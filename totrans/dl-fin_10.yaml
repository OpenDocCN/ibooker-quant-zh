- en: Chapter 10\. Deep Reinforcement Learning for Time Series Prediction
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章。深度强化学习用于时间序列预测
- en: '*Reinforcement learning* is a branch of machine learning that deals with sequential
    decision-making problems. Algorithms in this branch learn to make optimal decisions
    by interacting with an environment and receiving feedback in the form of rewards.
    In the context of time series forecasting, it can be used to develop models that
    make sequential predictions based on historical data. Traditional forecasting
    approaches often rely on statistical methods or supervised learning techniques,
    which assume independence between data points. However, time series data exhibits
    temporal dependencies and patterns, which may be effectively captured using reinforcement
    learning.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*强化学习*是处理顺序决策问题的机器学习分支。该分支中的算法通过与环境的交互和奖励反馈来学习做出最优决策。在时间序列预测的背景下，它可以用来开发基于历史数据进行顺序预测的模型。传统的预测方法通常依赖于统计方法或监督学习技术，这些方法假设数据点之间是独立的。然而，时间序列数据表现出时间依赖性和模式，这些可以通过强化学习有效地捕捉。'
- en: Reinforcement learning models for time series forecasting typically involve
    an agent that takes actions based on observed states and receives rewards based
    on the accuracy of its predictions. The agent learns through trial and error to
    maximize cumulative rewards over time. The key challenge is finding an optimal
    balance between *exploration* (trying out new actions) and *exploitation* (using
    learned knowledge).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 用于时间序列预测的强化学习模型通常涉及一个基于观察到的状态采取行动并根据其预测准确性获得奖励的代理程序。代理通过试错学习以在时间上最大化累积奖励。关键挑战在于找到*探索*（尝试新行动）与*利用*（使用已学知识）之间的最佳平衡。
- en: This chapter gives a basic overview of reinforcement learning and deep reinforcement
    learning with regard to predicting time series data.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章概述了关于预测时间序列数据的强化学习和深度强化学习的基本概念。
- en: Intuition of Reinforcement Learning
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习的直觉
- en: Simplification is always the right path toward understanding more advanced details.
    So let’s look at reinforcement learning from a simple point of view before digging
    deeper.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 简化总是理解更高级细节的正确途径。因此，在深入研究之前，让我们从一个简单的角度来看待强化学习。
- en: Reinforcement learning deals primarily with rewards and penalties. Imagine a
    child who gets a reward for doing good things and a punishment for doing bad things.
    Over time, that child will grow and will develop their experience so that they
    do good things and try to avoid doing bad things as much as possible (no one is
    perfect). Therefore, the learning is done through experience.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习主要涉及奖励和惩罚。想象一个孩子因做好事而受到奖励，因做坏事而受到惩罚。随着时间的推移，孩子会成长并积累经验，以便做好事并尽量避免做坏事（没有人是完美的）。因此，学习是通过经验来完成的。
- en: From a time series perspective, the main idea is the same. Imagine training
    a model on past data and letting it then learn by experience, rewarding it for
    good predictions and calibrating its parameters when it makes a mistake so that
    it can achieve better accuracy next time. The algorithm is greedy in nature and
    wants to maximize its rewards; therefore, over time it becomes better at predicting
    the next likely value, which is of course dependent on the quality and the signal-to-noise
    ratio of the analyzed time series.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 从时间序列的角度来看，基本思想是一样的。想象在过去数据上训练模型，然后让其通过经验学习，在做出良好预测时奖励它，在犯错时校正其参数，以便下次实现更好的准确性。算法本质上是贪婪的，并且希望最大化其奖励；因此，随着时间的推移，它在预测下一个可能值时变得更加优秀，这当然依赖于分析的时间序列的质量和信噪比。
- en: 'The term *reinforcement learning* comes from the fact that *positive reinforcement*
    is given to the algorithm when it makes right decisions and *negative reinforcement*
    is given when it makes bad decisions. The first three concepts you must know are
    states, actions, and rewards:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*强化学习*这个术语来源于算法在做出正确决策时给予*正向强化*，在做出错误决策时给予*负向强化*。你必须了解的前三个概念是状态、行动和奖励：'
- en: States
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 状态
- en: The features at every time step. For example, at a certain time step, the current
    state of the market is its OHLC data and its volume data. In more familiar words,
    states are the explanatory variables.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 每个时间步骤的特征。例如，在某个时间步骤，市场的当前状态是其OHLC数据和其交易量数据。更简单的话来说，状态是解释变量。
- en: Actions
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 行动
- en: The decisions a trader may make at every time step. They generally involve buying,
    selling, or holding. In more familiar words, actions are the algorithms’ decisions
    when faced with certain states (a simple discretionary example of this would be
    a trader noticing an overvalued market and deciding to initiate a buy order).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 交易员在每个时间步骤可能做出的决策。一般涉及买入、卖出或持有。更通俗的说法，动作是算法在面对特定状态时的决策（例如，交易员注意到市场估值过高并决定发起买单的简单自主性例子）。
- en: Rewards
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励
- en: The results of correct actions. The simplest reward is the positive return.
    Note that a poorly designed reward function can lead to model issues such as a
    buy-and-hold strategy.^([1](ch10.html#id771))
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 正确动作的结果。最简单的奖励是正回报。请注意，设计不良的奖励函数可能导致模型问题，例如买入并持有策略[^1]。
- en: '[Table 10-1](#table-10-1) shows the three main elements of reinforcement learning.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 10-1](#table-10-1) 展示了强化学习的三个主要元素。'
- en: Table 10-1\. A hypothetical decision table
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10-1\. 一个假设的决策表
- en: '|   | Time | Open | High | Low | Close | &#124; | **Action** | **Reward** |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '|   | 时间 | 开盘 | 最高 | 最低 | 收盘 | &#124; | **动作** | **奖励** |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| **States** |   1 | 10 | 14 | 8 | 10 | &#124; | BUY |     0 |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| **状态** |   1 | 10 | 14 | 8 | 10 | &#124; | 买入 |     0 |'
- en: '| **States** |   2 | 10 | 15 | 6 | 13 | &#124; | BUY |     3 |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| **状态** |   2 | 10 | 15 | 6 | 13 | &#124; | 买入 |     3 |'
- en: '| **States** |   3 | 13 | 16 | 8 | 14 | &#124; | SELL |   –1 |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| **状态** |   3 | 13 | 16 | 8 | 14 | &#124; | 卖出 |   –1 |'
- en: '| **States** |   4 | 10 | 16 | 8 | 14 | &#124; | HOLD |     0 |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| **状态** |   4 | 10 | 16 | 8 | 14 | &#124; | 持有 |     0 |'
- en: States are the rows that go from the Time column to the Close column. Actions
    can be categorical, as you can see from the Action column, and Rewards can either
    be numerical (e.g., a positive or negative profit) or categorical (e.g., profit
    or loss label).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 状态是从时间列到收盘列的行，动作可以是分类的，正如您从动作列看到的，奖励可以是数值（例如，正或负的利润）或分类的（例如，利润或亏损标签）。
- en: From the preceding list, it seems complicated to just design a system that looks
    for rewards. A *reward function* quantifies the desirability or utility of being
    in a particular state or taking a specific action. The reward function therefore
    provides feedback to the agent, indicating the immediate quality of its actions
    and guiding its learning process. Before we discuss reward functions in more detail,
    let’s look at what a state-action table is (also known as a Q-table).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述列表中可以看出，仅设计一个寻找奖励的系统似乎很复杂。*奖励函数*量化了处于特定状态或采取特定行动的可取性或效用。因此，奖励函数向代理提供反馈，指示其行动的即时质量，并引导其学习过程。在我们详细讨论奖励函数之前，让我们先看看什么是状态-动作表（也称为
    Q 表）。
- en: A *Q-table*, short for *quality table*, is a data structure to store and update
    the expected value (called the *Q-value*) of taking a particular action in a given
    state. The Q-value of a state-action pair (*s*, *a*) at time *t* represents the
    expected cumulative reward that an agent can achieve by taking action *a* in state
    *s* following a specific policy. The Q-table is therefore a table-like structure
    that maps each state-action pair to its corresponding Q-value.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*Q 表*，即*质量表*，是一种数据结构，用于存储和更新在给定状态下采取特定行动的期望值（称为*Q 值*）。状态-动作对（*s*, *a*）在时间 *t*
    的 Q 值表示代理通过遵循特定策略，在状态 *s* 中采取行动 *a* 能够达到的预期累积奖励。因此，Q 表是一种表格结构，将每个状态-动作对映射到其对应的
    Q 值。'
- en: Initially, the Q-table is usually initialized with arbitrary values or set to
    zero. As the algorithm explores the environment (market) and receives rewards,
    it updates the Q-values in the table based on the observed rewards and the estimated
    future rewards. This process is typically done using an algorithm such as Q-learning.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 初始时，Q 表通常使用任意值或设为零进行初始化。随着算法探索环境（市场）并接收奖励，它基于观察到的奖励和预估的未来奖励更新表中的 Q 值。这一过程通常使用
    Q 学习等算法完成。
- en: Note
  id: totrans-27
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注
- en: Over time, through repeated exploration and exploitation, the Q-table gradually
    converges to more accurate estimates of the optimal Q-values, representing the
    best actions to take in each state. By using the Q-table, the agent can make informed
    decisions and learn to maximize its cumulative rewards in the given environment.
    Remember, a reward can be a profit, Sharpe ratio, or any other performance metric.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 随着反复的探索和利用，Q 表逐渐收敛到对最优 Q 值更准确的估计，代表在每个状态下采取的最佳行动。通过使用 Q 表，代理可以做出明智的决策，并学会在给定环境中最大化累积奖励。记住，奖励可以是利润、夏普比率或任何其他绩效指标。
- en: '*Q-learning* is a popular reinforcement learning algorithm that enables an
    agent to learn optimal actions by iteratively updating its action-value function,
    known as the *Bellman equation*, defined as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*Q-learning* 是一种流行的强化学习算法，通过迭代更新其动作值函数（即 *贝尔曼方程*），使代理程序能够学习到最优的行动：'
- en: <math alttext="upper Q left-parenthesis s Subscript t Baseline comma a Subscript
    t Baseline right-parenthesis equals upper R left-parenthesis s Subscript t Baseline
    comma a Subscript t Baseline right-parenthesis plus gamma m a x left-bracket upper
    Q left-parenthesis s Subscript t plus 1 Baseline comma a Subscript t plus 1 Baseline
    right-parenthesis right-bracket"><mrow><mi>Q</mi> <mrow><mo>(</mo> <msub><mi>s</mi>
    <mi>t</mi></msub> <mo>,</mo> <msub><mi>a</mi> <mi>t</mi></msub> <mo>)</mo></mrow>
    <mo>=</mo> <mi>R</mi> <mrow><mo>(</mo> <msub><mi>s</mi> <mi>t</mi></msub> <mo>,</mo>
    <msub><mi>a</mi> <mi>t</mi></msub> <mo>)</mo></mrow> <mo>+</mo> <mi>γ</mi> <mi>m</mi>
    <mi>a</mi> <mi>x</mi> <mrow><mo>[</mo> <mi>Q</mi> <mrow><mo>(</mo> <msub><mi>s</mi>
    <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub> <mo>,</mo> <msub><mi>a</mi>
    <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper Q left-parenthesis s Subscript t Baseline comma a Subscript
    t Baseline right-parenthesis equals upper R left-parenthesis s Subscript t Baseline
    comma a Subscript t Baseline right-parenthesis plus gamma m a x left-bracket upper
    Q left-parenthesis s Subscript t plus 1 Baseline comma a Subscript t plus 1 Baseline
    right-parenthesis right-bracket"><mrow><mi>Q</mi> <mrow><mo>(</mo> <msub><mi>s</mi>
    <mi>t</mi></msub> <mo>,</mo> <msub><mi>a</mi> <mi>t</mi></msub> <mo>)</mo></mrow>
    <mo>=</mo> <mi>R</mi> <mrow><mo>(</mo> <msub><mi>s</mi> <mi>t</mi></msub> <mo>,</mo>
    <msub><mi>a</mi> <mi>t</mi></msub> <mo>)</mo></mrow> <mo>+</mo> <mi>γ</mi> <mi>m</mi>
    <mi>a</mi> <mi>x</mi> <mrow><mo>[</mo> <mi>Q</mi> <mrow><mo>(</mo> <msub><mi>s</mi>
    <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub> <mo>,</mo> <msub><mi>a</mi>
    <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>
- en: <math alttext="StartLayout 1st Row  upper Q left-parenthesis s Subscript t Baseline
    comma a Subscript t Baseline right-parenthesis is the expected reward 2nd Row  upper
    R left-parenthesis s Subscript t Baseline comma a Subscript t Baseline right-parenthesis
    is the reward table 3rd Row  gamma is the learning rate left-parenthesis known
    as g a m m a right-parenthesis EndLayout"><mtable><mtr><mtd columnalign="left"><mrow><mi>Q</mi>
    <mo>(</mo> <msub><mi>s</mi> <mi>t</mi></msub> <mo>,</mo> <msub><mi>a</mi> <mi>t</mi></msub>
    <mo>)</mo> <mtext>is</mtext> <mtext>the</mtext> <mtext>expected</mtext> <mtext>reward</mtext></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mi>R</mi> <mo>(</mo> <msub><mi>s</mi> <mi>t</mi></msub>
    <mo>,</mo> <msub><mi>a</mi> <mi>t</mi></msub> <mo>)</mo> <mtext>is</mtext> <mtext>the</mtext>
    <mtext>reward</mtext> <mtext>table</mtext></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mi>γ</mi>
    <mtext>is</mtext> <mtext>the</mtext> <mtext>learning</mtext> <mtext>rate</mtext>
    <mtext>(known</mtext> <mtext>as</mtext> <mi>g</mi> <mi>a</mi> <mi>m</mi> <mi>m</mi>
    <mi>a</mi> <mtext>)</mtext></mrow></mtd></mtr></mtable></math>
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row  upper Q left-parenthesis s Subscript t Baseline
    comma a Subscript t Baseline right-parenthesis is the expected reward 2nd Row  upper
    R left-parenthesis s Subscript t Baseline comma a Subscript t Baseline right-parenthesis
    is the reward table 3rd Row  gamma is the learning rate left-parenthesis known
    as g a m m a right-parenthesis EndLayout"><mtable><mtr><mtd columnalign="left"><mrow><mi>Q</mi>
    <mo>(</mo> <msub><mi>s</mi> <mi>t</mi></msub> <mo>,</mo> <msub><mi>a</mi> <mi>t</mi></msub>
    <mo>)</mo> <mtext>is</mtext> <mtext>the</mtext> <mtext>expected</mtext> <mtext>reward</mtext></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mi>R</mi> <mo>(</mo> <msub><mi>s</mi> <mi>t</mi></msub>
    <mo>,</mo> <msub><mi>a</mi> <mi>t</mi></msub> <mo>)</mo> <mtext>is</mtext> <mtext>the</mtext>
    <mtext>reward</mtext> <mtext>table</mtext></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mi>γ</mi>
    <mtext>is</mtext> <mtext>the</mtext> <mtext>learning</mtext> <mtext>rate</mtext>
    <mtext>(known</mtext> <mtext>as</mtext> <mi>g</mi> <mi>a</mi> <mi>m</mi> <mi>m</mi>
    <mi>a</mi> <mtext>)</mtext></mrow></mtd></mtr></mtable></math>
- en: The larger the learning rate (γ), the more the algorithm takes into account
    the previous experiences. Notice that if γ is equal to zero, it would be synonymous
    to learning nothing as the second term will cancel itself out. As a simple example,
    consider [Table 10-2](#table-10-2).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率（γ）越大，算法越多考虑先前的经验。注意，如果 γ 等于零，那么学习就等同于什么都没学到，因为第二项将会自我抵消。举个简单的例子，考虑 [表格 10-2](#table-10-2)。
- en: Table 10-2\. R-table
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 10-2\. R 表
- en: '| Time (State) | Act (Action) | Wait (Action) |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 时间（状态） | 行动（动作） | 等待（动作） |'
- en: '| --- | --- | --- |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '|        1 | 2 reward units | 0 reward units |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|        1 | 2 个奖励单位 | 0 个奖励单位 |'
- en: '|        2 | 2 reward units | 0 reward units |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|        2 | 2 个奖励单位 | 0 个奖励单位 |'
- en: '|        3 | 2 reward units | 0 reward units |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|        3 | 2 个奖励单位 | 0 个奖励单位 |'
- en: '|        4 | 2 reward units | 0 reward units |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|        4 | 2 个奖励单位 | 0 个奖励单位 |'
- en: '|        5 | 2 reward units | 4 reward units |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|        5 | 2 个奖励单位 | 4 个奖励单位 |'
- en: 'The table describes the results of actions through time. At every time step,
    acting (doing something) will give a reward of 2, while waiting to act on the
    fifth time step will give a reward of 4\. This means that the agent can make one
    of the following choices:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 该表格描述了随时间的行动结果。每个时间步长，执行（做某事）将获得 2 的奖励，而在第五个时间步长等待行动将获得 4 的奖励。这意味着代理程序可以做出以下选择之一：
- en: Act now and get 2 reward units.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 立即行动并获得 2 个奖励单位。
- en: Wait before acting and get 4 reward units.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 等待一段时间再行动，可以获得 4 个奖励单位。
- en: 'Let’s assume γ = 0.80\. Using the Bellman equation and working backward will
    get you the following results:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 γ = 0.80\. 使用贝尔曼方程并向后工作，将得到以下结果：
- en: <math alttext="StartLayout 1st Row  upper Q left-parenthesis s 1 comma a 1 right-parenthesis
    equals 0 plus 0.8 left-parenthesis 2.04 right-parenthesis equals 1.63 2nd Row  upper
    Q left-parenthesis s 2 comma a 2 right-parenthesis equals 0 plus 0.8 left-parenthesis
    2.56 right-parenthesis equals 2.04 3rd Row  upper Q left-parenthesis s 3 comma
    a 3 right-parenthesis equals 0 plus 0.8 left-parenthesis 3.20 right-parenthesis
    equals 2.56 4th Row  upper Q left-parenthesis s 4 comma a 4 right-parenthesis
    equals 0 plus 0.8 left-parenthesis 4.00 right-parenthesis equals 3.20 EndLayout"><mtable><mtr><mtd
    columnalign="left"><mrow><mi>Q</mi> <mrow><mo>(</mo> <msub><mi>s</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>a</mi> <mn>1</mn></msub> <mo>)</mo></mrow> <mo>=</mo> <mn>0</mn>
    <mo>+</mo> <mn>0</mn> <mo lspace="0%" rspace="0%">.</mo> <mn>8</mn> <mrow><mo>(</mo>
    <mn>2</mn> <mo lspace="0%" rspace="0%">.</mo> <mn>04</mn> <mo>)</mo></mrow> <mo>=</mo>
    <mn>1</mn> <mo lspace="0%" rspace="0%">.</mo> <mn>63</mn></mrow></mtd></mtr> <mtr><mtd
    columnalign="left"><mrow><mi>Q</mi> <mrow><mo>(</mo> <msub><mi>s</mi> <mn>2</mn></msub>
    <mo>,</mo> <msub><mi>a</mi> <mn>2</mn></msub> <mo>)</mo></mrow> <mo>=</mo> <mn>0</mn>
    <mo>+</mo> <mn>0</mn> <mo lspace="0%" rspace="0%">.</mo> <mn>8</mn> <mrow><mo>(</mo>
    <mn>2</mn> <mo lspace="0%" rspace="0%">.</mo> <mn>56</mn> <mo>)</mo></mrow> <mo>=</mo>
    <mn>2</mn> <mo lspace="0%" rspace="0%">.</mo> <mn>04</mn></mrow></mtd></mtr> <mtr><mtd
    columnalign="left"><mrow><mi>Q</mi> <mrow><mo>(</mo> <msub><mi>s</mi> <mn>3</mn></msub>
    <mo>,</mo> <msub><mi>a</mi> <mn>3</mn></msub> <mo>)</mo></mrow> <mo>=</mo> <mn>0</mn>
    <mo>+</mo> <mn>0</mn> <mo lspace="0%" rspace="0%">.</mo> <mn>8</mn> <mrow><mo>(</mo>
    <mn>3</mn> <mo lspace="0%" rspace="0%">.</mo> <mn>20</mn> <mo>)</mo></mrow> <mo>=</mo>
    <mn>2</mn> <mo lspace="0%" rspace="0%">.</mo> <mn>56</mn></mrow></mtd></mtr> <mtr><mtd
    columnalign="left"><mrow><mi>Q</mi> <mrow><mo>(</mo> <msub><mi>s</mi> <mn>4</mn></msub>
    <mo>,</mo> <msub><mi>a</mi> <mn>4</mn></msub> <mo>)</mo></mrow> <mo>=</mo> <mn>0</mn>
    <mo>+</mo> <mn>0</mn> <mo lspace="0%" rspace="0%">.</mo> <mn>8</mn> <mrow><mo>(</mo>
    <mn>4</mn> <mo lspace="0%" rspace="0%">.</mo> <mn>00</mn> <mo>)</mo></mrow> <mo>=</mo>
    <mn>3</mn> <mo lspace="0%" rspace="0%">.</mo> <mn>20</mn></mrow></mtd></mtr></mtable></math>
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row  upper Q left-parenthesis s 1 comma a 1 right-parenthesis
    equals 0 plus 0.8 left-parenthesis 2.04 right-parenthesis equals 1.63 2nd Row  upper
    Q left-parenthesis s 2 comma a 2 right-parenthesis equals 0 plus 0.8 left-parenthesis
    2.56 right-parenthesis equals 2.04 3rd Row  upper Q left-parenthesis s 3 comma
    a 3 right-parenthesis equals 0 plus 0.8 left-parenthesis 3.20 right-parenthesis
    equals 2.56 4th Row  upper Q left-parenthesis s 4 comma a 4 right-parenthesis
    equals 0 plus 0.8 left-parenthesis 4.00 right-parenthesis equals 3.20 EndLayout"><mtable><mtr><mtd
    columnalign="left"><mrow><mi>Q</mi> <mrow><mo>(</mo> <msub><mi>s</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>a</mi> <mn>1</mn></msub> <mo>)</mo></mrow> <mo>=</mo> <mn>0</mn>
    <mo>+</mo> <mn>0</mn> <mo lspace="0%" rspace="0%">.</mo> <mn>8</mn> <mrow><mo>(</mo>
    <mn>2</mn> <mo lspace="0%" rspace="0%">.</mo> <mn>04</mn> <mo>)</mo></mrow> <mo>=</mo>
    <mn>1</mn> <mo lspace="0%" rspace="0%">.</mo> <mn>63</mn></mrow></mtd></mtr> <mtr><mtd
    columnalign="left"><mrow><mi>Q</mi> <mrow><mo>(</mo> <msub><mi>s</mi> <mn>2</mn></msub>
    <mo>,</mo> <msub><mi>a</mi> <mn>2</mn></msub> <mo>)</mo></mrow> <mo>=</mo> <mn>0</mn>
    <mo>+</mo> <mn>0</mn> <mo lspace="0%" rspace="0%">.</mo> <mn>8</mn> <mrow><mo>(</mo>
    <mn>2</mn> <mo lspace="0%" rspace="0%">.</mo> <mn>56</mn> <mo>)</mo></mrow> <mo>=</mo>
    <mn>2</mn> <mo lspace="0%" rspace="0%">.</mo> <mn>04</mn></mrow></mtd></mtr> <mtr><mtd
    columnalign="left"><mrow><mi>Q</mi> <mrow><mo>(</mo> <msub><mi>s</mi> <mn>3</mn></msub>
    <mo>,</mo> <msub><mi>a</mi> <mn>3</mn></msub> <mo>)</mo></mrow> <mo>=</mo> <mn>0</mn>
    <mo>+</mo> <mn>0</mn> <mo lspace="0%" rspace="0%">.</mo> <mn>8</mn> <mrow><mo>(</mo>
    <mn>3</mn> <mo lspace="0%" rspace="0%">.</mo> <mn>20</mn> <mo>)</mo></mrow> <mo>=</mo>
    <mn>2</mn> <mo lspace="0%" rspace="0%">.</mo> <mn>56</mn></mrow></mtd></mtr> <mtr><mtd
    columnalign="left"><mrow><mi>Q</mi> <mrow><mo>(</mo> <msub><mi>s</mi> <mn>4</mn></msub>
    <mo>,</mo> <msub><mi>a</mi> <mn>4</mn></msub> <mo>)</mo></mrow> <mo>=</mo> <mn>0</mn>
    <mo>+</mo> <mn>0</mn> <mo lspace="0%" rspace="0%">.</mo> <mn>8</mn> <mrow><mo>(</mo>
    <mn>4</mn> <mo lspace="0%" rspace="0%">.</mo> <mn>00</mn> <mo>)</mo></mrow> <mo>=</mo>
    <mn>3</mn> <mo lspace="0%" rspace="0%">.</mo> <mn>20</mn></mrow></mtd></mtr></mtable></math>
- en: '[Table 10-2](#table-10-2) may be updated to become [Table 10-3](#table-10-3)
    as follows.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[表格 10-2](#table-10-2) 可能会更新为以下的 [表格 10-3](#table-10-3) 。'
- en: Table 10-3\. Q-table
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 10-3\. Q 表
- en: '| Time (State) | Act (Action) | Wait (Action) |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 时间（状态） | 行动（动作） | 等待（动作） |'
- en: '| --- | --- | --- |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '|       1 | 2 reward units | 1.63 reward units |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|       1 | 2 个奖励单位 | 1.63 个奖励单位 |'
- en: '|       2 | 2 reward units | 2.04 reward units |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|       2 | 2 个奖励单位 | 2.04 个奖励单位 |'
- en: '|       3 | 2 reward units | 2.56 reward units |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|       3 | 2 个奖励单位 | 2.56 个奖励单位 |'
- en: '|       4 | 2 reward units | 3.20 reward units |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|       4 | 2 个奖励单位 | 3.20 个奖励单位 |'
- en: '|       5 | 2 reward units | 4.00 reward units |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|       5 | 2 个奖励单位 | 4.00 个奖励单位 |'
- en: Therefore, the Q-table is continuously updated with the implied rewards to help
    maximize the final reward. To understand why the term *max* is in the Bellman
    equation, consider the example in [Table 10-4](#table-10-4).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Q 表会持续更新隐含的奖励，以帮助最大化最终的奖励。要理解贝尔曼方程中为什么有 *max* 这个术语，可以参考 [表格 10-4](#table-10-4)
    中的例子。
- en: Table 10-4\. R-table
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 10-4\. R 表
- en: '| Time (State) | Buy | Sell | Hold |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 时间（状态） | 买入 | 卖出 | 持有 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '|       1 |  5 |  8 |  8 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|       1 |  5 |  8 |  8 |'
- en: '|       2 |  3 |  2 |  1 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|       2 |  3 |  2 |  1 |'
- en: '|       3 |  2 |  5 |  6 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|       3 |  2 |  5 |  6 |'
- en: Calculate the would-be value of *x* in a Q-table ([Table 10-5](#table-10-5))
    assuming a learning rate of 0.4.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 计算在 Q 表中（[表格 10-5](#table-10-5)）假设学习率为 0.4 时，*x* 的预期值。
- en: Table 10-5\. Q-table
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 10-5\. Q 表
- en: '| Time (State) | Buy | Sell | Hold |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 时间（状态） | 买入 | 卖出 | 持有 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '|      1 |  ?   |  ? |  ? |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|      1 |  ?   |  ? |  ? |'
- en: '|      2 |  ? |  x |  ? |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|      2 |  ? |  x |  ? |'
- en: '|      3 |  2 |  5 |  6 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|      3 |  2 |  5 |  6 |'
- en: 'Following the formula, you should get this result:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 按照公式，您应该得到以下结果：
- en: <math alttext="x equals 2 plus 0.4 left-parenthesis m a x left-parenthesis 2
    comma 5 comma 6 right-parenthesis right-parenthesis equals 4.4"><mrow><mi>x</mi>
    <mo>=</mo> <mn>2</mn> <mo>+</mo> <mn>0</mn> <mo lspace="0%" rspace="0%">.</mo>
    <mn>4</mn> <mo>(</mo> <mi>m</mi> <mi>a</mi> <mi>x</mi> <mo>(</mo> <mn>2</mn> <mo>,</mo>
    <mn>5</mn> <mo>,</mo> <mn>6</mn> <mo>)</mo> <mo>)</mo> <mo>=</mo> <mn>4</mn> <mo
    lspace="0%" rspace="0%">.</mo> <mn>4</mn></mrow></math>
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="x equals 2 plus 0.4 left-parenthesis m a x left-parenthesis 2
    comma 5 comma 6 right-parenthesis right-parenthesis equals 4.4"><mrow><mi>x</mi>
    <mo>=</mo> <mn>2</mn> <mo>+</mo> <mn>0</mn> <mo lspace="0%" rspace="0%">.</mo>
    <mn>4</mn> <mo>(</mo> <mi>m</mi> <mi>a</mi> <mi>x</mi> <mo>(</mo> <mn>2</mn> <mo>,</mo>
    <mn>5</mn> <mo>,</mo> <mn>6</mn> <mo>)</mo> <mo>)</mo> <mo>=</mo> <mn>4</mn> <mo
    lspace="0%" rspace="0%">.</mo> <mn>4</mn></mrow></math>
- en: States (features) must be predictive in nature so that the reinforcement learning
    algorithm predicts the next value with an accuracy better than random. Examples
    of features can be the values of the relative strength index (RSI), moving averages,
    and lagged close prices.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 状态（特征）必须具有预测性质，以便强化学习算法能够比随机预测更准确地预测下一个值。特征的例子可以是相对强弱指数（RSI）、移动平均和滞后收盘价格的值。
- en: 'It is crucial to keep in mind that the inputs’ statistical preference remains
    the same, that is, stationary. This begs the question: how are moving averages
    used as inputs if they are not stationary? The simple answer is through the usual
    transformation, which is to take the percentage difference.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在输入统计偏好保持不变的情况下至关重要，即具有固定性。这带来了一个问题：如果移动平均值不是固定的，那么如何将其用作输入？简单的答案是通过通常的转换，即进行百分比差异。
- en: Note
  id: totrans-73
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: It is possible to use fractional differencing to transform a nonstationary time
    series into a stationary one while retaining its memory.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用分数差分来将非平稳时间序列转换为平稳时间序列，同时保留其记忆。
- en: A *policy* defines the behavior of an agent in an environment. It is a mapping
    from states to actions, indicating what action the agent should take in a given
    state. The policy essentially guides the agent’s decision-making process by specifying
    the action to be executed based on the observed state.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*策略* 定义了智能体在环境中的行为。它是一个从状态到动作的映射，指示智能体在给定状态下应该采取什么动作。策略本质上通过指定基于观察到的状态应执行的动作来引导智能体的决策过程。'
- en: The goal of reinforcement learning is to find an optimal policy that maximizes
    the agent’s long-term cumulative reward. This is typically achieved through a
    trial-and-error process, where the agent interacts with the environment, takes
    actions, receives rewards, and adjusts its policy based on the observed outcomes.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习的目标是找到最大化智能体长期累积奖励的最佳策略。这通常通过试错过程实现，智能体与环境互动，采取行动，获得奖励，并根据观察到的结果调整其策略。
- en: The exploitation policy is generally faster than the exploration policy but
    may be more limited as it seeks a greater and immediate reward, while there may
    be a path afterward that leads to an even greater reward. Ideally, the best policy
    to take is a combination of both. But how do you determine this optimal mix? That
    question is answered by epsilon (ε).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 开发策略通常比探索策略更快，但可能更受限，因为它寻求更大和即时的奖励，而在此之后可能有一条路径导致更大的奖励。理想情况下，最佳策略是两者的组合。但是你如何确定这种最佳混合？这个问题由
    epsilon（ε）回答。
- en: '*Epsilon* is a parameter used in exploration–exploitation trade-offs. It determines
    the probability with which an agent selects a random action (exploration) versus
    selecting the action with the highest estimated value (exploitation).'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*Epsilon* 是用于探索和开发权衡的参数。它决定了智能体选择随机动作（探索）与选择估计值最高的动作（开发）的概率。'
- en: Commonly used exploration strategies include epsilon-greedy and softmax. In
    *epsilon-greedy*, the agent selects the action with the highest estimated value
    with a probability of (1 – ε), and then it selects a random action with a probability
    of ε. This allows the agent to explore different actions and potentially discover
    better policies. As the agent learns over time, the epsilon value is often decayed
    gradually to reduce exploration and focus more on exploitation.^([2](ch10.html#id778))
    *Softmax* action selection considers the estimated action values but introduces
    stochasticity in the decision-making process. The temperature parameter associated
    with softmax determines the randomness in action selection, where a higher temperature
    leads to more exploration.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 常用的探索策略包括 epsilon-greedy 和 softmax。在 *epsilon-greedy* 中，智能体以概率 (1 – ε) 选择估计值最高的动作，然后以
    ε 的概率选择随机动作。这允许智能体探索不同的动作，并有可能发现更好的策略。随着智能体逐渐学习，epsilon 值通常会逐渐衰减，以减少探索并更多地专注于开发[^2]。*Softmax*
    动作选择考虑了估计的动作值，但在决策过程中引入了随机性。与 softmax 相关的温度参数决定了动作选择的随机性，温度越高，探索就越多。
- en: Warning
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'Do not mix up epsilon and gamma:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 不要混淆 epsilon 和 gamma：
- en: '*Gamma* is a parameter that determines the importance of future rewards. It
    controls the extent to which the agent values immediate rewards compared to delayed
    rewards (hence, it is related to a delayed gratification issue). The value of
    gamma is typically a number between 0 and 1, where a value closer to 1 means the
    agent considers future rewards more heavily, while a value closer to 0 gives less
    importance to future rewards. To understand this more, consider having another
    look at the Bellman equation.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Gamma* 是一个参数，它决定了未来奖励的重要性。它控制了智能体对即时奖励与延迟奖励的价值比重（因此，它与延迟满足问题相关）。Gamma 的值通常在
    0 和 1 之间，数值越接近 1，智能体就越重视未来奖励，而数值越接近 0，就越不重视未来奖励。要更好地理解这一点，请再次查看贝尔曼方程。'
- en: '*Epsilon* is a parameter used in exploration–exploitation trade-offs. It determines
    the probability with which an agent selects a random action (exploration) versus
    selecting the action with the highest estimated value (exploitation).'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Epsilon* 是用于探索和开发权衡的参数。它决定了智能体选择随机动作（探索）与选择估计值最高的动作（开发）的概率。'
- en: At this point, you may feel overwhelmed by the amount of new information presented,
    especially because it differs from what you have seen so far in the book. Before
    moving to the more complex deep reinforcement learning discussion, a quick summary
    of what you have seen in this chapter until now may be beneficial. Reinforcement
    learning is essentially giving the machine a task that it will then learn how
    to do on its own.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，你可能会感到对所呈现的大量新信息感到不知所措，特别是因为它与你在本书中迄今所见的内容不同。在进入更复杂的深度强化学习讨论之前，快速总结一下本章至今所见的内容可能会有益。强化学习本质上是给机器一个任务，然后让它自己学会如何完成。
- en: With time series analysis, states represent the current situation or condition
    of the environment at a particular time. An example of state is a technical indicator’s
    value. States are represented by Q-tables. Actions are self-explanatory and can
    be buy, sell, or hold (or even a more complex combination such as decrease weight
    and increase weight). Rewards are what the algorithm is trying to maximize and
    can be profit per trade, Sharpe ratio, or any sort of performance evaluation metric.
    A reward can also be a penalty such as the number of trades or maximum drawdown (in
    such a case, you are aiming to minimize it). The reinforcement learning algorithm
    will go through many iterations and variables through different policies to try
    to detect hidden patterns and optimize trading decision so that profitability
    is maximized. This is easier said than done (or coded).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间序列分析中，状态表示环境在特定时间的当前情况或状态。例如，技术指标的值就是一个状态的例子。状态由Q表表示。动作是不言自明的，可以是买入、卖出或持有（或者更复杂的组合，如减少权重和增加权重）。奖励是算法试图最大化的，可以是每笔交易的利润，夏普比率或任何性能评估指标。奖励也可以是惩罚，如交易次数或最大回撤（在这种情况下，你的目标是将其最小化）。强化学习算法将通过不同策略的多次迭代和变量来尝试检测隐藏的模式，并优化交易决策以最大化盈利。这说起来容易，实现起来难（或者编码起来难）。
- en: 'One question is begging an answer: is using a Q-table to represent the different
    states of financial time series efficient? This question is answered in the next
    section.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 一个问题迫切需要回答：使用Q表来表示金融时间序列的不同状态是否高效？这个问题将在下一节中回答。
- en: Deep Reinforcement Learning
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度强化学习
- en: '*Deep reinforcement learning* combines reinforcement learning techniques with
    deep learning architectures, particularly deep neural networks. It involves training
    agents to learn optimal behavior and make decisions by interacting with an environment,
    using deep neural networks to approximate value functions or policies.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '*深度强化学习* 将强化学习技术与深度学习架构，特别是深度神经网络结合在一起。它涉及训练代理程序通过与环境的互动来学习最优行为并做出决策，使用深度神经网络来逼近价值函数或策略。'
- en: The main difference between a reinforcement learning algorithm and a deep reinforcement
    learning algorithm is that the former estimates Q-values using the Q-table, while
    the latter estimates Q-values using ANNs (see [Chapter 8](ch08.html#ch08) for
    details on artificial neural networks).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习算法和深度强化学习算法的主要区别在于，前者使用Q表估计Q值，而后者使用人工神经网络来估计Q值（详见[第8章](ch08.html#ch08)关于人工神经网络的详细信息）。
- en: Note
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: As a reminder, *artificial neural networks* (ANNs) are a type of computational
    model inspired by the structure and functioning of the human brain. A neural network
    consists of interconnected nodes organized into layers. The three main types of
    layers are the input layer, hidden layers, and the output layer. The input layer
    receives the initial data, which is then processed through the hidden layers,
    and finally, the output layer produces the network’s prediction.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 作为提醒，*人工神经网络*（ANNs）是一种受人脑结构和功能启发的计算模型。神经网络由连接的节点组成，组织成层。主要的层类型包括输入层、隐藏层和输出层。输入层接收初始数据，然后通过隐藏层处理，最后输出层产生网络的预测。
- en: The main objective of this section is to understand and design a deep reinforcement
    learning algorithm with the aim of data prediction. Keep in mind that reinforcement
    learning is still not heavily applied since it suffers from a few issues (discussed
    at the end of this section) that need to be resolved before making it one of the
    main trading algorithms in quantitative finance.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的主要目标是理解并设计一个深度强化学习算法，其目的是数据预测。请记住，强化学习由于一些问题（在本节末尾讨论），目前还没有广泛应用，需要在将其作为量化金融主要交易算法之前解决这些问题。
- en: 'Therefore, deep reinforcement learning will have two main elements with important
    tasks:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，深度强化学习将有两个主要元素具有重要任务：
- en: A deep neural network architecture to recognize patterns and approximate the
    best function that relates dependent and independent variables
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个深度神经网络架构来识别模式并逼近关联的因变量和自变量之间的最佳函数
- en: A reinforcement learning architecture that allows the algorithm to learn by
    trial and error how to maximize a certain profit function
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习架构允许算法通过试错学习如何最大化某个利润函数
- en: Let’s continue defining a few key concepts before putting things together. *Replay
    memory*, also known as *experience replay*, involves storing and reusing past
    experiences to enhance the learning process and improve the stability and efficiency
    of the training.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续将各个关键概念定义清楚之前，让我们先介绍几个关键概念。*重放内存*，又称*经验重放*，涉及存储和重用过去的经验以增强学习过程并提高训练的稳定性和效率。
- en: In deep reinforcement learning, an agent interacts with an environment, observes
    states, takes actions, and receives rewards. Each observation, action, reward,
    and resulting next state is considered an experience. The replay memory serves
    as a buffer that stores a collection of these experiences.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度强化学习中，一个代理与环境互动，观察状态，采取行动并获得奖励。每次观察、行动、奖励和导致的下一个状态被视为一次经验。重放内存作为一个缓冲区，存储了这些经验的集合。
- en: 'The replay memory has the following key features:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 重放内存具有以下关键特点：
- en: Storage
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 存储
- en: The replay memory is a data structure that can store a fixed number of experiences.
    Each experience typically consists of the current state, the action taken, the
    resulting reward, the next state, and a flag indicating whether the episode terminated.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 重放内存是一种可以存储固定数量经验的数据结构。每个经验通常包括当前状态、采取的行动、导致的奖励、下一个状态以及一个指示该情节是否终止的标志。
- en: Sampling
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 抽样
- en: During the training process, instead of using experiences immediately as they
    occur, the agent samples a batch of experiences from the replay memory. Randomly
    sampling experiences from a large pool of stored transitions helps in decorrelating
    the data and breaking the temporal dependencies that exist in consecutive experiences.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，代理程序不是立即使用发生的经验，而是从重放内存中抽取一批经验。从存储的大量转换中随机抽取经验有助于解相关数据并打破连续经验中存在的时间依赖关系。
- en: Batch learning
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 批量学习
- en: The sampled batch of experiences is then used to update the agent’s neural network.
    By learning from a batch of experiences rather than individual experiences, the
    agent can make more efficient use of computation and improve the learning stability.
    Batch learning also allows for the application of optimization techniques, such
    as stochastic gradient descent, to update the network weights.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 随机抽取的一批经验随后用于更新代理的神经网络。通过批量学习而不是单个经验的学习，代理可以更有效地利用计算资源并改善学习稳定性。批量学习还允许应用优化技术，如随机梯度下降，来更新网络权重。
- en: The replay memory provides several benefits to deep reinforcement learning algorithms.
    Among those benefits is experience reuse, as the agent can learn from a more diverse
    set of data, reducing the bias that can arise from sequential updates. Breaking
    correlations is another benefit since the sequential nature of experience collection
    in reinforcement learning can introduce correlations between consecutive experiences.
    Randomly sampling experiences from the replay memory helps break these correlations,
    making the learning process more stable.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 重放内存为深度强化学习算法提供了几个好处。这些好处包括经验复用，因为代理可以从更多样化的数据集中学习，减少顺序更新可能引起的偏差。打破相关性是另一个好处，因为在强化学习的经验收集过程中，顺序的性质可能会在连续的经验之间引入相关性。从重放内存中随机抽取经验有助于打破这些相关性，使学习过程更加稳定。
- en: 'So far, we have discussed the following steps:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了以下步骤：
- en: Defining and initializing the environment
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义和初始化环境
- en: Designing the neural network architecture
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设计神经网络架构
- en: Designing the reinforcement learning architecture with experience replay to
    stabilize the learning process
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设计带有经验重放的强化学习架构以稳定学习过程
- en: Interacting with the environment and storing experiences until the learning
    process is done and predictions on new data are done
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与环境交互并存储经验，直到学习过程完成并对新数据进行预测
- en: One thing we have not discussed is how to reduce overestimations, which can
    be achieved by doubling down on the neural network architecture.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尚未讨论的一件事是如何通过加倍神经网络架构来减少过度估计。
- en: The *Double Deep Q-Network* (DDQN) model is an extension of the original DQN
    architecture introduced by DeepMind in 2015\. The primary motivation behind DDQN
    is to address a known issue in the DQN algorithm called *overestimation bias*,
    which can lead to suboptimal action selection.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '*双深度 Q 网络*（DDQN）模型是由 DeepMind 在 2015 年推出的原始 DQN 架构的扩展。DDQN 的主要动机是解决 DQN 算法中的已知问题，称为*过度估计偏差*，这可能导致次优的动作选择。'
- en: In the original DQN, the action values (Q-values) for each state-action pair
    are estimated using a single neural network. However, during the learning process,
    the Q-values are estimated using the maximum Q-value among all possible actions
    in the next state (take a look at [Table 10-5](#table-10-5)). This maximum Q-value
    can sometimes result in an overestimation of the true action values, leading to
    a suboptimal policy.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始 DQN 中，每个状态-动作对的动作值（Q 值）是使用单个神经网络估算的。然而，在学习过程中，Q 值是使用下一个状态中所有可能动作的最大 Q 值来估算的（查看
    [表 10-5](#table-10-5)）。这个最大 Q 值有时会导致真实动作值的过度估计，从而导致次优策略。
- en: 'The DDQN addresses this overestimation bias by utilizing two separate neural
    networks: the Q-network and the target-network. The *Q-network* is a deep neural
    network that approximates the action-value function (Q-function). In other words,
    it estimates the value of each possible action in a given state. The Q-network’s
    parameters (weights and biases) are learned through training to minimize the difference
    between predicted Q-values and target Q-values. The *target network* is a separate
    copy of the Q-network that is used to estimate the target Q-values during training.
    It helps stabilize the learning process and improve the convergence of the Q-network.
    The weights of the target network are not updated during training; instead, they
    are periodically updated to match the weights of the Q-network.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: DDQN 通过利用两个独立的神经网络解决这种过度估计偏差：Q 网络和目标网络。*Q 网络*是一个深度神经网络，用于逼近动作值函数（Q 函数）。换句话说，它估算了给定状态下每个可能动作的价值。Q
    网络的参数（权重和偏置）通过训练来学习，以最小化预测 Q 值和目标 Q 值之间的差异。*目标网络*是 Q 网络的一个单独副本，用于在训练期间估算目标 Q 值。它有助于稳定学习过程并改善
    Q 网络的收敛性。目标网络的权重在训练过程中不会更新；相反，它们会定期更新以匹配 Q 网络的权重。
- en: The key idea behind the DDQN is to decouple the selection of actions from the
    estimation of their values.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: DDQN 的关键思想是将动作的选择与其值的估算分离开来。
- en: Note
  id: totrans-116
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The algorithm updates the Q-network regularly and the target network occasionally.
    This is done to avoid the issue of the same model being used to estimate the Q-value
    from the next state and then giving it to the Bellman equation to estimate the
    Q-value for the current state.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 算法定期更新 Q 网络和偶尔更新目标网络。这样做是为了避免同一模型用于估算下一个状态的 Q 值，然后将其传递给贝尔曼方程，以估算当前状态的 Q 值的问题。
- en: 'So, to put these elements into an ordered sequence, here’s how the deep reinforcement
    learning architecture may look:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了将这些元素放入有序序列中，深度强化学习架构可能如下所示：
- en: The environment is initialized.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化环境。
- en: The epsilon value is selected. Remember, epsilon is the exploration–exploitation
    trade-off parameter used to control the agent’s behavior during training.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择 epsilon 值。请记住，epsilon 是控制训练期间代理行为的探索-利用权衡参数。
- en: The current state is fetched. Remember, an example of the current state may
    be the OHLC data, the RSI, the standard deviation of the returns, or even the
    day of the week.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取当前状态。请记住，当前状态的示例可能是 OHLC 数据、RSI、收益标准差，甚至是星期几。
- en: In the first round, the algorithm selects the action through exploration as
    the model is not trained yet; therefore, the action is randomly selected (e.g.,
    from a choice panel of buy, sell, and hold). If it’s not the first step, then
    exploitation may be used to select the action. Exploitation is where the action
    is determined by the neural network model.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第一轮中，由于模型尚未训练，算法通过探索选择动作；因此，动作是随机选择的（例如，从买入、卖出和持有的选择面板）。如果不是第一步，则可能使用利用来选择动作。利用是指动作由神经网络模型确定。
- en: The action is applied.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用动作。
- en: The previous elements are stored in replay memory.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将前述元素存储在回放内存中。
- en: The inputs and the target array are fetched and the Q-network is trained.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入和目标数组被获取，并训练 Q 网络。
- en: If the round is not over, repeat the process starting at step 3\. Otherwise,
    train the target network and repeat from step 1.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果轮次尚未结束，请从第 3 步开始重复该过程。否则，训练目标网络并从第 1 步重复。
- en: To illustrate the algorithm, let’s use it on the synthetic sine wave time series.
    Create the time series and then apply the deep reinforcement learning algorithm
    with the aim of predicting the future values.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 要说明算法，让我们在合成正弦波时间序列上使用它。创建时间序列，然后应用深度强化学习算法，旨在预测未来值。
- en: The full code can be found in the [GitHub repository](https://oreil.ly/5YGHI)
    (for replication purposes).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的代码可以在 [GitHub 仓库](https://oreil.ly/5YGHI) 中找到（供复制目的）。
- en: '[Figure 10-1](#figure-10-1) shows the test data (solid line) versus the predicted
    data (dashed line) using 1 epoch, 5 inputs (lagged values), a batch size of 64,
    and 1 hidden layer with 6 neurons.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10-1](#figure-10-1) 显示了测试数据（实线）与预测数据（虚线），使用了 1 个周期，5 个输入（滞后值），批量大小为 64，并且有
    1 个隐藏层，每个隐藏层有 6 个神经元。'
- en: '![](assets/dlff_1001.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlff_1001.png)'
- en: Figure 10-1\. Test data versus predicted data using 1 epoch, 5 inputs, a batch
    size of 64, and 1 hidden layer with 6 neurons
  id: totrans-131
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-1\. 使用了 1 个周期，5 个输入，批量大小为 64，并且有 1 个隐藏层，每个隐藏层有 6 个神经元的测试数据与预测数据对比
- en: '[Figure 10-2](#figure-10-2) shows the test data (solid line) versus the predicted
    data (dashed line) using 1 epoch, 5 inputs (lagged values), a batch size of 64,
    and 2 hidden layers with each having 6 neurons.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10-2](#figure-10-2) 显示了测试数据（实线）与预测数据（虚线），使用了 1 个周期，5 个输入（滞后值），批量大小为 64，并且有
    2 个隐藏层，每个隐藏层有 6 个神经元。'
- en: '![](assets/dlff_1002.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlff_1002.png)'
- en: Figure 10-2\. Predicted values versus actual values using 1 epoch, 5 inputs,
    a batch size of 64, and 2 hidden layers with each having 6 neurons
  id: totrans-134
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-2\. 使用了 1 个周期，5 个输入，批量大小为 64，并且有 2 个隐藏层，每个隐藏层有 6 个神经元的预测值与实际值对比
- en: '[Figure 10-3](#figure-10-3) shows the predictions using 10 epochs, 5 inputs
    (lagged values), a batch size of 32, and 2 hidden layers with each having 6 neurons.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10-3](#figure-10-3) 显示了使用了 10 个周期，5 个输入（滞后值），批量大小为 32，并且有 2 个隐藏层，每个隐藏层有
    6 个神经元的预测。'
- en: '![](assets/dlff_1003.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlff_1003.png)'
- en: Figure 10-3\. Predicted values versus actual values using 10 epochs, 5 inputs,
    a batch size of 32, and 2 hidden layers with each having 6 neurons
  id: totrans-137
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-3\. 使用了 10 个周期，5 个输入，批量大小为 32，并且有 2 个隐藏层，每个隐藏层有 6 个神经元的预测值与实际值对比
- en: '[Figure 10-4](#figure-10-4) shows the predictions using 10 epochs, 5 inputs
    (lagged values), a batch size of 32, and 2 hidden layers with each having 24 neurons.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10-4](#figure-10-4) 显示了使用了 10 个周期，5 个输入（滞后值），批量大小为 32，并且有 2 个隐藏层，每个隐藏层有
    24 个神经元的预测。'
- en: '![](assets/dlff_1004.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlff_1004.png)'
- en: Figure 10-4\. Predicted values versus actual values using 10 epochs, 5 inputs,
    a batch size of 32, and 2 hidden layers with each having 24 neurons
  id: totrans-140
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-4\. 使用了 10 个周期，5 个输入，批量大小为 32，并且有 2 个隐藏层，每个隐藏层有 24 个神经元的预测值与实际值对比
- en: '[Figure 10-5](#figure-10-5) shows the predictions using 10 epochs, 8 inputs
    (lagged values), a batch size of 32, and 2 hidden layers with each having 64 neurons.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10-5](#figure-10-5) 显示了使用了 10 个周期，8 个输入（滞后值），批量大小为 32，并且有 2 个隐藏层，每个隐藏层有
    64 个神经元的预测。'
- en: '![](assets/dlff_1005.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlff_1005.png)'
- en: Figure 10-5\. Predicted values versus actual values using 10 epochs, 8 inputs,
    a batch size of 32, and 2 hidden layers with each having 64 neurons
  id: totrans-143
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-5\. 使用了 10 个周期，8 个输入，批量大小为 32，并且有 2 个隐藏层，每个隐藏层有 64 个神经元的预测值与实际值对比
- en: As you know, the more epochs, the better the fit—up to a certain point, where
    overfitting may start to become an issue. Fortunately, by now you know how to
    reduce that risk.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所知，更多周期意味着更好的拟合——直到某一点，过拟合可能开始成为问题。幸运的是，现在您知道如何减少这种风险。
- en: Note
  id: totrans-145
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Note that the sine wave example is a very basic one, and more complex data can
    be used with the algorithm. The choice of the sine wave time series is for illustrative
    purposes only, and you must use more sophisticated methods on more complex time
    series to be able to judge the algorithm.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，正弦波示例非常基础，算法可以使用更复杂的数据。选择正弦波时间序列仅用于说明目的，您必须在更复杂的时间序列上使用更复杂的方法来评估算法。
- en: Reinforcement learning is easily overfit and is more likely to learn simple
    patterns and not hidden and complicated ones. Also, you should now be aware of
    the difficulty of reward function design and choice of features. Furthermore,
    such models are often considered mystery boxes, making it difficult to explain
    the reasoning behind their predictions. All of these issues are now a barrier
    to implementing a stable and profitable deep reinforcement learning algorithm
    for trading.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习容易过拟合，更容易学习简单的模式而不是隐藏且复杂的模式。此外，您现在应该意识到奖励函数设计的难度和特征选择的重要性。此外，这些模型通常被认为是神秘盒子，这使得解释其预测背后的推理变得困难。所有这些问题现在都成为实施稳定且盈利性强的深度强化学习算法用于交易的障碍。
- en: Summary
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Reinforcement learning can be applied to time series prediction tasks, where
    the goal is to make predictions about future values based on historical data.
    In this approach, an agent interacts with an environment representing the time
    series data. The agent receives observations of past values and takes actions
    to predict future values. The agent’s actions involve adjusting its internal model
    or parameters to make predictions. It uses reinforcement learning algorithms to
    learn from past experiences and improve its prediction accuracy over time.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习可以应用于时间序列预测任务，其目标是根据历史数据预测未来值。在这种方法中，代理与代表时间序列数据的环境互动。代理接收过去值的观察结果，并采取行动预测未来值。代理的行动涉及调整其内部模型或参数以进行预测。它使用强化学习算法从过去的经验中学习，并随着时间推移提高其预测准确性。
- en: The agent receives rewards or penalties based on the accuracy of its predictions.
    Rewards can be designed to reflect the prediction error or the utility of the
    predictions for the specific application. Through a process of trial and error,
    the agent learns to associate certain patterns or features in the time series
    data with future outcomes. It learns to make predictions that maximize rewards
    and minimize errors.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 代理根据其预测的准确性接收奖励或惩罚。奖励可以设计成反映预测误差或预测对特定应用程序的效用。通过反复试验的过程，代理学会将时间序列数据中的某些模式或特征与未来结果联系起来。它学会了做出既能最大化奖励又能最小化误差的预测。
- en: The reinforcement learning process involves a balance between exploration and
    exploitation. The agent explores different prediction strategies, trying to discover
    patterns and make accurate predictions. It also exploits its existing knowledge
    to make predictions based on what it has learned so far. The goal of reinforcement
    learning for time series prediction is to train the agent to make accurate and
    reliable predictions. By continually receiving feedback and updating its prediction
    strategies, the agent adapts to changing patterns in the time series and improves
    its forecasting abilities.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习过程涉及在探索和利用之间取得平衡。代理探索不同的预测策略，试图发现模式并进行准确预测。它还利用其现有知识基础进行预测，根据迄今为止所学到的内容。强化学习用于时间序列预测的目标是训练代理以进行准确可靠的预测。通过不断接收反馈并更新其预测策略，代理适应时间序列中变化的模式并改进其预测能力。
- en: '[Chapter 11](ch11.html#ch11) will show how to employ more deep learning techniques
    and applications.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '[第 11 章](ch11.html#ch11) 将展示如何运用更多的深度学习技术和应用。'
- en: ^([1](ch10.html#id771-marker)) A buy-and-hold strategy is a passive action whereby
    the trader or the algorithm initiates one buy order and holds it for a long time
    in an attempt to replicate the market’s return and minimize transaction costs
    incurred from excessive trading.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch10.html#id771-marker)) 买入并持有策略是一种被动行为，交易者或算法发起一笔买单，并长期持有以试图复制市场的回报，并最小化由于过度交易而产生的交易成本。
- en: ^([2](ch10.html#id778-marker)) Keep epsilon decay in mind as it will be used
    as a variable in the code later.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch10.html#id778-marker)) 切记保持 epsilon 衰减，因为它将在稍后的代码中用作变量。
