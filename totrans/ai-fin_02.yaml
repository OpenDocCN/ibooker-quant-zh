- en: Chapter 1\. Artificial Intelligence
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一章 人工智能
- en: This is the first time that a computer program has defeated a human professional
    player in the full-sized game of Go, a feat previously thought to be at least
    a decade away.
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这是计算机程序第一次在全尺寸围棋比赛中击败专业人类选手，这一壮举以前被认为至少还需十年时间。
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: David Silver et al. (2016)
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: David Silver等（2016年）
- en: This chapter introduces general notions, ideas, and definitions from the field
    of artificial intelligence (AI) for the purposes of this book. It also provides
    worked-out examples for different types of major learning algorithms. In particular,
    [“Algorithms”](#ai_algorithms) takes a broad perspective and categorizes types
    of data, types of learning, and types of problems typically encountered in an
    AI context. This chapter also presents examples for unsupervised and reinforcement
    learning. [“Neural Networks”](#ai_neural_networks) jumps right into the world
    of neural networks, which not only are central to what follows in later chapters
    of the book but also have proven to be among the most powerful algorithms AI has
    to offer nowadays. [“Importance of Data”](#ai_importance_of_data) discusses the
    importance of data volume and variety in the context of AI.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了人工智能领域的一般概念、思想和定义，用于本书的目的。它还为不同类型的主要学习算法提供了实例。特别是，[“算法”](#ai_algorithms)
    从广泛的角度来看分类了数据类型、学习类型和通常在AI环境中遇到的问题。本章还介绍了无监督学习和强化学习的示例。[“神经网络”](#ai_neural_networks)
    直接进入神经网络的世界，这不仅是本书后续章节的核心内容，而且被证明是当今AI最强大的算法之一。[“数据的重要性”](#ai_importance_of_data)
    讨论了在AI背景下数据量和多样性的重要性。
- en: Algorithms
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 算法
- en: This section introduces basic notions from the field of AI relevant to this
    book. It discusses the different types of data, learning, problems, and approaches
    that can be subsumed under the general term *AI*. Alpaydin (2016) provides an
    informal introduction to and overview of many of the topics covered only briefly
    in this section, along with many examples.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了与本书相关的人工智能领域的基本概念。它讨论了不同类型的数据、学习、问题和方法，这些可以归纳为通用术语*AI*。Alpaydin（2016年）提供了一个非正式的介绍，以及许多本节中仅简要涉及的主题的概述和许多示例。
- en: Types of Data
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据类型
- en: 'Data in general has two major components:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 数据通常具有两个主要组成部分：
- en: Features
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 特征
- en: Features data (or input data) is data that is given as input to an algorithm.
    In a financial context, this might be, for example, the income and the savings
    of a potential debtor.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 特征数据（或输入数据）是作为算法输入的数据。在金融背景下，例如可能是潜在债务人的收入和储蓄。
- en: Labels
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 标签
- en: Labels data (or output data) is data that is given as the relevant output to
    be learned, for example, by a supervised learning algorithm. In a financial context,
    this might be the creditworthiness of a potential debtor.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 标签数据（或输出数据）是作为相关输出给定的数据，例如通过监督学习算法学习的内容。在金融背景下，例如可能是潜在债务人的信用价值。
- en: Types of Learning
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习类型
- en: 'There are three major types of learning algorithms:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 主要有三种类型的学习算法：
- en: Supervised learning (SL)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习（SL）
- en: These are algorithms that learn from a given sample data set of features (input)
    and labels (output) values. The next section presents examples for such algorithms,
    like ordinary least-squares (OLS) regression and neural networks. The purpose
    of supervised learning is to learn the relationship between the input and output
    values. In finance, such algorithms might be trained to predict whether a potential
    debtor is creditworthy or not. For the purposes of this book, these are the most
    important types of algorithms.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这些算法从给定的特征（输入）和标签（输出）数值样本数据集中学习。接下来的部分展示了这类算法的实例，如普通最小二乘回归（OLS）和神经网络。监督学习的目的是学习输入和输出值之间的关系。在金融领域，这类算法可能被训练用于预测潜在债务人是否信用良好。对于本书而言，这些是最重要的算法类型。
- en: Unsupervised learning (UL)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习（UL）
- en: These are algorithms that learn from a given sample data set of features (input)
    values only, often with the goal of finding structure in the data. They are supposed
    to learn about the input data set, given, for example, some guiding parameters.
    Clustering algorithms fall into that category. In a financial context, such algorithms
    might cluster stocks into certain groups.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这些算法仅从给定的特征（输入）数值样本数据集中学习，通常目标是找出数据中的结构。它们被设计来学习输入数据集，例如通过一些引导参数。聚类算法属于这一类别。在金融背景下，这种算法可能会将股票聚类到某些组中。
- en: Reinforcement learning (RL)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）
- en: These are algorithms that learn from trial and error by receiving a reward for
    taking an action. They update an optimal action policy according to what rewards
    and punishments they receive. Such algorithms are, for example, used for environments
    where actions need to be taken continuously and rewards are received immediately,
    such as in a computer game.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是通过试错学习的算法，通过在采取行动后获得奖励来更新最佳行动策略。它们根据所接收到的奖励和惩罚更新最佳行动策略。例如，此类算法用于需要持续采取行动并立即获得奖励的环境，例如在电脑游戏中。
- en: Because supervised learning is addressed in the subsequent section in some detail,
    brief examples will illustrate unsupervised learning and reinforcement learning.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 因为后续章节详细介绍了监督学习，所以简要示例将说明无监督学习和强化学习。
- en: Unsupervised Learning
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 无监督学习
- en: Simply speaking, a *k-means clustering algorithm* sorts <math alttext="n"><mi>n</mi></math>
    observations into <math alttext="k"><mi>k</mi></math> clusters. Each observation
    belongs to the cluster to which its mean (center) is nearest. The following Python
    code generates sample data for which the features data is clustered. [Figure 1-1](#figure_ai_08)
    visualizes the clustered sample data and also shows that the `scikit-learn` `KMeans`
    algorithm used here has identified the clusters perfectly. The coloring of the
    dots is based on what the algorithm has learned.^([1](ch01.xhtml#idm45625352526520))
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 简单地说，*k-means聚类算法*将<math alttext="n"><mi>n</mi></math>个观测值分成<math alttext="k"><mi>k</mi></math>个聚类。每个观测值属于其均值（中心）最近的聚类。以下Python代码生成了聚类的特征数据的样本数据。[图1-1](#figure_ai_08)可视化了聚类的样本数据，还显示了这里使用的`scikit-learn`
    `KMeans`算法完美地识别了聚类。点的着色基于算法学习的内容。^([1](ch01.xhtml#idm45625352526520))
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[![1](Images/1.png)](#co_artificial_intelligence_CO1-1)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_artificial_intelligence_CO1-1)'
- en: A sample data set is created with clustered features data.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个带有聚类特征数据的样本数据集。
- en: '[![2](Images/2.png)](#co_artificial_intelligence_CO1-2)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_artificial_intelligence_CO1-2)'
- en: A `KMeans` model object is instantiated, fixing the number of clusters.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 实例化一个`KMeans`模型对象，固定聚类数。
- en: '[![3](Images/3.png)](#co_artificial_intelligence_CO1-3)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_artificial_intelligence_CO1-3)'
- en: The model is fitted to the features data.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 模型适配特征数据。
- en: '[![4](Images/4.png)](#co_artificial_intelligence_CO1-4)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_artificial_intelligence_CO1-4)'
- en: The predictions are generated given the fitted model.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 给定适配模型，生成预测结果。
- en: '[![5](Images/5.png)](#co_artificial_intelligence_CO1-5)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_artificial_intelligence_CO1-5)'
- en: The predictions are numbers from 0 to 3, each representing one cluster.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 预测结果是从0到3的数字，每个数字代表一个聚类。
- en: '![aiif 0101](Images/aiif_0101.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![aiif 0101](Images/aiif_0101.png)'
- en: Figure 1-1\. Unsupervised learning of clusters
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-1. 聚类的无监督学习
- en: 'Once an algorithm such as `KMeans` is trained, it can, for instance, predict
    the cluster for a new (not yet seen) combination of features values. Assume that
    such an algorithm is trained on features data that describes potential and real
    debtors of a bank. It might learn about the creditworthiness of potential debtors
    by generating two clusters. New potential debtors can then be sorted into a certain
    cluster: “creditworthy” versus “not creditworthy.”'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦像`KMeans`这样的算法训练完成，它可以预测一个新的（尚未见过）特征值组合的聚类情况。假设这样的算法是在描述银行潜在和真实债务人的特征数据上进行训练的。它可以通过生成两个聚类来了解潜在债务人的信用状况。然后，新的潜在债务人可以被分类到某个聚类中：“有信用”还是“没有信用”。
- en: Reinforcement learning
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 强化学习
- en: 'The following example is based on a coin tossing game that is played with a
    coin that lands 80% of the time on heads and 20% of the time on tails. The coin
    tossing game is heavily biased to emphasize the benefits of learning as compared
    to an uninformed baseline algorithm. The baseline algorithm, which bets randomly
    and equally distributes on heads and tails, achieves a total reward of around
    50, on average, per epoch of 100 bets played:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例基于一个硬币抛掷游戏，该游戏使用一枚硬币，正面的概率为80%，反面的概率为20%。硬币抛掷游戏被大大偏向以强调学习的好处，与一个未经训练的基线算法相比。随机押注并平均分配在正反面的基线算法，在每轮100次押注游戏中平均获得约50的总奖励：
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[![1](Images/1.png)](#co_artificial_intelligence_CO2-1)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_artificial_intelligence_CO2-1)'
- en: The state space (1 = heads, 0 = tails).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 状态空间（1 = 正面，0 = 反面）。
- en: '[![2](Images/2.png)](#co_artificial_intelligence_CO2-2)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_artificial_intelligence_CO2-2)'
- en: The action space (1 = bet on heads, 0 = bet on tails).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 行动空间（1 = 押注正面，0 = 押注反面）。
- en: '[![3](Images/3.png)](#co_artificial_intelligence_CO2-3)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_artificial_intelligence_CO2-3)'
- en: An action is randomly chosen from the action space.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 从动作空间中随机选择一个动作。
- en: '[![4](Images/4.png)](#co_artificial_intelligence_CO2-4)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_artificial_intelligence_CO2-4)'
- en: A state is randomly chosen from the state space.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 从状态空间中随机选择一个状态。
- en: '[![5](Images/5.png)](#co_artificial_intelligence_CO2-5)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_artificial_intelligence_CO2-5)'
- en: The total reward `tr` is increased by one if the bet is correct.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果赌注正确，则总奖励`tr`增加1。
- en: '[![6](Images/6.png)](#co_artificial_intelligence_CO2-6)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](Images/6.png)](#co_artificial_intelligence_CO2-6)'
- en: The game is played for a number of epochs; each epoch is 100 bets.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏进行多个时期；每个时期为100次赌注。
- en: '[![7](Images/7.png)](#co_artificial_intelligence_CO2-7)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[![7](Images/7.png)](#co_artificial_intelligence_CO2-7)'
- en: The average total reward of the epochs played is calculated.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 计算已播放的时期的平均总奖励。
- en: 'Reinforcement learning tries to learn from what is observed after an action
    is taken, usually based on a reward. To keep things simple, the following learning
    algorithm only keeps track of the states that are observed in each round insofar
    as they are appended to the action space `list` object. In this way, the algorithm
    learns the bias in the game, though maybe not perfectly. By randomly sampling
    from the updated action space, the bias is reflected because naturally the bet
    will more often be heads. Over time, heads is chosen, on average, around 80% of
    the time. The average total reward of around 65 reflects the improvement of the
    learning algorithm as compared to the uninformed baseline algorithm:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习试图从采取行动后观察到的内容中学习，通常基于奖励。为了简化问题，以下学习算法仅在每轮中跟踪观察到的状态，只要它们被附加到动作空间的`list`对象中。通过这种方式，算法学习了游戏中的偏差，尽管可能不完美。通过从更新后的动作空间中随机抽样，反映了偏差，因为自然地，赌注更容易更多地选择正面。随着时间的推移，平均而言，头部将被选择约80%的时间。大约65的平均总奖励反映了与未知基线算法相比学习算法的改进：
- en: '[PRE2]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[![1](Images/1.png)](#co_artificial_intelligence_CO3-1)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_artificial_intelligence_CO3-1)'
- en: Resets the action space before starting (over)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前重置动作空间（over）
- en: '[![2](Images/2.png)](#co_artificial_intelligence_CO3-2)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_artificial_intelligence_CO3-2)'
- en: Adds the observed state to the action space
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 将观察到的状态添加到动作空间
- en: Types of Tasks
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 任务类型
- en: 'Depending on the type of labels data and the problem at hand, two types of
    tasks to be learned are important:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 根据标签数据类型和手头问题的不同，学习的两种重要任务类型如下：
- en: Estimation
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 估计
- en: Estimation (or approximation, regression) refers to the cases in which the labels
    data is real-valued (continuous); that is, it is technically represented as floating
    point numbers.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 估计（或近似，回归）是指标签数据为实值（连续）的情况；也就是说，它在技术上表示为浮点数。
- en: Classification
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 分类
- en: Classification refers to the cases in which the labels data consists of a finite
    number of classes or categories that are typically represented by discrete values
    (positive natural numbers), which in turn are represented technically as integers.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 分类是指标签数据包含有限数量的类别或类别，通常由离散值（正整数）表示，技术上表示为整数。
- en: The following section provides examples for both types of tasks.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分提供了这两种任务类型的示例。
- en: Types of Approaches
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 方法类型
- en: 'Some more definitions might be in order before finishing this section. This
    book follows the common differentiation between the following three major terms:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成本节之前，可能需要进一步定义一些术语。本书遵循以下三个主要术语之间的常见区分：
- en: Artificial intelligence (AI)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能（AI）
- en: AI encompasses all types of learning (algorithms), as defined before, and some
    more (for example, expert systems).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: AI包含了所有类型的学习（算法），如前所定义，以及一些其他类型（例如专家系统）。
- en: Machine learning (ML)
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习（ML）
- en: ML is the discipline of learning relationships and other information about given
    data sets based on an algorithm and a measure of success; a measure of success
    might, for example, be the mean-squared error (MSE) given labels values and output
    values to be estimated and the predicted values from the algorithm. ML is a sub-set
    of AI.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习（ML）是根据算法和成功度量学习给定数据集的关系和其他信息的学科；例如，成功度量可能是均方误差（MSE），给定标签值和要估计的输出值与算法预测值。ML是AI的一个子集。
- en: Deep learning (DL)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（DL）
- en: DL encompasses all algorithms based on neural networks. The term *deep* is usually
    only used when the neural network has more than one hidden layer. DL is a sub-set
    of machine learning and so is therefore also a sub-set of AI.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: DL包括所有基于神经网络的算法。术语*深度*通常仅在神经网络具有多个隐藏层时使用。DL是机器学习的子集，因此也是AI的子集。
- en: DL has proven useful for a number of broad problem areas. It is suited for estimation
    and classification tasks, as well as for RL. In many cases, DL-based approaches
    perform better than alternative algorithms, such as logistic regression or kernel-based
    ones, like support vector machines.^([2](ch01.xhtml#idm45625338167000)) That is
    why this book mainly focuses on DL. DL approaches used include dense neural networks
    (DNNs), recurrent neural networks (RNNs), and convolutional neural networks (CNNs).
    More details appear in later chapters, particularly in [Part III](part03.xhtml#part_statistical_inefficiencies).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: DL已被证明在许多广泛的问题领域中非常有用。它适用于估计和分类任务，以及强化学习。在许多情况下，基于DL的方法表现优于其他算法，例如逻辑回归或基于核的方法，如支持向量机。^([2](ch01.xhtml#idm45625338167000))
    这就是为什么本书主要侧重于DL。所使用的DL方法包括密集神经网络（DNNs）、循环神经网络（RNNs）和卷积神经网络（CNNs）。更多细节将在后面的章节中出现，特别是在[第三部分](part03.xhtml#part_statistical_inefficiencies)。
- en: Neural Networks
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络
- en: The previous sections provide a broader overview of algorithms in AI. This section
    shows how neural networks fit in. A simple example will illustrate what characterizes
    neural networks in comparison to traditional statistical methods, such as ordinary
    least-squares (OLS) regression. The example starts with mathematics and then uses
    linear regression for *estimation* (or function approximation) and finally applies
    neural networks to accomplish the estimation. The approach taken here is a supervised
    learning approach where the task is to estimate labels data based on features
    data. This section also illustrates the use of neural networks in the context
    of *classification* problems.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的章节提供了人工智能算法的更广泛概述。本节展示了神经网络的适应性。一个简单的例子将说明神经网络与传统统计方法（例如普通最小二乘回归）相比的特点。该例子从数学开始，然后使用线性回归进行*估计*（或函数逼近），最后应用神经网络来完成估计。这里采用的方法是一种监督学习方法，任务是基于特征数据估计标签数据。本节还说明了神经网络在*分类*问题背景下的使用。
- en: OLS Regression
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 普通最小二乘回归
- en: 'Assume that a mathematical function is given as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 假设给出数学函数如下：
- en: <math alttext="f colon double-struck upper R right-arrow double-struck upper
    R comma y equals 2 x squared minus one-third x cubed" display="block"><mrow><mi>f</mi>
    <mo>:</mo> <mi>ℝ</mi> <mo>→</mo> <mi>ℝ</mi> <mo>,</mo> <mi>y</mi> <mo>=</mo> <mn>2</mn>
    <msup><mi>x</mi> <mn>2</mn></msup> <mo>-</mo> <mfrac><mn>1</mn> <mn>3</mn></mfrac>
    <msup><mi>x</mi> <mn>3</mn></msup></mrow></math>
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="f colon double-struck upper R right-arrow double-struck upper
    R comma y equals 2 x squared minus one-third x cubed" display="block"><mrow><mi>f</mi>
    <mo>:</mo> <mi>ℝ</mi> <mo>→</mo> <mi>ℝ</mi> <mo>,</mo> <mi>y</mi> <mo>=</mo> <mn>2</mn>
    <msup><mi>x</mi> <mn>2</mn></msup> <mo>-</mo> <mfrac><mn>1</mn> <mn>3</mn></mfrac>
    <msup><mi>x</mi> <mn>3</mn></msup></mrow></math>
- en: 'Such a function transforms an input value <math alttext="x"><mi>x</mi></math>
    to an output value <math alttext="y"><mi>y</mi></math> . Or it transforms a series
    of input values <math alttext="x 1 comma x 2 comma ellipsis comma x Subscript
    upper N Baseline"><mrow><msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi>
    <mn>2</mn></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>x</mi> <mi>N</mi></msub></mrow></math>
    into a series of output values <math alttext="y 1 comma y 2 comma ellipsis comma
    y Subscript upper N Baseline"><mrow><msub><mi>y</mi> <mn>1</mn></msub> <mo>,</mo>
    <msub><mi>y</mi> <mn>2</mn></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>y</mi>
    <mi>N</mi></msub></mrow></math> . The following Python code implements the mathematical
    function as a Python function and creates a number of input and output values.
    [Figure 1-2](#figure_ai_01) plots the output values against the input values:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的函数将一个输入值<math alttext="x"><mi>x</mi></math>转换为一个输出值<math alttext="y"><mi>y</mi></math>。或者将一系列输入值<math
    alttext="x 1 comma x 2 comma ellipsis comma x Subscript upper N Baseline"><mrow><msub><mi>x</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>,</mo> <mo>...</mo>
    <mo>,</mo> <msub><mi>x</mi> <mi>N</mi></msub></mrow></math>转换为一系列输出值<math alttext="y
    1 comma y 2 comma ellipsis comma y Subscript upper N Baseline"><mrow><msub><mi>y</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>y</mi> <mn>2</mn></msub> <mo>,</mo> <mo>...</mo>
    <mo>,</mo> <msub><mi>y</mi> <mi>N</mi></msub></mrow></math>。以下Python代码将数学函数实现为Python函数，并创建一些输入和输出值。[图 1-2](#figure_ai_01)绘制了输出值与输入值的关系：
- en: '[PRE3]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[![1](Images/1.png)](#co_artificial_intelligence_CO4-1)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_artificial_intelligence_CO4-1)'
- en: The mathematical function as a Python function
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 数学函数作为Python函数
- en: '[![2](Images/2.png)](#co_artificial_intelligence_CO4-2)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_artificial_intelligence_CO4-2)'
- en: The input values
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 输入值
- en: '[![3](Images/3.png)](#co_artificial_intelligence_CO4-4)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_artificial_intelligence_CO4-4)'
- en: The output values
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 输出值
- en: '![aiif 0102](Images/aiif_0102.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![aiif 0102](Images/aiif_0102.png)'
- en: Figure 1-2\. Output values against input values
  id: totrans-91
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-2\. 输出值与输入值
- en: Whereas in the mathematical example the function comes first, the input data
    second, and the output data third, the sequence is different in *statistical learning*.
    Assume that the previous input values and output values are given. They represent
    the *sample* (data). The problem in *statistical regression* is to find a function
    that approximates the functional relationship between the input values (also called
    the *independent values*) and the output values (also called the *dependent values*)
    as well as possible.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 而在数学示例中，函数首先出现，然后是输入数据，最后是输出数据，在*统计学习*中，顺序不同。假设给出了前述输入值和输出值。它们代表*样本*（数据）。*统计回归*问题是找到一种尽可能好地近似输入值（也称为*自变量*）和输出值（也称为*因变量*）之间的功能关系的函数。
- en: 'Assume simple OLS linear regression. In this case, the functional relationship
    between the input and output values is assumed to be linear, and the problem is
    to find optimal parameters <math alttext="alpha"><mi>α</mi></math> and <math alttext="beta"><mi>β</mi></math>
    for the following linear equation:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 假设简单OLS线性回归。在这种情况下，假设输入和输出值之间的功能关系是线性的，问题是找到以下线性方程的最优参数 <math alttext="alpha"><mi>α</mi></math>
    和 <math alttext="beta"><mi>β</mi></math>：
- en: <math alttext="ModifyingAbove f With caret colon double-struck upper R right-arrow
    double-struck upper R comma ModifyingAbove y With caret equals alpha plus beta
    x" display="block"><mrow><mover accent="true"><mi>f</mi> <mo>^</mo></mover> <mo>:</mo>
    <mi>ℝ</mi> <mo>→</mo> <mi>ℝ</mi> <mo>,</mo> <mover accent="true"><mi>y</mi> <mo>^</mo></mover>
    <mo>=</mo> <mi>α</mi> <mo>+</mo> <mi>β</mi> <mi>x</mi></mrow></math>
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="ModifyingAbove f With caret colon double-struck upper R right-arrow
    double-struck upper R comma ModifyingAbove y With caret equals alpha plus beta
    x" display="block"><mrow><mover accent="true"><mi>f</mi> <mo>^</mo></mover> <mo>:</mo>
    <mi>ℝ</mi> <mo>→</mo> <mi>ℝ</mi> <mo>,</mo> <mover accent="true"><mi>y</mi> <mo>^</mo></mover>
    <mo>=</mo> <mi>α</mi> <mo>+</mo> <mi>β</mi> <mi>x</mi></mrow></math>
- en: 'For given input values <math alttext="x 1 comma x 2 comma ellipsis comma x
    Subscript upper N Baseline"><mrow><msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo>
    <msub><mi>x</mi> <mn>2</mn></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>x</mi>
    <mi>N</mi></msub></mrow></math> and output values <math alttext="y 1 comma y 2
    comma ellipsis comma y Subscript upper N Baseline"><mrow><msub><mi>y</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>y</mi> <mn>2</mn></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo>
    <msub><mi>y</mi> <mi>N</mi></msub></mrow></math> , *optimal* in this case means
    that they minimize the mean squared error (MSE) between the real output values
    and the approximated output values:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的输入值 <math alttext="x 1 comma x 2 comma ellipsis comma x Subscript upper
    N Baseline"><mrow><msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi>
    <mn>2</mn></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>x</mi> <mi>N</mi></msub></mrow></math>
    和输出值 <math alttext="y 1 comma y 2 comma ellipsis comma y Subscript upper N Baseline"><mrow><msub><mi>y</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>y</mi> <mn>2</mn></msub> <mo>,</mo> <mo>...</mo>
    <mo>,</mo> <msub><mi>y</mi> <mi>N</mi></msub></mrow></math>，在这种情况下*最优*意味着它们最小化了真实输出值和近似输出值之间的均方误差（MSE）：
- en: <math alttext="min Underscript alpha comma beta Endscripts StartFraction 1 Over
    upper N EndFraction sigma-summation Underscript n Overscript upper N Endscripts
    left-parenthesis y Subscript n Baseline minus ModifyingAbove f With caret left-parenthesis
    x Subscript n Baseline right-parenthesis right-parenthesis squared" display="block"><mrow><munder><mo
    movablelimits="true" form="prefix">min</mo> <mrow><mi>α</mi><mo>,</mo><mi>β</mi></mrow></munder>
    <mfrac><mn>1</mn> <mi>N</mi></mfrac> <munderover><mo>∑</mo> <mi>n</mi> <mi>N</mi></munderover>
    <msup><mfenced separators="" open="(" close=")"><msub><mi>y</mi> <mi>n</mi></msub>
    <mo>-</mo><mover accent="true"><mi>f</mi> <mo>^</mo></mover><mrow><mo>(</mo><msub><mi>x</mi>
    <mi>n</mi></msub> <mo>)</mo></mrow></mfenced> <mn>2</mn></msup></mrow></math>
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="min Underscript alpha comma beta Endscripts StartFraction 1 Over
    upper N EndFraction sigma-summation Underscript n Overscript upper N Endscripts
    left-parenthesis y Subscript n Baseline minus ModifyingAbove f With caret left-parenthesis
    x Subscript n Baseline right-parenthesis right-parenthesis squared" display="block"><mrow><munder><mo
    movablelimits="true" form="prefix">min</mo> <mrow><mi>α</mi><mo>,</mo><mi>β</mi></mrow></munder>
    <mfrac><mn>1</mn> <mi>N</mi></mfrac> <munderover><mo>∑</mo> <mi>n</mi> <mi>N</mi></munderover>
    <msup><mfenced separators="" open="(" close=")"><msub><mi>y</mi> <mi>n</mi></msub>
    <mo>-</mo><mover accent="true"><mi>f</mi> <mo>^</mo></mover><mrow><mo>(</mo><msub><mi>x</mi>
    <mi>n</mi></msub> <mo>)</mo></mrow></mfenced> <mn>2</mn></msup></mrow></math>
- en: 'For the case of simple linear regression, the solution <math alttext="left-parenthesis
    alpha Superscript asterisk Baseline comma beta Superscript asterisk Baseline right-parenthesis"><mrow><mo>(</mo>
    <msup><mi>α</mi> <mo>*</mo></msup> <mo>,</mo> <msup><mi>β</mi> <mo>*</mo></msup>
    <mo>)</mo></mrow></math> is known in closed form, as shown in the following equation.
    Bars on the variables indicate sample mean values:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 对于简单线性回归情况，解 <math alttext="left-parenthesis alpha Superscript asterisk Baseline
    comma beta Superscript asterisk Baseline right-parenthesis"><mrow><mo>(</mo> <msup><mi>α</mi>
    <mo>*</mo></msup> <mo>,</mo> <msup><mi>β</mi> <mo>*</mo></msup> <mo>)</mo></mrow></math>
    在闭合形式中是已知的，如下方程所示。变量上的条表示样本均值：
- en: <math alttext="StartLayout 1st Row 1st Column beta Superscript asterisk 2nd
    Column equals 3rd Column StartFraction upper C o v left-parenthesis x comma y
    right-parenthesis Over upper V a r left-parenthesis x right-parenthesis EndFraction
    2nd Row 1st Column alpha Superscript asterisk 2nd Column equals 3rd Column y overbar
    minus beta x overbar EndLayout" display="block"><mtable><mtr><mtd><msup><mi>β</mi>
    <mo>*</mo></msup></mtd> <mtd><mo>=</mo></mtd> <mtd><mfrac><mrow><mi>C</mi><mi>o</mi><mi>v</mi><mo>(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>)</mo></mrow>
    <mrow><mi>V</mi><mi>a</mi><mi>r</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac></mtd></mtr>
    <mtr><mtd><msup><mi>α</mi> <mo>*</mo></msup></mtd> <mtd><mo>=</mo></mtd> <mtd><mrow><mover
    accent="true"><mi>y</mi> <mo>¯</mo></mover> <mo>-</mo> <mi>β</mi> <mover accent="true"><mi>x</mi>
    <mo>¯</mo></mover></mrow></mtd></mtr></mtable></math>
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column beta Superscript asterisk 2nd
    Column equals 3rd Column StartFraction upper C o v left-parenthesis x comma y
    right-parenthesis Over upper V a r left-parenthesis x right-parenthesis EndFraction
    2nd Row 1st Column alpha Superscript asterisk 2nd Column equals 3rd Column y overbar
    minus beta x overbar EndLayout" display="block"><mtable><mtr><mtd><msup><mi>β</mi>
    <mo>*</mo></msup></mtd> <mtd><mo>=</mo></mtd> <mtd><mfrac><mrow><mi>C</mi><mi>o</mi><mi>v</mi><mo>(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>)</mo></mrow>
    <mrow><mi>V</mi><mi>a</mi><mi>r</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac></mtd></mtr>
    <mtr><mtd><msup><mi>α</mi> <mo>*</mo></msup></mtd> <mtd><mo>=</mo></mtd> <mtd><mrow><mover
    accent="true"><mi>y</mi> <mo>¯</mo></mover> <mo>-</mo> <mi>β</mi> <mover accent="true"><mi>x</mi>
    <mo>¯</mo></mover></mrow></mtd></mtr></mtable></math>
- en: 'The following Python code calculates the optimal parameter values, linearly
    estimates (approximates) the output values, and plots the linear regression line
    alongside the sample data (see [Figure 1-3](#figure_ai_02)). The linear regression
    approach does not work too well here in approximating the functional relationship.
    This is confirmed by the relatively high MSE value:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 以下 Python 代码计算了最优参数值，线性估计（近似）输出值，并在示例数据旁边绘制了线性回归线（参见 [Figure 1-3](#figure_ai_02)）。在此处，线性回归方法在近似功能关系方面表现不佳。这一点由相对较高的
    MSE 值确认：
- en: '[PRE4]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[![1](Images/1.png)](#co_artificial_intelligence_CO5-1)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_artificial_intelligence_CO5-1)'
- en: Calculation of optimal <math alttext="beta"><mi>β</mi></math>
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 计算最优参数 <math alttext="beta"><mi>β</mi></math>
- en: '[![2](Images/2.png)](#co_artificial_intelligence_CO5-3)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_artificial_intelligence_CO5-3)'
- en: Calculation of optimal <math alttext="alpha"><mi>α</mi></math>
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 计算最优参数 <math alttext="alpha"><mi>α</mi></math>
- en: '[![3](Images/3.png)](#co_artificial_intelligence_CO5-5)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_artificial_intelligence_CO5-5)'
- en: Calculation of estimated output values
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 计算估计输出值
- en: '[![4](Images/4.png)](#co_artificial_intelligence_CO5-6)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_artificial_intelligence_CO5-6)'
- en: Calculation of the MSE given the approximation
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 计算近似值时的 MSE
- en: '![aiif 0103](Images/aiif_0103.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![aiif 0103](Images/aiif_0103.png)'
- en: Figure 1-3\. Sample data and linear regression line
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-3\. 样本数据和线性回归线
- en: 'How can the MSE value be improved (decreased)—maybe even to 0, that is, to
    a “perfect estimation?” Of course, OLS regression is not constrained to a simple
    linear relationship. In addition to the constant and linear terms, higher order
    monomials, for instance, can be easily added as basis functions. To this end,
    compare the regression results shown in [Figure 1-4](#figure_ai_03) and the following
    code that creates the figure. The improvements that come from using quadratic
    and cubic monomials as basis functions are obvious and also are numerically confirmed
    by the calculated MSE values. For basis functions up to and including the cubic
    monomial, the estimation is perfect, and the functional relationship is perfectly
    recovered:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如何改善（降低）MSE值，甚至可能达到0，即“完美估计”？当然，OLS回归不仅限于简单的线性关系。除了常数和线性项外，例如，可以轻松添加更高阶的单项式作为基础函数。为此，请参考[图1-4](#figure_ai_03)中显示的回归结果和创建该图的以下代码。使用二次和三次单项式作为基础函数带来的改进是显而易见的，并且通过计算得到的MSE值也得到了数值确认。对于包括三次单项式在内的基础函数，估计是完美的，功能关系也完全恢复：
- en: '[PRE5]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[![1](Images/1.png)](#co_artificial_intelligence_CO6-1)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_artificial_intelligence_CO6-1)'
- en: Regression step
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 回归步骤
- en: '[![2](Images/2.png)](#co_artificial_intelligence_CO6-2)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_artificial_intelligence_CO6-2)'
- en: Approximation step
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 近似步骤
- en: '[![3](Images/3.png)](#co_artificial_intelligence_CO6-3)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_artificial_intelligence_CO6-3)'
- en: MSE calculation
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: MSE 计算
- en: '[![4](Images/4.png)](#co_artificial_intelligence_CO6-4)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_artificial_intelligence_CO6-4)'
- en: Optimal (“perfect”) parameter values
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳（“完美”）参数值
- en: '![aiif 0104](Images/aiif_0104.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![aiif 0104](Images/aiif_0104.png)'
- en: Figure 1-4\. Sample data and OLS regression lines
  id: totrans-122
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-4\. 样本数据和OLS回归线
- en: Exploiting the knowledge of the form of the mathematical function to be approximated
    and accordingly adding more basis functions to the regression leads to a “perfect
    approximation.” That is, the OLS regression recovers the exact factors of the
    quadratic and cubic part, respectively, of the original function.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 利用数学函数形式的知识进行近似，并相应地添加更多的基础函数到回归中，可以实现“完美逼近”。也就是说，OLS回归分别恢复了原始函数的二次和三次部分的确切因素。
- en: Estimation with Neural Networks
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用神经网络进行估计
- en: However, not all relationships are of this kind. This is where, for instance,
    *neural networks* can help. Without going into the details, neural networks can
    approximate a wide range of functional relationships. Knowledge of the form of
    the relationship is generally not required.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，并非所有的关系都是这种类型。例如，*神经网络* 可以帮助。不详细讨论，神经网络可以近似广泛的功能关系。通常不需要了解关系形式的具体知识。
- en: Scikit-learn
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Scikit-learn
- en: 'The following Python code uses the `MLPRegressor` class of `scikit-learn`,
    which implements a DNN for estimation. DNNs are sometimes also called multi-layer
    perceptron (MLP).^([3](ch01.xhtml#idm45625340102264)) The results are not perfect,
    as [Figure 1-5](#figure_ai_04) and the MSE illustrate. However, they are quite
    good already for the simple configuration used:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的Python代码使用了`scikit-learn`的`MLPRegressor`类，它实现了用于估计的DNN。有时也称为多层感知器（MLP）。^([3](ch01.xhtml#idm45625340102264))
    如图[1-5](#figure_ai_04) 和MSE所示，结果并非完美。但对于所用的简单配置来说，它们已经相当不错：
- en: '[PRE6]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[![1](Images/1.png)](#co_artificial_intelligence_CO7-1)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_artificial_intelligence_CO7-1)'
- en: Instantiates the `MLPRegressor` object
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 实例化`MLPRegressor`对象
- en: '[![2](Images/2.png)](#co_artificial_intelligence_CO7-2)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_artificial_intelligence_CO7-2)'
- en: Implements the fitting or learning step
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 实现拟合或学习步骤
- en: '[![3](Images/3.png)](#co_artificial_intelligence_CO7-3)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_artificial_intelligence_CO7-3)'
- en: Implements the prediction step
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 实现预测步骤
- en: Just having a look at the results in [Figure 1-4](#figure_ai_03) and [Figure 1-5](#figure_ai_04),
    one might assume that the methods and approaches are not too dissimilar after
    all. However, there is a fundamental difference worth highlighting. Although the
    OLS regression approach, as shown explicitly for the simple linear regression,
    is based on the calculation of certain well-specified quantities and parameters,
    the neural network approach relies on *incremental learning*. This in turn means
    that a set of parameters, the *weights* within the neural network, are first initialized
    randomly and then adjusted gradually given the differences between the neural
    network output and the sample output values. This approach lets you retrain (update)
    a neural network incrementally.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅看一下[图1-4](#figure_ai_03)和[图1-5](#figure_ai_04)中的结果，人们可能会认为这些方法和方法实际上并没有太大的不同。然而，有一个值得强调的根本性差异。尽管OLS回归方法，如对简单线性回归明确显示的那样，是基于计算某些明确定义的量和参数的，但神经网络方法依赖于*增量学习*。这又意味着，一组参数，神经网络内的*权重*，首先是随机初始化的，然后根据神经网络输出与样本输出值之间的差异逐渐调整。这种方法让你可以增量地重新训练（更新）神经网络。
- en: '![aiif 0105](Images/aiif_0105.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![aiif 0105](Images/aiif_0105.png)'
- en: Figure 1-5\. Sample data and neural network–based estimations
  id: totrans-137
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-5。样本数据和基于神经网络的估计
- en: Keras
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Keras
- en: 'The next example uses a sequential model with the `Keras` deep learning package.^([4](ch01.xhtml#idm45625340095720))
    The model is fitted, or *trained*, for 100 epochs. The procedure is repeated for
    five rounds. After every such round, the approximation by the neural network is
    updated and plotted. [Figure 1-6](#figure_ai_05) shows how the approximation gradually
    improves with every round. This is also reflected in the decreasing MSE values.
    The end result is not perfect, but again, it is quite good given the simplicity
    of the model:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '下一个例子使用了`Keras`深度学习包中的顺序模型^([4](ch01.xhtml#idm45625340095720))。该模型进行了100个时期的拟合或*训练*。该过程重复进行五轮。在每一轮之后，神经网络的近似值都会更新并绘制出来。[图1-6](#figure_ai_05)显示了每一轮近似值的逐渐改善。这也反映在逐渐减少的MSE值中。最终结果并不完美，但考虑到模型的简单性，它还是相当不错的： '
- en: '[PRE7]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[![1](Images/1.png)](#co_artificial_intelligence_CO8-1)'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_artificial_intelligence_CO8-1)'
- en: Instantiates the `Sequential` model object
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 实例化`Sequential`模型对象
- en: '[![2](Images/2.png)](#co_artificial_intelligence_CO8-2)'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_artificial_intelligence_CO8-2)'
- en: Adds a densely connected hidden layer with rectified linear unit (ReLU) activation^([5](ch01.xhtml#idm45625339723528))
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 添加具有修正线性单元（ReLU）激活的密集连接隐藏层^([5](ch01.xhtml#idm45625339723528))
- en: '[![3](Images/3.png)](#co_artificial_intelligence_CO8-3)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_artificial_intelligence_CO8-3)'
- en: Adds the output layer with linear activation
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 添加具有线性激活的输出层
- en: '[![4](Images/4.png)](#co_artificial_intelligence_CO8-4)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_artificial_intelligence_CO8-4)'
- en: Compiles the model for usage
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 编译模型以供使用
- en: '[![5](Images/5.png)](#co_artificial_intelligence_CO8-5)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_artificial_intelligence_CO8-5)'
- en: Trains the neural network for a fixed number of epochs
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 对神经网络进行固定次数的训练
- en: '[![6](Images/6.png)](#co_artificial_intelligence_CO8-6)'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](Images/6.png)](#co_artificial_intelligence_CO8-6)'
- en: Implements the approximation step
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 实现近似步骤
- en: '[![7](Images/7.png)](#co_artificial_intelligence_CO8-7)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '[![7](Images/7.png)](#co_artificial_intelligence_CO8-7)'
- en: Calculates the current MSE
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 计算当前的MSE
- en: '[![8](Images/8.png)](#co_artificial_intelligence_CO8-8)'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[![8](Images/8.png)](#co_artificial_intelligence_CO8-8)'
- en: Plots the current approximation results
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制当前的近似结果
- en: Roughly speaking, one can say that the neural network does almost as well in
    the estimation as the OLS regression, which delivers a perfect result. Therefore,
    why use neural networks at all? A more comprehensive answer might need to come
    later in this book, but a somewhat different example might give some hint.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 粗略地说，可以说神经网络在估计方面几乎与OLS回归一样出色，后者提供了完美的结果。那么，为什么还要使用神经网络呢？一个更全面的答案可能需要在本书的后面给出，但一个稍微不同的例子可能会给出一些提示。
- en: Consider instead the previous sample data set, as generated from a well-defined
    mathematical function, now a *random sample data set*, for which both features
    and labels are randomly chosen. Of course, this example is for illustration and
    does not allow for a deep interpretation.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一下之前的样本数据集，这是从一个明确定义的数学函数生成的，现在是一个*随机样本数据集*，其中特征和标签都是随机选择的。当然，这个例子是为了说明，并不允许深入解释。
- en: '![aiif 0106](Images/aiif_0106.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![aiif 0106](Images/aiif_0106.png)'
- en: Figure 1-6\. Sample data and estimations after multiple training rounds
  id: totrans-160
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-6。多轮训练后的样本数据和估计
- en: 'The following code generates the random sample data set and creates the OLS
    regression estimation based on a varying number of monomial basis functions. [Figure 1-7](#figure_ai_06)
    visualizes the results. Even for the highest number of monomials in the example,
    the estimation results are still not too good. The MSE value is accordingly relatively
    high:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码生成随机样本数据集，并基于不同数量的单项式基函数创建OLS回归估计。[图 1-7](#figure_ai_06) 可视化了结果。即使在示例中单项式数量最高的情况下，估计结果仍然不太理想。相应的均方误差（MSE）值相对较高：
- en: '[PRE8]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The results for the OLS regression are not too surprising. OLS regression in
    this case assumes that the approximation can be achieved through an appropriate
    combination of a finite number of basis functions. Since the sample data set has
    been generated randomly, the OLS regression does not perform well in this case.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: OLS回归的结果并不令人意外。在这种情况下，OLS回归假设可以通过有限数量的基础函数的适当组合来实现近似。由于样本数据集是随机生成的，因此OLS回归在这种情况下表现不佳。
- en: '![aiif 0107](Images/aiif_0107.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![aiif 0107](Images/aiif_0107.png)'
- en: Figure 1-7\. Random sample data and OLS regression lines
  id: totrans-165
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-7\. 随机样本数据和OLS回归线
- en: 'What about neural networks? The application is as straightforward as before
    and yields estimations as shown in [Figure 1-8](#figure_ai_07). While the end
    result is not perfect, it is obvious that the neural network performs better than
    the OLS regression in estimating the random label values from the random features
    values. Given its architecture, however, the neural network has almost 200,000
    trainable parameters (weights), which offers relatively high flexibility, particularly
    when compared to the OLS regression, for which a maximum of 15 + 1 parameters
    are used:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络怎么样？应用方式与以前一样简单，并显示了如[图 1-8](#figure_ai_07)所示的估算结果。虽然最终结果并非完美，但显然神经网络在从随机特征值估计随机标签值方面表现更好。然而，考虑到其架构，神经网络几乎有200,000个可训练参数（权重），这提供了相对高的灵活性，尤其是与仅使用最多15+1参数的OLS回归相比：
- en: '[PRE9]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[![1](Images/1.png)](#co_artificial_intelligence_CO9-1)'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_artificial_intelligence_CO9-1)'
- en: Multiple hidden layers are added.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 添加多个隐藏层。
- en: '[![2](Images/2.png)](#co_artificial_intelligence_CO9-2)'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_artificial_intelligence_CO9-2)'
- en: Network architecture and number of trainable parameters are shown.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 展示网络架构和可训练参数的数量。
- en: '![aiif 0108](Images/aiif_0108.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![aiif 0108](Images/aiif_0108.png)'
- en: Figure 1-8\. Random sample data and neural network estimations
  id: totrans-173
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-8\. 随机样本数据和神经网络估算结果
- en: Classification with Neural Networks
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用神经网络进行分类
- en: 'Another benefit of neural networks is that they can be easily used for classification
    tasks as well. Consider the following Python code that implements a classification
    using a neural network based on `Keras`. The binary features data and labels data
    are generated randomly. The major adjustment to be made modeling-wise is to change
    the activation function from the output layer to `sigmoid` from `linear`. More
    details on this appear in later chapters. The classification is not perfect. However,
    it reaches a high level of accuracy. How the accuracy, expressed as the relationship
    between correct results to all label values, changes with the number of training
    epochs is shown in [Figure 1-9](#figure_ai_09). The accuracy starts out low and
    then improves step-wise, though not necessarily with every step:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的另一个好处是，它们也可以轻松用于分类任务。考虑下面这段使用基于`Keras`的神经网络进行分类的Python代码。二进制特征数据和标签数据是随机生成的。建模上的主要调整是将输出层的激活函数从`linear`改为`sigmoid`。更多详细信息将在后续章节中讨论。分类并不完美。但是，它达到了很高的准确率。如何准确率，即正确结果与所有标签值的关系随着训练时期数的增加而改变，显示在[图 1-9](#figure_ai_09)中。准确率开始较低，然后逐步改善，尽管并非每个步骤都如此：
- en: '[PRE10]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[![1](Images/1.png)](#co_artificial_intelligence_CO10-1)'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_artificial_intelligence_CO10-1)'
- en: Creates random features data
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 创建随机特征数据。
- en: '[![2](Images/2.png)](#co_artificial_intelligence_CO10-3)'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_artificial_intelligence_CO10-3)'
- en: Creates random labels data
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 创建随机标签数据。
- en: '[![3](Images/3.png)](#co_artificial_intelligence_CO10-5)'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_artificial_intelligence_CO10-5)'
- en: Defines the activation function for the output layer as `sigmoid`
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 将输出层的激活函数定义为`sigmoid`。
- en: '[![4](Images/4.png)](#co_artificial_intelligence_CO10-6)'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_artificial_intelligence_CO10-6)'
- en: Defines the loss function to be `binary_crossentropy`^([6](ch01.xhtml#idm45625343405032))
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 将损失函数定义为`binary_crossentropy`^([6](ch01.xhtml#idm45625343405032))
- en: '[![5](Images/5.png)](#co_artificial_intelligence_CO10-7)'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_artificial_intelligence_CO10-7)'
- en: Compares the predicted values with the labels data
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 将预测值与标签数据进行比较。
- en: '[![6](Images/6.png)](#co_artificial_intelligence_CO10-8)'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](Images/6.png)](#co_artificial_intelligence_CO10-8)'
- en: Plots the loss function and accuracy values for every training step
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制每个训练步骤的损失函数和准确度值。
- en: '![aiif 0109](Images/aiif_0109.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![aiif 0109](Images/aiif_0109.png)'
- en: Figure 1-9\. Classification accuracy and loss against number of epochs
  id: totrans-190
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-9\. 根据周期数的分类准确度和损失
- en: 'The examples in this section illustrate some fundamental characteristics of
    neural networks as compared to OLS regression:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的示例展示了神经网络与OLS回归相比的一些基本特征：
- en: Problem-agnostic
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 问题不可知性
- en: The neural network approach is agnostic when it comes to estimating and classifying
    label values, given a set of feature values. Statistical methods, such as OLS
    regression, might perform well for a smaller set of problems, but not too well
    or not at all for others.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络方法在估计和分类标签值时是不可知的，只要给定一组特征值。统计方法，如OLS回归，对于一小部分问题可能表现良好，但对其他问题则可能效果不佳甚至根本无法使用。
- en: Incremental learning
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 增量学习
- en: The optimal weights within a neural network, given a target measure of success,
    are learned incrementally based on a random initialization and incremental improvements.
    These incremental improvements are achieved by considering the differences between
    the predicted values and the sample label values and backpropagating weights updates
    through the neural network.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络中，给定一个目标成功度量，最优权重是通过随机初始化和增量改进逐步学习的。这些增量改进通过考虑预测值与样本标签值之间的差异，并通过神经网络反向传播权重更新来实现。
- en: Universal approximation
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 通用逼近
- en: There are strong mathematical theorems showing that neural networks (even with
    one hidden layer only) can approximate almost any function.^([7](ch01.xhtml#idm45625335928248))
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 存在强大的数学定理表明，神经网络（即使只有一个隐藏层）可以近似几乎任何函数。^([7](ch01.xhtml#idm45625335928248))
- en: These characteristics might justify why this book puts neural networks at the
    core with regard to the algorithms used. [Chapter 2](ch02.xhtml#superintelligence)
    discusses more good reasons.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这些特征可能正是本书将神经网络置于所用算法核心位置的理由。[第2章](ch02.xhtml#superintelligence)将讨论更多充分的理由。
- en: Neural Networks
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络
- en: Neural networks are good at learning relationships between input and output
    data. They can be applied to a number of problem types, such as estimation in
    the presence of complex relationships or classification, for which traditional
    statistical methods are not well suited.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络擅长学习输入和输出数据之间的关系。它们可应用于多种问题类型，例如在存在复杂关系的情况下的估计或分类问题，而传统的统计方法则不太适用。
- en: Importance of Data
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据的重要性
- en: The example at the end of the previous section shows that neural networks are
    capable of solving classification problems quite well. The neural network with
    one hidden layer reaches a high degree of accuracy on the given data set, or *in-sample*.
    However, what about the predictive power of a neural network? This hinges significantly
    on the volume and variety of the data available to train the neural network. Another
    numerical example, based on larger data sets, will illustrate this point.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 前一节末尾的例子表明，神经网络能够相当好地解决分类问题。具有一个隐藏层的神经网络在给定数据集上达到了很高的准确度，或称为*样本内*。然而，神经网络的预测能力如何？这在很大程度上取决于用于训练神经网络的数据量和种类的多样性。基于更大数据集的另一个数值例子将阐明这一点。
- en: Small Data Set
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 小数据集
- en: 'Consider a random sample data set similar to the one used before in the classification
    example, but with more features and more samples. Most algorithms used in AI are
    about *pattern recognition*. In the following Python code, the number of binary
    features defines the number of possible patterns about which the algorithm can
    learn something. Given that the labels data is also binary, the algorithm tries
    to learn whether a `0` or `1` is more likely given a certain pattern, say `[0,
    0, 1, 1, 1, 1, 0, 0, 0, 0]`. Because all numbers are randomly chosen with equal
    probability, there is not that much to learn beyond the fact that the labels `0`
    and `1` are equally likely no matter what (random) pattern is observed. Therefore,
    a baseline prediction algorithm should be accurate about 50% of the time, no matter
    what (random) pattern it is presented with:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个与分类示例中使用的相似的随机样本数据集，但具有更多的特征和更多的样本。人工智能中使用的大多数算法都是关于*模式识别*的。在以下 Python 代码中，二进制特征的数量定义了算法可以学习的可能模式的数量。鉴于标签数据也是二进制的，算法试图学习在给定某个模式时`0`或`1`更可能出现的情况，比如`[0,
    0, 1, 1, 1, 1, 0, 0, 0, 0]`。因为所有数字都是以相等概率随机选择的，除了标签`0`和`1`不管观察到什么（随机）模式都是同等可能发生的外，没有太多可学习的东西。因此，基线预测算法应该在任何（随机）模式被呈现时准确率约为50%：
- en: '[PRE11]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[![1](Images/1.png)](#co_artificial_intelligence_CO11-1)'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_artificial_intelligence_CO11-1)'
- en: Features data
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 特征数据
- en: '[![2](Images/2.png)](#co_artificial_intelligence_CO11-3)'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_artificial_intelligence_CO11-3)'
- en: Labels data
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 标签数据
- en: '[![3](Images/3.png)](#co_artificial_intelligence_CO11-5)'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_artificial_intelligence_CO11-5)'
- en: Number of patterns
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 模式数量
- en: 'In order to proceed, the raw data is put into a `pandas` `DataFrame` object,
    which simplifies certain operations and analyses:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 为了继续进行，原始数据被放入了一个`pandas`的`DataFrame`对象中，这简化了某些操作和分析：
- en: '[PRE12]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[![1](Images/1.png)](#co_artificial_intelligence_CO12-1)'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_artificial_intelligence_CO12-1)'
- en: Defines column names for the features data
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 为特征数据定义列名
- en: '[![2](Images/2.png)](#co_artificial_intelligence_CO12-3)'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_artificial_intelligence_CO12-3)'
- en: Puts the features data into a `DataFrame` object
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 将特征数据放入一个`DataFrame`对象中
- en: '[![3](Images/3.png)](#co_artificial_intelligence_CO12-4)'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_artificial_intelligence_CO12-4)'
- en: Puts the labels data into the same `DataFrame` object
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 将标签数据放入同一个`DataFrame`对象中
- en: '[![4](Images/4.png)](#co_artificial_intelligence_CO12-5)'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_artificial_intelligence_CO12-5)'
- en: Shows the meta information for the data set
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 显示数据集的元信息
- en: 'Two major problems can be identified given the results from executing the following
    Python code. First, not all patterns are in the sample data set. Second, the sample
    size is much too small per observed pattern. Even without digging deeper, it is
    clear that no classification algorithm can really learn about all the possible
    patterns in a meaningful way:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 通过执行以下 Python 代码的结果，可以确定两个主要问题。首先，样本数据集中并不包含所有模式。其次，每个观察到的模式的样本量太小。即使不深入挖掘，也很明显没有分类算法能够真正有意义地学习所有可能的模式：
- en: '[PRE13]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[![1](Images/1.png)](#co_artificial_intelligence_CO13-1)'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_artificial_intelligence_CO13-1)'
- en: Groups the data along all columns
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 沿所有列对数据进行分组
- en: '[![2](Images/2.png)](#co_artificial_intelligence_CO13-2)'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_artificial_intelligence_CO13-2)'
- en: Unstacks the grouped data for the labels column
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 展开标签列的分组数据
- en: '[![3](Images/3.png)](#co_artificial_intelligence_CO13-3)'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_artificial_intelligence_CO13-3)'
- en: Adds up the frequency for a `0` and a `1`
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 对`0`和`1`的频率进行求和
- en: '[![4](Images/4.png)](#co_artificial_intelligence_CO13-4)'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_artificial_intelligence_CO13-4)'
- en: Shows the frequencies for a `0` and a `1` given a certain pattern
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 显示给定某个模式的`0`和`1`的频率
- en: '[![5](Images/5.png)](#co_artificial_intelligence_CO13-5)'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_artificial_intelligence_CO13-5)'
- en: Provides statistics for the sum of the frequencies
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 提供频率总和的统计信息
- en: 'The following Python code uses the `MLPClassifier` model from `scikit-learn`.^([8](ch01.xhtml#idm45625338923864))
    The model is trained on the whole data set. What about the ability of a neural
    network to learn about the relationships within a given data set? The ability
    is pretty high, as the in-sample accuracy score shows. It is in fact close to
    100%, a result driven to a large extent by the relatively high neural network
    capacity given the relatively small data set:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 以下 Python 代码使用了`scikit-learn`中的`MLPClassifier`模型。^([8](ch01.xhtml#idm45625338923864))
    该模型在整个数据集上进行了训练。神经网络在学习给定数据集内部关系方面的能力如何？从样本内准确率分数来看，这种能力相当高。事实上，这个准确率接近于100%，这在很大程度上是由于相对较小的数据集给出了相对较高的神经网络容量所致：
- en: '[PRE14]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'But what about the *predictive power* of a trained neural network? To this
    end, the given data set can be split into a training and a test data sub-set.
    The model is trained on the training data sub-set only and then tested with regard
    to its predictive power on the test data set. As before, the accuracy of the trained
    neural network is pretty high in-sample (that is, on the training data set). However,
    it is more than 10 percentage points worse than an uninformed baseline algorithm
    on the test data set:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，对于经过训练的神经网络的*预测能力*又如何呢？为此，给定的数据集可以分为训练和测试数据子集。模型仅在训练数据子集上进行训练，然后针对测试数据集测试其预测能力。与之前相同，训练好的神经网络在样本内（即训练数据集）的准确率非常高。然而，在测试数据集上，它比一个无信息基准算法差了超过
    10 个百分点：
- en: '[PRE15]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[![1](Images/1.png)](#co_artificial_intelligence_CO14-1)'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_artificial_intelligence_CO14-1)'
- en: Splits the data into `train` and `test` data sub-sets
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据分割为`train`和`test`数据子集
- en: '[![2](Images/2.png)](#co_artificial_intelligence_CO14-4)'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_artificial_intelligence_CO14-4)'
- en: Trains the model on the training data set only
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 仅在训练数据集上训练模型
- en: '[![3](Images/3.png)](#co_artificial_intelligence_CO14-5)'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_artificial_intelligence_CO14-5)'
- en: Reports the accuracy in-sample (training data set)
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 报告了样本内的准确率（训练数据集）
- en: '[![4](Images/4.png)](#co_artificial_intelligence_CO14-6)'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_artificial_intelligence_CO14-6)'
- en: Reports the accuracy out-of-sample (test data set)
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 报告了样本外的准确率（测试数据集）
- en: Roughly speaking, the neural network, trained on a small data set only, learns
    wrong relationships due to the identified two major problem areas. The problems
    are not really relevant in the context of learning relationships *in-sample*.
    To the contrary, the smaller a data set is, the more easily in-sample relationships
    can be learned in general. However, the problem areas are highly relevant when
    using the trained neural network to generate predictions *out-of-sample*.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 简言之，仅在小数据集上训练的神经网络，由于识别出的两个主要问题区域，学习到了错误的关系。在样本内学习关系时，这些问题实际上并不重要。相反，数据集越小，通常越容易学习样本内的关系。然而，当使用训练好的神经网络进行样本外预测时，这些问题区域就变得非常重要。
- en: Larger Data Set
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更大的数据集
- en: 'Fortunately, there is often a clear way out of this problematic situation:
    a *larger data set*. Faced with real-world problems, this theoretical insight
    might be equally correct. From a practical point of view, though, such larger
    data sets are not always available, nor can they often be generated so easily.
    However, in the context of the example of this section, a larger data set is indeed
    easily created.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，通常有一种明显的方法来摆脱这个问题情境：*更大的数据集*。面对现实世界中的问题，这种理论洞见可能同样正确。然而，从实际的角度来看，这样的更大数据集并不总是可用的，也不容易生成。然而，在本节示例的上下文中，确实可以轻松地创建一个更大的数据集。
- en: 'The following Python code increases the number of samples in the initial sample
    data set significantly. The result is that the prediction accuracy of the trained
    neural network increases by more than 10 percentage points, to a level of about
    50%, which is to be expected given the nature of the labels data. It is now in
    line with an uninformed baseline algorithm:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 以下 Python 代码显著增加了初始样本数据集中的样本数量。结果是，训练好的神经网络的预测准确率提高了超过 10 个百分点，达到约 50%，这是可以预期的，考虑到标签数据的性质。现在，它与无信息基准算法的一致性已经达到了：
- en: '[PRE16]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[![1](Images/1.png)](#co_artificial_intelligence_CO15-1)'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_artificial_intelligence_CO15-1)'
- en: Prediction accuracy in-sample (training data set)
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 样本内的预测准确率（训练数据集）
- en: '[![2](Images/2.png)](#co_artificial_intelligence_CO15-2)'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_artificial_intelligence_CO15-2)'
- en: Prediction accuracy out-of-sample (test data set)
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 样本外的预测准确率（测试数据集）
- en: 'A quick analysis of the available data, as shown next, explains the increase
    in the prediction accuracy. First, all possible patterns are now represented in
    the data set. Second, all patterns have an average frequency of above 10 in the
    data set. In other words, the neural network sees basically *all the patterns
    multiple times*. This allows the neural network to “learn” that both labels `0`
    and `1` are equally likely for all possible patterns. Of course, it is a rather
    involved way of learning this, but it is a good illustration of the fact that
    a *relatively small* data set might often be *too small* in the context of neural
    networks:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 如下所示，对可用数据的快速分析说明了预测准确性的提高。首先，现在数据集中包含所有可能的模式。其次，所有模式在数据集中的平均频率均超过10。换句话说，神经网络基本上*多次看到所有模式*。这使得神经网络能够“学习”，即所有可能的模式对标签`0`和`1`同等可能。当然，这是一种相对复杂的学习方式，但它很好地说明了在神经网络背景下，*相对较小*的数据集经常可能*太小*的事实：
- en: '[PRE17]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[![1](Images/1.png)](#co_artificial_intelligence_CO16-1)'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_artificial_intelligence_CO16-1)'
- en: Adds the frequency for the `0` and `1` values
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 添加`0`和`1`值的频率
- en: '[![2](Images/2.png)](#co_artificial_intelligence_CO16-2)'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_artificial_intelligence_CO16-2)'
- en: Shows summary statistics for the sum values
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 显示和值的汇总统计信息
- en: Volume and Variety
  id: totrans-261
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据量和多样性
- en: In the context of neural networks that perform prediction tasks, the volume
    and variety of the available data used to train the neural network are decisive
    for its prediction performance. The numerical, hypothetical examples in this section
    show that the same neural network trained on a relatively small and not-as-varied
    data set underperforms its counterpart trained on a relatively large and varied
    data set by more than 10 percentage points. This difference can be considered
    huge given that AI practitioners and companies often fight for improvements as
    small as a tenth of a percentage point.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行预测任务的神经网络背景下，用于训练神经网络的可用数据的数量和多样性对其预测性能至关重要。本节中的数值假设示例表明，同一神经网络在相对较小且数据不够丰富的数据集上训练，其预测性能要比在相对较大且数据更加多样化的数据集上训练的网络低超过10个百分点。考虑到人工智能从业者和公司通常为提高预测准确性而奋斗，这种差异可以被视为巨大的。
- en: Big Data
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大数据
- en: What is the difference between a *larger* data set and a *big* data set? The
    term *big data* has been used for more than a decade now to mean a number of things.
    For the purposes of this book, one might say that a *big data set* is large enough—in
    terms of volume, variety, and also maybe velocity—for an AI algorithm to be trained
    properly such that the algorithm performs better at a prediction task as compared
    to a baseline algorithm.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '*更大*数据集和*大*数据集之间的区别是什么？*大数据*这个术语已经用了十多年，有多种含义。对于本书的目的，可以说*大数据集*足够大——在数据量、多样性甚至速度方面——使得AI算法能够适当地训练，从而使算法在预测任务中表现优于基准算法。'
- en: The larger data set used before is still small in practical terms. However,
    it is large enough to accomplish the specified goal. The required volume and variety
    of the data set are mainly driven by the structure and characteristics of the
    features and labels data.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 之前使用的更大数据集在实际条件下仍然很小。然而，它足够实现指定的目标。数据集所需的数量和多样性主要由特征和标签数据的结构和特征驱动。
- en: 'In this context, assume that a retail bank implements a neural network–based
    classification approach for credit scoring. Given in-house data, the responsible
    data scientist designs 25 categorical features, every one of which can take on
    8 different values. The resulting number of patterns is astronomically large:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，假设一家零售银行实施基于神经网络的信用评分分类方法。根据内部数据，负责的数据科学家设计了25个分类特征，每个特征可以取8个不同的值。由此产生的模式数量是天文数字：
- en: '[PRE18]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: It is clear that no single data set can provide a neural network with exposure
    to every single one of these patterns.^([9](ch01.xhtml#idm45625339136664)) Fortunately,
    in practice this is not necessary for the neural network to learn about the creditworthiness
    based on data for regular, defaulting, and/or rejected debtors. It is also not
    necessary in general to generate “good” predictions with regard to the creditworthiness
    of every potential debtor.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，没有单个数据集可以为神经网络提供接触每个模式的机会。^([9](ch01.xhtml#idm45625339136664)) 幸运的是，在实践中，对于神经网络根据常规、违约和/或拒绝的债务人的数据学习信用价值的情况，并不需要单个数据集对所有潜在债务人的信用价值进行“好”的预测。
- en: This is due to a number of reasons. To name only a few, first, not every pattern
    will be relevant in practice—some patterns might simply not exist, might be impossible,
    and so forth. Second, not all features might be equally important, reducing the
    number of relevant features and thereby the number of possible patterns. Third,
    a value of `4` or `5` for feature number `7`, say, might not make a difference
    at all, further reducing the number of relevant patterns.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 这是由于多种原因。仅举几例，首先，并非每种模式在实践中都是相关的——有些模式可能根本不存在，可能是不可能的等等。其次，并非所有特征都可能同等重要，减少相关特征数量，从而减少可能的模式数量。第三，例如，特征编号为`7`的值为`4`或`5`可能根本没有任何差别，进一步减少了相关模式的数量。
- en: Conclusions
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: For this book, artificial intelligence, or AI, encompasses methods, techniques,
    algorithms, and so on that are able to learn relationships, rules, probabilities,
    and more from data. The focus lies on supervised learning algorithms, such as
    those for estimation and classification. With regard to algorithms, neural networks
    and deep learning approaches are at the core.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本书，人工智能（AI）涵盖了能够从数据中学习关系、规则、概率等的方法、技术、算法等。重点放在监督学习算法上，例如用于估计和分类的算法。关于算法，神经网络和深度学习方法处于核心位置。
- en: 'The central theme of this book is the application of neural networks to one
    of the core problems in finance: the prediction of future market movements. More
    specifically, the problem might be to predict the direction of movement for a
    stock index or the exchange rate for a currency pair. The prediction of the future
    market direction (that is, whether a target level or price goes up or down) is
    a problem that can be easily cast into a classification setting.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书的核心主题是将神经网络应用于金融领域的核心问题之一：预测未来市场走势。更具体地说，问题可能是预测股票指数的方向或货币对的汇率。预测未来市场走向（即目标水平或价格是上升还是下降）可以很容易地转化为分类问题。
- en: Before diving deeper into the core theme itself, the next chapter first discusses
    selected topics related to what is called *superintelligence* and *technological
    singularity*. That discussion will provide useful background for the chapters
    that follow, which focus on finance and the application of AI to the financial
    domain.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入讨论核心主题本身之前，下一章首先讨论与所谓的*超智能*和*技术奇点*相关的选定主题。这些讨论将为接下来专注于金融和人工智能在金融领域应用的章节提供有用的背景。
- en: References
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Books and papers cited in this chapter:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 本章引用的书籍和论文：
- en: Alpaydin, Ethem. 2016\. *Machine Learning*. MIT Press, Cambridge.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alpaydin，Ethem。2016。*机器学习*. MIT Press，Cambridge。
- en: 'Chollet, Francois. 2017\. *Deep Learning with Python*. Shelter Island: Manning.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chollet，Francois。2017。*Python深度学习*. Shelter Island：Manning。
- en: 'Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016\. *Deep Learning*.
    Cambridge: MIT Press. [*http://deeplearningbook.org*](http://deeplearningbook.org).'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow，Ian，Yoshua Bengio和Aaron Courville。2016。*深度学习*. Cambridge：MIT Press。[*http://deeplearningbook.org*](http://deeplearningbook.org)。
- en: Kratsios, Anastasis. 2019\. “Universal Approximation Theorems.” [*https://oreil.ly/COOdI*](https://oreil.ly/COOdI).
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kratsios，Anastasis。2019。《通用逼近定理》。[*https://oreil.ly/COOdI*](https://oreil.ly/COOdI)。
- en: 'Silver, David et al. 2016\. “Mastering the Game of Go with Deep Neural Networks
    and Tree Search.” *Nature* 529 (January): 484-489.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Silver，David等。2016。《使用深度神经网络和树搜索掌握围棋》。*Nature* 529（一月）：484-489。
- en: 'Shanahan, Murray. 2015\. *The Technological Singularity*. Cambridge: MIT Press.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shanahan，Murray。2015。*技术奇点*. Cambridge：MIT Press。
- en: 'Tegmark, Max. 2017\. *Life 3.0: Being Human in the Age of Artificial Intelligence*.
    United Kingdom: Penguin Random House.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tegmark，Max。2017。*生命3.0：在人工智能时代的人类存在*. United Kingdom：Penguin Random House。
- en: 'VanderPlas, Jake. 2017\. *Python Data Science Handbook*. Sebastopol: O’Reilly.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VanderPlas，Jake。2017。*Python数据科学手册*. Sebastopol：O’Reilly。
- en: ^([1](ch01.xhtml#idm45625352526520-marker)) For details, see [`sklearn.cluster.KMeans`](https://oreil.ly/cRcJo)
    and VanderPlas (2017, ch. 5).
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch01.xhtml#idm45625352526520-marker)) 详细信息请参阅[`sklearn.cluster.KMeans`](https://oreil.ly/cRcJo)，以及VanderPlas
    (2017，第5章)。
- en: ^([2](ch01.xhtml#idm45625338167000-marker)) For details, see VanderPlas (2017,
    ch. 5).
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch01.xhtml#idm45625338167000-marker)) 详细信息请参阅VanderPlas (2017，第5章)。
- en: ^([3](ch01.xhtml#idm45625340102264-marker)) For details, see [`sklearn.neural_network.MLPRegressor`](https://oreil.ly/Oimd8).
    For more background, see Goodfellow et al. (2016, ch. 6).
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch01.xhtml#idm45625340102264-marker)) 详细信息请参阅[`sklearn.neural_network.MLPRegressor`](https://oreil.ly/Oimd8)。有关更多背景信息，请参阅Goodfellow等人（2016，第6章）。
- en: ^([4](ch01.xhtml#idm45625340095720-marker)) For details, see Chollet (2017,
    ch. 3).
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch01.xhtml#idm45625340095720-marker)) 详细信息请参阅Chollet (2017, ch. 3)。
- en: ^([5](ch01.xhtml#idm45625339723528-marker)) For details on activation functions
    with `Keras`, see [*https://keras.io/activations*](https://keras.io/activations).
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch01.xhtml#idm45625339723528-marker)) 有关`Keras`激活函数的详细信息，请参阅[*https://keras.io/activations*](https://keras.io/activations)。
- en: ^([6](ch01.xhtml#idm45625343405032-marker)) The *loss function* calculates the
    prediction error of the neural network (or other ML algorithms). *Binary cross
    entropy* is an appropriate loss function for binary classification problems, while
    the *mean squared error* (MSE) is, for example, appropriate for estimation problems.
    For details on loss functions with `Keras`, see [*https://keras.io/losses*](https://keras.io/losses).
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch01.xhtml#idm45625343405032-marker)) *损失函数*计算神经网络（或其他ML算法）的预测误差。对于二元分类问题，*二元交叉熵*是一种合适的损失函数，而对于估计问题，例如*均方误差*（MSE）则更合适。有关`Keras`损失函数的详细信息，请参阅[*https://keras.io/losses*](https://keras.io/losses)。
- en: ^([7](ch01.xhtml#idm45625335928248-marker)) See, for example, Kratsios (2019).
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch01.xhtml#idm45625335928248-marker)) 例如，参见Kratsios (2019)。
- en: ^([8](ch01.xhtml#idm45625338923864-marker)) For details, see [`sklearn.neural_network.MLPClassifier`](https://oreil.ly/hCR4h).
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch01.xhtml#idm45625338923864-marker)) 详细信息请参阅[`sklearn.neural_network.MLPClassifier`](https://oreil.ly/hCR4h)。
- en: ^([9](ch01.xhtml#idm45625339136664-marker)) Nor would current compute technology
    allow one to model and train a neural network based on such a data set if it would
    be available. In this context, the next chapter discusses the importance of hardware
    for AI.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch01.xhtml#idm45625339136664-marker)) 如果有此类数据集可用，当前计算技术也无法对基于此数据集的神经网络进行建模和训练。在这个背景下，下一章讨论了硬件对AI的重要性。
