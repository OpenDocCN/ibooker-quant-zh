<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 9. Reinforcement Learning"><div class="chapter" id="Chapter9">
<h1><span class="label">Chapter 9. </span>Reinforcement Learning</h1>


<p><a data-type="indexterm" data-primary="reinforcement learning (RL)" id="ix_Chapter9-asciidoc0"/>Incentives drive nearly everything, and finance is not an exception. Humans do not learn from millions of labeled examples. Instead, we often learn from positive or negative experiences that we associate with our actions. Learning from experiences and the associated rewards or punishments is the core idea behind reinforcement learning (RL).<sup><a data-type="noteref" id="idm45174907850488-marker" href="ch09.xhtml#idm45174907850488">1</a></sup></p>

<p>Reinforcement learning is an approach toward training a machine to find the best course of action through optimal policies that maximize rewards and minimize punishments.</p>

<p>The RL algorithms that empowered <em>AlphaGo</em> (the first computer program to defeat a professional human Go player) are also finding inroads into finance. Reinforcement learning’s main idea of <em>maximizing the rewards</em> aligns beautifully with several areas in finance, including algorithmic trading and portfolio management. Reinforcement learning is particularly suitable for algorithmic trading, because the concept of a <em>return-maximizing agent</em> in an uncertain, dynamic environment has much in common with an investor or a trading strategy that interacts with financial markets. Reinforcement learning–based models go one step further than the price prediction–based trading strategies discussed in previous chapters and determine rule-based policies for actions (i.e., place an order, do nothing, cancel an order, and so on).</p>

<p>Similarly, in portfolio management and asset allocation, reinforcement learning–based algorithms do not yield predictions and do not learn the structure of the market implicitly. They do more. They directly learn the policy of changing the portfolio allocation weights dynamically in the continuously changing market. Reinforcement learning models are also useful for order execution problems, which involve the 
<span class="keep-together">process</span> of completing a buy or sell order for a market instrument. Here, the algorithms learn through trial and error, figuring out the optimal path of execution on their own.</p>

<p>Reinforcement learning algorithms, with their ability to tackle more nuances and parameters within the operational environment, can also produce derivatives hedging strategies. Unlike traditional finance-based hedging strategies, these hedging strategies are optimal and valid under real-world market frictions, such as transaction costs, market impact, liquidity constraints, and risk limits.</p>

<p>In this chapter, we cover three reinforcement learning–based case studies covering major finance applications: algorithmic trading, derivatives hedging, and portfolio allocation. In terms of the model development steps, the case studies follow a standardized seven-step model development process presented in <a data-type="xref" href="ch02.xhtml#Chapter2">Chapter 2</a>. Model development and evaluation are key steps for reinforcement learning, and these steps will be emphasized. With multiple concepts in machine learning and finance implemented, these case studies can be used as a blueprint for any other reinforcement learning–based problem in finance.</p>

<p>In <a data-type="xref" href="#CaseStudy1RL">“Case Study 1: Reinforcement Learning–Based Trading Strategy”</a>, we demonstrate the use of RL to develop an algorithmic trading strategy.</p>

<p>In <a data-type="xref" href="#CaseStudy2RL">“Case Study 2: Derivatives Hedging”</a>, we implement and analyze reinforcement learning–based techniques to calculate the optimal hedging strategies for portfolios of derivatives under market frictions.</p>

<p>In <a data-type="xref" href="#CaseStudy3RL">“Case Study 3: Portfolio Allocation”</a>, we illustrate the use of a 
<span class="keep-together">reinforcement</span> learning–based technique on a dataset of cryptocurrency in order to allocate capital into different cryptocurrencies to maximize risk-adjusted returns. We also introduce a reinforcement learning–based <em>simulation environment</em> to train and test the model.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45174907837160">
<h5/>
<p>In addition to the points mentioned above, readers will understand the following
points by the end of this chapter:</p>

<ul>
<li>
<p>Key components of reinforcement learning (i.e., reward, agent, environment, action, and policy).</p>
</li>
<li>
<p>Model-based and model-free algorithms for reinforcement learning along with policy and value-based models.</p>
</li>
<li>
<p>Fundamental approaches to solving reinforcement learning problems, such as Markov decision processes (MDP), temporal difference (TD) learning, and artificial neural networks (ANNs).</p>
</li>
<li>
<p>Methods to train and test value-based and policy-based reinforcement learning algorithms using artificial neural networks and deep learning.</p>
</li>
<li>
<p>How to set up an agent or simulation environment for reinforcement learning problems using Python.</p>
</li>
<li>
<p>How to design and implement a problem statement related to algorithmic trading strategy, portfolio management, and instrument hedging in a classification-based machine learning framework.</p>
</li>
</ul>
</div></aside>
<div data-type="note" epub:type="note"><h1>This Chapter’s Code Repository</h1>
<p>A Python-based Jupyter notebook for all the case studies presented in this chapter is included under the folder <a href="https://oreil.ly/Fp0xD">Chapter 9 - Reinforcement Learning</a> in the code repository for this book. To work through any machine learning problems in Python involving RL models (such as DQN or policy gradient) presented in this chapter, readers need to modify the template slightly to align with their problem statement.<a data-type="indexterm" data-startref="ix_Chapter9-asciidoc1" id="idm45174907788440"/></p>
</div>






<section data-type="sect1" data-pdf-bookmark="Reinforcement Learning—Theory and Concepts"><div class="sect1" id="idm45174907787512">
<h1>Reinforcement Learning—Theory and Concepts</h1>

<p><a data-type="indexterm" data-primary="reinforcement learning (RL)" data-secondary="theory and concepts" id="ix_Chapter9-asciidoc1"/>Reinforcement learning is an extensive topic covering a wide range of concepts and terminology. The theory section of this chapter covers the items and topics listed in <a data-type="xref" href="#RLConcepts">Figure 9-1</a>.<sup><a data-type="noteref" id="idm45174907783768-marker" href="ch09.xhtml#idm45174907783768">2</a></sup></p>

<figure><div id="RLConcepts" class="figure">
<img src="Images/mlbf_0901.png" alt="mlbf 0901" width="1039" height="469"/>
<h6><span class="label">Figure 9-1. </span>RL summary of concepts</h6>
</div></figure>

<p>In order to solve any problem using RL, it is important to first understand and define the RL components.</p>








<section data-type="sect2" data-pdf-bookmark="RL Components"><div class="sect2" id="idm45174907779352">
<h2>RL Components</h2>

<p><a data-type="indexterm" data-primary="reinforcement learning (RL)" data-secondary="components" id="ix_Chapter9-asciidoc2"/>The main components of an RL system are agent, actions, environment, state, and reward.</p>
<dl>
<dt>Agent</dt>
<dd>
<p><a data-type="indexterm" data-primary="agent (RL system)" data-secondary="definition" id="idm45174907774648"/>The entity that performs actions.</p>
</dd>
<dt>Actions</dt>
<dd>
<p><a data-type="indexterm" data-primary="action" data-secondary="in reinforcement learning" id="idm45174907772392"/>The things an agent can do within its environment.</p>
</dd>
<dt>Environment</dt>
<dd>
<p><a data-type="indexterm" data-primary="environment" data-secondary="reinforcement learning" id="idm45174907770056"/>The world in which the agent resides.</p>
</dd>
<dt>State</dt>
<dd>
<p><a data-type="indexterm" data-primary="state (definition)" id="idm45174907767960"/>The current situation.</p>
</dd>
<dt>Reward</dt>
<dd>
<p><a data-type="indexterm" data-primary="reward, defined" id="idm45174907766040"/>The immediate return sent by the environment to evaluate the last action by the agent.</p>
</dd>
</dl>

<p>The goal of reinforcement learning is to learn an optimal strategy through experimental trials and relatively simple feedback loops. With the optimal strategy, the agent is capable of actively adapting to the environment to maximize the rewards. Unlike in supervised learning, these reward signals are not given to the model immediately. Instead, they are returned as a consequence of a sequence of actions that the agent makes.</p>

<p>An agent’s actions are usually conditioned on what the agent perceives from the environment. What the agent perceives is referred to as the observation or the state of the environment. <a data-type="xref" href="#RLComp">Figure 9-2</a> summarizes the components of a reinforcement learning system.</p>

<figure><div id="RLComp" class="figure">
<img src="Images/mlbf_0902.png" alt="mlbf 0902" width="836" height="594"/>
<h6><span class="label">Figure 9-2. </span>RL components</h6>
</div></figure>

<p>The interaction between the agent and the environment involves a sequence of actions and observed rewards in time, <math alttext="t equals 1 comma 2 period period period upper T">
  <mrow>
    <mi>t</mi>
    <mo>=</mo>
    <mn>1</mn>
    <mo>,</mo>
    <mn>2</mn>
    <mo>.</mo>
    <mo>.</mo>
    <mo>.</mo>
    <mi>T</mi>
  </mrow>
</math>. During the process, the agent accumulates knowledge about the environment, learns the optimal policy, and makes decisions on which action to take next so as to efficiently learn the best policy. Let’s label the state, action, and reward at time step <em>t</em> as <math alttext="upper S Subscript t Baseline comma upper A Subscript t Baseline period period period upper R Subscript t Baseline">
  <mrow>
    <msub><mi>S</mi> <mi>t</mi> </msub>
    <mo>,</mo>
    <msub><mi>A</mi> <mi>t</mi> </msub>
    <mo>.</mo>
    <mo>.</mo>
    <mo>.</mo>
    <msub><mi>R</mi> <mi>t</mi> </msub>
  </mrow>
</math>, respectively. Thus, the interaction sequence is fully described by one episode (also known as “trial” or “trajectory”), and the sequence ends at the terminal state <math alttext="upper S Subscript upper T Baseline colon upper S 1 comma upper A 1 comma upper R 2 comma upper S 2 comma upper A 2 period period period upper A Subscript upper T Baseline">
  <mrow>
    <msub><mi>S</mi> <mi>T</mi> </msub>
    <mo>:</mo>
    <msub><mi>S</mi> <mn>1</mn> </msub>
    <mo>,</mo>
    <msub><mi>A</mi> <mn>1</mn> </msub>
    <mo>,</mo>
    <msub><mi>R</mi> <mn>2</mn> </msub>
    <mo>,</mo>
    <msub><mi>S</mi> <mn>2</mn> </msub>
    <mo>,</mo>
    <msub><mi>A</mi> <mn>2</mn> </msub>
    <mo>.</mo>
    <mo>.</mo>
    <mo>.</mo>
    <msub><mi>A</mi> <mi>T</mi> </msub>
  </mrow>
</math>.</p>

<p>In addition to the five components of reinforcement learning mentioned so far, there are three additional components of reinforcement learning: policy, value function (and Q-value), and model of the environment.  Let us discuss the components in detail.</p>










<section data-type="sect3" data-pdf-bookmark="Policy"><div class="sect3" id="idm45174907732920">
<h3>Policy</h3>

<p><a data-type="indexterm" data-primary="policy, defined" id="idm45174907731512"/><a data-type="indexterm" data-primary="reinforcement learning (RL)" data-secondary="policy" id="idm45174907730808"/>A policy is an algorithm or a set of rules that describes how an agent makes its decisions. More formally, a policy is a function, usually denoted as <em>π</em>, that maps a state (<em>s</em>) and an action (<em>a</em>):</p>
<div data-type="equation">
<math alttext="a Subscript t Baseline equals pi left-parenthesis s Subscript t Baseline right-parenthesis" display="block">
  <mrow>
    <msub><mi>a</mi> <mi>t</mi> </msub>
    <mo>=</mo>
    <mi>π</mi>
    <mrow>
      <mo>(</mo>
      <msub><mi>s</mi> <mi>t</mi> </msub>
      <mo>)</mo>
    </mrow>
  </mrow>
</math>
</div>

<p>This means that an agent decides its action given its current state. The policy can be can be either deterministic or stochastic. <a data-type="indexterm" data-primary="deterministic policy" id="idm45174907720632"/>A deterministic policy maps a state to actions. <a data-type="indexterm" data-primary="stochastic policy" id="idm45174907719736"/>On the other hand, a stochastic policy outputs a probability distribution over actions. It means that instead of being sure of taking action <em>a</em>, there is a probability assigned to the action given a state.</p>

<p>Our goal in reinforcement learning is to learn an optimal policy (which is also referred to as <math alttext="pi Superscript asterisk">
  <msup><mi>π</mi> <mo>*</mo> </msup>
</math>). An optimal policy tells us how to act to maximize return in every state.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Value function (and Q-value)"><div class="sect3" id="idm45174907715720">
<h3>Value function (and Q-value)</h3>

<p><a data-type="indexterm" data-primary="reinforcement learning (RL)" data-secondary="value function" id="idm45174907714216"/><a data-type="indexterm" data-primary="state (value) function" id="idm45174907713048"/><a data-type="indexterm" data-primary="value function" id="idm45174907712376"/>The goal of a reinforcement learning agent is to learn to perform a task well in an environment. <a data-type="indexterm" data-primary="cumulative discounted reward" id="idm45174907711464"/><a data-type="indexterm" data-primary="future reward" id="idm45174907710824"/>Mathematically, this means maximizing the future reward, or cumulative discounted reward, <math alttext="upper G">
  <mi>G</mi>
</math>, which can be expressed in the following equation as a function of reward function <math alttext="upper R">
  <mi>R</mi>
</math> at different times:</p>
<div data-type="equation">
<math alttext="upper G Subscript t Baseline equals upper R Subscript t plus 1 Baseline plus gamma upper R Subscript t plus 2 Baseline plus period period period equals sigma-summation Underscript 0 Overscript normal infinity Endscripts y Superscript k Baseline upper R Subscript t plus k plus 1 Baseline" display="block">
  <mrow>
    <msub><mi>G</mi> <mi>t</mi> </msub>
    <mo>=</mo>
    <msub><mi>R</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow> </msub>
    <mo>+</mo>
    <mi>γ</mi>
    <msub><mi>R</mi> <mrow><mi>t</mi><mo>+</mo><mn>2</mn></mrow> </msub>
    <mo>+</mo>
    <mo>.</mo>
    <mo>.</mo>
    <mo>.</mo>
    <mo>=</mo>
    <munderover><mo>∑</mo> <mrow><mn>0</mn></mrow> <mi>∞</mi> </munderover>
    <msup><mi>y</mi> <mi>k</mi> </msup>
    <msub><mi>R</mi> <mrow><mi>t</mi><mo>+</mo><mi>k</mi><mo>+</mo><mn>1</mn></mrow> </msub>
  </mrow>
</math>
</div>

<p><a data-type="indexterm" data-primary="discounting factor" id="idm45174907690744"/>The discounting factor <math alttext="gamma">
  <mi>γ</mi>
</math> is a value between 0 and 1 to penalize the rewards in the future, as future rewards do not provide immediate benefits and may have higher uncertainty. Future reward is an important input to the value function.</p>

<p>The value function (or state value) measures the attractiveness of a state through a prediction of future reward <math alttext="upper G Subscript t">
  <msub><mi>G</mi> <mi>t</mi> </msub>
</math>. The value function of a state <em>s</em> is the expected return, with a policy <math alttext="pi">
  <mi>π</mi>
</math> if we are in this state at time <em>t</em>:</p>
<div data-type="equation">
<math alttext="StartLayout 1st Row  upper V left-parenthesis s right-parenthesis equals upper E left-bracket upper G Subscript t Baseline vertical-bar upper S Subscript t Baseline equals s right-bracket EndLayout" display="block">
  <mrow>
    <mi>V</mi>
    <mrow>
      <mo>(</mo>
      <mi>s</mi>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <mi>E</mi>
    <mrow>
      <mo>[</mo>
      <msub><mi>G</mi> <mi>t</mi> </msub>
      <mo>|</mo>
      <msub><mi>S</mi> <mi>t</mi> </msub>
      <mo>=</mo>
      <mi>s</mi>
      <mo>]</mo>
    </mrow>
  </mrow>
</math>
</div>

<p><a data-type="indexterm" data-primary="action-value function (Q-value)" id="idm45174907672760"/><a data-type="indexterm" data-primary="Q-value" id="idm45174907672056"/><a data-type="indexterm" data-primary="reinforcement learning (RL)" data-secondary="Q-value" id="idm45174907671384"/>Similarly, we define the action-value function (Q-value) of a state-action pair (<math alttext="s comma a">
  <mrow>
    <mi>s</mi>
    <mo>,</mo>
    <mi>a</mi>
  </mrow>
</math>) as:</p>
<div data-type="equation">
<math alttext="StartLayout 1st Row  upper Q left-parenthesis s comma a right-parenthesis equals upper E left-bracket upper G Subscript t Baseline vertical-bar upper S Subscript t Baseline equals s comma upper A Subscript t Baseline equals a right-bracket EndLayout" display="block">
  <mrow>
    <mi>Q</mi>
    <mrow>
      <mo>(</mo>
      <mi>s</mi>
      <mo>,</mo>
      <mi>a</mi>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <mi>E</mi>
    <mrow>
      <mo>[</mo>
      <msub><mi>G</mi> <mi>t</mi> </msub>
      <mo>|</mo>
      <msub><mi>S</mi> <mi>t</mi> </msub>
      <mo>=</mo>
      <mi>s</mi>
      <mo>,</mo>
      <msub><mi>A</mi> <mi>t</mi> </msub>
      <mo>=</mo>
      <mi>a</mi>
      <mo>]</mo>
    </mrow>
  </mrow>
</math>
</div>

<p>So the value function is the expected return for a state following a policy <math alttext="pi">
  <mi>π</mi>
</math>. The Q-value is the expected reward for the state-action pair following a policy <math alttext="pi">
  <mi>π</mi>
</math>.</p>

<p><a data-type="indexterm" data-primary="value function" data-secondary="Q-value and" id="idm45174907650168"/>The value function and the Q-value are interconnected as well. Since we follow the target policy <math alttext="pi">
  <mi>π</mi>
</math>, we can make use of the probability distribution over possible actions and the Q-values to recover the value function:</p>
<div data-type="equation">
<math alttext="StartLayout 1st Row  upper V left-parenthesis s right-parenthesis equals sigma-summation Underscript a element-of upper A Endscripts upper Q left-parenthesis s comma a right-parenthesis pi left-parenthesis a vertical-bar s right-parenthesis EndLayout" display="block">
  <mrow>
    <mi>V</mi>
    <mrow>
      <mo>(</mo>
      <mi>s</mi>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <munder><mo>∑</mo> <mrow><mi>a</mi><mo>∈</mo><mi>A</mi></mrow> </munder>
    <mi>Q</mi>
    <mrow>
      <mo>(</mo>
      <mi>s</mi>
      <mo>,</mo>
      <mi>a</mi>
      <mo>)</mo>
    </mrow>
    <mi>π</mi>
    <mrow>
      <mo>(</mo>
      <mi>a</mi>
      <mo>|</mo>
      <mi>s</mi>
      <mo>)</mo>
    </mrow>
  </mrow>
</math>
</div>

<p>The preceding equation represents the relationship between the value function and Q-value.</p>

<p>The relationship between reward function (<math alttext="upper R">
  <mi>R</mi>
</math>), future rewards (<math alttext="upper G">
  <mi>G</mi>
</math>), value function, and Q-value is used to derive the Bellman equations (discussed later in this chapter), which are one of the key components of many reinforcement learning models.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Model"><div class="sect3" id="idm45174907715128">
<h3>Model</h3>

<p><a data-type="indexterm" data-primary="reinforcement learning (RL)" data-secondary="model definition" id="idm45174907629384"/>The model is a descriptor of the environment. With the model, we can learn or infer how the environment would interact with and provide feedback to the agent. Models are used for <em>planning</em>, by which we mean any way of deciding on a course of action by considering possible future situations. A model of the stock market, for example, is tasked with predicting what the prices will look like in the future. The model has two major parts: <a data-type="indexterm" data-primary="P (transition probability function)" id="idm45174907627240"/><a data-type="indexterm" data-primary="transition probability function (P)" id="idm45174907626568"/><em>transition probability function</em> (<em>P</em>) and <em>reward function</em>. We already discussed the reward function. The transition function (<em>P</em>) records the probability of transitioning from one state to another after taking an action.</p>

<p>Overall, an RL agent may be directly or indirectly trying to learn a policy or value function shown in <a data-type="xref" href="#ModelValPolicy">Figure 9-3</a>. The approach to learning a policy varies depending on the RL model type. When we fully know the environment, we can find the optimal solution by using <em>model-based approaches</em>.<sup><a data-type="noteref" id="idm45174907622280-marker" href="ch09.xhtml#idm45174907622280">3</a></sup> When we do not know the environment, we follow a <em>model-free approach</em> and try to learn the model explicitly as part of the algorithm.</p>

<figure><div id="ModelValPolicy" class="figure">
<img src="Images/mlbf_0903.png" alt="mlbf 0903" width="659" height="508"/>
<h6><span class="label">Figure 9-3. </span>Model, value, and policy</h6>
</div></figure>
</div></section>













<section data-type="sect3" data-pdf-bookmark="RL components in a trading context"><div class="sect3" id="idm45174907617880">
<h3>RL components in a trading context</h3>

<p><a data-type="indexterm" data-primary="reinforcement learning (RL)" data-secondary="components in trading context" id="idm45174907616664"/>Let’s try to understand what the RL components correspond to in a trading setting:</p>
<dl>
<dt>Agent</dt>
<dd>
<p><a data-type="indexterm" data-primary="agent (RL system)" data-secondary="trading and" id="idm45174907613816"/>The agent is our trading agent. We can think of the agent as a human trader who makes trading decisions based on the current state of the exchange and their account.</p>
</dd>
<dt>Action</dt>
<dd>
<p><a data-type="indexterm" data-primary="action" data-secondary="in trading" id="idm45174907611384"/>There would be three actions: <em>Buy, Hold,</em> and <em>Sell</em>.</p>
</dd>
<dt>Reward function</dt>
<dd>
<p><a data-type="indexterm" data-primary="reward function" id="idm45174907608136"/>An obvious reward function would be the <em>realized PnL (Profit and Loss)</em>. Other reward functions can be <em>Sharpe ratio</em> or <em>maximum drawdown</em>.<sup><a data-type="noteref" id="idm45174907605976-marker" href="ch09.xhtml#idm45174907605976">4</a></sup> There can be a wide range of complex reward functions that offer a trade-off between profit and risk.</p>
</dd>
<dt>Environment</dt>
<dd>
<p><a data-type="indexterm" data-primary="environment" data-secondary="trading and" id="idm45174907603800"/>The environment in a trading context would be the <em>exchange</em>. In the case of trading on an exchange, we do not observe the complete state of the environment. Specifically, we are unaware of the other agents, and
what an agent observes is not the true state of the environment but some derivation of it.</p>
</dd>
</dl>

<p><a data-type="indexterm" data-primary="partially observable Markov decision process (POMDP)" id="idm45174907601608"/><a data-type="indexterm" data-primary="POMDP (partially observable Markov decision process)" id="idm45174907600776"/>This is referred to as a <em>partially observable Markov decision process</em> (POMDP). This is the most common type of environment that we encounter in finance.<a data-type="indexterm" data-startref="ix_Chapter9-asciidoc2" id="idm45174907599560"/></p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="RL Modeling Framework"><div class="sect2" id="idm45174907778728">
<h2>RL Modeling Framework</h2>

<p><a data-type="indexterm" data-primary="reinforcement learning (RL)" data-secondary="modeling framework" id="ix_Chapter9-asciidoc3"/>In this section, we describe the core framework of reinforcement learning used across several RL models.</p>










<section data-type="sect3" data-pdf-bookmark="Bellman equations"><div class="sect3" id="idm45174907595784">
<h3>Bellman equations</h3>

<p><a data-type="indexterm" data-primary="Bellman equations" id="idm45174907594248"/><a data-type="indexterm" data-primary="reinforcement learning (RL)" data-secondary="Bellman equations" id="idm45174907593544"/>Bellman equations refer to a set of equations that decompose the value function and Q-value into the immediate reward plus the discounted future values.</p>

<p>In RL, the main aim of an agent is to get the most expected sum of rewards from every state it lands in. To achieve that, we must try to get the optimal value function and Q-value; the Bellman equations help us to do so.</p>

<p>We use the relationship between reward function (R), future rewards (G), value function, and Q-value to derive the Bellman equation for value function, as shown in <a data-type="xref" href="#BelEqValFunc">Equation 9-1</a>.</p>
<div id="BelEqValFunc" data-type="equation">
<h5><span class="label">Equation 9-1. </span>Bellman equation for value function</h5>
<math alttext="StartLayout 1st Row  upper V left-parenthesis s right-parenthesis equals upper E left-bracket upper R Subscript t plus 1 Baseline plus gamma upper V left-parenthesis upper S Subscript t plus 1 Baseline right-parenthesis vertical-bar upper S Subscript t Baseline equals s right-bracket EndLayout" display="block">
  <mrow>
    <mi>V</mi>
    <mrow>
      <mo>(</mo>
      <mi>s</mi>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <mi>E</mi>
    <mo>[</mo>
    <msub><mi>R</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow> </msub>
    <mo>+</mo>
    <mi>γ</mi>
    <mi>V</mi>
    <mrow>
      <mo>(</mo>
      <msub><mi>S</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow> </msub>
      <mo>)</mo>
    </mrow>
    <mo>|</mo>
    <msub><mi>S</mi> <mi>t</mi> </msub>
    <mo>=</mo>
    <mi>s</mi>
    <mo>]</mo>
  </mrow>
</math>
</div>

<p>Here, the value function is decomposed into two parts; an immediate reward, <math alttext="upper R Subscript t plus 1">
  <msub><mi>R</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow> </msub>
</math>, and the discounted value of the successor state, <math alttext="gamma upper V left-parenthesis upper S Subscript t plus 1 Baseline right-parenthesis">
  <mrow>
    <mi>γ</mi>
    <mi>V</mi>
    <mo>(</mo>
    <msub><mi>S</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow> </msub>
    <mo>)</mo>
  </mrow>
</math>, as shown in the preceding equation. Hence, we have broken down the problem into the immediate reward and the discounted successor state. The state value <em>V(s)</em> for the state <em>s</em> at time <em>t</em> can be computed using the current reward <math alttext="upper R Subscript t plus 1">
  <msub><mi>R</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow> </msub>
</math> and the value function at the time <em>t</em>+1. This is the Bellman equation for value function. This equation can be maximized to get an equation called <a data-type="indexterm" data-primary="Bellman optimality equation" id="idm45174907560632"/>Bellman Optimality Equation for value function, represented by <em>V*(s)</em>.</p>

<p>We follow a very similar algorithm to estimate the optimal state-action values (Q-values). The simplified iteration algorithms for value function and Q-value are shown in Equations <a href="#IterationAlgoV">9-2</a> and <a href="#IterationAlgoQ">9-3</a>.</p>
<div data-type="equation" id="IterationAlgoV">
<h5><span class="label">Equation 9-2. </span>Iteration algorithm for value function</h5>
<math display="block">
  <mrow>
    <msub><mi>V</mi> <mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow> </msub>
    <mrow>
      <mo>(</mo>
      <mi>s</mi>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <munder><mi>m</mi> <mi>a</mi></munder>
    <mi>a</mi>
    <mi>x</mi>
    <munder><mo>∑</mo> <msup><mi>s</mi> <mo>′</mo> </msup> </munder>
    <msubsup><mi>P</mi> <mrow><mi>s</mi><msup><mi>s</mi> <mo>′</mo> </msup></mrow> <mi>a</mi> </msubsup>
    <mfenced separators="" open="(" close=")"><msubsup><mi>R</mi> <mrow><mi>s</mi><msup><mi>s</mi> <mo>′</mo> </msup></mrow> <mi>a</mi> </msubsup> <mo>+</mo> <mi>γ</mi> <msub><mi>V</mi> <mi>k</mi> </msub> <mrow><mo>(</mo><msup><mi>s</mi> <mo>′</mo> </msup><mo>)</mo></mrow></mfenced>
  </mrow>
</math>
</div>
<div data-type="equation" id="IterationAlgoQ">
<h5><span class="label">Equation 9-3. </span>Iteration algorithm for Q-value</h5>
<math display="block">
  <mrow>
    <msub><mi>Q</mi> <mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow> </msub>
    <mrow>
      <mo>(</mo>
      <mi>s</mi>
      <mo>,</mo>
      <mi>a</mi>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <munder><mo>∑</mo> <msup><mi>s</mi> <mo>′</mo> </msup> </munder>
    <msubsup><mi>P</mi> <mrow><mi>s</mi><msup><mi>s</mi> <mo>′</mo> </msup></mrow> <mi>a</mi> </msubsup>
    <mrow>
      <mo stretchy="false">[</mo>
      <msubsup><mi>R</mi> <mrow><mi>s</mi><msup><mi>s</mi> <mo>′</mo> </msup></mrow> <mi>a</mi> </msubsup>
      <mo>+</mo>
      <mi>γ</mi>
      <mo>*</mo>
      <munder><mi>m</mi> <msup><mi>a</mi> <mo>′</mo> </msup></munder>
      <mi>a</mi>
      <mi>x</mi>
      <mo>*</mo>
      <msub><mi>Q</mi> <mi>k</mi> </msub>
      <mrow>
        <mo>(</mo>
        <msup><mi>s</mi> <mo>′</mo> </msup>
        <mo>,</mo>
        <msup><mi>a</mi> <mo>′</mo> </msup>
        <mo>)</mo>
      </mrow>
          <mo stretchy="false">]</mo>
    </mrow>
  </mrow>
</math>
</div>

<p>where</p>

<ul>
<li>
<p><math display="inline">
  <msubsup><mi>P</mi> <mrow><mi>s</mi><msup><mi>s</mi> <mo>′</mo> </msup></mrow> <mi>a</mi> </msubsup>
</math> is the transition probability from state <em>s</em> to state s′, given that action <em>a</em> was chosen.</p>
</li>
<li>
<p><math display="inline">
  <msubsup><mi>R</mi> <mrow><mi>s</mi><msup><mi>s</mi> <mo>′</mo> </msup></mrow> <mi>a</mi> </msubsup>
</math> is the reward that the agent gets when it goes from state <em>s</em> to state s′, given that action <em>a</em> was chosen.</p>
</li>
</ul>

<p>Bellman equations are important because they let us express values of states as values of other states. This means that if we know the value function or Q-value of <em>s</em><sub>t+1</sub>, we can very easily calculate the value of <em>s</em><sub>t</sub>. This opens a lot of doors for iterative approaches for calculating the value for each state, since if we know the value of the next state, we can know the value of the current state.</p>

<p>If we have complete information about the environment, the iteration algorithms shown in Equations <a href="#IterationAlgoV">9-2</a> and <a href="#IterationAlgoQ">9-3</a> turn into a planning problem, solvable by dynamic programming that we will demonstrate in the next section. Unfortunately, in most scenarios, we do not know <math display="inline">
  <msub><mi>R</mi> <mrow><mi>s</mi><msup><mi>s</mi> <mo>′</mo> </msup></mrow> </msub>
</math> or <math display="inline">
  <msub><mi>P</mi> <mrow><mi>s</mi><msup><mi>s</mi> <mo>′</mo> </msup></mrow> </msub>
</math> and thus cannot apply the Bellman equations directly, but they lay the theoretical foundation for many RL algorithms.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Markov decision processes"><div class="sect3" id="idm45174907595160">
<h3>Markov decision processes</h3>

<p><a data-type="indexterm" data-primary="Markov decision processes (MDPs)" id="ix_Chapter9-asciidoc4"/><a data-type="indexterm" data-primary="MDPs (Markov decision processes)" id="ix_Chapter9-asciidoc5"/><a data-type="indexterm" data-primary="reinforcement learning (RL)" data-secondary="Markov decision processes" id="ix_Chapter9-asciidoc6"/>Almost all RL problems can be framed as Markov decision processes (MDPs). MDPs formally describe an environment for reinforcement learning. A Markov decision process consists of five elements: <math>
  <mrow>
    <mi>M</mi>
    <mo>=</mo>
    <mi>S</mi>
    <mo>,</mo>
    <mi>A</mi>
    <mo>,</mo>
    <mi>P</mi>
    <mo>,</mo>
    <mi>R</mi>
    <mo>,</mo>
    <mi>γ</mi>
  </mrow>
</math>, where the symbols carry the same meanings as defined in the previous section:</p>

<ul>
<li>
<p><em>S</em>: a set of states</p>
</li>
<li>
<p><em>A</em>: a set of actions</p>
</li>
<li>
<p><em>P</em>: transition probability</p>
</li>
<li>
<p><em>R</em>: reward function</p>
</li>
<li>
<p><em>γ</em>: discounting factor for future rewards</p>
</li>
</ul>

<p>MDPs frame the agent–environment interaction as a sequential decision problem over a series of time steps t = 1, …, T.  The agent and the environment interact continually, the agent selecting actions and the environment responding to these actions and presenting new situations to the agent, with the aim of coming up with an optimal policy or strategy. Bellman equations form the basis for the overall algorithm.</p>

<p>All states in MDP have the Markov property, referring to the fact that the future depends only on the current state, not on the history.</p>

<p>Let us look into an example of MDP in a financial context and analyze the Bellman equation. Trading in the market can be formalized as an MDP, which is a process that has specified transition probabilities from state to state.  <a data-type="xref" href="#MDP">Figure 9-4</a> shows an example of MDP in the financial market, with a set of states, transition probability, action, and reward.</p>

<figure><div id="MDP" class="figure">
<img src="Images/mlbf_0904.png" alt="mlbf 0904" width="1014" height="862"/>
<h6><span class="label">Figure 9-4. </span>Markov decision process</h6>
</div></figure>

<p>The MDP presented here has three states: bull, bear, and stagnant market, represented by three states (s<sub>0</sub>, s<sub>1</sub>, s<sub>2</sub>). The three actions of a trader are hold, buy, and sell, represented by a<sub>0</sub>, a<sub>1</sub>, a<sub>2</sub>, respectively. This is a hypothetical setup in which we assume that transition probabilities are known and the action of the trader leads to a change in the state of the market. In the subsequent sections we will look at approaches for solving RL problems without making such assumptions. The chart also shows the transition probabilities and the rewards for different actions. If we start in state s<sub>0</sub> (bull market), the agent can choose between actions a<sub>0</sub>, a<sub>1</sub>, a<sub>2</sub> (sell, buy, or hold). If it chooses action buy (a<sub>1</sub>), it remains in state s<sub>0</sub> with certainty, and without any reward. It can thus decide to stay there forever if it wants. But if it chooses action hold (a<sub>0</sub>), it has a 70% probability of gaining a reward of +50, and remaining in state s<sub>0</sub>. It can then try again to gain as much reward as possible. But at some point, it is going to end up instead in state s<sub>1</sub> (stagnant market). In state s<sub>1</sub> it has only two possible actions: hold (a<sub>0</sub>) or buy (a<sub>1</sub>). It can choose to stay put by repeatedly choosing action a<sub>1</sub>, or it can choose to move on to state s<sub>2</sub> (bear market) and get a negative reward of –250. In state s<sub>3</sub> it has no other choice than to take action buy (a<sub>1</sub>), which will most likely lead it back to state s<sub>0</sub> (bull market), gaining a reward of +200 on the way.</p>

<p>Now, by looking at this MDP, it is possible to come up with an optimal policy or a strategy to achieve the most reward over time. In state s<sub>0</sub> it is clear that action a<sub>0</sub> is the best option, and in state s<sub>2</sub> the agent has no choice but to take action a<sub>1</sub>, but in state s<sub>1</sub> it is not obvious whether the agent should stay put (a<sub>0</sub>) or sell (a<sub>2</sub>).</p>

<p>Let’s apply the following Bellman equation as per <a data-type="xref" href="#IterationAlgoQ">Equation 9-3</a> to get the optimal Q-value:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <msub><mi>Q</mi> <mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow> </msub>
    <mrow>
      <mo>(</mo>
      <mi>s</mi>
      <mo>,</mo>
      <mi>a</mi>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <munder><mo>∑</mo> <msup><mi>s</mi> <mo>′</mo> </msup> </munder>
    <msubsup><mi>P</mi> <mrow><mi>s</mi><msup><mi>s</mi> <mo>′</mo> </msup></mrow> <mi>a</mi> </msubsup>
    <mo stretchy="false">[</mo><msubsup><mi>R</mi> <mrow><mi>s</mi><msup><mi>s</mi> <mo>′</mo> </msup></mrow> <mi>a</mi> </msubsup> <mo>+</mo> <mi>γ</mi> <mo>*</mo> <munder><mi>m</mi> <msup><mi>a</mi> <mo>′</mo> </msup></munder> <mi>a</mi> <mi>x</mi> <mo>*</mo> <msub><mi>Q</mi> <mi>k</mi> </msub> <mrow><mo>(</mo><msup><mi>s</mi> <mo>′</mo> </msup><mo>,</mo><msup><mi>a</mi> <mo>′</mo> </msup><mo>)</mo></mrow><mo stretchy="false">]</mo>
  </mrow>
</math>
</div>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="n">nan</code><code class="o">=</code><code class="n">np</code><code class="o">.</code><code class="n">nan</code> <code class="c"># represents impossible actions</code>
<code class="c">#Array for transition probability</code>
<code class="n">P</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([</code> <code class="c"># shape=[s, a, s']</code>
<code class="p">[[</code><code class="mf">0.7</code><code class="p">,</code> <code class="mf">0.3</code><code class="p">,</code> <code class="mf">0.0</code><code class="p">],</code> <code class="p">[</code><code class="mf">1.0</code><code class="p">,</code> <code class="mf">0.0</code><code class="p">,</code> <code class="mf">0.0</code><code class="p">],</code> <code class="p">[</code><code class="mf">0.8</code><code class="p">,</code> <code class="mf">0.2</code><code class="p">,</code> <code class="mf">0.0</code><code class="p">]],</code>
<code class="p">[[</code><code class="mf">0.0</code><code class="p">,</code> <code class="mf">1.0</code><code class="p">,</code> <code class="mf">0.0</code><code class="p">],</code> <code class="p">[</code><code class="n">nan</code><code class="p">,</code> <code class="n">nan</code><code class="p">,</code> <code class="n">nan</code><code class="p">],</code> <code class="p">[</code><code class="mf">0.0</code><code class="p">,</code> <code class="mf">0.0</code><code class="p">,</code> <code class="mf">1.0</code><code class="p">]],</code>
<code class="p">[[</code><code class="n">nan</code><code class="p">,</code> <code class="n">nan</code><code class="p">,</code> <code class="n">nan</code><code class="p">],</code> <code class="p">[</code><code class="mf">0.8</code><code class="p">,</code> <code class="mf">0.1</code><code class="p">,</code> <code class="mf">0.1</code><code class="p">],</code> <code class="p">[</code><code class="n">nan</code><code class="p">,</code> <code class="n">nan</code><code class="p">,</code> <code class="n">nan</code><code class="p">]],</code>
<code class="p">])</code>

<code class="c"># Array for the return</code>
<code class="n">R</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([</code> <code class="c"># shape=[s, a, s']</code>
<code class="p">[[</code><code class="mf">50.</code><code class="p">,</code> <code class="mf">0.0</code><code class="p">,</code> <code class="mf">0.0</code><code class="p">],</code> <code class="p">[</code><code class="mf">0.0</code><code class="p">,</code> <code class="mf">0.0</code><code class="p">,</code> <code class="mf">0.0</code><code class="p">],</code> <code class="p">[</code><code class="mf">0.0</code><code class="p">,</code> <code class="mf">0.0</code><code class="p">,</code> <code class="mf">0.0</code><code class="p">]],</code>
<code class="p">[[</code><code class="mf">50.</code><code class="p">,</code> <code class="mf">0.0</code><code class="p">,</code> <code class="mf">0.0</code><code class="p">],</code> <code class="p">[</code><code class="n">nan</code><code class="p">,</code> <code class="n">nan</code><code class="p">,</code> <code class="n">nan</code><code class="p">],</code> <code class="p">[</code><code class="mf">0.0</code><code class="p">,</code> <code class="mf">0.0</code><code class="p">,</code> <code class="o">-</code><code class="mf">250.</code><code class="p">]],</code>
<code class="p">[[</code><code class="n">nan</code><code class="p">,</code> <code class="n">nan</code><code class="p">,</code> <code class="n">nan</code><code class="p">],</code> <code class="p">[</code><code class="mf">200.</code><code class="p">,</code> <code class="mf">0.0</code><code class="p">,</code> <code class="mf">0.0</code><code class="p">],</code> <code class="p">[</code><code class="n">nan</code><code class="p">,</code> <code class="n">nan</code><code class="p">,</code> <code class="n">nan</code><code class="p">]],</code>
<code class="p">])</code>
<code class="c">#Actions</code>
<code class="n">A</code> <code class="o">=</code> <code class="p">[[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">],</code> <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">2</code><code class="p">],</code> <code class="p">[</code><code class="mi">1</code><code class="p">]]</code>
<code class="c">#The data already obtained from yahoo finance is imported.</code>

<code class="c">#Now let's run the Q-Value Iteration algorithm:</code>
<code class="n">Q</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">full</code><code class="p">((</code><code class="mi">3</code><code class="p">,</code> <code class="mi">3</code><code class="p">),</code> <code class="o">-</code><code class="n">np</code><code class="o">.</code><code class="n">inf</code><code class="p">)</code> <code class="c"># -inf for impossible actions</code>
<code class="k">for</code> <code class="n">state</code><code class="p">,</code> <code class="n">actions</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">A</code><code class="p">):</code>
    <code class="n">Q</code><code class="p">[</code><code class="n">state</code><code class="p">,</code> <code class="n">actions</code><code class="p">]</code> <code class="o">=</code> <code class="mf">0.0</code> <code class="c"># Initial value = 0.0, for all possible actions</code>
<code class="n">discount_rate</code> <code class="o">=</code> <code class="mf">0.95</code>
<code class="n">n_iterations</code> <code class="o">=</code> <code class="mi">100</code>
<code class="k">for</code> <code class="n">iteration</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">n_iterations</code><code class="p">):</code>
    <code class="n">Q_prev</code> <code class="o">=</code> <code class="n">Q</code><code class="o">.</code><code class="n">copy</code><code class="p">()</code>
    <code class="k">for</code> <code class="n">s</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">3</code><code class="p">):</code>
        <code class="k">for</code> <code class="n">a</code> <code class="ow">in</code> <code class="n">A</code><code class="p">[</code><code class="n">s</code><code class="p">]:</code>
            <code class="n">Q</code><code class="p">[</code><code class="n">s</code><code class="p">,</code> <code class="n">a</code><code class="p">]</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">sum</code><code class="p">([</code>
                <code class="n">T</code><code class="p">[</code><code class="n">s</code><code class="p">,</code> <code class="n">a</code><code class="p">,</code> <code class="n">sp</code><code class="p">]</code> <code class="o">*</code> <code class="p">(</code><code class="n">R</code><code class="p">[</code><code class="n">s</code><code class="p">,</code> <code class="n">a</code><code class="p">,</code> <code class="n">sp</code><code class="p">]</code> <code class="o">+</code> <code class="n">discount_rate</code> <code class="o">*</code> <code class="n">np</code><code class="o">.</code><code class="n">max</code><code class="p">(</code><code class="n">Q_prev</code><code class="p">[</code><code class="n">sp</code><code class="p">]))</code>
        <code class="k">for</code> <code class="n">sp</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">3</code><code class="p">)])</code>
<code class="nb">print</code><code class="p">(</code><code class="n">Q</code><code class="p">)</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">[[109.43230584 103.95749333  84.274035  ]
 [  5.5402017          -inf   5.83515676]
 [        -inf 269.30353051         -inf]]</pre>

<p>This gives us the optimal policy (Q-value) for this MDP, when using a discount rate of 0.95. Looking for the highest Q-value for each of the states: in a bull market (s<sub>0</sub>) choose action hold (a<sub>0</sub>); in a stagnant market (s<sub>1</sub>) choose action sell (a<sub>2</sub>); and in a bear market (s<sub>2</sub>) choose action buy (a<sub>1</sub>).</p>

<p>The preceding example is a demonstration of a dynamic programming (DP) algorithm for obtaining optimal policy. These methods make an unrealistic assumption of complete knowledge of the environment but are the conceptual foundations for most other approaches.<a data-type="indexterm" data-startref="ix_Chapter9-asciidoc6" id="idm45174907410296"/><a data-type="indexterm" data-startref="ix_Chapter9-asciidoc5" id="idm45174907129736"/><a data-type="indexterm" data-startref="ix_Chapter9-asciidoc4" id="idm45174907129064"/></p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Temporal difference learning"><div class="sect3" id="idm45174907486104">
<h3>Temporal difference learning</h3>

<p><a data-type="indexterm" data-primary="reinforcement learning (RL)" data-secondary="temporal difference learning" id="idm45174907127016"/><a data-type="indexterm" data-primary="temporal difference (TD) learning" id="idm45174907125960"/>Reinforcement learning problems with discrete actions can often be modeled as Markov decision processes, as we saw in the previous example, but in most cases the agent initially has no insight into the transition probabilities. It also does not know what the rewards are going to be. This is where temporal difference (TD) learning can be useful.</p>

<p>A TD learning algorithm is very similar to the value iteration algorithm (<a data-type="xref" href="#IterationAlgoV">Equation 9-2</a>) based on the Bellman equation but is tweaked to take into account the fact that the agent has only partial knowledge of the MDP. In general, we assume that the agent initially knows only the possible states and actions and nothing more. For example, the agent uses an exploration policy, a purely random policy, to explore the MDP, and as it progresses, the TD learning algorithm updates the estimates of the state values based on the transitions and rewards that are actually observed.</p>

<p>The key idea in TD learning is to update the value function V(<em>S<sub>t</sub></em>) toward an estimated return <math alttext="upper R Subscript t plus 1 Baseline plus gamma upper V left-parenthesis upper S Subscript t plus 1 Baseline right-parenthesis">
  <mrow>
    <msub><mi>R</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow> </msub>
    <mo>+</mo>
    <mi>γ</mi>
    <mi>V</mi>
    <mrow>
      <mo>(</mo>
      <msub><mi>S</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow> </msub>
      <mo>)</mo>
    </mrow>
  </mrow>
</math> (known as the <em>TD target</em>). The extent to which we want to update the value function is controlled by the <em>learning rate</em> hyperparameter <em>α</em>, which defines how aggressive we want to be when updating our value. When <em>α</em> is close to zero, we’re not updating very aggressively. When <em>α</em> is close to one, we’re simply replacing the old value with the updated value:</p>
<div data-type="equation">
<math display="block">
<mrow>
    <mi>V</mi>
    <mrow>
      <mo>(</mo>
      <msub><mi>s</mi> <mi>t</mi> </msub>
      <mo>)</mo>
    </mrow>
    <mo>←</mo>
    <mi>V</mi>
    <mrow>
      <mo>(</mo>
      <msub><mi>s</mi> <mi>t</mi> </msub>
      <mo>)</mo>
    </mrow>
    <mo>+</mo>
    <mi>α</mi>
    <mrow>
      <mo>(</mo>
      <msub><mi>R</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow> </msub>
      <mo>+</mo>
      <mi>γ</mi>
      <mi>V</mi>
      <mrow>
        <mo>(</mo>
        <msub><mi>s</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow> </msub>
        <mo>)</mo>
      </mrow>
      <mo>–</mo>
      <mi>V</mi>
      <mrow>
        <mo>(</mo>
        <msub><mi>s</mi> <mi>t</mi> </msub>
        <mo>)</mo>
      </mrow>
      <mo>)</mo>
    </mrow>
  </mrow>
</math>
</div>

<p>Similarly, for Q-value estimation:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <mi>Q</mi>
    <mrow>
      <mo>(</mo>
      <msub><mi>s</mi> <mi>t</mi> </msub>
      <mo>,</mo>
      <msub><mi>a</mi> <mi>t</mi> </msub>
      <mo>)</mo>
    </mrow>
    <mo>←</mo>
    <mi>Q</mi>
    <mrow>
      <mo>(</mo>
      <msub><mi>s</mi> <mi>t</mi> </msub>
      <mo>,</mo>
      <msub><mi>a</mi> <mi>t</mi> </msub>
      <mo>)</mo>
    </mrow>
    <mo>+</mo>
    <mi>α</mi>
    <mrow>
      <mo>(</mo>
      <msub><mi>R</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow> </msub>
      <mo>+</mo>
      <mi>γ</mi>
      <mi>Q</mi>
      <mrow>
        <mo>(</mo>
        <msub><mi>s</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow> </msub>
        <mo>,</mo>
        <msub><mi>a</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow> </msub>
        <mo>)</mo>
      </mrow>
      <mo>–</mo>
      <mi>Q</mi>
      <mrow>
        <mo>(</mo>
        <msub><mi>s</mi> <mi>t</mi> </msub>
        <mo>,</mo>
        <msub><mi>a</mi> <mi>t</mi> </msub>
        <mo>)</mo>
      </mrow>
      <mo>)</mo>
    </mrow>
  </mrow>
</math>
</div>

<p>Many RL models use the TD learning algorithm that we will see in the next section.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Artificial neural network and deep learning"><div class="sect3" id="idm45174907061784">
<h3>Artificial neural network and deep learning</h3>

<p><a data-type="indexterm" data-primary="artificial neural networks (ANNs)" data-secondary="reinforcement learning and" id="idm45174907060552"/><a data-type="indexterm" data-primary="deep learning" data-secondary="reinforcement learning and" id="idm45174907059448"/><a data-type="indexterm" data-primary="reinforcement learning (RL)" data-secondary="artificial neural networks/deep learning" id="idm45174907058504"/>Reinforcement learning models often leverage an artificial neural network and deep learning methods to approximate a value or policy function. That is, ANN can learn to map states to values, or state-action pairs to Q-values. ANNs use <em>coefficients</em>, or <em>weights</em>, to approximate the function relating inputs to outputs. In the context of RL, the learning of ANNs means finding the right weights by iteratively adjusting them in such a way that the rewards are maximized. Refer to <a href="ch03.xhtml#Chapter3">3</a> and <a href="ch05.xhtml#Chapter5">5</a> for more details on methods related to ANN (including deep learning).<a data-type="indexterm" data-startref="ix_Chapter9-asciidoc3" id="idm45174907054584"/></p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Reinforcement Learning Models"><div class="sect2" id="reinforcement_learning_models">
<h2>Reinforcement Learning Models</h2>

<p><a data-type="indexterm" data-primary="reinforcement learning (RL)" data-secondary="models" id="ix_Chapter9-asciidoc7"/>Reinforcement learning can be categorized into <em>model-based</em> and <em>model-free</em> algorithms, based on whether the rewards and probabilities for each step are readily accessible.</p>










<section data-type="sect3" data-pdf-bookmark="Model-based algorithms"><div class="sect3" id="idm45174907049672">
<h3>Model-based algorithms</h3>

<p><a data-type="indexterm" data-primary="model-based algorithms" id="idm45174907048472"/><a data-type="indexterm" data-primary="reinforcement learning (RL)" data-secondary="model-based algorithms" id="idm45174907047768"/>Model-based algorithms try to understand the environment and create a model to represent it. When the RL problem includes well-defined transition probabilities and a limited number of states and actions, it can be framed as a <em>finite MDP</em> for which dynamic programming (DP) can compute an exact solution, similar to the previous example.<sup><a data-type="noteref" id="idm45174907045944-marker" href="ch09.xhtml#idm45174907045944">5</a></sup></p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Model-free algorithms"><div class="sect3" id="idm45174907045016">
<h3>Model-free algorithms</h3>

<p><a data-type="indexterm" data-primary="model-free algorithms" id="idm45174907043608"/><a data-type="indexterm" data-primary="reinforcement learning (RL)" data-secondary="model-free algorithms" id="idm45174907042744"/>Model-free algorithms try to maximize the expected reward only from real experience, without a model or prior knowledge. Model-free algorithms are used when we have incomplete information about the model. The agent’s policy <em>π(s)</em> provides the guideline on what is the optimal action to take in a certain state with the goal of maximizing the total rewards. Each state is associated with a value function <em>V(s)</em> predicting the expected amount of future rewards we are able to receive in this state by acting on the corresponding policy. In other words, the value function quantifies how good a state is. Model-free algorithms are further divided into <em>value-based</em> and <em>policy-based</em>. Value-based algorithms learn the state, or Q-value, by choosing the best action in a state. These algorithms are generally based upon temporal difference learning that we discussed in the RL framework section. <a data-type="indexterm" data-primary="direct policy search" id="idm45174907039128"/><a data-type="indexterm" data-primary="policy-based algorithms" id="idm45174907038424"/>Policy-based algorithms (also known as <em>direct policy search</em>) directly learn an optimal policy that maps state to action (or tries to approximate optimal policy, if true optimal policy is not attainable).</p>

<p>In most situations in finance, we do not fully know the environment, rewards, or transition probabilities, and we must fall back to model-free algorithms and related approaches.<sup><a data-type="noteref" id="idm45174907036600-marker" href="ch09.xhtml#idm45174907036600">6</a></sup> Hence, the focus of the next section and of the case studies will be the model-free methods and related algorithms.</p>

<p><a data-type="xref" href="#TaxRLModels">Figure 9-5</a> shows a taxonomy of model-free reinforcement learning. We highly recommend that readers refer to <em>Reinforcement Learning: An Introduction</em> for a more in-depth understanding of the algorithms and the concepts.</p>

<figure><div id="TaxRLModels" class="figure">
<img src="Images/mlbf_0905.png" alt="mlbf 0905" width="1184" height="782"/>
<h6><span class="label">Figure 9-5. </span>Taxonomy of RL models</h6>
</div></figure>

<p>In the context of model-free methods, temporal difference learning is one of the most used approaches. In TD, the algorithm refines its estimates based on its own prior estimates. The value-based algorithms <em>Q-learning</em> and <em>SARSA</em> use this approach.</p>

<p>Model-free methods often leverage an artificial neural network to approximate a value or policy function. <em>Policy gradient</em> and <em>deep Q-network (DQN)</em> are two commonly used model-free algorithms that use artificial neutral networks. Policy gradient is a policy-based approach that directly parameterizes the policy. Deep Q-network is a value-based method that combines deep learning with <em>Q-learning</em>, which sets the learning objective to optimize the estimates of Q-value.<sup><a data-type="noteref" id="idm45174907028376-marker" href="ch09.xhtml#idm45174907028376">7</a></sup></p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Q-Learning"><div class="sect3" id="idm45174907026872">
<h3>Q-Learning</h3>

<p><a data-type="indexterm" data-primary="Q-learning" id="idm45174907025592"/><a data-type="indexterm" data-primary="reinforcement learning (RL)" data-secondary="Q-learning" id="idm45174907024696"/><a data-type="indexterm" data-primary="temporal difference (TD) learning" data-secondary="Q-learning and" id="idm45174907023784"/><em>Q-learning</em> is an adaptation of TD learning. The algorithm evaluates which action to take based on a Q-value (or action-value) function that determines the value of being in a certain state and taking a certain action at that state. For each state-action pair <em>(s, a)</em>, this algorithm keeps track of a running average of the rewards, <em>R</em>, which the agent gets upon leaving the state <em>s</em> with action <em>a</em>, plus the rewards it expects to earn later. Since the target policy would act optimally, we take the maximum of the Q-value estimates for the next state.</p>

<p>The learning proceeds <em>off-policy</em>—that is, the algorithm does <em>not</em> need to select actions based on the policy that is implied by the value function alone. However, convergence requires that all state-action pairs continue to be updated throughout the training process, and a straightforward way to ensure that this occurs is to use an <em>ε-greedy</em> policy, which is defined further in the following section.</p>

<p>The steps of Q-learning are:</p>
<ol>
<li>
<p>At time step <em>t</em>, we start from state <em>s<sub>t</sub></em> and pick an action according to Q-values,
<math alttext="a Subscript t Baseline equals m a x Subscript a Baseline upper Q left-parenthesis s Subscript t Baseline comma a right-parenthesis">
  <mrow>
    <msub><mi>a</mi> <mi>t</mi> </msub>
    <mo>=</mo>
    <mi>m</mi>
    <mi>a</mi>
    <msub><mi>x</mi> <mi>a</mi> </msub>
    <mi>Q</mi>
    <mrow>
      <mo>(</mo>
      <msub><mi>s</mi> <mi>t</mi> </msub>
      <mo>,</mo>
      <mi>a</mi>
      <mo>)</mo>
    </mrow>
  </mrow>
</math>.</p>
</li>
<li>
<p>We apply an <em>ε-greedy</em> approach that selects an action randomly with a probability of <em>ε</em> or otherwise chooses the best action according to the Q-value function. This ensures the exploration of new actions in a given state while also exploiting the learning experience.<sup><a data-type="noteref" id="idm45174907004296-marker" href="ch09.xhtml#idm45174907004296">8</a></sup></p>
</li>
<li>
<p>With action <em>a<sub>t</sub></em>, we observe reward <em>R<sub>t+1</sub></em> and get into the next state <em>S<sub>t+1</sub></em>.</p>
</li>
<li>
<p>We update the action-value function:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <mi>Q</mi>
    <mrow>
      <mo>(</mo>
      <msub><mi>s</mi> <mi>t</mi> </msub>
      <mo>,</mo>
      <msub><mi>a</mi> <mi>t</mi> </msub>
      <mo>)</mo>
    </mrow>
    <mo>←</mo>
    <mi>Q</mi>
    <mrow>
      <mo>(</mo>
      <msub><mi>s</mi> <mi>t</mi> </msub>
      <mo>,</mo>
      <msub><mi>a</mi> <mi>t</mi> </msub>
      <mo>)</mo>
    </mrow>
    <mo>+</mo>
    <mi>α</mi>
    <mrow>
      <mo>(</mo>
      <msub><mi>R</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow> </msub>
      <mo>+</mo>
      <mi>γ</mi>
      <munder><mo movablelimits="true" form="prefix">max</mo> <mi>a</mi> </munder>
      <mi>Q</mi>
      <mrow>
        <mo>(</mo>
        <msub><mi>s</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow> </msub>
        <mo>,</mo>
        <msub><mi>a</mi> <mi>t</mi> </msub>
        <mo>)</mo>
      </mrow>
      <mo>–</mo>
      <mi>Q</mi>
      <mrow>
        <mo>(</mo>
        <msub><mi>s</mi> <mi>t</mi> </msub>
        <mo>,</mo>
        <msub><mi>a</mi> <mi>t</mi> </msub>
        <mo>)</mo>
      </mrow>
      <mo>)</mo>
    </mrow>
  </mrow>
</math>
</div>
</li>
<li>
<p>We increment the time step, <em>t = t+1</em>, and repeat the steps.</p>
</li>

</ol>

<p>Given enough iterations of the steps above, this algorithm will converge to the optimal Q-value.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="SARSA"><div class="sect3" id="idm45174906966632">
<h3>SARSA</h3>

<p><a data-type="indexterm" data-primary="reinforcement learning (RL)" data-secondary="SARSA" id="idm45174906965432"/><a data-type="indexterm" data-primary="SARSA" id="idm45174906964264"/><a data-type="indexterm" data-primary="temporal difference (TD) learning" data-secondary="SARSA" id="idm45174906963592"/>SARSA is also a TD learning–based algorithm. It refers to the procedure of updating the Q-value by following a sequence of
<math alttext="period period period upper S Subscript t Baseline comma upper A Subscript t Baseline comma upper R Subscript t plus 1 Baseline comma upper S Subscript t plus 1 Baseline comma upper A Subscript t plus 1 Baseline comma period period period">
  <mrow>
    <mo>.</mo>
    <mo>.</mo>
    <mo>.</mo>
    <msub><mi>S</mi> <mi>t</mi> </msub>
    <mo>,</mo>
    <msub><mi>A</mi> <mi>t</mi> </msub>
    <mo>,</mo>
    <msub><mi>R</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow> </msub>
    <mo>,</mo>
    <msub><mi>S</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow> </msub>
    <mo>,</mo>
    <msub><mi>A</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow> </msub>
    <mo>,</mo>
    <mo>.</mo>
    <mo>.</mo>
    <mo>.</mo>
  </mrow>
</math>. The first two steps of SARSA are similar to the steps of Q-learning. However, unlike Q-learning, SARSA is an <em>on-policy</em> algorithm in which the agent grasps the optimal policy and uses the same to act. In this algorithm, the policies used for <em>updating</em> and for <em>acting</em> are the same. Q-learning is considered an <em>off-policy</em> algorithm.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Deep Q-Network"><div class="sect3" id="idm45174906945896">
<h3>Deep Q-Network</h3>

<p><a data-type="indexterm" data-primary="deep Q-network (DQN)" id="idm45174906944424"/><a data-type="indexterm" data-primary="DQN (deep Q-network)" id="idm45174906943720"/><a data-type="indexterm" data-primary="reinforcement learning (RL)" data-secondary="deep Q-network" id="idm45174906943048"/>In the previous section, we saw how Q-learning allows us to learn the optimal Q-value function in an environment with discrete state actions using iterative updates based on the Bellman equation. However, Q-learning may have the following 
<span class="keep-together">drawbacks</span>:</p>

<ul>
<li>
<p>In cases where the state and action space are large, the optimal Q-value table quickly becomes computationally infeasible.</p>
</li>
<li>
<p>Q-learning may suffer from instability and divergence.</p>
</li>
</ul>

<p>To address these shortcomings, we use ANNs to approximate Q-values. For example, if we use a function with parameter <em>θ</em> to calculate Q-values, we can label the Q-value function as <em>Q(s,a;θ)</em>. The deep Q-learning algorithm approximates the Q-values by learning a set of weights, <em>θ</em>, of a multilayered deep Q-network that maps states to actions. The algorithm aims to greatly improve and stabilize the training procedure of Q-learning through two innovative mechanisms:</p>
<dl>
<dt>Experience replay</dt>
<dd>
<p><a data-type="indexterm" data-primary="experience replay" id="idm45174906935176"/>Instead of running Q-learning on state-action pairs as they occur during simulation or actual experience, the algorithm stores the history of state, action, reward, and next state transitions that are experienced by the agent in one large <em>replay memory</em>. This can be referred to as a <em>mini-batch</em> of observations. During Q-learning updates, samples are drawn at random from the replay memory, and thus one sample could be used multiple times. Experience replay improves data efficiency, removes correlations in the observation sequences, and smooths over changes in the data distribution.</p>
</dd>
<dt>Periodically updated target</dt>
<dd>
<p><em>Q</em> is optimized toward target values that are only periodically updated. The Q-network is cloned and kept frozen as the optimization targets every <em>C</em> step (<em>C</em> is a hyperparameter). This modification makes the training more stable as it overcomes the short-term oscillations. To learn the network parameters, the algorithm applies <em>gradient descent</em><sup><a data-type="noteref" id="idm45174906929688-marker" href="ch09.xhtml#idm45174906929688">9</a></sup> to a loss function defined as the squared difference between the DQN’s estimate of the target and its estimate of the Q-value of the current state-action pair, <em>Q(s,a:θ)</em>. The loss function is as follows:</p>
</dd>
</dl>
<div data-type="equation">
<math display="block">
  <mrow>
    <mi>L</mi>
    <mrow>
      <mo>(</mo>
      <msub><mi>θ</mi> <mi>i</mi> </msub>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <mi>𝔼</mi>
    <mo stretchy="false">[</mo>
      <msup><mfenced separators="" open="(" close=")"><mi>r</mi> <mo>+</mo> <mi>γ</mi> <munder><mo movablelimits="true" form="prefix">max</mo> <msup><mi>a</mi> <mo>′</mo> </msup> </munder> <mi>Q</mi> <mrow><mo>(</mo><msup><mi>s</mi> <mo>′</mo> </msup><mo>,</mo><mi>a</mi><mo>′</mo><mo>;</mo><msub><mi>θ</mi> <mrow><mi>i</mi><mo>–</mo><mn>1</mn></mrow> </msub><mo>)</mo></mrow> <mo>–</mo> <mi>Q</mi> <mrow><mo>(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo>;</mo><msub><mi>θ</mi> <mi>i</mi> </msub><mo>)</mo></mrow></mfenced> <mn>2</mn> </msup>
    <mo stretchy="false">]</mo>
  </mrow>
</math>
</div>

<p>The loss function is essentially a mean squared error (MSE) function, where <math display="inline">
  <mfenced separators="" open="(" close=")"><mi>r</mi> <mo>+</mo> <mi>γ</mi> <msub><mo movablelimits="true" form="prefix">max</mo> <msup><mi>a</mi> <mo>′</mo> </msup> </msub> <mi>Q</mi> <mrow><mo>(</mo><msup><mi>s</mi> <mo>′</mo> </msup><mo>,</mo><mi>a</mi><mo>′</mo><mo>;</mo><msub><mi>θ</mi> <mrow><mi>i</mi><mo>–</mo><mn>1</mn></mrow> </msub><mo>)</mo></mrow></mfenced>
</math> represents the target value and <math alttext="upper Q left-bracket s comma a semicolon theta Subscript i Baseline right-bracket">
  <mrow>
    <mi>Q</mi>
    <mo>[</mo>
    <mi>s</mi>
    <mo>,</mo>
    <mi>a</mi>
    <mo>;</mo>
    <msub><mi>θ</mi> <mi>i</mi> </msub>
    <mo>]</mo>
  </mrow>
</math> represents the predicted value. <em>θ</em> are the weights of the network, which are computed when the loss function is minimized. Both the target and the current estimate depend on the set of weights, underlining the distinction from supervised learning, in which targets are fixed prior to training.</p>

<p>An example of the DQN for the trading example containing buy, sell, and hold actions is represented in <a data-type="xref" href="#DQN">Figure 9-6</a>. Here, we provide the network only the state (<em>s</em>) as input, and we receive Q-values for all possible actions (i.e., buy, sell, and hold) at once. We will be using DQN in the first and third case studies of this chapter.</p>

<figure><div id="DQN" class="figure">
<img src="Images/mlbf_0906.png" alt="mlbf 0906" width="762" height="412"/>
<h6><span class="label">Figure 9-6. </span>DQN</h6>
</div></figure>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Policy gradient"><div class="sect3" id="idm45174906883784">
<h3>Policy gradient</h3>

<p><a data-type="indexterm" data-primary="policy gradient" id="idm45174906882152"/><a data-type="indexterm" data-primary="reinforcement learning (RL)" data-secondary="policy gradient method" id="idm45174906881448"/><em>Policy gradient</em> is a policy-based method in which we learn a policy function, <em>π</em>, which is a direct map from each state to the best corresponding action at that state. It is a more straightforward approach than the value-based method, without the need for a Q-value function.</p>

<p>Policy gradient methods learn the policy directly with a parameterized function respect to <em>θ, π(a|s;θ)</em>. This function can be a complex function and might require a sophisticated model. In policy gradient methods, we use ANNs to map state to action because they are efficient at learning complex functions. The loss function of the ANN is the opposite of the expected return (cumulative future rewards).</p>

<p>The objective function of the policy gradient method can be defined as:</p>
<div data-type="equation">
<math alttext="upper J left-parenthesis theta right-parenthesis equals upper V Subscript pi Sub Subscript theta Baseline left-parenthesis upper S 1 right-parenthesis equals double-struck upper E Subscript pi Sub Subscript theta Baseline left-bracket upper V 1 right-bracket" display="block">
  <mrow>
    <mi>J</mi>
    <mrow>
      <mo>(</mo>
      <mi>θ</mi>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <msub><mi>V</mi> <msub><mi>π</mi> <mi>θ</mi> </msub> </msub>
    <mrow>
      <mo>(</mo>
      <msub><mi>S</mi> <mn>1</mn> </msub>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <msub><mi>𝔼</mi> <msub><mi>π</mi> <mi>θ</mi> </msub> </msub>
    <mrow>
      <mo>[</mo>
      <msub><mi>V</mi> <mn>1</mn> </msub>
      <mo>]</mo>
    </mrow>
  </mrow>
</math>
</div>

<p>where <em>θ</em> represents a set of weights of the ANN that maps states to actions. The idea here is to maximize the objective function and compute the weights (<em>θ</em>) of the ANN.</p>

<p><a data-type="indexterm" data-primary="gradient ascent" id="idm45174906747336"/>Since this is a maximization problem, we optimize the policy by taking the <em>gradient ascent</em> (as opposed to gradient descent, which is used to minimize the loss function), with the partial derivative of the objective with respect to the policy parameter <em>θ</em>:</p>
<div data-type="equation">
<math alttext="theta left-arrow theta plus StartFraction normal partial-differential Over normal partial-differential theta EndFraction upper J left-parenthesis theta right-parenthesis" display="block">
  <mrow>
    <mi>θ</mi>
    <mo>←</mo>
    <mi>θ</mi>
    <mo>+</mo>
    <mfrac><mi>∂</mi> <mrow><mi>∂</mi><mi>θ</mi></mrow></mfrac>
    <mi>J</mi>
    <mrow>
      <mo>(</mo>
      <mi>θ</mi>
      <mo>)</mo>
    </mrow>
  </mrow>
</math>
</div>

<p>Using gradient ascent, we can find the best <em>θ</em> that produces the highest return. Computing the gradient numerically can be done by perturbing <em>θ</em> by a small amount <em>ε</em> in the kth dimension or by using an analytical approach for deriving the gradient.</p>

<p>We will be using the policy gradient method for case study 2 later in this chapter.<a data-type="indexterm" data-startref="ix_Chapter9-asciidoc7" id="idm45174906735416"/></p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Key Challenges in Reinforcement Learning"><div class="sect2" id="idm45174906883128">
<h2>Key Challenges in Reinforcement Learning</h2>

<p><a data-type="indexterm" data-primary="reinforcement learning (RL)" data-secondary="shortcomings" id="idm45174906733576"/>So far, we have covered only what reinforcement learning algorithms can do. However, several shortcomings are outlined below:</p>
<dl>
<dt>Resource efficiency</dt>
<dd>
<p>Current deep reinforcement learning algorithms require vast amounts of time, training data, and computational resources in order to reach a desirable level of proficiency. Thus, making reinforcement learning algorithms trainable under limited resources will continue to be an important issue.</p>
</dd>
<dt>Credit assignment</dt>
<dd>
<p>In RL, reward signals can occur significantly later than actions that contributed to the result, complicating the association of actions with their consequences.</p>
</dd>
<dt>Interpretability</dt>
<dd>
<p>In RL, it is relatively difficult for a model to provide any meaningful, intuitive relationships between input and their corresponding output that can be easily understood. Most advanced reinforcement learning algorithms incorporate deep neural networks, which make interpretability even more difficult due to a large number of layers and nodes inside the neural network.</p>
</dd>
</dl>

<p>Let us look at the case studies now.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Case Study 1: Reinforcement Learning–Based Trading Strategy"><div class="sect1" id="CaseStudy1RL">
<h1>Case Study 1: Reinforcement Learning–Based Trading Strategy</h1>

<p><a data-type="indexterm" data-primary="trading strategy, reinforcement-learning based" id="ix_Chapter9-asciidoc8"/>Algorithmic trading primarily has three components: <em>policy development</em>, <em>parameter optimization</em>, and <em>backtesting</em>. <a data-type="indexterm" data-primary="policy development" id="idm45174906721848"/>The policy determines what actions to take based on the current state of the market. <a data-type="indexterm" data-primary="parameter optimization" id="idm45174906720984"/>Parameter optimization is performed using a search over possible values of strategy parameters, such as thresholds or coefficients. <a data-type="indexterm" data-primary="backtesting" data-secondary="defined" id="idm45174906720040"/>Finally, backtesting assesses the viability of a trading strategy by exploring how it would have played out using historical data.</p>

<p>RL is based around coming up with a policy to maximize the reward in a given environment. Instead of needing to hand code a rule-based trading policy, RL learns one directly. There is no need to explicitly specify rules and thresholds. Their ability to decide policy on their own makes RL models very suitable machine learning algorithms to create automated algorithmic trading models, or <em>trading bots</em>.</p>

<p>In terms of <em>parameter optimization</em> and <em>backtesting</em> steps, RL allows for end-to-end optimization and maximizes (potentially delayed) rewards. Reinforcement learning agents are trained in a simulation, which can be as complex as desired. Taking into account latencies, liquidity, and fees, we can seamlessly combine the backtesting and parameter optimization steps without needing to go through separate stages.</p>

<p>Additionally, RL algorithms learn powerful policies parameterized by artificial neural networks. RL algorithms can also learn to adapt to various market conditions by experiencing them in historical data, given that they are trained over a long-time horizon and have sufficient memory. This allows them to be much more robust to changing markets than supervised learning–based trading strategies, which, due to the simplistic nature of the policy, may not have a parameterization powerful enough to learn to adapt to changing market conditions.</p>

<p>Reinforcement learning, with its capability to easily handle policy, parameter optimization, and backtesting, is ideal for the next wave of algorithmic trading. Anecdotally, it seems that several of the more sophisticated algorithmic execution desks at large investment banks and hedge funds are beginning to use reinforcement learning to optimize their decision making.</p>

<p>In this case study, we will create an end-to-end trading strategy based on reinforcement learning. We will use the Q-learning approach with deep Q-network (DQN) to come up with a policy and an implementation of the trading strategy. As discussed before, the name “Q-learning” is in reference to the <math alttext="upper Q left-parenthesis s comma a right-parenthesis">
  <mrow>
    <mi>Q</mi>
    <mo>(</mo>
    <mi>s</mi>
    <mo>,</mo>
    <mi>a</mi>
    <mo>)</mo>
  </mrow>
</math> function, which returns the expected reward based on the state <em>s</em> and provided action <em>a</em>. In addition to 
<span class="keep-together">developing</span> a specific trading strategy, this case study will discuss the general framework and components of a reinforcement learning–based trading strategy.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45174906707400">
<h5/>
<p>In this case study, we will focus on:</p>

<ul>
<li>
<p>Understanding the key components of an RL framework from a trading strategy standpoint.</p>
</li>
<li>
<p>Evaluating the Q-learning method of RL in Python by defining an agent, followed by training and testing setup.</p>
</li>
<li>
<p>Implementing a deep neural network to be used for RL problems in Python using the Keras package.</p>
</li>
<li>
<p>Understanding the class structure of Python programming while implementing an RL-based model.</p>
</li>
<li>
<p>Understanding the intuition and interpretation of RL-based algorithms.</p>
</li>
</ul>
</div></aside>
<img class="blueprint top" src="Images/bracket_top.png" width="1625" height="32"/>








<section data-type="sect2" class="blueprint" data-pdf-bookmark="Blueprint for Creating a Reinforcement Learning–Based Trading Strategy"><div class="sect2" id="idm45174906699752">
<h2>Blueprint for Creating a Reinforcement Learning–Based Trading Strategy</h2>










<section data-type="sect3" data-pdf-bookmark="1. Problem definition"><div class="sect3" id="idm45174906698136">
<h3>1. Problem definition</h3>

<p>In the reinforcement learning framework for this case study, the algorithm takes an action (buy, sell, or hold) depending on the current state of the stock price. The algorithm is trained using a deep Q-learning model to perform the best action. The key components of the reinforcement learning framework for this case study are:</p>
<dl>
<dt>Agent</dt>
<dd>
<p>Trading agent.</p>
</dd>
<dt>Action</dt>
<dd>
<p>Buy, sell, or hold.</p>
</dd>
<dt>Reward function</dt>
<dd>
<p>Realized profit and loss (PnL) is used as the reward function for this case study. The reward depends on the action: sell (realized profit and loss), buy (no reward), or hold (no reward).</p>
</dd>
<dt>State</dt>
<dd>
<p><a data-type="indexterm" data-primary="sigmoid function" id="idm45174906690312"/><a data-type="indexterm" data-primary="state (definition)" id="idm45174906689384"/>A sigmoid function<sup><a data-type="noteref" id="idm45174906688584-marker" href="ch09.xhtml#idm45174906688584">10</a></sup> of the differences of past stock prices for a given time window is used as the state. State <em>S<sub>t</sub></em> is described as <math alttext="left-parenthesis d Subscript t minus tau plus 1 Baseline comma d Subscript t minus 1 Baseline comma d Subscript t Baseline right-parenthesis">
  <mrow>
    <mo>(</mo>
    <msub><mi>d</mi> <mrow><mi>t</mi><mo>-</mo><mi>τ</mi><mo>+</mo><mn>1</mn></mrow> </msub>
    <mo>,</mo>
    <msub><mi>d</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow> </msub>
    <mo>,</mo>
    <msub><mi>d</mi> <mi>t</mi> </msub>
    <mo>)</mo>
  </mrow>
</math>, where
<math display="inline">
  <mrow>
    <msub><mi>d</mi> <mi>T</mi> </msub>
    <mo>=</mo>
    <mi>s</mi>
    <mi>i</mi>
    <mi>g</mi>
    <mi>m</mi>
    <mi>o</mi>
    <mi>i</mi>
    <mi>d</mi>
    <mrow>
      <mo>(</mo>
      <msub><mi>p</mi> <mi>t</mi> </msub>
      <mo>–</mo>
      <msub><mi>p</mi> <mrow><mi>t</mi><mo>–</mo><mn>1</mn></mrow> </msub>
      <mo>)</mo>
    </mrow>
  </mrow>
</math>, <math alttext="p Subscript t">
  <msub><mi>p</mi> <mi>t</mi> </msub>
</math> is price at time <em>t</em>, and
<math alttext="tau">
  <mi>τ</mi>
</math> is the time window size. A sigmoid function converts the differences of the past stock prices into a number between zero and one, which helps to normalize the values to probabilities and makes the state simpler to interpret.</p>
</dd>
<dt>Environment</dt>
<dd>
<p>Stock exchange or the stock market.</p>
</dd>
</dl>
<div data-type="tip"><h1>Selecting the RL Components for a Trading Strategy</h1>
<p>Formulating an intelligent behavior for a reinforcement learning–based trading strategy begins with identification of the correct components of the RL model. Hence, before we go into the model development, we should carefully identify the following RL components:</p>
<dl>
<dt>Reward function</dt>
<dd>
<p><a data-type="indexterm" data-primary="reward function" id="idm45174906658152"/>This is an important parameter, as it decides whether the RL algorithm will learn to optimize the appropriate metric. In addition to the return or PnL, the reward function can incorporate risk embedded in the underlying instrument or include other parameters such as volatility or maximum drawdown. It can also include the transaction costs of the buy/sell actions.</p>
</dd>
<dt>State</dt>
<dd>
<p>State determines the observations that the agent receives from the environment for taking a decision. The state should be representative of current market behavior as compared to the past and can also include values of any signals that are believed to be predictive or items related to market microstructure, such as volume traded.</p>
</dd>
</dl>
</div>

<p>The data that we will use will be the S&amp;P 500 closing prices. The data is extracted from Yahoo Finance and contains ten years of daily data from 2010 to 2019.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="2. Getting started—loading the data and Python packages"><div class="sect3" id="idm45174906654200">
<h3>2. Getting started—loading the data and Python packages</h3>












<section data-type="sect4" data-pdf-bookmark="2.1. Loading the Python packages"><div class="sect4" id="idm45174906653192">
<h4>2.1. Loading the Python packages</h4>

<p><a data-type="indexterm" data-primary="trading strategy, reinforcement-learning based" data-secondary="loading data and Python packages" id="idm45174906651768"/>The list of libraries used for all of the steps of model implementation, from <em>data loading</em> to <em>model evaluation</em>, including deep learning–based model development, are included here. The details of most of these packages and functions have been provided in Chapters <a href="ch02.xhtml#Chapter2">2</a>, <a href="ch03.xhtml#Chapter3">3</a>, and <a href="ch04.xhtml#Chapter4">4</a>. The packages used for different purposes have been separated in the Python code here, and their usage will be demonstrated in different steps of the model development process.</p>

<p><code>Packages for reinforcement learning</code></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">import</code> <code class="nn">keras</code>
<code class="kn">from</code> <code class="nn">keras</code> <code class="k">import</code> <code class="n">layers</code><code class="p">,</code> <code class="n">models</code><code class="p">,</code> <code class="n">optimizers</code> <code class="kn">from</code> <code class="nn">keras</code> <code class="k">import</code> <code class="n">backend</code> <code class="k">as</code> <code class="n">K</code>
<code class="kn">from</code> <code class="nn">collections</code> <code class="k">import</code> <code class="n">namedtuple</code><code class="p">,</code> <code class="n">deque</code>
<code class="kn">from</code> <code class="nn">keras.models</code> <code class="k">import</code> <code class="n">Sequential</code>
<code class="kn">from</code> <code class="nn">keras.models</code> <code class="k">import</code> <code class="n">load_model</code>
<code class="kn">from</code> <code class="nn">keras.layers</code> <code class="k">import</code> <code class="n">Dense</code>
<code class="kn">from</code> <code class="nn">keras.optimizers</code> <code class="k">import</code> <code class="n">Adam</code></pre>

<p class="pagebreak-before"><code>Packages/modules for data processing and visualization</code></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">import</code> <code class="nn">pandas</code> <code class="k">as</code> <code class="nn">pd</code>
<code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="k">as</code> <code class="nn">plt</code>
<code class="kn">from</code> <code class="nn">pandas</code> <code class="k">import</code> <code class="n">read_csv</code><code class="p">,</code> <code class="n">set_option</code>
<code class="kn">import</code> <code class="nn">datetime</code>
<code class="kn">import</code> <code class="nn">math</code>
<code class="kn">from</code> <code class="nn">numpy.random</code> <code class="k">import</code> <code class="n">choice</code>
<code class="kn">import</code> <code class="nn">random</code>
<code class="kn">from</code> <code class="nn">collections</code> <code class="k">import</code> <code class="n">deque</code></pre>
</div></section>













<section data-type="sect4" data-pdf-bookmark="2.2. Loading the data"><div class="sect4" id="idm45174906600136">
<h4>2.2. Loading the data</h4>

<p>The fetched data for the time period of 2010 to 2019 is loaded:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">dataset</code> <code class="o">=</code> <code class="n">read_csv</code><code class="p">(</code><code class="s">'data/SP500.csv'</code><code class="p">,</code> <code class="n">index_col</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code></pre>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="3. Exploratory data analysis"><div class="sect3" id="idm45174906520008">
<h3>3. Exploratory data analysis</h3>

<p><a data-type="indexterm" data-primary="trading strategy, reinforcement-learning based" data-secondary="exploratory data analysis" id="idm45174906518568"/>We will look at descriptive statistics and data visualization in this section. Let us have a look at the dataset we have:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c"># shape</code>
<code class="n">dataset</code><code class="o">.</code><code class="n">shape</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">(2515, 6)</pre>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c"># peek at data</code>
<code class="n">set_option</code><code class="p">(</code><code class="s">'display.width'</code><code class="p">,</code> <code class="mi">100</code><code class="p">)</code>
<code class="n">dataset</code><code class="o">.</code><code class="n">head</code><code class="p">(</code><code class="mi">5</code><code class="p">)</code></pre>

<p><code>Output</code></p>

<figure><div class="figure">
<img src="Images/mlbf_09in02.png" alt="mlbf 09in02" width="500" height="182"/>
<h6/>
</div></figure>

<p>The data has a total of 2,515 rows and six columns, which contain the categories <em>open</em>, <em>high</em>, <em>low</em>, <em>close</em>, <em>adjusted close price</em>, and <em>total volume</em>. The adjusted close price is the closing price adjusted for the split and dividends. For the purpose of this case study, we will be focusing on the closing price.</p>

<figure><div class="figure">
<img src="Images/mlbf_09in03.png" alt="mlbf 09in03" width="393" height="263"/>
<h6/>
</div></figure>

<p>The chart shows that S&amp;P 500 has been in an upward-trending series between 2010 and 2019. Let us perform the data preparation.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="4. Data preparation"><div class="sect3" id="idm45174906519416">
<h3>4. Data preparation</h3>

<p><a data-type="indexterm" data-primary="trading strategy, reinforcement-learning based" data-secondary="data preparation" id="idm45174906420984"/>This step is important in order to create a meaningful, reliable, and clean dataset that can be used without any errors in the reinforcement learning algorithm.</p>












<section data-type="sect4" data-pdf-bookmark="4.1. Data cleaning"><div class="sect4" id="idm45174906419480">
<h4>4.1. Data cleaning</h4>

<p>In this step, we check for NAs in the rows and either drop them or fill them with the mean of the column:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c">#Checking for any null values and removing the null values'''</code>
<code class="nb">print</code><code class="p">(</code><code class="s">'Null Values ='</code><code class="p">,</code> <code class="n">dataset</code><code class="o">.</code><code class="n">isnull</code><code class="p">()</code><code class="o">.</code><code class="n">values</code><code class="o">.</code><code class="n">any</code><code class="p">())</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">Null Values = False</pre>

<p>As there are no null values in the data, there is no need to perform any further data cleaning.</p>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="5. Evaluate algorithms and models"><div class="sect3" id="idm45174906408680">
<h3>5. Evaluate algorithms and models</h3>

<p><a data-type="indexterm" data-primary="trading strategy, reinforcement-learning based" data-secondary="evaluation of algorithms and models" id="ix_Chapter9-asciidoc9"/>This is the key step of the reinforcement learning model development, where we will define all the relevant functions and classes and train the algorithm. In the first step, we prepare the data for the training set and the test set.</p>












<section data-type="sect4" data-pdf-bookmark="5.1. Train-test split"><div class="sect4" id="idm45174906405864">
<h4>5.1. Train-test split</h4>

<p>In this step, we partition the original dataset into training set and test set. We use the test set to confirm the performance of our final model and to understand if there is any overfitting. We will use 80% of the dataset for modeling and 20% for testing:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">X</code><code class="o">=</code><code class="nb">list</code><code class="p">(</code><code class="n">dataset</code><code class="p">[</code><code class="s">"Close"</code><code class="p">])</code>
<code class="n">X</code><code class="o">=</code><code class="p">[</code><code class="nb">float</code><code class="p">(</code><code class="n">x</code><code class="p">)</code> <code class="k">for</code> <code class="n">x</code> <code class="ow">in</code> <code class="n">X</code><code class="p">]</code>
<code class="n">validation_size</code> <code class="o">=</code> <code class="mf">0.2</code>
<code class="n">train_size</code> <code class="o">=</code> <code class="nb">int</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">X</code><code class="p">)</code> <code class="o">*</code> <code class="p">(</code><code class="mi">1</code><code class="o">-</code><code class="n">validation_size</code><code class="p">))</code>
<code class="n">X_train</code><code class="p">,</code> <code class="n">X_test</code> <code class="o">=</code> <code class="n">X</code><code class="p">[</code><code class="mi">0</code><code class="p">:</code><code class="n">train_size</code><code class="p">],</code> <code class="n">X</code><code class="p">[</code><code class="n">train_size</code><code class="p">:</code><code class="nb">len</code><code class="p">(</code><code class="n">X</code><code class="p">)]</code></pre>
</div></section>













<section data-type="sect4" data-pdf-bookmark="5.2. Implementation steps and modules"><div class="sect4" id="idm45174906378680">
<h4>5.2. Implementation steps and modules</h4>

<p><a data-type="indexterm" data-primary="trading strategy, reinforcement-learning based" data-secondary="implementation steps and modules" id="idm45174906297256"/>The overall algorithm of this case study (and of reinforcement learning in general) is a bit complex as it requires building <em>class-based code structure</em> and the simultaneous use of many modules and functions. This additional section was added for this case study to provide a functional explanation of what is happening in the program.</p>

<p>The algorithm, in simple terms, decides whether to buy, sell, or hold when provided with the current market price.</p>

<p><a data-type="xref" href="#RLTrading">Figure 9-7</a> provides an overview of the training of the Q-learning-based algorithm in the context of this case study. The algorithm evaluates which action to take based on a Q-value, which determines the value of being in a certain state and taking a certain action at that state.</p>

<p>As per <a data-type="xref" href="#RLTrading">Figure 9-7</a>, the state (<em>s</em>) is decided on the basis of the current and historical behavior of the price (P<sub>t</sub>, P<sub>t–1</sub>,…). Based on the current state, the action is “buy.” With this action, we observe a reward of <em>$10</em> (i.e., the PnL associated with the action) and move into the next state. Using the current reward and the next state’s Q-value, the algorithm updates the Q-value function. The algorithm keeps on moving through the next time steps. Given sufficient iterations of the steps above, this algorithm will converge to the optimal Q-value.</p>

<figure><div id="RLTrading" class="figure">
<img src="Images/mlbf_0907.png" alt="mlbf 0907" width="1060" height="542"/>
<h6><span class="label">Figure 9-7. </span>Reinforcement learning for trading</h6>
</div></figure>

<p>The deep Q-network that we use in this case study uses an ANN to approximate Q-values; hence, the action value function is defined as <em>Q(s,a;θ)</em>. The deep Q-learning algorithm approximates the Q-value function by learning a set of weights, <em>θ</em>, of a multilayered DQN that maps states to actions.</p>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="Modules and functions"><div class="sect3" id="idm45174906286376">
<h3>Modules and functions</h3>

<p>Implementing this DQN algorithm requires implementation of several functions and modules that interact with each other during the model training. Here is a summary of the modules and functions:</p>
<dl>
<dt>Agent class</dt>
<dd>
<p>The agent is defined as “Agent” class. This holds the variables and member functions that perform the Q-learning. An object of the <code>Agent</code> class is created using the training phase and is used for training the model.</p>
</dd>
<dt>Helper functions</dt>
<dd>
<p>In this module, we create additional functions that are helpful for training.</p>
</dd>
<dt>Training module</dt>
<dd>
<p>In this step, we perform the training of the data using the variables and the functions defined in the agent and helper methods. During training, the prescribed action for each day is predicted, the rewards are computed, and the deep learning–based Q-learning model weights are updated iteratively over a number of episodes. Additionally, the profit and loss of each action is summed to determine whether an overall profit has occurred. The aim is to maximize the total profit.</p>
</dd>
</dl>

<p>We provide a deep dive into the interaction between the different modules and functions in <a data-type="xref" href="#training_model_cs">“5.5. Training the model”</a>.</p>

<p>Let us look at each of these in detail.</p>












<section data-type="sect4" data-pdf-bookmark="5.3. Agent class"><div class="sect4" id="idm45174906277160">
<h4>5.3. Agent class</h4>

<p><a data-type="indexterm" data-primary="agent class" id="ix_Chapter9-asciidoc10"/><a data-type="indexterm" data-primary="trading strategy, reinforcement-learning based" data-secondary="agent class" id="ix_Chapter9-asciidoc11"/>The <code>agent</code> class consists of the following components:</p>

<ul>
<li>
<p><code>Constructor</code></p>
</li>
<li>
<p>Function <code>model</code></p>
</li>
<li>
<p>Function <code>act</code></p>
</li>
<li>
<p>Function <code>expReplay</code></p>
</li>
</ul>

<p>The <code>Constructor</code> is defined as <code>init</code> function and contains important parameters such as <code>discount factor</code> for reward function, <code>epsilon</code> for the <em>ε-greedy</em> approach, <code>state size</code>, and <code>action size</code>. The number of actions is set at three (i.e., buy, sell, and hold). The <code>memory</code> variable defines the <code>replay memory</code> size. The input parameter of this function also consists of <code>is_eval</code> parameter, which defines whether training is ongoing. This variable is changed to <code>True</code> during the evaluation/testing phase. Also, if the pretrained model has to be used in the evaluation/training phase, it is passed using the <code>model_name</code> variable:</p>

<pre data-type="programlisting" data-code-language="ipython3" class="pagebreak-before"><code class="k">class</code> <code class="nc">Agent</code><code class="p">:</code>
    <code class="k">def</code> <code class="nf">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">state_size</code><code class="p">,</code> <code class="n">is_eval</code><code class="o">=</code><code class="k">False</code><code class="p">,</code> <code class="n">model_name</code><code class="o">=</code><code class="s">""</code><code class="p">):</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">state_size</code> <code class="o">=</code> <code class="n">state_size</code> <code class="c"># normalized previous days</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">action_size</code> <code class="o">=</code> <code class="mi">3</code> <code class="c"># hold, buy, sell</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">memory</code> <code class="o">=</code> <code class="n">deque</code><code class="p">(</code><code class="n">maxlen</code><code class="o">=</code><code class="mi">1000</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">inventory</code> <code class="o">=</code> <code class="p">[]</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">model_name</code> <code class="o">=</code> <code class="n">model_name</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">is_eval</code> <code class="o">=</code> <code class="n">is_eval</code>

        <code class="bp">self</code><code class="o">.</code><code class="n">gamma</code> <code class="o">=</code> <code class="mf">0.95</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">epsilon</code> <code class="o">=</code> <code class="mf">1.0</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">epsilon_min</code> <code class="o">=</code> <code class="mf">0.01</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">epsilon_decay</code> <code class="o">=</code> <code class="mf">0.995</code>

        <code class="bp">self</code><code class="o">.</code><code class="n">model</code> <code class="o">=</code> <code class="n">load_model</code><code class="p">(</code><code class="s">"models/"</code> <code class="o">+</code> <code class="n">model_name</code><code class="p">)</code> <code class="k">if</code> <code class="n">is_eval</code> \
         <code class="k">else</code> <code class="bp">self</code><code class="o">.</code><code class="n">_model</code><code class="p">()</code></pre>

<p>The function <code>model</code> is a deep learning model that maps the states to actions. This function takes in the state of the environment and returns a <em>Q-value</em> table or a policy that refers to a probability distribution over actions. This function is built using the Keras Python library.<sup><a data-type="noteref" id="idm45174906222520-marker" href="ch09.xhtml#idm45174906222520">11</a></sup> The architecture for the deep learning model used is:</p>

<ul>
<li>
<p>The model expects rows of data with number of variables equal to the <em>state size</em>, which comes as an input.</p>
</li>
<li>
<p>The first, second, and third hidden layers have <em>64</em>, <em>32</em>, and <em>8</em> nodes, respectively, and all of these layers use the ReLU activation function.</p>
</li>
<li>
<p>The output layer has the number of nodes equal to the action size (i.e., three), and the node uses a linear activation function:<sup><a data-type="noteref" id="idm45174906129432-marker" href="ch09.xhtml#idm45174906129432">12</a></sup></p>
</li>
</ul>

<pre data-type="programlisting" data-code-language="ipython3">    <code class="k">def</code> <code class="nf">_model</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
        <code class="n">model</code> <code class="o">=</code> <code class="n">Sequential</code><code class="p">()</code>
        <code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dense</code><code class="p">(</code><code class="n">units</code><code class="o">=</code><code class="mi">64</code><code class="p">,</code> <code class="n">input_dim</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">state_size</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s">"relu"</code><code class="p">))</code>
        <code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dense</code><code class="p">(</code><code class="n">units</code><code class="o">=</code><code class="mi">32</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s">"relu"</code><code class="p">))</code>
        <code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dense</code><code class="p">(</code><code class="n">units</code><code class="o">=</code><code class="mi">8</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s">"relu"</code><code class="p">))</code>
        <code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dense</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">action_size</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s">"linear"</code><code class="p">))</code>
        <code class="n">model</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">loss</code><code class="o">=</code><code class="s">"mse"</code><code class="p">,</code> <code class="n">optimizer</code><code class="o">=</code><code class="n">Adam</code><code class="p">(</code><code class="n">lr</code><code class="o">=</code><code class="mf">0.001</code><code class="p">))</code>

        <code class="k">return</code> <code class="n">model</code></pre>

<p>The function <code>act</code> returns an action given a state. The function uses the <code>model</code> function and returns a buy, sell, or hold action:</p>

<pre data-type="programlisting" data-code-language="ipython3">    <code class="k">def</code> <code class="nf">act</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">state</code><code class="p">):</code>
        <code class="k">if</code> <code class="ow">not</code> <code class="bp">self</code><code class="o">.</code><code class="n">is_eval</code> <code class="ow">and</code> <code class="n">random</code><code class="o">.</code><code class="n">random</code><code class="p">()</code> <code class="o">&lt;=</code> <code class="bp">self</code><code class="o">.</code><code class="n">epsilon</code><code class="p">:</code>
            <code class="k">return</code> <code class="n">random</code><code class="o">.</code><code class="n">randrange</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">action_size</code><code class="p">)</code>

        <code class="n">options</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">state</code><code class="p">)</code>
        <code class="k">return</code> <code class="n">np</code><code class="o">.</code><code class="n">argmax</code><code class="p">(</code><code class="n">options</code><code class="p">[</code><code class="mi">0</code><code class="p">])</code></pre>

<p>The function <code>expReplay</code> is the key function, where the neural network is trained based on the observed experience. <a data-type="indexterm" data-primary="experience replay" id="idm45174905853512"/>This function implements the <em>Experience replay</em> mechanism as previously discussed. Experience replay stores a history of state, action, reward, and next state transitions that are experienced by the agent. It takes a minibatch of the observations (<em>replay memory</em>) as an input and updates the deep learning–based Q-learning model weights by minimizing the loss function. <a data-type="indexterm" data-primary="epsilon greedy algorithm" id="idm45174905851624"/>The <em>epsilon greedy</em> approach implemented in this function prevents overfitting. In order to explain the function, different steps are numbered in the comments of the following Python code, along with an outline of the steps:</p>
<ol>
<li>
<p>Prepare the replay buffer memory, which is the set of observation used for training. New experiences are added to the replay buffer memory using a for loop.</p>
</li>
<li>
<p><em>Loop</em> across all the observations of state, action, reward, and next state transitions in the mini-batch.</p>
</li>
<li>
<p>The target variable for the Q-table is updated based on the Bellman equation. The update happens if the current state is the terminal state or the end of the episode. This is represented by the variable <code>done</code> and is defined further in the training function. If it is not <code>done</code>, the target is just set to reward.</p>
</li>
<li>
<p>Predict the Q-value of the next state using a deep learning model.</p>
</li>
<li>
<p>The Q-value of this state for the action in the current replay buffer is set to the target.</p>
</li>
<li>
<p>The deep learning model weights are updated by using the <code>model.fit</code> function.</p>
</li>
<li>
<p>The epsilon greedy approach is implemented. Recall that this approach selects an action randomly with a probability of <em>ε</em> or the best action, according to the Q-value function, with probability 1–<em>ε</em>.<a data-type="indexterm" data-startref="ix_Chapter9-asciidoc11" id="idm45174905952984"/><a data-type="indexterm" data-startref="ix_Chapter9-asciidoc10" id="idm45174905952248"/></p>
</li>

</ol>

<pre data-type="programlisting" data-code-language="ipython3">    <code class="k">def</code> <code class="nf">expReplay</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">batch_size</code><code class="p">):</code>
        <code class="n">mini_batch</code> <code class="o">=</code> <code class="p">[]</code>
        <code class="n">l</code> <code class="o">=</code> <code class="nb">len</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">memory</code><code class="p">)</code>
        <code class="c">#1: prepare replay memory</code>
        <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">l</code> <code class="o">-</code> <code class="n">batch_size</code> <code class="o">+</code> <code class="mi">1</code><code class="p">,</code> <code class="n">l</code><code class="p">):</code>
            <code class="n">mini_batch</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">memory</code><code class="p">[</code><code class="n">i</code><code class="p">])</code>

        <code class="c">#2: Loop across the replay memory batch.</code>
        <code class="k">for</code> <code class="n">state</code><code class="p">,</code> <code class="n">action</code><code class="p">,</code> <code class="n">reward</code><code class="p">,</code> <code class="n">next_state</code><code class="p">,</code> <code class="n">done</code> <code class="ow">in</code> <code class="n">mini_batch</code><code class="p">:</code>
            <code class="n">target</code> <code class="o">=</code> <code class="n">reward</code> <code class="c"># reward or Q at time t</code>
            <code class="c">#3: update the target for Q table. table equation</code>
            <code class="k">if</code> <code class="ow">not</code> <code class="n">done</code><code class="p">:</code>
                <code class="n">target</code> <code class="o">=</code> <code class="n">reward</code> <code class="o">+</code> <code class="bp">self</code><code class="o">.</code><code class="n">gamma</code> <code class="o">*</code> \
                 <code class="n">np</code><code class="o">.</code><code class="n">amax</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">next_state</code><code class="p">)[</code><code class="mi">0</code><code class="p">])</code>
            <code class="c">#set_trace()</code>

            <code class="c"># 4: Q-value of the state currently from the table</code>
            <code class="n">target_f</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">state</code><code class="p">)</code>
            <code class="c"># 5: Update the output Q table for the given action in the table</code>
            <code class="n">target_f</code><code class="p">[</code><code class="mi">0</code><code class="p">][</code><code class="n">action</code><code class="p">]</code> <code class="o">=</code> <code class="n">target</code>
            <code class="c"># 6. train and fit the model.</code>
            <code class="bp">self</code><code class="o">.</code><code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">state</code><code class="p">,</code> <code class="n">target_f</code><code class="p">,</code> <code class="n">epochs</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">verbose</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>

        <code class="c">#7. Implement epsilon greedy algorithm</code>
        <code class="k">if</code> <code class="bp">self</code><code class="o">.</code><code class="n">epsilon</code> <code class="o">&gt;</code> <code class="bp">self</code><code class="o">.</code><code class="n">epsilon_min</code><code class="p">:</code>
            <code class="bp">self</code><code class="o">.</code><code class="n">epsilon</code> <code class="o">*=</code> <code class="bp">self</code><code class="o">.</code><code class="n">epsilon_decay</code></pre>
</div></section>













<section data-type="sect4" data-pdf-bookmark="5.4. Helper functions"><div class="sect4" id="idm45174906276536">
<h4>5.4. Helper functions</h4>

<p><a data-type="indexterm" data-primary="helper functions" id="idm45174905947800"/><a data-type="indexterm" data-primary="trading strategy, reinforcement-learning based" data-secondary="helper functions" id="idm45174905947096"/>In this module, we create additional functions that are helpful for training. Some of the important helper functions are discussed here. For details about other helper functions, refer to the Jupyter notebook in the GitHub repository for this book.</p>

<p>The function <code>getState</code> generates the states given the stock data, time <em>t</em> (the day of prediction), and window <em>n</em> (number of days to go back in time). First, the vector of price difference is computed, followed by scaling this vector from zero to one with a <code>sigmoid</code> function. This is returned as the state.</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="k">def</code> <code class="nf">getState</code><code class="p">(</code><code class="n">data</code><code class="p">,</code> <code class="n">t</code><code class="p">,</code> <code class="n">n</code><code class="p">):</code>
    <code class="n">d</code> <code class="o">=</code> <code class="n">t</code> <code class="o">-</code> <code class="n">n</code> <code class="o">+</code> <code class="mi">1</code>
    <code class="n">block</code> <code class="o">=</code> <code class="n">data</code><code class="p">[</code><code class="n">d</code><code class="p">:</code><code class="n">t</code> <code class="o">+</code> <code class="mi">1</code><code class="p">]</code> <code class="k">if</code> <code class="n">d</code> <code class="o">&gt;=</code> <code class="mi">0</code> <code class="k">else</code> <code class="o">-</code><code class="n">d</code> <code class="o">*</code> <code class="p">[</code><code class="n">data</code><code class="p">[</code><code class="mi">0</code><code class="p">]]</code> <code class="o">+</code> <code class="n">data</code><code class="p">[</code><code class="mi">0</code><code class="p">:</code><code class="n">t</code> <code class="o">+</code> <code class="mi">1</code><code class="p">]</code>
    <code class="n">res</code> <code class="o">=</code> <code class="p">[]</code>
    <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">n</code> <code class="o">-</code> <code class="mi">1</code><code class="p">):</code>
        <code class="n">res</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">sigmoid</code><code class="p">(</code><code class="n">block</code><code class="p">[</code><code class="n">i</code> <code class="o">+</code> <code class="mi">1</code><code class="p">]</code> <code class="o">-</code> <code class="n">block</code><code class="p">[</code><code class="n">i</code><code class="p">]))</code>
    <code class="k">return</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="n">res</code><code class="p">])</code></pre>

<p>The function <code>plot_behavior</code> returns the plot of the market price along with indicators for the buy and sell actions. It is used for the overall evaluation of the algorithm during the training and testing phase.</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="k">def</code> <code class="nf">plot_behavior</code><code class="p">(</code><code class="n">data_input</code><code class="p">,</code> <code class="n">states_buy</code><code class="p">,</code> <code class="n">states_sell</code><code class="p">,</code> <code class="n">profit</code><code class="p">):</code>
    <code class="n">fig</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">figure</code><code class="p">(</code><code class="n">figsize</code> <code class="o">=</code> <code class="p">(</code><code class="mi">15</code><code class="p">,</code> <code class="mi">5</code><code class="p">))</code>
    <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">data_input</code><code class="p">,</code> <code class="n">color</code><code class="o">=</code><code class="s">'r'</code><code class="p">,</code> <code class="n">lw</code><code class="o">=</code><code class="mf">2.</code><code class="p">)</code>
    <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">data_input</code><code class="p">,</code> <code class="s">'^'</code><code class="p">,</code> <code class="n">markersize</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">color</code><code class="o">=</code><code class="s">'m'</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s">'Buying signal'</code><code class="p">,</code>\
     <code class="n">markevery</code><code class="o">=</code><code class="n">states_buy</code><code class="p">)</code>
    <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">data_input</code><code class="p">,</code> <code class="s">'v'</code><code class="p">,</code> <code class="n">markersize</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">color</code><code class="o">=</code><code class="s">'k'</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s">'Selling signal'</code><code class="p">,</code>\
     <code class="n">markevery</code> <code class="o">=</code> <code class="n">states_sell</code><code class="p">)</code>
    <code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="s">'Total gains: %f'</code><code class="o">%</code><code class="p">(</code><code class="n">profit</code><code class="p">))</code>
    <code class="n">plt</code><code class="o">.</code><code class="n">legend</code><code class="p">()</code>
    <code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>
</div></section>













<section data-type="sect4" data-pdf-bookmark="5.5. Training the model"><div class="sect4" id="training_model_cs">
<h4>5.5. Training the model</h4>

<p><a data-type="indexterm" data-primary="trading strategy, reinforcement-learning based" data-secondary="training the model" id="ix_Chapter9-asciidoc12"/>We will proceed to train the data. Based on our agent, we define the following variables and instantiate the stock agent:</p>
<dl>
<dt>Episode</dt>
<dd>
<p>The number of times the code is trained through the entire data. In this case study, we use 10 episodes.</p>
</dd>
<dt>Windows size</dt>
<dd>
<p>Number of market days to consider to evaluate the state.</p>
</dd>
<dt>Batch size</dt>
<dd>
<p>Size of the replay buffer or memory use during training.</p>
</dd>
</dl>

<p>Once these variables are defined, we train the model iterating through the episodes. <a data-type="xref" href="#TraingStepsQTrd">Figure 9-8</a> provides a deep dive into the training steps and brings together all the elements discussed so far. The upper section showing steps 1 to 7 describes the steps in the <em>training</em> module, and the lower section describes the steps in the <code>replay buffer</code> function (i.e., <code>exeReplay</code> function).</p>

<figure><div id="TraingStepsQTrd" class="figure">
<img src="Images/mlbf_0908.png" alt="mlbf 0908" width="1389" height="677"/>
<h6><span class="label">Figure 9-8. </span>Training steps of Q-trading</h6>
</div></figure>

<p>Steps 1 to 6 shown in <a data-type="xref" href="#TraingStepsQTrd">Figure 9-8</a> are numbered in the following Python code and are described as follows:</p>
<ol>
<li>
<p>Get the current state using the helper function <code>getState</code>. It returns a vector of states, where the length of the vector is defined by windows size and the values of the states are between zero and one.</p>
</li>
<li>
<p>Get the action for the given state using the <code>act</code> function of the agent class.</p>
</li>
<li>
<p>Get the reward for the given action. The mapping of the action and reward is described in the problem definition section of this case study.</p>
</li>
<li>
<p>Get the next state using the <code>getState</code> function. The detail of the next state is further used in the Bellman equation for updating the Q-function.</p>
</li>
<li>
<p>The details of the state, next state, action, etc., are saved in the memory of the agent object, which is used further by the <code>exeReply</code> function. A sample mini-batch is as follows:</p>

<figure class="width-50"><div class="figure">
<img src="Images/mlbf_09in04.png" alt="mlbf 09in04" width="591" height="443"/>
<h6/>
</div></figure>
</li>
<li>
<p>Check if the batch is complete. The size of a batch is defined by the batch size variable. If the batch is complete, then we move to the <code>Replay buffer</code> function and update the Q-function by minimizing the MSE between the Q-predicted and the Q-target. If not, then we move to the next time step.</p>
</li>

</ol>

<p>The code produces the final results of each episode, along with the plot showing the buy and sell actions and the total profit for each episode of the training phase.</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">window_size</code> <code class="o">=</code> <code class="mi">1</code>
<code class="n">agent</code> <code class="o">=</code> <code class="n">Agent</code><code class="p">(</code><code class="n">window_size</code><code class="p">)</code>
<code class="n">l</code> <code class="o">=</code> <code class="nb">len</code><code class="p">(</code><code class="n">data</code><code class="p">)</code> <code class="o">-</code> <code class="mi">1</code>
<code class="n">batch_size</code> <code class="o">=</code> <code class="mi">10</code>
<code class="n">states_sell</code> <code class="o">=</code> <code class="p">[]</code>
<code class="n">states_buy</code> <code class="o">=</code> <code class="p">[]</code>
<code class="n">episode_count</code> <code class="o">=</code> <code class="mi">3</code>

<code class="k">for</code> <code class="n">e</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">episode_count</code> <code class="o">+</code> <code class="mi">1</code><code class="p">):</code>
    <code class="nb">print</code><code class="p">(</code><code class="s">"Episode "</code> <code class="o">+</code> <code class="nb">str</code><code class="p">(</code><code class="n">e</code><code class="p">)</code> <code class="o">+</code> <code class="s">"/"</code> <code class="o">+</code> <code class="nb">str</code><code class="p">(</code><code class="n">episode_count</code><code class="p">))</code>
    <code class="c"># 1-get state</code>
    <code class="n">state</code> <code class="o">=</code> <code class="n">getState</code><code class="p">(</code><code class="n">data</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="n">window_size</code> <code class="o">+</code> <code class="mi">1</code><code class="p">)</code>

    <code class="n">total_profit</code> <code class="o">=</code> <code class="mi">0</code>
    <code class="n">agent</code><code class="o">.</code><code class="n">inventory</code> <code class="o">=</code> <code class="p">[]</code>

    <code class="k">for</code> <code class="n">t</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">l</code><code class="p">):</code>
        <code class="c"># 2-apply best action</code>
        <code class="n">action</code> <code class="o">=</code> <code class="n">agent</code><code class="o">.</code><code class="n">act</code><code class="p">(</code><code class="n">state</code><code class="p">)</code>

        <code class="c"># sit</code>
        <code class="n">next_state</code> <code class="o">=</code> <code class="n">getState</code><code class="p">(</code><code class="n">data</code><code class="p">,</code> <code class="n">t</code> <code class="o">+</code> <code class="mi">1</code><code class="p">,</code> <code class="n">window_size</code> <code class="o">+</code> <code class="mi">1</code><code class="p">)</code>
        <code class="n">reward</code> <code class="o">=</code> <code class="mi">0</code>

        <code class="k">if</code> <code class="n">action</code> <code class="o">==</code> <code class="mi">1</code><code class="p">:</code> <code class="c"># buy</code>
            <code class="n">agent</code><code class="o">.</code><code class="n">inventory</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">data</code><code class="p">[</code><code class="n">t</code><code class="p">])</code>
            <code class="n">states_buy</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">t</code><code class="p">)</code>
            <code class="nb">print</code><code class="p">(</code><code class="s">"Buy: "</code> <code class="o">+</code> <code class="n">formatPrice</code><code class="p">(</code><code class="n">data</code><code class="p">[</code><code class="n">t</code><code class="p">]))</code>

        <code class="k">elif</code> <code class="n">action</code> <code class="o">==</code> <code class="mi">2</code> <code class="ow">and</code> <code class="nb">len</code><code class="p">(</code><code class="n">agent</code><code class="o">.</code><code class="n">inventory</code><code class="p">)</code> <code class="o">&gt;</code> <code class="mi">0</code><code class="p">:</code> <code class="c"># sell</code>
            <code class="n">bought_price</code> <code class="o">=</code> <code class="n">agent</code><code class="o">.</code><code class="n">inventory</code><code class="o">.</code><code class="n">pop</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>
             <code class="c">#3: Get Reward</code>

            <code class="n">reward</code> <code class="o">=</code> <code class="nb">max</code><code class="p">(</code><code class="n">data</code><code class="p">[</code><code class="n">t</code><code class="p">]</code> <code class="o">-</code> <code class="n">bought_price</code><code class="p">,</code> <code class="mi">0</code><code class="p">)</code>
            <code class="n">total_profit</code> <code class="o">+=</code> <code class="n">data</code><code class="p">[</code><code class="n">t</code><code class="p">]</code> <code class="o">-</code> <code class="n">bought_price</code>
            <code class="n">states_sell</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">t</code><code class="p">)</code>
            <code class="nb">print</code><code class="p">(</code><code class="s">"Sell: "</code> <code class="o">+</code> <code class="n">formatPrice</code><code class="p">(</code><code class="n">data</code><code class="p">[</code><code class="n">t</code><code class="p">])</code> <code class="o">+</code> <code class="s">" | Profit: "</code> \
            <code class="o">+</code> <code class="n">formatPrice</code><code class="p">(</code><code class="n">data</code><code class="p">[</code><code class="n">t</code><code class="p">]</code> <code class="o">-</code> <code class="n">bought_price</code><code class="p">))</code>

        <code class="n">done</code> <code class="o">=</code> <code class="k">True</code> <code class="k">if</code> <code class="n">t</code> <code class="o">==</code> <code class="n">l</code> <code class="o">-</code> <code class="mi">1</code> <code class="k">else</code> <code class="k">False</code>
        <code class="c"># 4: get next state to be used in bellman's equation</code>
        <code class="n">next_state</code> <code class="o">=</code> <code class="n">getState</code><code class="p">(</code><code class="n">data</code><code class="p">,</code> <code class="n">t</code> <code class="o">+</code> <code class="mi">1</code><code class="p">,</code> <code class="n">window_size</code> <code class="o">+</code> <code class="mi">1</code><code class="p">)</code>

        <code class="c"># 5: add to the memory</code>
        <code class="n">agent</code><code class="o">.</code><code class="n">memory</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="n">state</code><code class="p">,</code> <code class="n">action</code><code class="p">,</code> <code class="n">reward</code><code class="p">,</code> <code class="n">next_state</code><code class="p">,</code> <code class="n">done</code><code class="p">))</code>
        <code class="n">state</code> <code class="o">=</code> <code class="n">next_state</code>

        <code class="k">if</code> <code class="n">done</code><code class="p">:</code>

            <code class="nb">print</code><code class="p">(</code><code class="s">"--------------------------------"</code><code class="p">)</code>
            <code class="nb">print</code><code class="p">(</code><code class="s">"Total Profit: "</code> <code class="o">+</code> <code class="n">formatPrice</code><code class="p">(</code><code class="n">total_profit</code><code class="p">))</code>
            <code class="nb">print</code><code class="p">(</code><code class="s">"--------------------------------"</code><code class="p">)</code>

        <code class="c"># 6: Run replay buffer function</code>
        <code class="k">if</code> <code class="nb">len</code><code class="p">(</code><code class="n">agent</code><code class="o">.</code><code class="n">memory</code><code class="p">)</code> <code class="o">&gt;</code> <code class="n">batch_size</code><code class="p">:</code>
            <code class="n">agent</code><code class="o">.</code><code class="n">expReplay</code><code class="p">(</code><code class="n">batch_size</code><code class="p">)</code>

    <code class="k">if</code> <code class="n">e</code> <code class="o">%</code> <code class="mi">10</code> <code class="o">==</code> <code class="mi">0</code><code class="p">:</code>
        <code class="n">agent</code><code class="o">.</code><code class="n">model</code><code class="o">.</code><code class="n">save</code><code class="p">(</code><code class="s">"models/model_ep"</code> <code class="o">+</code> <code class="nb">str</code><code class="p">(</code><code class="n">e</code><code class="p">))</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">Running episode 0/10
Total Profit: $6738.87</pre>

<figure><div class="figure">
<img src="Images/mlbf_09in05.png" alt="mlbf 09in05" width="870" height="303"/>
<h6/>
</div></figure>

<pre data-type="programlisting">Running episode 1/10
Total Profit: –$45.07</pre>

<figure><div class="figure">
<img src="Images/mlbf_09in06.png" alt="mlbf 09in06" width="870" height="303"/>
<h6/>
</div></figure>

<pre data-type="programlisting">Running episode 9/10
Total Profit: $1971.54</pre>

<figure><div class="figure">
<img src="Images/mlbf_09in07.png" alt="mlbf 09in07" width="870" height="303"/>
<h6/>
</div></figure>

<pre data-type="programlisting">Running episode 10/10
Total Profit: $1926.84</pre>

<figure><div class="figure">
<img src="Images/mlbf_09in08.png" alt="mlbf 09in08" width="870" height="303"/>
<h6/>
</div></figure>

<p>The charts show the details of the buy/sell pattern and the total gains of the first two (zero and one) and last two (9 and 10) episodes. The details of other episodes can be seen in Jupyter notebook under the GitHub repository for this book.</p>

<p>As we can see, in the beginning of episodes 0 and 1, since the agent has no preconception of the consequences of its actions, it takes randomized actions to observe the rewards associated with it. In episode zero, there is an overall profit of $6,738, a strong result indeed, but in episode one we experience an overall loss of $45. The fact that the cumulative reward per episode fluctuates substantially in the beginning illustrates the exploration process the algorithm is going through. Looking at episodes 9 and 10, it seems as though the agent begins learning from its training. It discovers the strategy and starts to exploit it consistently. The buy and sell actions of these last two episodes lead a PnL that is perhaps less than that of episode zero, but far more robust. The buy and sell actions in the later episodes have been performed uniformly over the entire time period, and the overall profit is stable.</p>

<p>Ideally, the number of training episodes should be higher than the number used in this case study. A higher number of training episodes will lead to a better training performance. Before we move on to the testing, let us go through the details about model tuning.<a data-type="indexterm" data-startref="ix_Chapter9-asciidoc12" id="idm45174904945192"/></p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="5.6. Model tuning"><div class="sect4" id="idm45174905423560">
<h4>5.6. Model tuning</h4>

<p><a data-type="indexterm" data-primary="trading strategy, reinforcement-learning based" data-secondary="model tuning" id="idm45174904943720"/>Similar to other machine learning techniques, we can find the best combination of model hyperparameters in RL by using techniques such as grid search. The grid search for RL-based problems are computationally intensive. Hence, in this section, rather than performing the grid search, we present the key hyperparameters to consider, along with their intuition and potential impact on the model 
<span class="keep-together">output</span>.</p>
<dl>
<dt>Gamma (discount factor)</dt>
<dd>
<p><a data-type="indexterm" data-primary="gamma" id="idm45174904939816"/>Decaying gamma will have the agent prioritize short-term rewards as it learns what those rewards are, and place less emphasis on long-term rewards. Lowering the discount factor in this case study may cause the algorithm to focus on the long-term rewards.</p>
</dd>
<dt>Epsilon</dt>
<dd>
<p><a data-type="indexterm" data-primary="epsilon variable" id="idm45174904937560"/>The epsilon variable drives the <em>exploration versus exploitation</em> property of the model. The more we get to know our environment, the less random exploration we want to do. When we reduce epsilon, the likelihood of a random action becomes smaller, and we take more opportunities to benefit from the high-valued actions that we already discovered. However, in the trading setup, we do not want the algorithm to <em>overfit</em> to the training data, and the epsilon should be modified accordingly.</p>
</dd>
<dt>Episodes and batch size</dt>
<dd>
<p>A higher number of episodes and larger batch size in the training set will lead to better training and a more optimal Q-value. However, there is a trade-off, as increasing the number of episodes and batch size increases the total training time.</p>
</dd>
<dt>Window size</dt>
<dd>
<p>Window size determines the number of market days to consider to evaluate the state. This can be increased in case we want the state to be determined by a greater number of days in the past.</p>
</dd>
<dt>Number of layers and nodes of the deep learning model</dt>
<dd>
<p>This can be modified for better training and a more optimal Q-value. The details about the impact of changing the layers and nodes of ANN models are discussed in <a data-type="xref" href="ch03.xhtml#Chapter3">Chapter 3</a>, and the grid search for a deep learning model is discussed in 
<span class="keep-together"><a data-type="xref" href="ch05.xhtml#Chapter5">Chapter 5</a></span>.<a data-type="indexterm" data-startref="ix_Chapter9-asciidoc9" id="idm45174904928984"/></p>
</dd>
</dl>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="6. Testing the data"><div class="sect3" id="idm45174904928056">
<h3>6. Testing the data</h3>

<p><a data-type="indexterm" data-primary="trading strategy, reinforcement-learning based" data-secondary="testing the data" id="ix_Chapter9-asciidoc13"/>After training the data, it is evaluated against the test dataset. This is an important step, especially for reinforcement learning, as the agent may mistakenly correlate reward with certain spurious features from the data, or it may overfit a particular chart pattern. In the testing step, we look at the performance of the already trained model (<em>model_ep10</em>) from the training step on the test data. The Python code looks similar to the training set we saw before. However, the <code>is_eval</code> flag is set to <code>true</code>, the <code>reply buffer</code> function is not called, and there is no training. Let us look at the results:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c">#agent is already defined in the training set above.</code>
<code class="n">test_data</code> <code class="o">=</code> <code class="n">X_test</code>
<code class="n">l_test</code> <code class="o">=</code> <code class="nb">len</code><code class="p">(</code><code class="n">test_data</code><code class="p">)</code> <code class="o">-</code> <code class="mi">1</code>
<code class="n">state</code> <code class="o">=</code> <code class="n">getState</code><code class="p">(</code><code class="n">test_data</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="n">window_size</code> <code class="o">+</code> <code class="mi">1</code><code class="p">)</code>
<code class="n">total_profit</code> <code class="o">=</code> <code class="mi">0</code>
<code class="n">is_eval</code> <code class="o">=</code> <code class="k">True</code>
<code class="n">done</code> <code class="o">=</code> <code class="k">False</code>
<code class="n">states_sell_test</code> <code class="o">=</code> <code class="p">[]</code>
<code class="n">states_buy_test</code> <code class="o">=</code> <code class="p">[]</code>
<code class="n">model_name</code> <code class="o">=</code> <code class="s">"model_ep10"</code>
<code class="n">agent</code> <code class="o">=</code> <code class="n">Agent</code><code class="p">(</code><code class="n">window_size</code><code class="p">,</code> <code class="n">is_eval</code><code class="p">,</code> <code class="n">model_name</code><code class="p">)</code>
<code class="n">state</code> <code class="o">=</code> <code class="n">getState</code><code class="p">(</code><code class="n">data</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="n">window_size</code> <code class="o">+</code> <code class="mi">1</code><code class="p">)</code>
<code class="n">total_profit</code> <code class="o">=</code> <code class="mi">0</code>
<code class="n">agent</code><code class="o">.</code><code class="n">inventory</code> <code class="o">=</code> <code class="p">[]</code>

<code class="k">for</code> <code class="n">t</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">l_test</code><code class="p">):</code>
    <code class="n">action</code> <code class="o">=</code> <code class="n">agent</code><code class="o">.</code><code class="n">act</code><code class="p">(</code><code class="n">state</code><code class="p">)</code>

    <code class="n">next_state</code> <code class="o">=</code> <code class="n">getState</code><code class="p">(</code><code class="n">test_data</code><code class="p">,</code> <code class="n">t</code> <code class="o">+</code> <code class="mi">1</code><code class="p">,</code> <code class="n">window_size</code> <code class="o">+</code> <code class="mi">1</code><code class="p">)</code>
    <code class="n">reward</code> <code class="o">=</code> <code class="mi">0</code>

    <code class="k">if</code> <code class="n">action</code> <code class="o">==</code> <code class="mi">1</code><code class="p">:</code>

        <code class="n">agent</code><code class="o">.</code><code class="n">inventory</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">test_data</code><code class="p">[</code><code class="n">t</code><code class="p">])</code>
        <code class="nb">print</code><code class="p">(</code><code class="s">"Buy: "</code> <code class="o">+</code> <code class="n">formatPrice</code><code class="p">(</code><code class="n">test_data</code><code class="p">[</code><code class="n">t</code><code class="p">]))</code>

    <code class="k">elif</code> <code class="n">action</code> <code class="o">==</code> <code class="mi">2</code> <code class="ow">and</code> <code class="nb">len</code><code class="p">(</code><code class="n">agent</code><code class="o">.</code><code class="n">inventory</code><code class="p">)</code> <code class="o">&gt;</code> <code class="mi">0</code><code class="p">:</code>
        <code class="n">bought_price</code> <code class="o">=</code> <code class="n">agent</code><code class="o">.</code><code class="n">inventory</code><code class="o">.</code><code class="n">pop</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>
        <code class="n">reward</code> <code class="o">=</code> <code class="nb">max</code><code class="p">(</code><code class="n">test_data</code><code class="p">[</code><code class="n">t</code><code class="p">]</code> <code class="o">-</code> <code class="n">bought_price</code><code class="p">,</code> <code class="mi">0</code><code class="p">)</code>
        <code class="n">total_profit</code> <code class="o">+=</code> <code class="n">test_data</code><code class="p">[</code><code class="n">t</code><code class="p">]</code> <code class="o">-</code> <code class="n">bought_price</code>
        <code class="nb">print</code><code class="p">(</code><code class="s">"Sell: "</code> <code class="o">+</code> <code class="n">formatPrice</code><code class="p">(</code><code class="n">test_data</code><code class="p">[</code><code class="n">t</code><code class="p">])</code> <code class="o">+</code> <code class="s">" | profit: "</code> <code class="o">+</code>\
         <code class="n">formatPrice</code><code class="p">(</code><code class="n">test_data</code><code class="p">[</code><code class="n">t</code><code class="p">]</code> <code class="o">-</code> <code class="n">bought_price</code><code class="p">))</code>

    <code class="k">if</code> <code class="n">t</code> <code class="o">==</code> <code class="n">l_test</code> <code class="o">-</code> <code class="mi">1</code><code class="p">:</code>
        <code class="n">done</code> <code class="o">=</code> <code class="k">True</code>
    <code class="n">agent</code><code class="o">.</code><code class="n">memory</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="n">state</code><code class="p">,</code> <code class="n">action</code><code class="p">,</code> <code class="n">reward</code><code class="p">,</code> <code class="n">next_state</code><code class="p">,</code> <code class="n">done</code><code class="p">))</code>
    <code class="n">state</code> <code class="o">=</code> <code class="n">next_state</code>

    <code class="k">if</code> <code class="n">done</code><code class="p">:</code>
        <code class="nb">print</code><code class="p">(</code><code class="s">"------------------------------------------"</code><code class="p">)</code>
        <code class="nb">print</code><code class="p">(</code><code class="s">"Total Profit: "</code> <code class="o">+</code> <code class="n">formatPrice</code><code class="p">(</code><code class="n">total_profit</code><code class="p">))</code>
        <code class="nb">print</code><code class="p">(</code><code class="s">"------------------------------------------"</code><code class="p">)</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">Total Profit: $1280.40</pre>

<figure><div class="figure">
<img src="Images/mlbf_09in09.png" alt="mlbf 09in09" width="869" height="303"/>
<h6/>
</div></figure>

<p>Looking at the results above, our model resulted in an overall profit of $1,280, and we can say that our DQN agent performs quite well on the test set.<a data-type="indexterm" data-startref="ix_Chapter9-asciidoc13" id="idm45174904912568"/></p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Conclusion"><div class="sect3" id="idm45174904911736">
<h3>Conclusion</h3>

<p>In this case study, we created an automated trading strategy, or a <em>trading bot</em>, that simply needs to be fed running stock market data to produce a trading signal. We saw that the algorithm decides the policy by itself, and the overall approach is much simpler and more principled than the supervised learning–based approach. The trained model was profitable in the test set, corroborating the effectiveness of the RL-based trading strategy.</p>

<p>In using a reinforcement learning model such as DQN, which is based on a deep neural network, we can learn policies that are more complex and powerful than what a human trader could learn.</p>

<p>Given the high complexity and low interpretability of the RL-based model, visualization and testing steps become quite important. For interpretability, we used the plots of the training episodes of the training algorithm and found that the model starts to learn over a period of time, discovers the strategy, and starts to exploit it. A sufficient number of tests should be conducted on different time periods before deploying the model for live trading.</p>

<p>While using RL-based models, we should carefully select the RL components, such as the reward function and state, and ensure understanding of their impact on the overall model results. Before implementing or training the model, it is important to think of questions, such as “How can we engineer the reward function or the state so that the RL algorithm has the potential to learn to optimize the right metric?”</p>

<p>Overall, these RL-based models can enable financial practitioners to create trading strategies with a very flexible approach. The framework provided in this case study can be a great starting point to develop more powerful models for algorithmic 
<span class="keep-together">trading</span>.<a data-type="indexterm" data-startref="ix_Chapter9-asciidoc8" id="idm45174904608152"/></p>
<img class="blueprint bottom" src="Images/bracket_bottom.png" width="1625" height="32"/>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Case Study 2: Derivatives Hedging"><div class="sect1" id="CaseStudy2RL">
<h1>Case Study 2: Derivatives Hedging</h1>

<p><a data-type="indexterm" data-primary="derivatives hedging" id="ix_Chapter9-asciidoc14"/>Much of traditional finance theory for handling derivatives pricing and risk management is based on the idealized complete markets assumption of perfect hedgability, without trading restrictions, transaction costs, market impact, or liquidity constraints. In practice, however, these frictions are very real. As a consequence, practical risk management using derivatives requires human oversight and maintenance; the models themselves are insufficient. Implementation is still partially driven by the trader’s intuitive understanding of the shortcomings of the existing tools.</p>

<p>Reinforcement learning algorithms, with their ability to tackle more nuances and parameters within the operational environment, are inherently aligned with the objective of hedging. These models can produce dynamic strategies that are optimal, even in a world with frictions. The model-free RL approaches demand very few theoretical assumptions. This allows for automation of hedging without requiring frequent human intervention, making the overall hedging process significantly faster. These models can learn from large amounts of historical data and can consider many variables to make more precise and accurate hedging decisions. Moreover, the availability of vast amounts of data makes RL-based models more useful and effective than ever before.</p>

<p>In this case study, we implement a reinforcement learning–based hedging strategy that adopts the ideas presented in the paper <a href="https://oreil.ly/6_Qvz">“Deep Hedging”</a> by Hans Bühler et al. We will build an optimal hedging strategy for a specific type of derivative (call options) by minimizing the risk-adjusted PnL. We use the measure <em>CVaR</em> (conditional value at risk), which quantifies the amount of tail risk of a position or portfolio as a risk assessment measure.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45174904600296">
<h5/>
<p>In this case study, we will focus on:</p>

<ul>
<li>
<p>Using policy-based (or direct policy search–based) reinforcement learning and implementing it using a deep neural network.</p>
</li>
<li>
<p>Comparing the effectiveness of an RL-based trading strategy to the traditional Black-Scholes model.</p>
</li>
<li>
<p>Setting up an agent for an RL problem using class structure in Python.</p>
</li>
<li>
<p>Implementing and evaluating a policy gradient–based RL method.</p>
</li>
<li>
<p>Introducing the basic concept of functions in the TensorFlow Python package.</p>
</li>
<li>
<p>Implementing a Monte Carlo simulation of stock price and the Black-Scholes pricing model, and computing option Greeks.</p>
</li>
</ul>
</div></aside>
<img class="blueprint top" src="Images/bracket_top.png" width="1625" height="32"/>








<section data-type="sect2" class="blueprint" data-pdf-bookmark="Blueprint for Implementing a Reinforcement Learning–Based Hedging Strategy"><div class="sect2" id="idm45174904591928">
<h2>Blueprint for Implementing a Reinforcement Learning–Based Hedging Strategy</h2>










<section data-type="sect3" data-pdf-bookmark="1. Problem definition"><div class="sect3" id="idm45174904590408">
<h3>1. Problem definition</h3>

<p>In the reinforcement learning framework for this case study, the algorithm decides the best hedging strategy for call options using market prices of the underlying asset. A direct policy search reinforcement learning strategy is used. The overall idea, derived from the “Deep Hedging” paper, is based on minimizing the hedge error under a risk assessment measure. The overall PnL of a call option hedging strategy over a period of time, from <em>t</em>=1 to <em>t</em>=T, can be written as:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <mi>P</mi>
    <mi>n</mi>
    <msub><mi>L</mi> <mi>T</mi> </msub>
    <mrow>
      <mo>(</mo>
      <mi>Z</mi>
      <mo>,</mo>
      <mi>δ</mi>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <mo>–</mo>
    <msub><mi>Z</mi> <mi>T</mi> </msub>
    <mo>+</mo>
    <munderover><mo>∑</mo> <mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow> <mi>T</mi> </munderover>
    <msub><mi>δ</mi> <mrow><mi>t</mi><mo>–</mo><mn>1</mn></mrow> </msub>
    <mrow>
      <mo>(</mo>
      <msub><mi>S</mi> <mi>t</mi> </msub>
      <mo>–</mo>
      <msub><mi>S</mi> <mrow><mi>t</mi><mo>–</mo><mn>1</mn></mrow> </msub>
      <mo>)</mo>
    </mrow>
    <mo>–</mo>
    <munderover><mo>∑</mo> <mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow> <mi>T</mi> </munderover>
    <msub><mi>C</mi> <mi>t</mi> </msub>
  </mrow>
</math>
</div>

<p>where</p>

<ul>
<li>
<p><math alttext="upper Z Subscript upper T">
  <msub><mi>Z</mi> <mi>T</mi> </msub>
</math> is the payoff of a call option at maturity.</p>
</li>
<li>
<p>
<math display="inline">
  <mrow>
    <msub><mi>δ</mi> <mrow><mi>t</mi><mo>–</mo><mn>1</mn></mrow> </msub>
    <mrow>
      <mo>(</mo>
      <msub><mi>S</mi> <mi>t</mi> </msub>
      <mo>–</mo>
      <msub><mi>S</mi> <mrow><mi>t</mi><mo>–</mo><mn>1</mn></mrow> </msub>
      <mo>)</mo>
    </mrow>
  </mrow>
</math> is the cash flow from the hedging instruments on day <math alttext="t">
  <mi>t</mi>
</math>, where <math alttext="delta">
  <mi>δ</mi>
</math> is the hedge and <math alttext="upper S Subscript t">
  <msub><mi>S</mi> <mi>t</mi> </msub>
</math> is the spot price on day <math alttext="t">
  <mi>t</mi>
</math>.</p>
</li>
<li>
<p><math alttext="upper C Subscript t">
  <msub><mi>C</mi> <mi>t</mi> </msub>
</math> is the transaction cost at time <math alttext="t">
  <mi>t</mi>
</math> and may be constant or proportional to the hedge size.</p>
</li>
</ul>

<p>The individual components in the equation are the components of the cash flow. However, it would be preferable to take into account the risk arising from any position while designing the reward function. We use the measure CVaR as the risk assessment measure. CVaR quantifies the amount of tail risk and is the <code>expected shortfall</code> (risk aversion parameter)<sup><a data-type="noteref" id="idm45174904541224-marker" href="ch09.xhtml#idm45174904541224">13</a></sup> for the confidence level <math alttext="alpha">
  <mi>α</mi>
</math>. Now the reward function is modified to the following:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <msub><mi>V</mi> <mi>T</mi> </msub>
    <mo>=</mo>
    <mi>f</mi>
    <mo>(</mo>
    <mo>–</mo>
    <msub><mi>Z</mi> <mi>T</mi> </msub>
    <mo>+</mo>
    <munderover><mo>∑</mo> <mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow> <mi>T</mi> </munderover>
    <msub><mi>δ</mi> <mrow><mi>t</mi><mo>–</mo><mn>1</mn></mrow> </msub>
    <mrow>
      <mo>(</mo>
      <msub><mi>S</mi> <mi>t</mi> </msub>
      <mo>–</mo>
      <msub><mi>S</mi> <mrow><mi>t</mi><mo>–</mo><mn>1</mn></mrow> </msub>
      <mo>)</mo>
    </mrow>
    <mo>–</mo>
    <munderover><mo>∑</mo> <mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow> <mi>T</mi> </munderover>
    <msub><mi>C</mi> <mi>t</mi> </msub>
    <mrow>
      <mo>)</mo>
    </mrow>
  </mrow>
</math>
</div>

<p>where <math alttext="f">
  <mi>f</mi>
</math> represents the CVaR.</p>

<p>We will train an <em>RNN-based</em> network to learn the optimal hedging strategy (i.e., <math alttext="delta 1 comma delta 2 period period period comma delta Subscript upper T Baseline">
  <mrow>
    <msub><mi>δ</mi> <mn>1</mn> </msub>
    <mo>,</mo>
    <msub><mi>δ</mi> <mn>2</mn> </msub>
    <mo>.</mo>
    <mo>.</mo>
    <mo>.</mo>
    <mo>,</mo>
    <msub><mi>δ</mi> <mi>T</mi> </msub>
  </mrow>
</math>) given the stock price, strike price, and risk aversion parameter, (<math alttext="alpha">
  <mi>α</mi>
</math>), by minimizing CVaR. We assume transaction costs to be zero for simplicity. The model can easily be extended to incorporate transaction costs and other market frictions.</p>

<p>The data used for the synthetic underlying stock price is generated using Monte Carlo simulation, assuming a lognormal price distribution. We assume an interest rate of 0% and annual volatility of 20%.</p>

<p>The key components of the model are:</p>
<dl>
<dt>Agent</dt>
<dd>
<p>Trader or trading agent.</p>
</dd>
<dt>Action</dt>
<dd>
<p>Hedging strategy (i.e., <math alttext="delta 1 comma delta 2 period period period comma delta Subscript upper T Baseline">
  <mrow>
    <msub><mi>δ</mi> <mn>1</mn> </msub>
    <mo>,</mo>
    <msub><mi>δ</mi> <mn>2</mn> </msub>
    <mo>.</mo>
    <mo>.</mo>
    <mo>.</mo>
    <mo>,</mo>
    <msub><mi>δ</mi> <mi>T</mi> </msub>
  </mrow>
</math>).</p>
</dd>
<dt>Reward function</dt>
<dd>
<p>CVaR—this is a convex function and is minimized during the model training.</p>
</dd>
<dt>State</dt>
<dd>
<p>State is the representation of the current market and relevant product variables. The state represents the model inputs, which include the simulated stock price path (i.e., <math alttext="upper S 1 comma upper S 2 period period period comma upper S Subscript upper T Baseline">
  <mrow>
    <msub><mi>S</mi> <mn>1</mn> </msub>
    <mo>,</mo>
    <msub><mi>S</mi> <mn>2</mn> </msub>
    <mo>.</mo>
    <mo>.</mo>
    <mo>.</mo>
    <mo>,</mo>
    <msub><mi>S</mi> <mi>T</mi> </msub>
  </mrow>
</math>), strike, and risk aversion parameter (<math alttext="alpha">
  <mi>α</mi>
</math>).</p>
</dd>
<dt>Environment</dt>
<dd>
<p>Stock exchange or stock market.</p>
</dd>
</dl>
</div></section>













<section data-type="sect3" data-pdf-bookmark="2. Getting started"><div class="sect3" id="idm45174904589912">
<h3>2. Getting started</h3>












<section data-type="sect4" data-pdf-bookmark="2.1. Loading the Python packages"><div class="sect4" id="idm45174904482168">
<h4>2.1. Loading the Python packages</h4>

<p><a data-type="indexterm" data-primary="derivatives hedging" data-secondary="loading Python packages" id="idm45174904480920"/>The loading of Python packages is similar to the previous case studies. Please refer to the Jupyter notebook for this case study for more details.</p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="2.2. Generating the data"><div class="sect4" id="idm45174904479400">
<h4>2.2. Generating the data</h4>

<p><a data-type="indexterm" data-primary="derivatives hedging" data-secondary="generating data" id="idm45174904477976"/>In this step we generate the data for this case study using a Black-Scholes simulation.</p>

<p>This function generates the Monte Carlo paths for the stock price and gets the option price on each of the Monte Carlo paths. The calculation as shown is based on the lognormal assumption of stock prices:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <msub><mi>S</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow> </msub>
    <mo>=</mo>
    <msub><mi>S</mi> <mi>t</mi> </msub>
    <msup><mi>e</mi> <mrow><mfenced separators="" open="(" close=")"><mi>μ</mi><mo>–</mo><mfrac><mn>1</mn> <mn>2</mn></mfrac><msup><mi>σ</mi> <mn>2</mn> </msup></mfenced><mi>Δ</mi><mi>t</mi><mo>+</mo><mi>σ</mi><msqrt><mrow><mi>Δ</mi><mi>t</mi></mrow></msqrt><mi>Z</mi></mrow> </msup>
  </mrow>
</math>
</div>

<p>where <math alttext="upper S">
  <mi>S</mi>
</math> is stock price, <math alttext="sigma">
  <mi>σ</mi>
</math> is volatility, <math alttext="mu">
  <mi>μ</mi>
</math> is the drift, <math alttext="t">
  <mi>t</mi>
</math> is time, and <math alttext="upper Z">
  <mi>Z</mi>
</math> is a standard normal variable.</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="k">def</code> <code class="nf">monte_carlo_paths</code><code class="p">(</code><code class="n">S_0</code><code class="p">,</code> <code class="n">time_to_expiry</code><code class="p">,</code> <code class="n">sigma</code><code class="p">,</code> <code class="n">drift</code><code class="p">,</code> <code class="n">seed</code><code class="p">,</code> <code class="n">n_sims</code><code class="p">,</code> \
  <code class="n">n_timesteps</code><code class="p">):</code>
    <code class="sd">"""</code>
<code class="sd">    Create random paths of a stock price following a brownian geometric motion</code>
<code class="sd">    return:</code>

<code class="sd">    a (n_timesteps x n_sims x 1) matrix</code>
<code class="sd">    """</code>
    <code class="k">if</code> <code class="n">seed</code> <code class="o">&gt;</code> <code class="mi">0</code><code class="p">:</code>
            <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="n">seed</code><code class="p">)</code>
    <code class="n">stdnorm_random_variates</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">randn</code><code class="p">(</code><code class="n">n_sims</code><code class="p">,</code> <code class="n">n_timesteps</code><code class="p">)</code>
    <code class="n">S</code> <code class="o">=</code> <code class="n">S_0</code>
    <code class="n">dt</code> <code class="o">=</code> <code class="n">time_to_expiry</code> <code class="o">/</code> <code class="n">stdnorm_random_variates</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code>
    <code class="n">r</code> <code class="o">=</code> <code class="n">drift</code>
    <code class="n">S_T</code> <code class="o">=</code> <code class="n">S</code> <code class="o">*</code> <code class="n">np</code><code class="o">.</code><code class="n">cumprod</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">exp</code><code class="p">((</code><code class="n">r</code><code class="o">-</code><code class="n">sigma</code><code class="o">**</code><code class="mi">2</code><code class="o">/</code><code class="mi">2</code><code class="p">)</code><code class="o">*</code><code class="n">dt</code><code class="o">+</code><code class="n">sigma</code><code class="o">*</code><code class="n">np</code><code class="o">.</code><code class="n">sqrt</code><code class="p">(</code><code class="n">dt</code><code class="p">)</code><code class="o">*</code>\
    <code class="n">stdnorm_random_variates</code><code class="p">),</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">np</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">transpose</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">c_</code><code class="p">[</code><code class="n">np</code><code class="o">.</code><code class="n">ones</code><code class="p">(</code><code class="n">n_sims</code><code class="p">)</code><code class="o">*</code><code class="n">S_0</code><code class="p">,</code> <code class="n">S_T</code><code class="p">]),</code> \
    <code class="p">(</code><code class="n">n_timesteps</code><code class="o">+</code><code class="mi">1</code><code class="p">,</code> <code class="n">n_sims</code><code class="p">,</code> <code class="mi">1</code><code class="p">))</code></pre>

<p>We generate 50,000 simulations of the spot price over a period of one month. The total number of time steps is 30. Hence, for each Monte Carlo scenario, there is one observation per day. The parameters needed for the simulation are defined below:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">S_0</code> <code class="o">=</code> <code class="mi">100</code><code class="p">;</code> <code class="n">K</code> <code class="o">=</code> <code class="mi">100</code><code class="p">;</code> <code class="n">r</code> <code class="o">=</code> <code class="mi">0</code><code class="p">;</code> <code class="n">vol</code> <code class="o">=</code> <code class="mf">0.2</code><code class="p">;</code> <code class="n">T</code> <code class="o">=</code> <code class="mi">1</code><code class="o">/</code><code class="mi">12</code>
<code class="n">timesteps</code> <code class="o">=</code> <code class="mi">30</code><code class="p">;</code> <code class="n">seed</code> <code class="o">=</code> <code class="mi">42</code><code class="p">;</code> <code class="n">n_sims</code> <code class="o">=</code> <code class="mi">5000</code>

<code class="c"># Generate the monte carlo paths</code>
<code class="n">paths_train</code> <code class="o">=</code> <code class="n">monte_carlo_paths</code><code class="p">(</code><code class="n">S_0</code><code class="p">,</code> <code class="n">T</code><code class="p">,</code> <code class="n">vol</code><code class="p">,</code> <code class="n">r</code><code class="p">,</code> <code class="n">seed</code><code class="p">,</code> <code class="n">n_sims</code><code class="p">,</code> <code class="n">timesteps</code><code class="p">)</code></pre>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="3. Exploratory data analysis"><div class="sect3" id="idm45174904143656">
<h3>3. Exploratory data analysis</h3>

<p><a data-type="indexterm" data-primary="derivatives hedging" data-secondary="exploratory data analysis" id="idm45174904187896"/>We will look at descriptive statistics and data visualization in this section. Given that the data was generated by the simulation, we simply inspect one path as a sanity check of the simulation algorithm:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c">#Plot Paths for one simulation</code>
<code class="n">plt</code><code class="o">.</code><code class="n">figure</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">20</code><code class="p">,</code> <code class="mi">10</code><code class="p">))</code>
<code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">paths_train</code><code class="p">[</code><code class="mi">1</code><code class="p">])</code>
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s">'Time Steps'</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="s">'Stock Price Sample Paths'</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>

<p><code>Output</code></p>

<figure><div class="figure">
<img src="Images/mlbf_09in10.png" alt="mlbf 09in10" width="748" height="468"/>
<h6/>
</div></figure>
</div></section>













<section data-type="sect3" class="pagebreak-before less_space" data-pdf-bookmark="4. Evaluate algorithms and models"><div class="sect3" id="idm45174904044088">
<h3>4. Evaluate algorithms and models</h3>

<p><a data-type="indexterm" data-primary="artificial neural networks (ANNs)" data-secondary="derivatives hedging with" id="ix_Chapter9-asciidoc16"/><a data-type="indexterm" data-primary="derivatives hedging" data-secondary="evaluation of algorithms and models" id="ix_Chapter9-asciidoc17"/>In this direct policy search approach, we use an artificial neural network (ANN) to map the state to action. In a traditional ANN, we assume that all inputs (and outputs) are independent of each other. However, the hedging decision at time <em>t</em> (represented by <em>δ<sub>t</sub></em>) is <em>path dependent</em> and is influenced by the stock price and hedging decisions at previous time steps. Hence, using a traditional ANN is not feasible. <a data-type="indexterm" data-primary="recurrent neural networks (RNNs)" data-secondary="derivatives hedging with" id="idm45174904037544"/><em>RNN</em> is a type of ANN that can capture the time-varying dynamics of the underlying system and is more appropriate in this context. RNNs have a memory, which captures information about what has been calculated so far. We used this property of the RNN model for time series modeling in <a data-type="xref" href="ch05.xhtml#Chapter5">Chapter 5</a>. <em>LSTM</em> (also discussed in <a data-type="xref" href="ch05.xhtml#Chapter5">Chapter 5</a>) is a special kind of RNN capable of learning long-term dependencies. Past state information is made available to the network when mapping to an action; the extraction of relevant past data is then learned as part of the training process. We will use an LSTM model to map the state to action and get the hedging strategy (i.e., δ<sub>1</sub>, δ<sub>2</sub>,…δ<sub>T</sub>).</p>












<section data-type="sect4" data-pdf-bookmark="4.1. Policy gradient script"><div class="sect4" id="idm45174904042104">
<h4>4.1. Policy gradient script</h4>

<p><a data-type="indexterm" data-primary="derivatives hedging" data-secondary="policy gradient script" id="ix_Chapter9-asciidoc18"/><a data-type="indexterm" data-primary="policy gradient" id="ix_Chapter9-asciidoc19"/>We will cover the implementation steps and model training in this section. We provide the input variables—stock price path (<math alttext="upper S 1 comma upper S 2 comma period period period upper S Subscript upper T Baseline">
  <mrow>
    <msub><mi>S</mi> <mn>1</mn> </msub>
    <mo>,</mo>
    <msub><mi>S</mi> <mn>2</mn> </msub>
    <mo>,</mo>
    <mo>.</mo>
    <mo>.</mo>
    <mo>.</mo>
    <msub><mi>S</mi> <mi>T</mi> </msub>
  </mrow>
</math>), strike, and risk aversion parameter, <math alttext="alpha">
  <mi>α</mi>
</math>—to the trained model and receive the hedging 
<span class="keep-together">strategy</span> (i.e., <math alttext="delta 1 comma delta 2 comma period period period delta Subscript upper T Baseline right-parenthesis">
  <mrow>
    <msub><mi>δ</mi> <mn>1</mn> </msub>
    <mo>,</mo>
    <msub><mi>δ</mi> <mn>2</mn> </msub>
    <mo>,</mo>
    <mo>.</mo>
    <mo>.</mo>
    <mo>.</mo>
    <msub><mi>δ</mi> <mi>T</mi> </msub>
    <mrow>
      <mo>)</mo>
    </mrow>
  </mrow>
</math> as the output. <a data-type="xref" href="#PGTraining">Figure 9-9</a> provides an overview of the training of the policy gradient for this case study.</p>

<figure><div id="PGTraining" class="figure">
<img src="Images/mlbf_0909.png" alt="mlbf 0909" width="1408" height="532"/>
<h6><span class="label">Figure 9-9. </span>Policy gradient training for derivatives hedging</h6>
</div></figure>

<p>We already performed step 1 in section 2 of this case study. Steps 2 to 5 are self-explanatory and are implemented in the <code>agent</code> class defined later. The <code>agent</code> holds the variables and member functions that perform the training. An <code>object</code> of the <code>agent</code> class is created using the training phase and is used for training the model.
After a sufficient number of iterations of steps 2 to 5, an optimal policy gradient model is generated.</p>

<p class="pagebreak-before">The class consists of the following modules:</p>

<ul>
<li>
<p><code>Constructor</code></p>
</li>
<li>
<p>The function <code>execute_graph_batchwise</code></p>
</li>
<li>
<p>The functions <code>training</code>, <code>predict</code>, and <code>restore</code></p>
</li>
</ul>

<p>Let us dig deeper into the Python code for each of the functions.</p>

<p>The <code>Constructor</code> is defined as an <code>init</code> function, where we define the model parameters. We can pass the <code>timesteps</code>, <code>batch_size</code>, and <code>number of nodes</code> in each layer of the LSTM model to the constructor. We define the input variables of the model (i.e., stock price path, strike, and risk aversion parameter) as <em>TensorFlow placeholders</em>. 
<span class="keep-together">Placeholders</span> are used to feed in data from outside the computational graph, and we feed the data of these input variables during the training phase. We implement an LSTM network in TensorFlow by using the <code>tf.MultiRNNCell</code> function. The LSTM model uses four layers with 62, 46, 46, and 1 nodes. The loss function is the CVaR, which is minimized when <code>tf.train</code> is called during the training step. We sort the negative realized PnLs of the trading strategy and calculate the mean of the (1−<em>α</em>) top losses:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="k">class</code> <code class="nc">Agent</code><code class="p">(</code><code class="nb">object</code><code class="p">):</code>
    <code class="k">def</code> <code class="nf">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">time_steps</code><code class="p">,</code> <code class="n">batch_size</code><code class="p">,</code> <code class="n">features</code><code class="p">,</code>\
       <code class="n">nodes</code> <code class="o">=</code> <code class="p">[</code><code class="mi">62</code><code class="p">,</code> <code class="mi">46</code><code class="p">,</code> <code class="mi">46</code><code class="p">,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">name</code><code class="o">=</code><code class="s">'model'</code><code class="p">):</code>

        <code class="c">#1. Initialize the variables</code>
        <code class="n">tf</code><code class="o">.</code><code class="n">reset_default_graph</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">batch_size</code> <code class="o">=</code> <code class="n">batch_size</code> <code class="c"># Number of options in a batch</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">S_t_input</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">placeholder</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">float32</code><code class="p">,</code> <code class="p">[</code><code class="n">time_steps</code><code class="p">,</code> <code class="n">batch_size</code><code class="p">,</code> \
          <code class="n">features</code><code class="p">])</code> <code class="c">#Spot</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">K</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">placeholder</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">float32</code><code class="p">,</code> <code class="n">batch_size</code><code class="p">)</code> <code class="c">#Strike</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">alpha</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">placeholder</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">float32</code><code class="p">)</code> <code class="c">#alpha for cVaR</code>

        <code class="n">S_T</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">S_t_input</code><code class="p">[</code><code class="o">-</code><code class="mi">1</code><code class="p">,:,</code><code class="mi">0</code><code class="p">]</code> <code class="c">#Spot at time T</code>
        <code class="c"># Change in the Spot</code>
        <code class="n">dS</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">S_t_input</code><code class="p">[</code><code class="mi">1</code><code class="p">:,</code> <code class="p">:,</code> <code class="mi">0</code><code class="p">]</code> <code class="o">-</code> <code class="bp">self</code><code class="o">.</code><code class="n">S_t_input</code><code class="p">[</code><code class="mi">0</code><code class="p">:</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="p">:,</code> <code class="mi">0</code><code class="p">]</code>
        <code class="c">#dS = tf.reshape(dS, (time_steps, batch_size))</code>


        <code class="c">#2. Prepare S_t for use in the RNN remove the \</code>
        <code class="c">#last time step (at T the portfolio is zero)</code>
        <code class="n">S_t</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">unstack</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">S_t_input</code><code class="p">[:</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="p">:,:],</code> <code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>

        <code class="c"># Build the lstm</code>
        <code class="n">lstm</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">contrib</code><code class="o">.</code><code class="n">rnn</code><code class="o">.</code><code class="n">MultiRNNCell</code><code class="p">([</code><code class="n">tf</code><code class="o">.</code><code class="n">contrib</code><code class="o">.</code><code class="n">rnn</code><code class="o">.</code><code class="n">LSTMCell</code><code class="p">(</code><code class="n">n</code><code class="p">)</code> \
        <code class="k">for</code> <code class="n">n</code> <code class="ow">in</code> <code class="n">nodes</code><code class="p">])</code>

        <code class="c">#3. So the state is a convenient tensor that holds the last</code>
        <code class="c">#actual RNN state,ignoring the zeros.</code>
        <code class="c">#The strategy tensor holds the outputs of all cells.</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">strategy</code><code class="p">,</code> <code class="n">state</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">static_rnn</code><code class="p">(</code><code class="n">lstm</code><code class="p">,</code> <code class="n">S_t</code><code class="p">,</code> <code class="n">initial_state</code><code class="o">=</code>\
          <code class="n">lstm</code><code class="o">.</code><code class="n">zero_state</code><code class="p">(</code><code class="n">batch_size</code><code class="p">,</code> <code class="n">tf</code><code class="o">.</code><code class="n">float32</code><code class="p">),</code> <code class="n">dtype</code><code class="o">=</code><code class="n">tf</code><code class="o">.</code><code class="n">float32</code><code class="p">)</code>

        <code class="bp">self</code><code class="o">.</code><code class="n">strategy</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">strategy</code><code class="p">,</code> <code class="p">(</code><code class="n">time_steps</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="n">batch_size</code><code class="p">))</code>

        <code class="c">#4. Option Price</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">option</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">maximum</code><code class="p">(</code><code class="n">S_T</code><code class="o">-</code><code class="bp">self</code><code class="o">.</code><code class="n">K</code><code class="p">,</code> <code class="mi">0</code><code class="p">)</code>

        <code class="bp">self</code><code class="o">.</code><code class="n">Hedging_PnL</code> <code class="o">=</code> <code class="o">-</code> <code class="bp">self</code><code class="o">.</code><code class="n">option</code> <code class="o">+</code> <code class="n">tf</code><code class="o">.</code><code class="n">reduce_sum</code><code class="p">(</code><code class="n">dS</code><code class="o">*</code><code class="bp">self</code><code class="o">.</code><code class="n">strategy</code><code class="p">,</code> \
          <code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>

        <code class="c">#5. Total Hedged PnL of each path</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">Hedging_PnL_Paths</code> <code class="o">=</code> <code class="o">-</code> <code class="bp">self</code><code class="o">.</code><code class="n">option</code> <code class="o">+</code> <code class="n">dS</code><code class="o">*</code><code class="bp">self</code><code class="o">.</code><code class="n">strategy</code>

        <code class="c"># 6. Calculate the CVaR for a given confidence level alpha</code>
        <code class="c"># Take the 1-alpha largest losses (top 1-alpha negative PnLs)</code>
        <code class="c">#and calculate the mean</code>
        <code class="n">CVaR</code><code class="p">,</code> <code class="n">idx</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">top_k</code><code class="p">(</code><code class="o">-</code><code class="bp">self</code><code class="o">.</code><code class="n">Hedging_PnL</code><code class="p">,</code> <code class="n">tf</code><code class="o">.</code><code class="n">cast</code><code class="p">((</code><code class="mi">1</code><code class="o">-</code><code class="bp">self</code><code class="o">.</code><code class="n">alpha</code><code class="p">)</code><code class="o">*</code>\
        <code class="n">batch_size</code><code class="p">,</code> <code class="n">tf</code><code class="o">.</code><code class="n">int32</code><code class="p">))</code>
        <code class="n">CVaR</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">reduce_mean</code><code class="p">(</code><code class="n">CVaR</code><code class="p">)</code>
        <code class="c">#7. Minimize the CVaR</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">train</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">train</code><code class="o">.</code><code class="n">AdamOptimizer</code><code class="p">()</code><code class="o">.</code><code class="n">minimize</code><code class="p">(</code><code class="n">CVaR</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">saver</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">train</code><code class="o">.</code><code class="n">Saver</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">modelname</code> <code class="o">=</code> <code class="n">name</code></pre>

<p>The function <code>execute_graph_batchwise</code> is the key function of the program, in which we train the neural network based on the observed experience. It takes a batch of the states as input and updates the policy gradient–based LSTM model weights by minimizing CVaR. This function trains the LSTM model to predict a hedging strategy by looping across the epochs and batches. First, it prepares a batch of market variables (stock price, strike, and risk aversion) and uses <code>sess.run</code> function for training. This <code>sess.run</code> is a TensorFlow function to run any operation defined within it. Here, it takes the inputs and runs the <code>tf.train</code> function that was defined in the constructor. After a sufficient number of iterations, an optimal policy gradient model is generated:</p>

<pre data-type="programlisting" data-code-language="ipython3">    <code class="k">def</code> <code class="nf">_execute_graph_batchwise</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">paths</code><code class="p">,</code> <code class="n">strikes</code><code class="p">,</code> <code class="n">riskaversion</code><code class="p">,</code> <code class="n">sess</code><code class="p">,</code> \
      <code class="n">epochs</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">train_flag</code><code class="o">=</code><code class="k">False</code><code class="p">):</code>
        <code class="c">#1: Initialize the variables.</code>
        <code class="n">sample_size</code> <code class="o">=</code> <code class="n">paths</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code>
        <code class="n">batch_size</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">batch_size</code>
        <code class="n">idx</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">arange</code><code class="p">(</code><code class="n">sample_size</code><code class="p">)</code>
        <code class="n">start</code> <code class="o">=</code> <code class="n">dt</code><code class="o">.</code><code class="n">datetime</code><code class="o">.</code><code class="n">now</code><code class="p">()</code>
        <code class="c">#2:Loop across all the epochs</code>
        <code class="k">for</code> <code class="n">epoch</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">epochs</code><code class="p">):</code>
            <code class="c"># Save the hedging Pnl for each batch</code>
            <code class="n">pnls</code> <code class="o">=</code> <code class="p">[]</code>
            <code class="n">strategies</code> <code class="o">=</code> <code class="p">[]</code>
            <code class="k">if</code> <code class="n">train_flag</code><code class="p">:</code>
                <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">shuffle</code><code class="p">(</code><code class="n">idx</code><code class="p">)</code>
            <code class="c">#3. Loop across the observations</code>
            <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="nb">int</code><code class="p">(</code><code class="n">sample_size</code><code class="o">/</code><code class="n">batch_size</code><code class="p">)):</code>
                <code class="n">indices</code> <code class="o">=</code> <code class="n">idx</code><code class="p">[</code><code class="n">i</code><code class="o">*</code><code class="n">batch_size</code> <code class="p">:</code> <code class="p">(</code><code class="n">i</code><code class="o">+</code><code class="mi">1</code><code class="p">)</code><code class="o">*</code><code class="n">batch_size</code><code class="p">]</code>
                <code class="n">batch</code> <code class="o">=</code> <code class="n">paths</code><code class="p">[:,</code><code class="n">indices</code><code class="p">,:]</code>

                <code class="c">#4. Train the LSTM</code>
                <code class="k">if</code> <code class="n">train_flag</code><code class="p">:</code><code class="c">#runs the train, hedging PnL and strategy.</code>
                    <code class="n">_</code><code class="p">,</code> <code class="n">pnl</code><code class="p">,</code> <code class="n">strategy</code> <code class="o">=</code> <code class="n">sess</code><code class="o">.</code><code class="n">run</code><code class="p">([</code><code class="bp">self</code><code class="o">.</code><code class="n">train</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">Hedging_PnL</code><code class="p">,</code> \
                      <code class="bp">self</code><code class="o">.</code><code class="n">strategy</code><code class="p">],</code> <code class="p">{</code><code class="bp">self</code><code class="o">.</code><code class="n">S_t_input</code><code class="p">:</code> <code class="n">batch</code><code class="p">,</code>\
                        <code class="bp">self</code><code class="o">.</code><code class="n">K</code> <code class="p">:</code> <code class="n">strikes</code><code class="p">[</code><code class="n">indices</code><code class="p">],</code>\
                        <code class="bp">self</code><code class="o">.</code><code class="n">alpha</code><code class="p">:</code> <code class="n">riskaversion</code><code class="p">})</code>
                        <code class="c">#5. Evaluation and no training</code>
                <code class="k">else</code><code class="p">:</code>
                    <code class="n">pnl</code><code class="p">,</code> <code class="n">strategy</code> <code class="o">=</code> <code class="n">sess</code><code class="o">.</code><code class="n">run</code><code class="p">([</code><code class="bp">self</code><code class="o">.</code><code class="n">Hedging_PnL</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">strategy</code><code class="p">],</code> \
                      <code class="p">{</code><code class="bp">self</code><code class="o">.</code><code class="n">S_t_input</code><code class="p">:</code> <code class="n">batch</code><code class="p">,</code>\
                      <code class="bp">self</code><code class="o">.</code><code class="n">K</code> <code class="p">:</code> <code class="n">strikes</code><code class="p">[</code><code class="n">indices</code><code class="p">],</code>
                      <code class="bp">self</code><code class="o">.</code><code class="n">alpha</code><code class="p">:</code> <code class="n">riskaversion</code><code class="p">})</code>\

                <code class="n">pnls</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">pnl</code><code class="p">)</code>
                <code class="n">strategies</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">strategy</code><code class="p">)</code>
            <code class="c">#6. Calculate the option price # given the risk aversion level alpha</code>

            <code class="n">CVaR</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="o">-</code><code class="n">np</code><code class="o">.</code><code class="n">sort</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">concatenate</code><code class="p">(</code><code class="n">pnls</code><code class="p">))</code>\
            <code class="p">[:</code><code class="nb">int</code><code class="p">((</code><code class="mi">1</code><code class="o">-</code><code class="n">riskaversion</code><code class="p">)</code><code class="o">*</code><code class="n">sample_size</code><code class="p">)])</code>
            <code class="c">#7. Return training metrics, \</code>
            <code class="c">#if it is in the training phase</code>
            <code class="k">if</code> <code class="n">train_flag</code><code class="p">:</code>
                <code class="k">if</code> <code class="n">epoch</code> <code class="o">%</code> <code class="mi">10</code> <code class="o">==</code> <code class="mi">0</code><code class="p">:</code>
                    <code class="nb">print</code><code class="p">(</code><code class="s">'Time elapsed:'</code><code class="p">,</code> <code class="n">dt</code><code class="o">.</code><code class="n">datetime</code><code class="o">.</code><code class="n">now</code><code class="p">()</code><code class="o">-</code><code class="n">start</code><code class="p">)</code>
                    <code class="nb">print</code><code class="p">(</code><code class="s">'Epoch'</code><code class="p">,</code> <code class="n">epoch</code><code class="p">,</code> <code class="s">'CVaR'</code><code class="p">,</code> <code class="n">CVaR</code><code class="p">)</code>
                    <code class="c">#Saving the model</code>
                    <code class="bp">self</code><code class="o">.</code><code class="n">saver</code><code class="o">.</code><code class="n">save</code><code class="p">(</code><code class="n">sess</code><code class="p">,</code> <code class="s">"model.ckpt"</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">saver</code><code class="o">.</code><code class="n">save</code><code class="p">(</code><code class="n">sess</code><code class="p">,</code> <code class="s">"model.ckpt"</code><code class="p">)</code>

        <code class="c">#8. return CVaR and other parameters</code>
        <code class="k">return</code> <code class="n">CVaR</code><code class="p">,</code> <code class="n">np</code><code class="o">.</code><code class="n">concatenate</code><code class="p">(</code><code class="n">pnls</code><code class="p">),</code> <code class="n">np</code><code class="o">.</code><code class="n">concatenate</code><code class="p">(</code><code class="n">strategies</code><code class="p">,</code><code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code></pre>

<p>The <code>training</code> function simply triggers the <code>execute_graph_batchwise</code> function and provides all the inputs required for training to this function. The <code>predict</code> function returns the action (hedging strategy) given a state (market variables). The <code>restore</code> function restores the saved trained model, to be used further for training or 
<span class="keep-together">prediction</span>:<a data-type="indexterm" data-startref="ix_Chapter9-asciidoc19" id="idm45174903628792"/><a data-type="indexterm" data-startref="ix_Chapter9-asciidoc18" id="idm45174903628056"/></p>

<pre data-type="programlisting" data-code-language="ipython3">    <code class="k">def</code> <code class="nf">training</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">paths</code><code class="p">,</code> <code class="n">strikes</code><code class="p">,</code> <code class="n">riskaversion</code><code class="p">,</code> <code class="n">epochs</code><code class="p">,</code> <code class="n">session</code><code class="p">,</code> <code class="n">init</code><code class="o">=</code><code class="k">True</code><code class="p">):</code>
        <code class="k">if</code> <code class="n">init</code><code class="p">:</code>
            <code class="n">sess</code><code class="o">.</code><code class="n">run</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">global_variables_initializer</code><code class="p">())</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">_execute_graph_batchwise</code><code class="p">(</code><code class="n">paths</code><code class="p">,</code> <code class="n">strikes</code><code class="p">,</code> <code class="n">riskaversion</code><code class="p">,</code> <code class="n">session</code><code class="p">,</code> \
          <code class="n">epochs</code><code class="p">,</code> <code class="n">train_flag</code><code class="o">=</code><code class="k">True</code><code class="p">)</code>


    <code class="k">def</code> <code class="nf">predict</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">paths</code><code class="p">,</code> <code class="n">strikes</code><code class="p">,</code> <code class="n">riskaversion</code><code class="p">,</code> <code class="n">session</code><code class="p">):</code>
        <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">_execute_graph_batchwise</code><code class="p">(</code><code class="n">paths</code><code class="p">,</code> <code class="n">strikes</code><code class="p">,</code> <code class="n">riskaversion</code><code class="p">,</code>\
          <code class="n">session</code><code class="p">,</code><code class="mi">1</code><code class="p">,</code> <code class="n">train_flag</code><code class="o">=</code><code class="k">False</code><code class="p">)</code>

    <code class="k">def</code> <code class="nf">restore</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">session</code><code class="p">,</code> <code class="n">checkpoint</code><code class="p">):</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">saver</code><code class="o">.</code><code class="n">restore</code><code class="p">(</code><code class="n">session</code><code class="p">,</code> <code class="n">checkpoint</code><code class="p">)</code></pre>
</div></section>













<section data-type="sect4" data-pdf-bookmark="4.2. Training the data"><div class="sect4" id="idm45174904031640">
<h4>4.2. Training the data</h4>

<p><a data-type="indexterm" data-primary="derivatives hedging" data-secondary="training the data" id="idm45174903096392"/>The steps of training our policy-based model are:</p>
<ol>
<li>
<p>Define the risk aversion parameter for CVaR, number of features (this is total number of stocks, and in this case we just have one), strike price, and batch size. The CVaR represents the amount of loss we want to minimize. For example, a CVaR of 99% means that we want to avoid extreme loss, while a CVaR of 50% minimizes average loss. We train with a CVaR of 50% to have smaller mean loss.</p>
</li>
<li>
<p>Instantiate the policy gradient agent, which has the RNN based-policy with the loss function based on the CVaR.</p>
</li>
<li>
<p>Iterate through the batches; the strategy is defined by the policy output of the LSTM-based network.</p>
</li>
<li>
<p>Finally, the trained model is saved.<a data-type="indexterm" data-startref="ix_Chapter9-asciidoc17" id="idm45174903041288"/><a data-type="indexterm" data-startref="ix_Chapter9-asciidoc16" id="idm45174903040584"/></p>
</li>

</ol>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">batch_size</code> <code class="o">=</code> <code class="mi">1000</code>
<code class="n">features</code> <code class="o">=</code> <code class="mi">1</code>
<code class="n">K</code> <code class="o">=</code> <code class="mi">100</code>
<code class="n">alpha</code> <code class="o">=</code> <code class="mf">0.50</code> <code class="c">#risk aversion parameter for CVaR</code>
<code class="n">epoch</code> <code class="o">=</code> <code class="mi">101</code> <code class="c">#It is set to 11, but should ideally be a high number</code>
<code class="n">model_1</code> <code class="o">=</code> <code class="n">Agent</code><code class="p">(</code><code class="n">paths_train</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="n">batch_size</code><code class="p">,</code> <code class="n">features</code><code class="p">,</code> <code class="n">name</code><code class="o">=</code><code class="s">'rnn_final'</code><code class="p">)</code>
<code class="c"># Training the model takes a few minutes</code>
<code class="n">start</code> <code class="o">=</code> <code class="n">dt</code><code class="o">.</code><code class="n">datetime</code><code class="o">.</code><code class="n">now</code><code class="p">()</code>
<code class="k">with</code> <code class="n">tf</code><code class="o">.</code><code class="n">Session</code><code class="p">()</code> <code class="k">as</code> <code class="n">sess</code><code class="p">:</code>
    <code class="c"># Train Model</code>
    <code class="n">model_1</code><code class="o">.</code><code class="n">training</code><code class="p">(</code><code class="n">paths_train</code><code class="p">,</code> <code class="n">np</code><code class="o">.</code><code class="n">ones</code><code class="p">(</code><code class="n">paths_train</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">])</code><code class="o">*</code><code class="n">K</code><code class="p">,</code> <code class="n">alpha</code><code class="p">,</code>\
     <code class="n">epoch</code><code class="p">,</code> <code class="n">sess</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="s">'Training finished, Time elapsed:'</code><code class="p">,</code> <code class="n">dt</code><code class="o">.</code><code class="n">datetime</code><code class="o">.</code><code class="n">now</code><code class="p">()</code><code class="o">-</code><code class="n">start</code><code class="p">)</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">Time elapsed: 0:00:03.326560
Epoch 0 CVaR 4.0718956
Epoch 100 CVaR 2.853285
Training finished, Time elapsed: 0:01:56.299444</pre>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="5. Testing the data"><div class="sect3" id="idm45174902995240">
<h3>5. Testing the data</h3>

<p><a data-type="indexterm" data-primary="derivatives hedging" data-secondary="testing the data" id="ix_Chapter9-asciidoc20"/>Testing is an important step, especially for RL, as it is difficult for a model to provide any meaningful, intuitive relationships between input and their corresponding output that is easily understood. In the testing step, we will compare the effectiveness of the hedging strategy and compare it to a delta hedging strategy based on the <a data-type="indexterm" data-primary="Black–Scholes model" data-secondary="reinforcement learning-based derivatives hedging compared to" id="ix_Chapter9-asciidoc21"/>Black-Scholes model. We first define the helper functions used in this step.</p>












<section data-type="sect4" data-pdf-bookmark="5.1. Helper functions for comparison against Black-Scholes"><div class="sect4" id="idm45174902990952">
<h4>5.1. Helper functions for comparison against Black-Scholes</h4>

<p>In this module, we create additional functions that are used for comparison against the traditional Black-Scholes model.</p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="5.1.1. Black-Scholes price and delta"><div class="sect4" id="idm45174902989288">
<h4>5.1.1. Black-Scholes price and delta</h4>

<p>The function <code>BlackScholes_price</code> implements the analytical formula for the call option price, and <code>BS_delta</code> implements the analytical formula for the delta of a call option:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="k">def</code> <code class="nf">BS_d1</code><code class="p">(</code><code class="n">S</code><code class="p">,</code> <code class="n">dt</code><code class="p">,</code> <code class="n">r</code><code class="p">,</code> <code class="n">sigma</code><code class="p">,</code> <code class="n">K</code><code class="p">):</code>
    <code class="k">return</code> <code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">log</code><code class="p">(</code><code class="n">S</code><code class="o">/</code><code class="n">K</code><code class="p">)</code> <code class="o">+</code> <code class="p">(</code><code class="n">r</code><code class="o">+</code><code class="n">sigma</code><code class="o">**</code><code class="mi">2</code><code class="o">/</code><code class="mi">2</code><code class="p">)</code><code class="o">*</code><code class="n">dt</code><code class="p">)</code> <code class="o">/</code> <code class="p">(</code><code class="n">sigma</code><code class="o">*</code><code class="n">np</code><code class="o">.</code><code class="n">sqrt</code><code class="p">(</code><code class="n">dt</code><code class="p">))</code>

<code class="k">def</code> <code class="nf">BlackScholes_price</code><code class="p">(</code><code class="n">S</code><code class="p">,</code> <code class="n">T</code><code class="p">,</code> <code class="n">r</code><code class="p">,</code> <code class="n">sigma</code><code class="p">,</code> <code class="n">K</code><code class="p">,</code> <code class="n">t</code><code class="o">=</code><code class="mi">0</code><code class="p">):</code>
    <code class="n">dt</code> <code class="o">=</code> <code class="n">T</code><code class="o">-</code><code class="n">t</code>
    <code class="n">Phi</code> <code class="o">=</code> <code class="n">stats</code><code class="o">.</code><code class="n">norm</code><code class="p">(</code><code class="n">loc</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">scale</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code><code class="o">.</code><code class="n">cdf</code>
    <code class="n">d1</code> <code class="o">=</code> <code class="n">BS_d1</code><code class="p">(</code><code class="n">S</code><code class="p">,</code> <code class="n">dt</code><code class="p">,</code> <code class="n">r</code><code class="p">,</code> <code class="n">sigma</code><code class="p">,</code> <code class="n">K</code><code class="p">)</code>
    <code class="n">d2</code> <code class="o">=</code> <code class="n">d1</code> <code class="o">-</code> <code class="n">sigma</code><code class="o">*</code><code class="n">np</code><code class="o">.</code><code class="n">sqrt</code><code class="p">(</code><code class="n">dt</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">S</code><code class="o">*</code><code class="n">Phi</code><code class="p">(</code><code class="n">d1</code><code class="p">)</code> <code class="o">-</code> <code class="n">K</code><code class="o">*</code><code class="n">np</code><code class="o">.</code><code class="n">exp</code><code class="p">(</code><code class="o">-</code><code class="n">r</code><code class="o">*</code><code class="n">dt</code><code class="p">)</code><code class="o">*</code><code class="n">Phi</code><code class="p">(</code><code class="n">d2</code><code class="p">)</code>

<code class="k">def</code> <code class="nf">BS_delta</code><code class="p">(</code><code class="n">S</code><code class="p">,</code> <code class="n">T</code><code class="p">,</code> <code class="n">r</code><code class="p">,</code> <code class="n">sigma</code><code class="p">,</code> <code class="n">K</code><code class="p">,</code> <code class="n">t</code><code class="o">=</code><code class="mi">0</code><code class="p">):</code>
    <code class="n">dt</code> <code class="o">=</code> <code class="n">T</code><code class="o">-</code><code class="n">t</code>
    <code class="n">d1</code> <code class="o">=</code> <code class="n">BS_d1</code><code class="p">(</code><code class="n">S</code><code class="p">,</code> <code class="n">dt</code><code class="p">,</code> <code class="n">r</code><code class="p">,</code> <code class="n">sigma</code><code class="p">,</code> <code class="n">K</code><code class="p">)</code>
    <code class="n">Phi</code> <code class="o">=</code> <code class="n">stats</code><code class="o">.</code><code class="n">norm</code><code class="p">(</code><code class="n">loc</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">scale</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code><code class="o">.</code><code class="n">cdf</code>
    <code class="k">return</code> <code class="n">Phi</code><code class="p">(</code><code class="n">d1</code><code class="p">)</code></pre>
</div></section>













<section data-type="sect4" data-pdf-bookmark="5.1.2. Test results and plotting"><div class="sect4" id="idm45174902984360">
<h4>5.1.2. Test results and plotting</h4>

<p>The following functions are used to compute the key metrics and related plots for evaluating the effectiveness of the hedge. The function <code>test_hedging_strategy</code> computes different types of PnL, including CVaR, PnL, and Hedge PnL. The function <code>plot_deltas</code> plots the comparison of the RL delta versus Black-Scholes hedging at different time points. The function <code>plot_strategy_pnl</code> is used to plot the total PnL of the RL-based strategy versus Black-Scholes hedging:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="k">def</code> <code class="nf">test_hedging_strategy</code><code class="p">(</code><code class="n">deltas</code><code class="p">,</code> <code class="n">paths</code><code class="p">,</code> <code class="n">K</code><code class="p">,</code> <code class="n">price</code><code class="p">,</code> <code class="n">alpha</code><code class="p">,</code> <code class="n">output</code><code class="o">=</code><code class="k">True</code><code class="p">):</code>
    <code class="n">S_returns</code> <code class="o">=</code> <code class="n">paths</code><code class="p">[</code><code class="mi">1</code><code class="p">:,:,</code><code class="mi">0</code><code class="p">]</code><code class="o">-</code><code class="n">paths</code><code class="p">[:</code><code class="o">-</code><code class="mi">1</code><code class="p">,:,</code><code class="mi">0</code><code class="p">]</code>
    <code class="n">hedge_pnl</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">deltas</code> <code class="o">*</code> <code class="n">S_returns</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
    <code class="n">option_payoff</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">maximum</code><code class="p">(</code><code class="n">paths</code><code class="p">[</code><code class="o">-</code><code class="mi">1</code><code class="p">,:,</code><code class="mi">0</code><code class="p">]</code> <code class="o">-</code> <code class="n">K</code><code class="p">,</code> <code class="mi">0</code><code class="p">)</code>
    <code class="n">replication_portfolio_pnls</code> <code class="o">=</code> <code class="o">-</code><code class="n">option_payoff</code> <code class="o">+</code> <code class="n">hedge_pnl</code> <code class="o">+</code> <code class="n">price</code>
    <code class="n">mean_pnl</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">replication_portfolio_pnls</code><code class="p">)</code>
    <code class="n">cvar_pnl</code> <code class="o">=</code> <code class="o">-</code><code class="n">np</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">sort</code><code class="p">(</code><code class="n">replication_portfolio_pnls</code><code class="p">)</code>\
    <code class="p">[:</code><code class="nb">int</code><code class="p">((</code><code class="mi">1</code><code class="o">-</code><code class="n">alpha</code><code class="p">)</code><code class="o">*</code><code class="n">replication_portfolio_pnls</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">])])</code>
    <code class="k">if</code> <code class="n">output</code><code class="p">:</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">hist</code><code class="p">(</code><code class="n">replication_portfolio_pnls</code><code class="p">)</code>
        <code class="nb">print</code><code class="p">(</code><code class="s">'BS price at t0:'</code><code class="p">,</code> <code class="n">price</code><code class="p">)</code>
        <code class="nb">print</code><code class="p">(</code><code class="s">'Mean Hedging PnL:'</code><code class="p">,</code> <code class="n">mean_pnl</code><code class="p">)</code>
        <code class="nb">print</code><code class="p">(</code><code class="s">'CVaR Hedging PnL:'</code><code class="p">,</code> <code class="n">cvar_pnl</code><code class="p">)</code>
    <code class="k">return</code> <code class="p">(</code><code class="n">mean_pnl</code><code class="p">,</code> <code class="n">cvar_pnl</code><code class="p">,</code> <code class="n">hedge_pnl</code><code class="p">,</code> <code class="n">replication_portfolio_pnls</code><code class="p">,</code> <code class="n">deltas</code><code class="p">)</code>

<code class="k">def</code> <code class="nf">plot_deltas</code><code class="p">(</code><code class="n">paths</code><code class="p">,</code> <code class="n">deltas_bs</code><code class="p">,</code> <code class="n">deltas_rnn</code><code class="p">,</code> <code class="n">times</code><code class="o">=</code><code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">5</code><code class="p">,</code> <code class="mi">10</code><code class="p">,</code> <code class="mi">15</code><code class="p">,</code> <code class="mi">29</code><code class="p">]):</code>
    <code class="n">fig</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">figure</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code><code class="mi">6</code><code class="p">))</code>
    <code class="k">for</code> <code class="n">i</code><code class="p">,</code> <code class="n">t</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">times</code><code class="p">):</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">subplot</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code><code class="mi">3</code><code class="p">,</code><code class="n">i</code><code class="o">+</code><code class="mi">1</code><code class="p">)</code>
        <code class="n">xs</code> <code class="o">=</code>  <code class="n">paths</code><code class="p">[</code><code class="n">t</code><code class="p">,:,</code><code class="mi">0</code><code class="p">]</code>
        <code class="n">ys_bs</code> <code class="o">=</code> <code class="n">deltas_bs</code><code class="p">[</code><code class="n">t</code><code class="p">,:]</code>
        <code class="n">ys_rnn</code> <code class="o">=</code> <code class="n">deltas_rnn</code><code class="p">[</code><code class="n">t</code><code class="p">,:]</code>
        <code class="n">df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">([</code><code class="n">xs</code><code class="p">,</code> <code class="n">ys_bs</code><code class="p">,</code> <code class="n">ys_rnn</code><code class="p">])</code><code class="o">.</code><code class="n">T</code>

        <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">df</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="n">df</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code> <code class="n">df</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="n">df</code><code class="p">[</code><code class="mi">2</code><code class="p">],</code> <code class="n">linestyle</code><code class="o">=</code><code class="s">''</code><code class="p">,</code> <code class="n">marker</code><code class="o">=</code><code class="s">'x'</code> <code class="p">)</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">legend</code><code class="p">([</code><code class="s">'BS delta'</code><code class="p">,</code> <code class="s">'RNN Delta'</code><code class="p">])</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="s">'Delta at Time %i'</code> <code class="o">%</code> <code class="n">t</code><code class="p">)</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s">'Spot'</code><code class="p">)</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s">'$\Delta$'</code><code class="p">)</code>
    <code class="n">plt</code><code class="o">.</code><code class="n">tight_layout</code><code class="p">()</code>

<code class="k">def</code> <code class="nf">plot_strategy_pnl</code><code class="p">(</code><code class="n">portfolio_pnl_bs</code><code class="p">,</code> <code class="n">portfolio_pnl_rnn</code><code class="p">):</code>
    <code class="n">fig</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">figure</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code><code class="mi">6</code><code class="p">))</code>
    <code class="n">sns</code><code class="o">.</code><code class="n">boxplot</code><code class="p">(</code><code class="n">x</code><code class="o">=</code><code class="p">[</code><code class="s">'Black-Scholes'</code><code class="p">,</code> <code class="s">'RNN-LSTM-v1 '</code><code class="p">],</code> <code class="n">y</code><code class="o">=</code><code class="p">[</code><code class="n">portfolio_pnl_bs</code><code class="p">,</code> \
    <code class="n">portfolio_pnl_rnn</code><code class="p">])</code>
    <code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="s">'Compare PnL Replication Strategy'</code><code class="p">)</code>
    <code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s">'PnL'</code><code class="p">)</code></pre>
</div></section>













<section data-type="sect4" data-pdf-bookmark="5.1.3. Hedging error for Black-Scholes replication"><div class="sect4" id="idm45174902586712">
<h4>5.1.3. Hedging error for Black-Scholes replication</h4>

<p>The following function is used to get the hedging strategy based on the traditional Black-Scholes model, which is further used for comparison against the RL-based hedging strategy:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="k">def</code> <code class="nf">black_scholes_hedge_strategy</code><code class="p">(</code><code class="n">S_0</code><code class="p">,</code> <code class="n">K</code><code class="p">,</code> <code class="n">r</code><code class="p">,</code> <code class="n">vol</code><code class="p">,</code> <code class="n">T</code><code class="p">,</code> <code class="n">paths</code><code class="p">,</code> <code class="n">alpha</code><code class="p">,</code> <code class="n">output</code><code class="p">):</code>
    <code class="n">bs_price</code> <code class="o">=</code> <code class="n">BlackScholes_price</code><code class="p">(</code><code class="n">S_0</code><code class="p">,</code> <code class="n">T</code><code class="p">,</code> <code class="n">r</code><code class="p">,</code> <code class="n">vol</code><code class="p">,</code> <code class="n">K</code><code class="p">,</code> <code class="mi">0</code><code class="p">)</code>
    <code class="n">times</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">zeros</code><code class="p">(</code><code class="n">paths</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">])</code>
    <code class="n">times</code><code class="p">[</code><code class="mi">1</code><code class="p">:]</code> <code class="o">=</code> <code class="n">T</code> <code class="o">/</code> <code class="p">(</code><code class="n">paths</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">-</code><code class="mi">1</code><code class="p">)</code>
    <code class="n">times</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">cumsum</code><code class="p">(</code><code class="n">times</code><code class="p">)</code>
    <code class="n">bs_deltas</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">zeros</code><code class="p">((</code><code class="n">paths</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="n">paths</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">]))</code>
    <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">paths</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">-</code><code class="mi">1</code><code class="p">):</code>
        <code class="n">t</code> <code class="o">=</code> <code class="n">times</code><code class="p">[</code><code class="n">i</code><code class="p">]</code>
        <code class="n">bs_deltas</code><code class="p">[</code><code class="n">i</code><code class="p">,:]</code> <code class="o">=</code> <code class="n">BS_delta</code><code class="p">(</code><code class="n">paths</code><code class="p">[</code><code class="n">i</code><code class="p">,:,</code><code class="mi">0</code><code class="p">],</code> <code class="n">T</code><code class="p">,</code> <code class="n">r</code><code class="p">,</code> <code class="n">vol</code><code class="p">,</code> <code class="n">K</code><code class="p">,</code> <code class="n">t</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">test_hedging_strategy</code><code class="p">(</code><code class="n">bs_deltas</code><code class="p">,</code> <code class="n">paths</code><code class="p">,</code> <code class="n">K</code><code class="p">,</code> <code class="n">bs_price</code><code class="p">,</code> <code class="n">alpha</code><code class="p">,</code> <code class="n">output</code><code class="p">)</code></pre>
</div></section>













<section data-type="sect4" data-pdf-bookmark="5.2. Comparison between Black-Scholes and reinforcement learning"><div class="sect4" id="idm45174902159752">
<h4>5.2. Comparison between Black-Scholes and reinforcement learning</h4>

<p>We will compare the effectiveness of the hedging strategy by looking at the influence of the CVaR risk aversion parameter and inspect how well the RL-based model can generalize the hedging strategy if we change the moneyness of the option, the drift, and the volatility of the underlying process.</p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="5.2.1. Test at 99% risk aversion"><div class="sect4" id="idm45174902158040">
<h4>5.2.1. Test at 99% risk aversion</h4>

<p>As mentioned before, the CVaR represents the amount of loss we want to minimize. We trained the model using a risk aversion of 50% to minimize average loss. However, for testing purposes we increase the risk aversion to 99%, meaning that we want to avoid extreme loss. These results are compared against the Black-Scholes model:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">n_sims_test</code> <code class="o">=</code> <code class="mi">1000</code>
<code class="c"># Monte Carlo Path for the test set</code>
<code class="n">alpha</code> <code class="o">=</code> <code class="mf">0.99</code>
<code class="n">paths_test</code> <code class="o">=</code>  <code class="n">monte_carlo_paths</code><code class="p">(</code><code class="n">S_0</code><code class="p">,</code> <code class="n">T</code><code class="p">,</code> <code class="n">vol</code><code class="p">,</code> <code class="n">r</code><code class="p">,</code> <code class="n">seed_test</code><code class="p">,</code> <code class="n">n_sims_test</code><code class="p">,</code> \
  <code class="n">timesteps</code><code class="p">)</code></pre>

<p>We use the trained function and compare the Black-Scholes and RL models in the following code:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="k">with</code> <code class="n">tf</code><code class="o">.</code><code class="n">Session</code><code class="p">()</code> <code class="k">as</code> <code class="n">sess</code><code class="p">:</code>
    <code class="n">model_1</code><code class="o">.</code><code class="n">restore</code><code class="p">(</code><code class="n">sess</code><code class="p">,</code> <code class="s">'model.ckpt'</code><code class="p">)</code>
    <code class="c">#Using the model_1 trained in the section above</code>
    <code class="n">test1_results</code> <code class="o">=</code> <code class="n">model_1</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">paths_test</code><code class="p">,</code> <code class="n">np</code><code class="o">.</code><code class="n">ones</code><code class="p">(</code><code class="n">paths_test</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">])</code><code class="o">*</code><code class="n">K</code><code class="p">,</code> \
    <code class="n">alpha</code><code class="p">,</code> <code class="n">sess</code><code class="p">)</code>

<code class="n">_</code><code class="p">,</code><code class="n">_</code><code class="p">,</code><code class="n">_</code><code class="p">,</code><code class="n">portfolio_pnl_bs</code><code class="p">,</code> <code class="n">deltas_bs</code> <code class="o">=</code> <code class="n">black_scholes_hedge_strategy</code>\
<code class="p">(</code><code class="n">S_0</code><code class="p">,</code><code class="n">K</code><code class="p">,</code> <code class="n">r</code><code class="p">,</code> <code class="n">vol</code><code class="p">,</code> <code class="n">T</code><code class="p">,</code> <code class="n">paths_test</code><code class="p">,</code> <code class="n">alpha</code><code class="p">,</code> <code class="k">True</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">figure</code><code class="p">()</code>
<code class="n">_</code><code class="p">,</code><code class="n">_</code><code class="p">,</code><code class="n">_</code><code class="p">,</code><code class="n">portfolio_pnl_rnn</code><code class="p">,</code> <code class="n">deltas_rnn</code> <code class="o">=</code> <code class="n">test_hedging_strategy</code>\
<code class="p">(</code><code class="n">test1_results</code><code class="p">[</code><code class="mi">2</code><code class="p">],</code> <code class="n">paths_test</code><code class="p">,</code> <code class="n">K</code><code class="p">,</code> <code class="mf">2.302974467802428</code><code class="p">,</code> <code class="n">alpha</code><code class="p">,</code> <code class="k">True</code><code class="p">)</code>
<code class="n">plot_deltas</code><code class="p">(</code><code class="n">paths_test</code><code class="p">,</code> <code class="n">deltas_bs</code><code class="p">,</code> <code class="n">deltas_rnn</code><code class="p">)</code>
<code class="n">plot_strategy_pnl</code><code class="p">(</code><code class="n">portfolio_pnl_bs</code><code class="p">,</code> <code class="n">portfolio_pnl_rnn</code><code class="p">)</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">BS price at t0: 2.3029744678024286
Mean Hedging PnL: -0.0010458505607415178
CVaR Hedging PnL: 1.2447953011695538
RL based BS price at t0: 2.302974467802428
RL based Mean Hedging PnL: -0.0019250998451393934
RL based CVaR Hedging PnL: 1.3832611348053374</pre>

<figure><div class="figure">
<img src="Images/mlbf_09in11.png" alt="mlbf 09in11" width="695" height="411"/>
<h6/>
</div></figure>

<figure><div class="figure">
<img src="Images/mlbf_09in12.png" alt="mlbf 09in12" width="595" height="357"/>
<h6/>
</div></figure>

<p>For the first test set (strike 100, same drift, same vol) with a risk aversion of 99%, the results look quite good. We see that the delta from both Black-Scholes and the RL-based approach converge over time from day 1 to 30. The CVaRs of both strategies are similar and lower in magnitude, with values of 1.24 and 1.38 for Black-Scholes and RL, respectively. Also, the volatility of the two strategies is similar, as illustrated in the second chart.</p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="5.2.2. Changing moneyness"><div class="sect4" id="idm45174902024584">
<h4>5.2.2. Changing moneyness</h4>

<p><a data-type="indexterm" data-primary="moneyness" id="idm45174901642296"/>Let us now look at the comparison of the strategies, when the moneyness, defined as the ratio of strike to spot price, is changed. In order to change the moneyness, we decrease the strike price by 10. The code snippet is similar to the previous case, and the output is shown below:</p>

<pre data-type="programlisting">BS price at t0: 10.07339936955367
Mean Hedging PnL: 0.0007508571761945107
CVaR Hedging PnL: 0.6977526775080665
RL based BS price at t0: 10.073
RL based Mean Hedging PnL: -0.038571546628968216
RL based CVaR Hedging PnL: 3.4732447615593975</pre>

<p>With the change in the moneyness, we see that the PnL of the RL strategy is significantly worse than that of the Black-Scholes strategy. We see a significant deviation of the delta between the two across all the days. The CVaR and the volatility of the RL-based strategy is much higher. The results indicate that we should be careful while generalizing the model to different levels of moneyness and should train the model with the option of using a variety of strikes before implementing it in a production environment.</p>

<figure><div class="figure">
<img src="Images/mlbf_09in13.png" alt="mlbf 09in13" width="695" height="411"/>
<h6/>
</div></figure>

<figure><div class="figure">
<img src="Images/mlbf_09in14.png" alt="mlbf 09in14" width="595" height="357"/>
<h6/>
</div></figure>
</div></section>













<section data-type="sect4" data-pdf-bookmark="5.2.3. Changing drift"><div class="sect4" id="idm45174901636024">
<h4>5.2.3. Changing drift</h4>

<p><a data-type="indexterm" data-primary="drift" id="idm45174901634744"/>Let us now look at the comparison of the strategies when the drift is changed. In order to change the drift, we assume the drift of the stock price is 4% per month, or 48% annualized. The output is shown below:</p>

<p class="pagebreak-before"><code>Output</code></p>

<pre data-type="programlisting">BS price at t0: 2.3029744678024286
Mean Hedging PnL: -0.01723902964827388
CVaR Hedging PnL: 1.2141220199385756
RL based BS price at t0: 2.3029
RL based Mean Hedging PnL: -0.037668804359885316
RL based CVaR Hedging PnL: 1.357201635552361</pre>

<figure><div class="figure">
<img src="Images/mlbf_09in15.png" alt="mlbf 09in15" width="697" height="411"/>
<h6/>
</div></figure>

<figure><div class="figure">
<img src="Images/mlbf_09in16.png" alt="mlbf 09in16" width="595" height="357"/>
<h6/>
</div></figure>

<p>The overall results look good for the change in drift. The conclusion is similar to results when the risk aversion was changed, with the deltas for the two approaches converging over time. Again, the CVaRs are similar in magnitude, with Black-Scholes producing a value of 1.21, and RL a value of 1.357.</p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="5.2.4. Shifted volatility"><div class="sect4" id="idm45174901627928">
<h4>5.2.4. Shifted volatility</h4>

<p>Finally, we look at the impact of shifting the volatility. In order to change the volatility, we increase it by 5%:</p>

<p><code>Output</code></p>

<pre data-type="programlisting">BS price at t0: 2.3029744678024286
Mean Hedging PnL: -0.5787493248269506
CVaR Hedging PnL: 2.5583922824407566
RL based BS price at t0: 2.309
RL based Mean Hedging PnL: -0.5735181045192523
RL based CVaR Hedging PnL: 2.835487824499669</pre>

<figure><div class="figure">
<img src="Images/mlbf_09in17.png" alt="mlbf 09in17" width="695" height="411"/>
<h6/>
</div></figure>

<p>Looking at the results, the delta, CVaR, and overall volatility of both models are similar. Hence looking at the different comparisons overall, the performance of this RL-based hedging is on par with Black-Scholes–based hedging<a data-type="indexterm" data-startref="ix_Chapter9-asciidoc21" id="idm45174901622520"/>.<a data-type="indexterm" data-startref="ix_Chapter9-asciidoc20" id="idm45174901621688"/></p>

<figure><div class="figure">
<img src="Images/mlbf_09in18.png" alt="mlbf 09in18" width="595" height="357"/>
<h6/>
</div></figure>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="Conclusion"><div class="sect3" id="idm45174902994648">
<h3>Conclusion</h3>

<p>In this case study, we compared the effectiveness of a call option hedging strategy using RL. The RL-based hedging strategy did quite well even when certain input parameters were modified. However, this strategy was not able to generalize the strategy for options at different moneyness levels. It underscores the fact that RL is a data-intensive approach, and it is important to train the model with different scenarios, which becomes more important if the model is intended to be used across a wide variety of derivatives.</p>

<p>Although we found the RL and traditional Black-Scholes strategies comparable, the RL approach offers a much higher ceiling for improvement. The RL model can be further trained using a wide variety of instruments with different hyperparameters, leading to performance enhancements. It would be interesting to explore the comparison of these two hedging models for more exotic derivatives, given the trade-off between these approaches.</p>

<p>Overall, the RL-based approach is model independent and scalable, and it offers efficiency boosts for many classical problems.<a data-type="indexterm" data-startref="ix_Chapter9-asciidoc14" id="idm45174901616104"/></p>
<img class="blueprint bottom" src="Images/bracket_bottom.png" width="1625" height="32"/>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" class="pagebreak-before less_space" data-pdf-bookmark="Case Study 3: Portfolio Allocation"><div class="sect1" id="CaseStudy3RL">
<h1>Case Study 3: Portfolio Allocation</h1>

<p><a data-type="indexterm" data-primary="portfolio allocation" id="ix_Chapter9-asciidoc22"/>As discussed in prior case studies, the most commonly used technique for portfolio allocation, <a data-type="indexterm" data-primary="mean-variance portfolio (MVP) optimization" data-secondary="weaknesses of" id="idm45174901611208"/><em>mean-variance portfolio optimization</em>, suffers from several weaknesses, including:</p>

<ul>
<li>
<p>Estimation errors in the expected returns and covariance matrix caused by the erratic nature of financial returns.</p>
</li>
<li>
<p>Unstable quadratic optimization that greatly jeopardizes the optimality of the resulting portfolios.</p>
</li>
</ul>

<p>We addressed some of these weaknesses in <a data-type="xref" href="ch07.xhtml#CaseStudy1DR">“Case Study 1: Portfolio Management: Finding an Eigen Portfolio”</a> in <a data-type="xref" href="ch07.xhtml#Chapter7">Chapter 7</a>, and in <a data-type="xref" href="ch08.xhtml#CaseStudy3CL">“Case Study 3: Hierarchical Risk Parity”</a> in <a data-type="xref" href="ch08.xhtml#Chapter8">Chapter 8</a>. Here, we approach this problem from an RL perspective.</p>

<p>Reinforcement learning algorithms, with the ability to decide the policy on their own, are strong models for performing portfolio allocation in an automated manner, without the need for continuous supervision. Automation of the manual steps involved in portfolio allocation can prove to be immensely useful, specifically for robo-advisors.</p>

<p>In an RL-based framework, we treat portfolio allocation not just as a one-step optimization problem but as <em>continuous control</em> of the portfolio with delayed rewards. We move from discrete optimal allocation to continuous control territory, and in the environment of a continuously changing market, RL algorithms can be leveraged to solve complex and dynamic portfolio allocation problems.</p>

<p>In this case study, we will use a Q-learning-based approach and DQN to come up with a policy for optimal portfolio allocation among a set of cryptocurrencies. Overall, the approach and framework in terms of the Python-based implementation is similar to that in case study 1. Therefore, some repetitive sections or code explanation is skipped in this case study.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45174901600760">
<h5/>
<p>In this case study, we will focus on:</p>

<ul>
<li>
<p>Defining the components of RL in a portfolio allocation problem.</p>
</li>
<li>
<p>Evaluating Q-learning in the context of portfolio allocation.</p>
</li>
<li>
<p>Creating a simulation environment to be used in the RL framework.</p>
</li>
<li>
<p>Extending the Q-learning framework used for trading strategy development to portfolio management.</p>
</li>
</ul>
</div></aside>
<img class="blueprint top" src="Images/bracket_top.png" width="1625" height="32"/>








<section data-type="sect2" class="blueprint" data-pdf-bookmark="Blueprint for Creating a Reinforcement Learning–Based Algorithm for Portfolio Allocation"><div class="sect2" id="idm45174901594488">
<h2>Blueprint for Creating a Reinforcement Learning–Based Algorithm for Portfolio Allocation</h2>










<section data-type="sect3" data-pdf-bookmark="1. Problem definition"><div class="sect3" id="idm45174901592856">
<h3>1. Problem definition</h3>

<p>In the reinforcement learning framework defined for this case study, the algorithm performs an action, which is <em>optimal portfolio allocation</em>, depending on the current state of the portfolio. The algorithm is trained using a deep Q-learning framework, and the components of the model are as follows:</p>
<dl>
<dt>Agent</dt>
<dd>
<p>A portfolio manager, a robo-advisor, or an individual investor.</p>
</dd>
<dt>Action</dt>
<dd>
<p>Assignment and rebalancing of the portfolio weights. The DQN model provides the Q-values, which are converted into portfolio weights.</p>
</dd>
<dt>Reward function</dt>
<dd>
<p>The Sharpe ratio. Although there can be a wide range of complex reward functions that provide a trade-off between profit and risk, such as percentage return or maximum drawdown.</p>
</dd>
<dt>State</dt>
<dd>
<p>The state is the correlation matrix of the instruments based on a specific time window. The correlation matrix is a suitable state variable for the portfolio allocation, as it contains the information about the relationships between different instruments and can be useful in performing portfolio allocation.</p>
</dd>
<dt>Environment</dt>
<dd>
<p>The cryptocurrency exchange.</p>
</dd>
</dl>

<p>The dataset used in this case study is from the <a href="https://oreil.ly/613O2">Kaggle</a> platform. It contains the daily prices of cryptocurrencies in 2018. The data contains some of the most liquid cryptocurrencies, including Bitcoin, Ethereum, Ripple, Litecoin, and Dash.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="2. Getting started—loading the data and Python packages"><div class="sect3" id="idm45174901581464">
<h3>2. Getting started—loading the data and Python packages</h3>












<section data-type="sect4" data-pdf-bookmark="2.1. Loading the Python packages"><div class="sect4" id="idm45174901580456">
<h4>2.1. Loading the Python packages</h4>

<p><a data-type="indexterm" data-primary="portfolio allocation" data-secondary="loading data and Python packages" id="idm45174901579288"/>The standard Python packages are loaded in this step. The details have already been presented in the previous case studies. Refer to the Jupyter notebook for this case study for more details.</p>
</div></section>













<section data-type="sect4" class="pagebreak-before less_space" data-pdf-bookmark="2.2. Loading the data"><div class="sect4" id="idm45174901577656">
<h4>2.2. Loading the data</h4>

<p>The fetched data is loaded in this step:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">dataset</code> <code class="o">=</code> <code class="n">read_csv</code><code class="p">(</code><code class="s">'data/crypto_portfolio.csv'</code><code class="p">,</code><code class="n">index_col</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code></pre>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="3. Exploratory data analysis"><div class="sect3" id="idm45174901569624">
<h3>3. Exploratory data analysis</h3>












<section data-type="sect4" data-pdf-bookmark="3.1. Descriptive statistics"><div class="sect4" id="idm45174901568744">
<h4>3.1. Descriptive statistics</h4>

<p><a data-type="indexterm" data-primary="portfolio allocation" data-secondary="exploratory data analysis" id="idm45174901567400"/>We will look at descriptive statistics and data visualizations of the data in this section:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c"># shape</code>
<code class="n">dataset</code><code class="o">.</code><code class="n">shape</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">(375, 15)</pre>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c"># peek at data</code>
<code class="n">set_option</code><code class="p">(</code><code class="s">'display.width'</code><code class="p">,</code> <code class="mi">100</code><code class="p">)</code>
<code class="n">dataset</code><code class="o">.</code><code class="n">head</code><code class="p">(</code><code class="mi">5</code><code class="p">)</code></pre>

<p><code>Output</code></p>

<figure><div class="figure">
<img src="Images/mlbf_09in19.png" alt="mlbf 09in19" width="1074" height="257"/>
<h6/>
</div></figure>

<p>The data has a total of 375 rows and 15 columns. These columns hold the daily prices of 15 different cryptocurrencies in 2018.</p>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="4. Evaluate algorithms and models"><div class="sect3" id="idm45174901523832">
<h3>4. Evaluate algorithms and models</h3>

<p><a data-type="indexterm" data-primary="portfolio allocation" data-secondary="evaluation of algorithms and models" id="ix_Chapter9-asciidoc23"/>This is the key step of the reinforcement learning model development, where we will define all the functions and classes and train the algorithm.</p>












<section data-type="sect4" data-pdf-bookmark="4.1. Agent and cryptocurrency environment script"><div class="sect4" id="idm45174901466760">
<h4>4.1. Agent and cryptocurrency environment script</h4>

<p><a data-type="indexterm" data-primary="portfolio allocation" data-secondary="agent and cryptocurrency environment script" id="idm45174901465384"/>We have an <code>Agent</code> class that holds the variables and member functions that perform the Q-learning. This is similar to the <code>Agent</code> class defined in case study 1, with an additional function to convert the Q-value output from the deep neural network to portfolio weights and vice versa. The training module implements iteration through several episodes and batches and saves the information of the state, action, reward, and next state to be used in training. We skip the detailed description of the Python code of <code>Agent</code> class and the training module in this case study. Readers can refer to the Jupyter notebook in the code repository for this book for more details.</p>

<p><a data-type="indexterm" data-primary="gym (simulation environment)" id="idm45174901462024"/><a data-type="indexterm" data-primary="simulation environment" id="idm45174901461160"/>We implement a simulation environment for cryptocurrencies using a class called <code>CryptoEnvironment</code>. The concept of a simulation environment, or <em>gym</em>, is quite common in RL problems. One of the challenges of reinforcement learning is the lack of available simulation environments on which to experiment. <a data-type="indexterm" data-primary="OpenAI gym" id="idm45174901459352"/><em>OpenAI gym</em> is a toolkit that provides a wide variety of simulated environments (e.g., Atari games, 2D/3D physical simulations), so we can train agents, compare them, or develop new RL algorithms. Additionally, it was developed with the aim of becoming a standardized environment and benchmark for RL research. We introduce a similar concept in the <code>CryptoEnvironment</code> class, where we create a simulation environment for cryptocurrencies. This class has the following key functions:</p>
<dl>
<dt><code>getState</code></dt>
<dd>
<p>This function returns the state as well as the historical return or raw historical data depending on the <code>is_cov_matrix</code> or <code>is_raw_time_series</code> flag</p>
</dd>
<dt><code>getReward</code></dt>
<dd>
<p>This function returns the reward (i.e., Sharpe ratio) of the portfolio, given the portfolio weights and lookback period</p>
</dd>
</dl>

<pre data-type="programlisting" data-code-language="ipython3"><code class="k">class</code> <code class="nc">CryptoEnvironment</code><code class="p">:</code>

    <code class="k">def</code> <code class="nf">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">prices</code> <code class="o">=</code> <code class="s">'./data/crypto_portfolio.csv'</code><code class="p">,</code> <code class="n">capital</code> <code class="o">=</code> <code class="mi">1</code><code class="n">e6</code><code class="p">):</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">prices</code> <code class="o">=</code> <code class="n">prices</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">capital</code> <code class="o">=</code> <code class="n">capital</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">data</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">load_data</code><code class="p">()</code>

    <code class="k">def</code> <code class="nf">load_data</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
        <code class="n">data</code> <code class="o">=</code>  <code class="n">pd</code><code class="o">.</code><code class="n">read_csv</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">prices</code><code class="p">)</code>
        <code class="k">try</code><code class="p">:</code>
            <code class="n">data</code><code class="o">.</code><code class="n">index</code> <code class="o">=</code> <code class="n">data</code><code class="p">[</code><code class="s">'Date'</code><code class="p">]</code>
            <code class="n">data</code> <code class="o">=</code> <code class="n">data</code><code class="o">.</code><code class="n">drop</code><code class="p">(</code><code class="n">columns</code> <code class="o">=</code> <code class="p">[</code><code class="s">'Date'</code><code class="p">])</code>
        <code class="k">except</code><code class="p">:</code>
            <code class="n">data</code><code class="o">.</code><code class="n">index</code> <code class="o">=</code> <code class="n">data</code><code class="p">[</code><code class="s">'date'</code><code class="p">]</code>
            <code class="n">data</code> <code class="o">=</code> <code class="n">data</code><code class="o">.</code><code class="n">drop</code><code class="p">(</code><code class="n">columns</code> <code class="o">=</code> <code class="p">[</code><code class="s">'date'</code><code class="p">])</code>
        <code class="k">return</code> <code class="n">data</code>

    <code class="k">def</code> <code class="nf">preprocess_state</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">state</code><code class="p">):</code>
        <code class="k">return</code> <code class="n">state</code>

    <code class="k">def</code> <code class="nf">get_state</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">t</code><code class="p">,</code> <code class="n">lookback</code><code class="p">,</code> <code class="n">is_cov_matrix</code><code class="o">=</code><code class="k">True</code>\
       <code class="n">is_raw_time_series</code><code class="o">=</code><code class="k">False</code><code class="p">):</code>

        <code class="k">assert</code> <code class="n">lookback</code> <code class="o">&lt;=</code> <code class="n">t</code>

        <code class="n">decision_making_state</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="n">t</code><code class="o">-</code><code class="n">lookback</code><code class="p">:</code><code class="n">t</code><code class="p">]</code>
        <code class="n">decision_making_state</code> <code class="o">=</code> <code class="n">decision_making_state</code><code class="o">.</code><code class="n">pct_change</code><code class="p">()</code><code class="o">.</code><code class="n">dropna</code><code class="p">()</code>

        <code class="k">if</code> <code class="n">is_cov_matrix</code><code class="p">:</code>
            <code class="n">x</code> <code class="o">=</code> <code class="n">decision_making_state</code><code class="o">.</code><code class="n">cov</code><code class="p">()</code>
            <code class="k">return</code> <code class="n">x</code>
        <code class="k">else</code><code class="p">:</code>
            <code class="k">if</code> <code class="n">is_raw_time_series</code><code class="p">:</code>
                <code class="n">decision_making_state</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="n">t</code><code class="o">-</code><code class="n">lookback</code><code class="p">:</code><code class="n">t</code><code class="p">]</code>
            <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">preprocess_state</code><code class="p">(</code><code class="n">decision_making_state</code><code class="p">)</code>

    <code class="k">def</code> <code class="nf">get_reward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">action</code><code class="p">,</code> <code class="n">action_t</code><code class="p">,</code> <code class="n">reward_t</code><code class="p">,</code> <code class="n">alpha</code> <code class="o">=</code> <code class="mf">0.01</code><code class="p">):</code>

        <code class="k">def</code> <code class="nf">local_portfolio</code><code class="p">(</code><code class="n">returns</code><code class="p">,</code> <code class="n">weights</code><code class="p">):</code>
            <code class="n">weights</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">weights</code><code class="p">)</code>
            <code class="n">rets</code> <code class="o">=</code> <code class="n">returns</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code> <code class="c"># * 252</code>
            <code class="n">covs</code> <code class="o">=</code> <code class="n">returns</code><code class="o">.</code><code class="n">cov</code><code class="p">()</code> <code class="c"># * 252</code>
            <code class="n">P_ret</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">rets</code> <code class="o">*</code> <code class="n">weights</code><code class="p">)</code>
            <code class="n">P_vol</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">sqrt</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">dot</code><code class="p">(</code><code class="n">weights</code><code class="o">.</code><code class="n">T</code><code class="p">,</code> <code class="n">np</code><code class="o">.</code><code class="n">dot</code><code class="p">(</code><code class="n">covs</code><code class="p">,</code> <code class="n">weights</code><code class="p">)))</code>
            <code class="n">P_sharpe</code> <code class="o">=</code> <code class="n">P_ret</code> <code class="o">/</code> <code class="n">P_vol</code>
            <code class="k">return</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="n">P_ret</code><code class="p">,</code> <code class="n">P_vol</code><code class="p">,</code> <code class="n">P_sharpe</code><code class="p">])</code>

        <code class="n">data_period</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">data</code><code class="p">[</code><code class="n">action_t</code><code class="p">:</code><code class="n">reward_t</code><code class="p">]</code>
        <code class="n">weights</code> <code class="o">=</code> <code class="n">action</code>
        <code class="n">returns</code> <code class="o">=</code> <code class="n">data_period</code><code class="o">.</code><code class="n">pct_change</code><code class="p">()</code><code class="o">.</code><code class="n">dropna</code><code class="p">()</code>

        <code class="n">sharpe</code> <code class="o">=</code> <code class="n">local_portfolio</code><code class="p">(</code><code class="n">returns</code><code class="p">,</code> <code class="n">weights</code><code class="p">)[</code><code class="o">-</code><code class="mi">1</code><code class="p">]</code>
        <code class="n">sharpe</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="n">sharpe</code><code class="p">]</code> <code class="o">*</code> <code class="nb">len</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">columns</code><code class="p">))</code>
        <code class="n">ret</code> <code class="o">=</code> <code class="p">(</code><code class="n">data_period</code><code class="o">.</code><code class="n">values</code><code class="p">[</code><code class="o">-</code><code class="mi">1</code><code class="p">]</code> <code class="o">-</code> <code class="n">data_period</code><code class="o">.</code><code class="n">values</code><code class="p">[</code><code class="mi">0</code><code class="p">])</code> <code class="o">/</code> \
        <code class="n">data_period</code><code class="o">.</code><code class="n">values</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>

        <code class="k">return</code> <code class="n">np</code><code class="o">.</code><code class="n">dot</code><code class="p">(</code><code class="n">returns</code><code class="p">,</code> <code class="n">weights</code><code class="p">),</code> <code class="n">ret</code></pre>

<p>Let’s explore the training of the RL model in the next step.</p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="4.3. Training the data"><div class="sect4" id="idm45174901431320">
<h4>4.3. Training the data</h4>

<p><a data-type="indexterm" data-primary="portfolio allocation" data-secondary="training the data" id="ix_Chapter9-asciidoc24"/>As a first step, we initialize the <code>Agent</code> class and <code>CryptoEnvironment</code> class. Then, we set the <code>number of episodes</code> and <code>batch size</code> for the training purpose. Given the volatility of cryptocurrencies, we set the state <code>window size</code> to 180 and <code>rebalancing frequency</code> to 90 days:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">N_ASSETS</code> <code class="o">=</code> <code class="mi">15</code>
<code class="n">agent</code> <code class="o">=</code> <code class="n">Agent</code><code class="p">(</code><code class="n">N_ASSETS</code><code class="p">)</code>
<code class="n">env</code> <code class="o">=</code> <code class="n">CryptoEnvironment</code><code class="p">()</code>
<code class="n">window_size</code> <code class="o">=</code> <code class="mi">180</code>
<code class="n">episode_count</code> <code class="o">=</code> <code class="mi">50</code>
<code class="n">batch_size</code> <code class="o">=</code> <code class="mi">32</code>
<code class="n">rebalance_period</code> <code class="o">=</code> <code class="mi">90</code></pre>

<p><a data-type="xref" href="#QLearnPort">Figure 9-10</a> provides a deep dive into the training of the DQN algorithm used for developing the RL-based portfolio allocation strategy. If we look carefully, the chart is similar to the steps defined in <a data-type="xref" href="#TraingStepsQTrd">Figure 9-8</a> in case study 1, with minor differences in the <em>Q-Matrix</em>, <em>reward function</em>, and <em>action</em>. Steps 1 to 7 describe the training and <code>CryptoEnvironment</code> module; steps 8 to 10 show what happens in the <code>replay buffer</code> function (i.e., <code>exeReplay</code> function) in the <code>Agent</code> module.</p>

<figure><div id="QLearnPort" class="figure">
<img src="Images/mlbf_0910.png" alt="mlbf 0910" width="1389" height="723"/>
<h6><span class="label">Figure 9-10. </span>DQN training for portfolio optimization</h6>
</div></figure>

<p>The details of steps 1 to 6 are:</p>
<ol>
<li>
<p>Get the <em>current state</em> using the helper function <code>getState</code> defined in the <code>CryptoEnvironment</code> module. It returns a correlation matrix of the cryptocurrencies based on the window size.</p>
</li>
<li>
<p>Get the <em>action</em> for the given state using the <code>act</code> function of the <code>Agent</code> class. The action is the weight of the cryptocurrency portfolio.</p>
</li>
<li>
<p>Get the <em>reward</em> for the given action using the <code>getReward</code> function in the <code>CryptoEnvironment</code> module.</p>
</li>
<li>
<p>Get the next state using the <code>getState</code> function. The detail of the next state is further used in the Bellman equation for updating the Q-function.</p>
</li>
<li>
<p>The details of the state, next state, and action are saved in the memory of the <code>Agent</code> object. This memory is used further by the <code>exeReply</code> function.</p>
</li>
<li>
<p>Check if the batch is complete. The size of a batch is defined by the batch size variable. If the batch is not complete, we move to the next time iteration. If the batch is complete, then we move to the <code>Replay buffer</code> function and update the Q-function by minimizing the MSE between the Q-predicted and the Q-target in steps 8, 9, and 10.</p>
</li>

</ol>

<p>As shown in the following charts, the code produces the final results along with two charts for each episode. The first chart shows the total cumulative return over time, while the second chart shows the percentage of each cryptocurrency in the portfolio.</p>

<p class="pagebreak-before"><code>Output</code></p>

<p><code>Episode 0/50 epsilon 1.0</code></p>

<figure><div class="figure">
<img src="Images/mlbf_09in20.png" alt="mlbf 09in20" width="692" height="124"/>
<h6/>
</div></figure>

<figure><div class="figure">
<img src="Images/mlbf_09in21.png" alt="mlbf 09in21" width="692" height="146"/>
<h6/>
</div></figure>

<p><code>Episode 1/50  epsilon 1.0</code></p>

<figure><div class="figure">
<img src="Images/mlbf_09in22.png" alt="mlbf 09in22" width="692" height="124"/>
<h6/>
</div></figure>

<figure><div class="figure">
<img src="Images/mlbf_09in23.png" alt="mlbf 09in23" width="692" height="146"/>
<h6/>
</div></figure>

<p><code>Episode 48/50 epsilon 1.0</code></p>

<figure><div class="figure">
<img src="Images/mlbf_09in24.png" alt="mlbf 09in24" width="701" height="124"/>
<h6/>
</div></figure>

<figure><div class="figure">
<img src="Images/mlbf_09in25.png" alt="mlbf 09in25" width="701" height="146"/>
<h6/>
</div></figure>

<p><code>Episode 49/50 epsilon 1.0</code></p>

<figure><div class="figure">
<img src="Images/mlbf_09in26.png" alt="mlbf 09in26" width="701" height="124"/>
<h6/>
</div></figure>

<figure><div class="figure">
<img src="Images/mlbf_09in27.png" alt="mlbf 09in27" width="692" height="146"/>
<h6/>
</div></figure>

<p>The charts outline the details of the portfolio allocation of the first two and last two episodes. The details of other episodes can be seen in the Jupyter notebook under the GitHub repository for this book. The black line shows the performance of the portfolio, and the dotted grey line shows the performance of the benchmark, which is an equally weighted portfolio of cryptocurrencies.</p>

<p>In the beginning of episodes zero and one, the agent has no preconception of the consequences of its actions, and it takes randomized actions to observe the returns, which are quite volatile. Episode zero shows a clear example of erratic performance behavior. Episode one displays more stable movement but ultimately underperforms the benchmark. This is evidence that the cumulative reward per episode fluctuates significantly in the beginning of training.</p>

<p>The last two charts of episodes 48 and 49 show the agent starting to learn from its training and discovering the optimal strategy. Overall returns are relatively stable and outperform the benchmark. However, the overall portfolio weights are still quite volatile due to the short time series and high volatility of the underlying cryptocurrency assets. Ideally, we would be able to increase the number of training episodes and the length of historical data to enhance the training performance<a data-type="indexterm" data-startref="ix_Chapter9-asciidoc24" id="idm45174900997272"/>.<a data-type="indexterm" data-startref="ix_Chapter9-asciidoc23" id="idm45174900996440"/></p>

<p class="pagebreak-before">Let us look at the testing results.</p>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="5. Testing the data"><div class="sect3" id="idm45174901430280">
<h3>5. Testing the data</h3>

<p><a data-type="indexterm" data-primary="portfolio allocation" data-secondary="testing the data" id="ix_Chapter9-asciidoc25"/>Recall that the black line shows the performance of the portfolio, and the dotted grey line is that of an equally weighted portfolio of cryptocurrencies:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">agent</code><code class="o">.</code><code class="n">is_eval</code> <code class="o">=</code> <code class="k">True</code>

<code class="n">actions_equal</code><code class="p">,</code> <code class="n">actions_rl</code> <code class="o">=</code> <code class="p">[],</code> <code class="p">[]</code>
<code class="n">result_equal</code><code class="p">,</code> <code class="n">result_rl</code> <code class="o">=</code> <code class="p">[],</code> <code class="p">[]</code>

<code class="k">for</code> <code class="n">t</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">window_size</code><code class="p">,</code> <code class="nb">len</code><code class="p">(</code><code class="n">env</code><code class="o">.</code><code class="n">data</code><code class="p">),</code> <code class="n">rebalance_period</code><code class="p">):</code>

    <code class="n">date1</code> <code class="o">=</code> <code class="n">t</code><code class="o">-</code><code class="n">rebalance_period</code>
    <code class="n">s_</code> <code class="o">=</code> <code class="n">env</code><code class="o">.</code><code class="n">get_state</code><code class="p">(</code><code class="n">t</code><code class="p">,</code> <code class="n">window_size</code><code class="p">)</code>
    <code class="n">action</code> <code class="o">=</code> <code class="n">agent</code><code class="o">.</code><code class="n">act</code><code class="p">(</code><code class="n">s_</code><code class="p">)</code>

    <code class="n">weighted_returns</code><code class="p">,</code> <code class="n">reward</code> <code class="o">=</code> <code class="n">env</code><code class="o">.</code><code class="n">get_reward</code><code class="p">(</code><code class="n">action</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="n">date1</code><code class="p">,</code> <code class="n">t</code><code class="p">)</code>
    <code class="n">weighted_returns_equal</code><code class="p">,</code> <code class="n">reward_equal</code> <code class="o">=</code> <code class="n">env</code><code class="o">.</code><code class="n">get_reward</code><code class="p">(</code>
        <code class="n">np</code><code class="o">.</code><code class="n">ones</code><code class="p">(</code><code class="n">agent</code><code class="o">.</code><code class="n">portfolio_size</code><code class="p">)</code> <code class="o">/</code> <code class="n">agent</code><code class="o">.</code><code class="n">portfolio_size</code><code class="p">,</code> <code class="n">date1</code><code class="p">,</code> <code class="n">t</code><code class="p">)</code>

    <code class="n">result_equal</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">weighted_returns_equal</code><code class="o">.</code><code class="n">tolist</code><code class="p">())</code>
    <code class="n">actions_equal</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">ones</code><code class="p">(</code><code class="n">agent</code><code class="o">.</code><code class="n">portfolio_size</code><code class="p">)</code> <code class="o">/</code> <code class="n">agent</code><code class="o">.</code><code class="n">portfolio_size</code><code class="p">)</code>

    <code class="n">result_rl</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">weighted_returns</code><code class="o">.</code><code class="n">tolist</code><code class="p">())</code>
    <code class="n">actions_rl</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">action</code><code class="p">[</code><code class="mi">0</code><code class="p">])</code>

<code class="n">result_equal_vis</code> <code class="o">=</code> <code class="p">[</code><code class="n">item</code> <code class="k">for</code> <code class="n">sublist</code> <code class="ow">in</code> <code class="n">result_equal</code> <code class="k">for</code> <code class="n">item</code> <code class="ow">in</code> <code class="n">sublist</code><code class="p">]</code>
<code class="n">result_rl_vis</code> <code class="o">=</code> <code class="p">[</code><code class="n">item</code> <code class="k">for</code> <code class="n">sublist</code> <code class="ow">in</code> <code class="n">result_rl</code> <code class="k">for</code> <code class="n">item</code> <code class="ow">in</code> <code class="n">sublist</code><code class="p">]</code>

<code class="n">plt</code><code class="o">.</code><code class="n">figure</code><code class="p">()</code>
<code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">result_equal_vis</code><code class="p">)</code><code class="o">.</code><code class="n">cumsum</code><code class="p">(),</code> <code class="n">label</code> <code class="o">=</code> <code class="s">'Benchmark'</code><code class="p">,</code> \
<code class="n">color</code> <code class="o">=</code> <code class="s">'grey'</code><code class="p">,</code><code class="n">ls</code> <code class="o">=</code> <code class="s">'--'</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">result_rl_vis</code><code class="p">)</code><code class="o">.</code><code class="n">cumsum</code><code class="p">(),</code> <code class="n">label</code> <code class="o">=</code> <code class="s">'Deep RL portfolio'</code><code class="p">,</code> \
<code class="n">color</code> <code class="o">=</code> <code class="s">'black'</code><code class="p">,</code><code class="n">ls</code> <code class="o">=</code> <code class="s">'-'</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s">'Time Period'</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s">'Cumulative Returnimage::images\Chapter9-b82b2.png[]'</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>

<p>Despite underperforming during the initial period, the model performance was better overall, primarily due to avoiding the steep decline that the benchmark portfolio experienced in the latter part of the test window. The returns appear very stable, perhaps due to rotating away from the most volatile cryptocurrencies.</p>

<p class="pagebreak-before"><code>Output</code></p>

<figure class="width-75"><div class="figure">
<img src="Images/mlbf_09in28.png" alt="mlbf 09in28" width="667" height="440"/>
<h6/>
</div></figure>

<p>Let us inspect the return, volatility, Sharpe ratio, alpha, and beta of the portfolio and benchmark:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">import</code> <code class="nn">statsmodels.api</code> <code class="k">as</code> <code class="nn">sm</code>
<code class="kn">from</code> <code class="nn">statsmodels</code> <code class="k">import</code> <code class="n">regression</code>
<code class="k">def</code> <code class="nf">sharpe</code><code class="p">(</code><code class="n">R</code><code class="p">):</code>
    <code class="n">r</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">diff</code><code class="p">(</code><code class="n">R</code><code class="p">)</code>
    <code class="n">sr</code> <code class="o">=</code> <code class="n">r</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code><code class="o">/</code><code class="n">r</code><code class="o">.</code><code class="n">std</code><code class="p">()</code> <code class="o">*</code> <code class="n">np</code><code class="o">.</code><code class="n">sqrt</code><code class="p">(</code><code class="mi">252</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">sr</code>

<code class="k">def</code> <code class="nf">print_stats</code><code class="p">(</code><code class="n">result</code><code class="p">,</code> <code class="n">benchmark</code><code class="p">):</code>

    <code class="n">sharpe_ratio</code> <code class="o">=</code> <code class="n">sharpe</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">result</code><code class="p">)</code><code class="o">.</code><code class="n">cumsum</code><code class="p">())</code>
    <code class="n">returns</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">result</code><code class="p">))</code>
    <code class="n">volatility</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">std</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">result</code><code class="p">))</code>

    <code class="n">X</code> <code class="o">=</code> <code class="n">benchmark</code>
    <code class="n">y</code> <code class="o">=</code> <code class="n">result</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">sm</code><code class="o">.</code><code class="n">add_constant</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
    <code class="n">model</code> <code class="o">=</code> <code class="n">regression</code><code class="o">.</code><code class="n">linear_model</code><code class="o">.</code><code class="n">OLS</code><code class="p">(</code><code class="n">y</code><code class="p">,</code> <code class="n">x</code><code class="p">)</code><code class="o">.</code><code class="n">fit</code><code class="p">()</code>
    <code class="n">alpha</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">params</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>
    <code class="n">beta</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">params</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code>

    <code class="k">return</code> <code class="n">np</code><code class="o">.</code><code class="n">round</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="n">returns</code><code class="p">,</code> <code class="n">volatility</code><code class="p">,</code> <code class="n">sharpe_ratio</code><code class="p">,</code> \
      <code class="n">alpha</code><code class="p">,</code> <code class="n">beta</code><code class="p">]),</code> <code class="mi">4</code><code class="p">)</code><code class="o">.</code><code class="n">tolist</code><code class="p">()</code></pre>

<pre data-type="programlisting" data-code-language="ipython3"><code class="nb">print</code><code class="p">(</code><code class="s">'EQUAL'</code><code class="p">,</code> <code class="n">print_stats</code><code class="p">(</code><code class="n">result_equal_vis</code><code class="p">,</code> <code class="n">result_equal_vis</code><code class="p">))</code>
<code class="nb">print</code><code class="p">(</code><code class="s">'RL AGENT'</code><code class="p">,</code> <code class="n">print_stats</code><code class="p">(</code><code class="n">result_rl_vis</code><code class="p">,</code> <code class="n">result_equal_vis</code><code class="p">))</code></pre>

<p class="pagebreak-before"><code>Output</code></p>

<pre data-type="programlisting">EQUAL [-0.0013, 0.0468, -0.5016, 0.0, 1.0]
RL AGENT [0.0004, 0.0231, 0.4445, 0.0002, -0.1202]</pre>

<p>Overall, the RL portfolio performs better across the board, with a higher return, higher Sharpe ratio, lower volatility, slight alpha, and negative correlation to the benchmark.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Conclusion"><div class="sect3" id="idm45174900994440">
<h3>Conclusion</h3>

<p>In this case study, we went beyond the classic efficient frontier for portfolio optimization and directly learned a policy of dynamically changing portfolio weights. We trained an RL-based model by setting up a standardized simulation environment. This approach facilitated the training process and can be explored further for general RL-based model training.</p>

<p>The trained RL-based model outperformed an equal-weight benchmark in the test set. The performance of the RL-based model can be further improved by optimizing the hyperparameters or using a longer time series for training. However, given the high complexity and low interpretability of an RL-based model, testing should occur across different time periods and market cycles before deploying the model for live trading. Also, as discussed in case study 1, we should carefully select the RL components, such as the reward function and state, and ensure we understand their impact on the overall model results<a data-type="indexterm" data-startref="ix_Chapter9-asciidoc25" id="idm45174900326184"/>.<a data-type="indexterm" data-startref="ix_Chapter9-asciidoc22" id="idm45174900325352"/></p>

<p>The framework provided in this case study can enable financial practitioners to perform portfolio allocation and rebalancing with a very flexible and automated approach.</p>
<img class="blueprint bottom" src="Images/bracket_bottom.png" width="1625" height="32"/>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Chapter Summary"><div class="sect1" id="idm45174901613880">
<h1>Chapter Summary</h1>

<p>Reward maximization is one of the key principles that drives algorithmic trading, portfolio management, derivative pricing, hedging, and trade execution. In this chapter, we saw that when we use RL-based approaches, explicitly defining the strategy or policy for trading, derivative hedging, or portfolio management is unnecessary. The algorithm determines the policy itself, which can lead to a much simpler and more principled approach than other machine learning techniques.</p>

<p>In <a data-type="xref" href="#CaseStudy1RL">“Case Study 1: Reinforcement Learning–Based Trading Strategy”</a>, we saw that RL makes algorithmic trading a simple game, which may or may not involve understanding fundamental information. In <a data-type="xref" href="#CaseStudy2RL">“Case Study 2: Derivatives Hedging”</a>, we explored the use of reinforcement learning for a traditional derivative hedging problem. This exercise demonstrated that we can leverage the efficient numerical calculation of RL in derivatives hedging to address some of the drawbacks of the more traditional models. In <a data-type="xref" href="#CaseStudy3RL">“Case Study 3: Portfolio Allocation”</a>, we performed portfolio allocation by learning a policy of changing portfolio weights dynamically in a continuously changing market environment, leading to further automation of the portfolio management process.</p>

<p>Although RL comes with some challenges, such as being computationally expensive and data intensive and lacking interpretability, it aligns perfectly with some areas in finance that are suited for policy frameworks based on reward maximization. Reinforcement learning has managed to achieve superhuman performance in finite action spaces, such as those in the games of Go, chess, and Atari. Looking ahead, with the availability of more data, refined RL algorithms, and superior infrastructure, RL will continue to prove to be immensely useful in finance.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Exercises"><div class="sect1" id="idm45174900316840">
<h1>Exercises</h1>

<ul>
<li>
<p>Using the ideas and concepts presented in case studies 1 and 2, implement a trading strategy based on a policy gradient algorithm for FX. Vary the key components (i.e., reward function, state, etc.) for this implementation.</p>
</li>
<li>
<p>Implement the hedging of a fixed income derivative using the concepts presented in case study 2.</p>
</li>
<li>
<p>Incorporate a transaction cost in case study 2 and see the impact on the overall results.</p>
</li>
<li>
<p>Based on the ideas presented in case study 3, implement a Q-learning-based portfolio allocation strategy on a portfolio of stocks, FX, or fixed income 
<span class="keep-together">instruments</span>.<a data-type="indexterm" data-startref="ix_Chapter9-asciidoc0" id="idm45174900310952"/></p>
</li>
</ul>
</div></section>







<div data-type="footnotes"><p data-type="footnote" id="idm45174907850488"><sup><a href="ch09.xhtml#idm45174907850488-marker">1</a></sup> Reinforcement learning is also referred to as RL throughout this chapter.</p><p data-type="footnote" id="idm45174907783768"><sup><a href="ch09.xhtml#idm45174907783768-marker">2</a></sup> For more details, be sure to check out <em>Reinforcement Learning: An Introduction</em> by Richard Sutton and Andrew Barto (MIT Press), or David Silver’s free online <a href="https://oreil.ly/niRu-">RL course at University College London</a>.</p><p data-type="footnote" id="idm45174907622280"><sup><a href="ch09.xhtml#idm45174907622280-marker">3</a></sup> See <a data-type="xref" href="#reinforcement_learning_models">“Reinforcement Learning Models”</a> for more details on model-based and model-free approaches.</p><p data-type="footnote" id="idm45174907605976"><sup><a href="ch09.xhtml#idm45174907605976-marker">4</a></sup> A maximum drawdown is the maximum observed loss from peak to trough of a portfolio before a new peak is attained; it is an indicator of downside risk over a specified time period.</p><p data-type="footnote" id="idm45174907045944"><sup><a href="ch09.xhtml#idm45174907045944-marker">5</a></sup> If the state and action spaces of MDP are finite, then it is called a finite Markov decision process.</p><p data-type="footnote" id="idm45174907036600"><sup><a href="ch09.xhtml#idm45174907036600-marker">6</a></sup> The MDP example based on dynamic programming that was discussed in the previous section was an example of a model-based algorithm. As seen there, example rewards and transition probabilities are needed for such algorithms.</p><p data-type="footnote" id="idm45174907028376"><sup><a href="ch09.xhtml#idm45174907028376-marker">7</a></sup> There are some models, such as the actor-critic model, that leverage both policy-based and value-based  <span class="keep-together">methods</span>.</p><p data-type="footnote" id="idm45174907004296"><sup><a href="ch09.xhtml#idm45174907004296-marker">8</a></sup> <em>Off-policy</em>, <em>ε-greedy</em>, <em>exploration</em>, and <em>exploitation</em> are commonly used terms in RL and will be used in other sections and case studies as well.</p><p data-type="footnote" id="idm45174906929688"><sup><a href="ch09.xhtml#idm45174906929688-marker">9</a></sup> Refer to <a data-type="xref" href="ch03.xhtml#Chapter3">Chapter 3</a> for more details on gradient descent.</p><p data-type="footnote" id="idm45174906688584"><sup><a href="ch09.xhtml#idm45174906688584-marker">10</a></sup> Refer to <a data-type="xref" href="ch03.xhtml#Chapter3">Chapter 3</a> for more details on the sigmoid function.</p><p data-type="footnote" id="idm45174906222520"><sup><a href="ch09.xhtml#idm45174906222520-marker">11</a></sup> The details of the Keras-based implementation of deep learning models are shown in <a data-type="xref" href="ch03.xhtml#Chapter3">Chapter 3</a>.</p><p data-type="footnote" id="idm45174906129432"><sup><a href="ch09.xhtml#idm45174906129432-marker">12</a></sup> Refer to <a data-type="xref" href="ch03.xhtml#Chapter3">Chapter 3</a> for more details on the linear and ReLU activation functions.</p><p data-type="footnote" id="idm45174904541224"><sup><a href="ch09.xhtml#idm45174904541224-marker">13</a></sup> The expected shortfall is the expected value of an investment in the tail scenario.</p></div></div></section></div>



  </body></html>