- en: Chapter 8\. Modeling Operational Risk
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第八章。建模操作风险
- en: '...It’s not necessarily the biggest missteps that deliver the biggest blows;
    share prices can plummet as a result of even the smallest events.'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '...并不一定是最大的错误行为造成最严重的打击；即使是最小的事件也可能导致股价暴跌。'
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Dunnett, Levy, and Simoes (2005)
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Dunnett, Levy, and Simoes (2005)
- en: 'Thus far, we have talked about three main financial risks: market, credit,
    and liquidity risks. Now it is time to discuss operational risk, which is more
    ambiguous than the other types of financial risks. This ambiguity arises from
    the huge variety of risk sources by which financial institutions may face huge
    losses.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了三种主要的财务风险：市场风险、信用风险和流动性风险。现在是时候讨论操作风险了，这种风险比其他类型的财务风险更加模糊不清。这种模糊性源于金融机构可能面临的风险来源种类繁多，从而可能导致巨大的损失。
- en: 'Operational risk is the risk of direct or indirect loss resulting from inadequate
    or failed interval processes, people, and systems or from external events (BIS
    2019). Please note that loss can be direct and/or indirect. Some direct losses
    would be:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 操作风险是由于内部流程、人员和系统的不足或失败，或外部事件导致的直接或间接损失的风险（BIS 2019）。请注意，损失可以是直接的和/或间接的。一些直接损失包括：
- en: Legal liability arising from judicial process
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自司法程序的法律责任
- en: Write-downs due to theft or reduction in assets
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于盗窃或资产减少而导致的减值
- en: Compliance emanating from tax, license, fines, etc.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 源自税收、许可、罚款等的合规性。
- en: Business interruption
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 业务中断
- en: Indirect cost is related to the opportunity cost in the way that a decision
    made by an institution may trigger a host of events resulting in a loss at an
    uncertain time in the future.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 间接成本与机会成本相关，机构的决策可能引发一系列事件，导致未来某个不确定时间的损失。
- en: Normally, financial institutions allocate a certain amount of money to cover
    the loss emanating from operational risk, which is known as *unexpected loss*.
    However, allocating an appropriate amount of funds to cover unexpected loss is
    not as easy as it sounds. It is necessary to determine the right amount of unexpected
    loss; otherwise, either more funds are devoted to it, which makes it idle and
    creates opportunity cost, or less than the required funds are allocated, resulting
    in a liquidity problem.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，金融机构会分配一定金额来覆盖源自操作风险的损失，即所谓的*意外损失*。然而，分配适当的资金以覆盖意外损失并非易事。必须确定正确的意外损失金额；否则，要么投入更多资金，导致资金闲置并产生机会成本，要么分配少于所需资金，导致流动性问题。
- en: As we briefly touched on earlier, operational risk can take on several forms.
    Among them, we’ll restrict our focus to the fraud risk, which is considered to
    be the most pervasive and disruptive type of operational risk.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前简要提到的，操作风险可以采取多种形式。其中，我们将重点放在欺诈风险上，这被认为是最普遍和最具破坏性的操作风险类型之一。
- en: Fraud may generally be characterized as an intentional act, misstatement, or
    omission designed to deceive others, resulting in the victim suffering a loss
    or the perpetrator achieving a gain (OCC 2019). A fraud can be an internal one
    if losses occurred from inside a financial institution or an external one if it
    is committed by a third party.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果损失是由金融机构内部引起的，则欺诈可能通常被描述为一种故意行为、误述或遗漏，旨在欺骗他人，导致受害者遭受损失或施行者获得利益（OCC 2019）。如果是由第三方实施的，则可以称为外部欺诈。
- en: 'What makes fraud a primary concern of financial institutions? What increases
    the likelihood of committing fraudulent activities? To address these questions,
    we can refer to three important factors:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么欺诈成为金融机构的主要关注点？什么会增加欺诈行为的可能性？为了回答这些问题，我们可以参考三个重要因素：
- en: Globalization
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全球化
- en: Lack of proper risk management
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺乏适当的风险管理
- en: Economic pressure
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经济压力
- en: Globalization led financial institutions to expand their operations across the
    world, and this came with a complexity that gave rise to a higher probability
    of corruption, bribery, and any kind of illegal act as financial institutions
    started operating in environments where they have no prior knowledge.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 全球化导致金融机构在全球范围内扩展其业务，这带来了复杂性，增加了腐败、贿赂和任何违法行为的可能性，因为金融机构开始在他们没有先前知识的环境中运营。
- en: Lack of proper risk management has been and is the most obvious reasons for
    fraud. Misleading reporting and rogue, unauthorized trading plants the seeds of
    fraudulent acts. A very well known example is the Barings case, in which Nick
    Leeson, a young trader at Barings, ran a speculative trading and subsequent cover-up
    operation using accounting tricks that cost Barings Bank a fortune, totaling $1.3
    billion. Thus, when there is a lack of well-defined risk management policies along
    with a well-established culture of risk, employees may tend to commit fraud.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 缺乏适当的风险管理一直以来都是欺诈的最明显原因。误导性报告和流氓、未经授权的交易为欺诈行为埋下了种子。巴林银行案例是一个非常著名的例子，尼克·利森（Barings）在此案中，一名年轻的巴林银行交易员运行了一个投机交易和随后的掩盖操作，使用会计把戏导致巴林银行损失了高达13亿美元的巨额财富。因此，当缺乏明确定义的风险管理政策以及良好建立的风险文化时，员工可能倾向于实施欺诈行为。
- en: Another motivation for fraud would be an employee’s worsening financial situation.
    Particularly during an economic downturn, employees might be tempted into fraudulent
    activities. In addition, financial institutions themselves might embrace illegal
    operations (such as accounting tricks) to find a way out of the downturn.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个导致欺诈行为的动机可能是员工经济状况恶化。特别是在经济衰退期间，员工可能会被诱使从事欺诈活动。此外，金融机构本身可能会采用非法操作（如会计把戏）来摆脱经济衰退的困境。
- en: Fraud does not only cause a considerable amount of loss, but it also poses a
    threat to a company’s reputation, which may in turn disrupt the long-term sustainability
    of the company. Take the case of Enron, a good example of accounting fraud, which
    broke out in 2001\. Enron was established in 1985 and became one of the biggest
    companies in the United States and the world. Let me briefly tell you the story
    of this big collapse.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 欺诈不仅会造成巨额损失，而且还会对公司的声誉构成威胁，这可能会进一步影响公司的长期可持续性。以安然公司为例，这是一个典型的会计欺诈案例，于2001年爆发。安然公司成立于1985年，曾经是美国和世界上最大的公司之一。让我简要告诉你这场大崩溃的故事。
- en: Due to the pressure that Enron faced in the energy market, executives were motivated
    to rely on dubious accounting practices, resulting in inflated profits from writing
    huge unrealized future gains. Thanks to whistleblower Sherron Watkins, who was
    the former vice president of corporate development, one of the biggest fraud cases
    in the history of modern finance came to light. This event also stresses the importance
    of preventing fraudulent activities, which otherwise might lead to huge damages
    to an individual’s or company’s reputation or financial collapse.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 由于安然公司在能源市场面临的压力，高管们被迫依赖可疑的会计实践，导致通过写入巨额未实现未来收益的方式产生的虚高利润。多亏了举报人雪伦·沃特金斯（Sherron
    Watkins），她曾是前企业发展副总裁，现代金融史上最大的欺诈案例之一得以曝光。此事件也强调了预防欺诈活动的重要性，否则可能导致个人或公司声誉或财务崩溃造成巨大损失。
- en: 'In this chapter, we aim to introduce an ML-based model to detect fraud or would-be
    fraud operations. This is and should be a constantly growing field to stay ahead
    of the perpetrators. Datasets related to fraud may come in two forms: *labeled*
    or *unlabeled data*. To take both into account, we first apply a supervised learning
    algorithm and then use an unsupervised learning algorithm pretending like we do
    not have labels, even though the dataset we’ll be using does include labels.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们旨在介绍一种基于机器学习的模型来检测欺诈或可能的欺诈操作。这是一个应不断发展的领域，以赶在作恶者之前。与欺诈相关的数据集可以分为*标记*或*未标记数据*。为了同时考虑两者，我们首先应用监督学习算法，然后使用无监督学习算法，假装我们没有标签，尽管我们将使用的数据集确实包含标签。
- en: The dataset we’ll use for our fraud analysis is known as the *Credit Card Transaction
    Fraud Detection Dataset* created by Brandon Harris. Credit card fraud is not a
    rare issue, and the goal is to detect the likelihood of fraud and inform the bank
    so that the bank can investigate the situation with due diligence. This is the
    way a bank protects itself from incurring huge losses. According to the Nilsen
    Report (2020), payment card fraud losses hit a record-high level of $32.04 billion,
    amounting to 6.8¢ for every $100 of total volume.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用于欺诈分析的数据集称为Brandon Harris创建的*信用卡交易欺诈检测数据集*。信用卡欺诈并不罕见，目标是检测欺诈可能性并通知银行，以便银行可以进行尽职调查。这是银行保护自身免受巨额损失的方式。根据尼尔森报告（2020年），支付卡欺诈损失达到创纪录的320.4亿美元，相当于每100美元总交易量的6.8美分。
- en: This dataset is a good example of a mix of attributes of variable types as we
    have continuous, discrete, and nominal data. You can find the data on [Kaggle](https://oreil.ly/fxxFg).
    An explanation of the data is provided in [Table 8-1](#att_exp).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集是各种属性类型混合的很好的例子，因为我们有连续、离散和名义数据。您可以在[Kaggle](https://oreil.ly/fxxFg)上找到这些数据。表8-1提供了数据的解释。
- en: Table 8-1\. Attributes and explanations
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 表8-1\. 属性及其解释
- en: '| Attribute | Explanation |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 属性 | 解释 |'
- en: '| --- | --- |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `trans_date_trans_time` | Date the transaction |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| `trans_date_trans_time` | 交易日期 |'
- en: '| `cc_num` | Credit card number of the customer |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| `cc_num` | 客户的信用卡号码 |'
- en: '| `merchant` | Merchant by whom the trade occurred |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| `merchant` | 进行交易的商家 |'
- en: '| `amt` | Amount of transaction |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| `amt` | 交易金额 |'
- en: '| `first` | First name of customer |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| `first` | 客户的名字 |'
- en: '| `last` | Last name of customer |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| `last` | 客户的姓氏 |'
- en: '| `gender` | Gender of the customer |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| `gender` | 客户的性别 |'
- en: '| `street, city, state` | Address of the customer |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| `street, city, state` | 客户的地址 |'
- en: '| `zip` | Zip code of the transaction |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| `zip` | 交易的邮政编码 |'
- en: '| `lat` | Latitude of the customer |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| `lat` | 客户的纬度 |'
- en: '| `long` | Longitude of the customer |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| `long` | 客户的经度 |'
- en: '| `city_pop` | Population of the city |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| `city_pop` | 城市的人口 |'
- en: '| `job` | Type of the customer’s profession |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| `job` | 客户职业的类型 |'
- en: '| `dob` | Date of birth of the customer |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| `dob` | 客户的出生日期 |'
- en: '| `trans_num` | Unique transaction number for each transaction |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| `trans_num` | 每笔交易的唯一交易号 |'
- en: '| `unix_time` | Time of the transaction in Unix |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| `unix_time` | 交易的Unix时间 |'
- en: '| `merch_lat` | Merchant latitude |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| `merch_lat` | 商家的纬度 |'
- en: '| `merch_long` | Merchant longitude |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| `merch_long` | 商家的经度 |'
- en: '| `is_fraud` | Whether the transaction is fraudulent or not |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| `is_fraud` | 交易是否欺诈 |'
- en: Getting Familiar with Fraud Data
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 熟悉欺诈数据
- en: As you probably noticed, ML algorithms work better if the number of observations
    among different classes are more or less equal to each other—that is, it works
    best with balanced data. We do not have balanced data in the fraud case, so this
    is called a *class imbalance*. In [Chapter 6](ch06.html#chapter_6), we learned
    how to handle class imbalance problems, and we’ll use this skill again in this
    chapter.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您可能注意到的，如果不同类别的观察数量相对均衡，机器学习算法的效果会更好——也就是说，它在平衡数据上运行得更好。在欺诈案例中我们没有平衡的数据，因此这被称为*类别不平衡*。在[第6章](ch06.html#chapter_6)中，我们学习了如何处理类别不平衡问题，我们将在本章再次使用这个技能。
- en: 'Let’s start off. To begin with, it makes sense to go through the data types
    of the variables in the Credit Card Transaction Fraud Detection Dataset:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。首先，查看信用卡交易欺诈检测数据集中变量的数据类型：
- en: '[PRE0]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'It turns out we have all types of data: object, integer, and float. However,
    the majority of the variables are of the object type, so additional analysis is
    required to turn these categorical variables into numerical ones.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，我们拥有所有类型的数据：对象、整数和浮点数。然而，大多数变量是对象类型，因此需要进一步分析将这些分类变量转换为数值变量。
- en: 'The dependent variable is of considerable importance in such an analysis, as
    it often has imbalance characteristics that require due attention. This is shown
    in the following snippet (and resultant [Figure 8-1](#pie_chart_fraud)), which
    indicates a highly disproportionate number of observations:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种分析中，因变量非常重要，因为它通常具有需要注意的不平衡特征。如下片段所示（及其结果图[Figure 8-1](#pie_chart_fraud)），表明观察数量极不均衡：
- en: '[PRE1]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![pie_chart_fraud](assets/mlfr_0801.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![pie_chart_fraud](assets/mlfr_0801.png)'
- en: Figure 8-1\. Pie chart for dependent variable
  id: totrans-56
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-1\. 因变量的饼图
- en: As we can see, the number of observations for the nonfraud case is 1,289,169,
    while there are only 7,506 for the fraud case, so we know that the data is highly
    imbalanced, as expected.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，非欺诈案例的观察数量为1,289,169，而欺诈案例仅有7,506，所以我们知道数据是高度不平衡的，这是预料之中的。
- en: 'At this point, we can use a rather different tool to detect the number of missing
    observations. This tool is known as `missingno`, and it also provides us with
    a visualization module for missing values (as can be seen in [Figure 8-2](#missingno_fraud)):'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们可以使用一个非常不同的工具来检测缺失观测数量。这个工具被称为`missingno`，它还为我们提供了一个用于缺失值的可视化模块（如[Figure 8-2](#missingno_fraud)所示）：
- en: '[PRE2]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[![1](assets/1.png)](#co_modeling_operational_risk_CO1-1)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_modeling_operational_risk_CO1-1)'
- en: Importing `missingno`
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 导入`missingno`
- en: '[![2](assets/2.png)](#co_modeling_operational_risk_CO1-2)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_modeling_operational_risk_CO1-2)'
- en: Creating a bar plot for missing values
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个用于缺失值的条形图
- en: '![missingno_fraud](assets/mlfr_0802.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![缺失值矩阵](assets/mlfr_0802.png)'
- en: Figure 8-2\. Missing observations
  id: totrans-65
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-2\. 缺失观测
- en: '[Figure 8-2](#missingno_fraud) indicates the number of nonmissing observations
    per variable at the top, and on the left-hand side we can see the percentage of
    nonmissing values. This analysis shows that the data has no missing values.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 8-2](#missingno_fraud) 表明在顶部每个变量的非缺失观测数量，而在左侧我们可以看到非缺失值的百分比。这个分析表明数据没有缺失值。'
- en: 'In the next step, first we convert the date variable, `trans_date_trans_time`,
    into a proper format, and then we break time down into days and hours, assuming
    that fraudulent activities surge during particular time periods. It makes sense
    to analyze the effect of fraud on the different categories of a variable. To do
    that, we’ll employ a bar plot. It becomes clearer that the number of fraud cases
    may change given the category of some variables. But it stays the same in gender
    variables, meaning that gender has no impact on fraudulent activities. Another
    striking and evident observation is that the fraud cases change wildly per day
    and hour. This can be visually confirmed in the resulting [Figure 8-3](#bar_fraud):'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，首先我们将日期变量 `trans_date_trans_time` 转换为适当的格式，然后我们将时间分解为天和小时，假设欺诈活动在特定时间段内激增。分析欺诈对变量的不同类别的影响是有意义的。为此，我们将使用柱状图。更清楚地看到，欺诈案件数量可能会随着某些变量类别的变化而变化。但是在性别变量中保持不变，这意味着性别对欺诈活动没有影响。另一个引人注目且显而易见的观察是，欺诈案件每天和每小时变化非常大。这可以在生成的
    [图 8-3](#bar_fraud) 中得到视觉确认。
- en: '[PRE3]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[![1](assets/1.png)](#co_modeling_operational_risk_CO2-1)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_modeling_operational_risk_CO2-1)'
- en: Sorting `fraud_data` based on fraudulent activities in an ascending order
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 根据欺诈活动对 `fraud_data` 进行排序，按升序排列
- en: 'Based on the analysis and our previous knowledge about the fraud analysis,
    we can decide on the number of variables to be used in our modeling. The categorical
    variables sort out so that we can create dummy variables using `pd.get_dummies`:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 基于分析和我们对欺诈分析的先前知识，我们可以决定在建模中使用的变量数量。分类变量进行排序，以便我们可以使用 `pd.get_dummies` 创建虚拟变量：
- en: '[PRE4]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![bar_fraud](assets/mlfr_0803.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![欺诈柱状图](assets/mlfr_0803.png)'
- en: Figure 8-3\. Bar plots per variable
  id: totrans-74
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-3\. 柱状图变量
- en: 'Subsequent to categorical variable analysis, it’s worth discussing the interactions
    between the numerical variables, namely, `amount`, `population`, and `hour`. A
    correlation analysis provides us with a strong tool for figuring out the interaction(s)
    among these variables, and the resulting heatmap ([Figure 8-4](#heatmap_fraud))
    suggests that the correlations are very low:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类变量分析之后，讨论数值变量 `amount`、`population` 和 `hour` 之间的交互作用是值得的。相关性分析为我们提供了一个强有力的工具，用于确定这些变量之间的相互作用，而生成的热图（[图 8-4](#heatmap_fraud)）表明相关性非常低：
- en: '[PRE5]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![heatmap_fraud](assets/mlfr_0804.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![欺诈热图](assets/mlfr_0804.png)'
- en: Figure 8-4\. Heatmap
  id: totrans-78
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-4\. 热图
- en: Supervised Learning Modeling for Fraud Examination
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 欺诈审查的监督学习建模
- en: 'We have determined the peculiar characteristics of the variables using interactions,
    missing values, and creating dummy variables. Now we are ready to move on and
    run ML models for fraud analysis. The models we are about to run are:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经确定了使用交互、缺失值和创建虚拟变量来确定变量的特殊特征。现在我们准备继续并运行欺诈分析的ML模型。我们即将运行的模型包括：
- en: Logistic regression
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: Decision tree
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树
- en: Random forest
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林
- en: XGBoost
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XGBoost
- en: As you can imagine, it’s key to have balanced data before doing our modeling.
    Even though there are numerous ways to get balanced data, we’ll choose the undersampling
    method because of its performance. Undersampling is a technique that matches the
    majority classes to minority classes, as shown in [Figure 8-5](#imbalance).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所想象的那样，在进行建模之前保持数据平衡是关键。尽管有许多方法可以获得平衡数据，但我们将选择欠采样方法，因为它的性能更好。欠采样是一种将多数类别与少数类别匹配的技术，如
    [图 8-5](#imbalance) 所示。
- en: '![imbalance](assets/mlfr_0805.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![不平衡情况](assets/mlfr_0805.png)'
- en: Figure 8-5\. Undersampling
  id: totrans-87
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-5\. 欠采样
- en: 'Alternatively, the number of observations from the majority class is removed
    until we get the same number of observations as the minority class. We’ll apply
    undersampling in the following code block, where the independent and dependent
    variables are named `X_under` and `y_under`, respectively. In what follows, train-test
    split is used to obtain the train and test splits in a random fashion:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，从多数类中删除观察次数，直到获得与少数类相同数量的观察次数。我们将在以下代码块中应用欠采样，其中独立和依赖变量分别命名为`X_under`和`y_under`。接下来，将随机分割训练和测试集以获取训练和测试分割：
- en: '[PRE6]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[![1](assets/1.png)](#co_modeling_operational_risk_CO3-1)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_modeling_operational_risk_CO3-1)'
- en: Sampling `fraud_count`
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 对`fraud_count`进行抽样
- en: '[![2](assets/2.png)](#co_modeling_operational_risk_CO3-2)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_modeling_operational_risk_CO3-2)'
- en: Concatenating the data including fraudulent cases with data including no fraudulent
    cases
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 将包含欺诈案例的数据与不包含欺诈案例的数据合并
- en: '[![3](assets/3.png)](#co_modeling_operational_risk_CO3-3)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_modeling_operational_risk_CO3-3)'
- en: Creating independent variables by dropping `is_fraud`
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 通过去除`is_fraud`创建独立变量
- en: '[![4](assets/4.png)](#co_modeling_operational_risk_CO3-4)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_modeling_operational_risk_CO3-4)'
- en: Creating dependent variables by `is_fraud`
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 通过`is_fraud`创建依赖变量
- en: 'After using the undersampling method, let’s now run some of the classification
    models we described earlier and observe the performance of these models in detecting
    the fraud:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 使用欠采样方法后，让我们现在运行之前描述的一些分类模型，并观察这些模型在检测欺诈方面的表现：
- en: '[PRE7]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: First, let’s look at the confusion matrix. The confusion matrix suggests that
    the number of observations in false positives and false negatives are 310 and
    486, respectively. We’ll be using the confusion matrix in the cost-based method.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看一下混淆矩阵。混淆矩阵表明误报和漏报的观察次数分别为310和486。我们将在基于成本的方法中使用混淆矩阵。
- en: The *F1 score* is the metric that is used to measure the performance of these
    models. It presents a weighted average of recall and precision, making it an ideal
    measure for a case such as this one.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '*F1分数*是用于衡量这些模型性能的指标。它呈现了召回率和精确率的加权平均值，使其成为这种情况下的理想衡量标准。'
- en: 'The second model is decision tree, which works well in modeling fraud. After
    tuning hyperparameters, it turns out that F1 score is much higher, indicating
    that decision tree does a relatively good job. As expected, the number of false
    positive and false negative observations are much fewer compared to logistic regression:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个模型是决策树，在建模欺诈方面表现良好。调整超参数后，F1分数显著提高，表明决策树表现相对良好。与逻辑回归相比，误报和漏报观察次数显著减少：
- en: '[PRE8]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'According to common belief, the random forest model, as an ensemble model,
    outperforms decision tree. However, this is true only if decision tree suffers
    from predictive instability in such a way that predictions of different samples
    vary wildly, and this is not the case here. As you can observe from the following
    result, random forest does not perform better than decision tree, even if it has
    an F1 score of 87:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 根据共同信念，作为集成模型的随机森林优于决策树。然而，在这种情况下，这只有在决策树预测不稳定且不同样本的预测结果差异巨大时才成立，而这里并非如此。正如您从以下结果中可以观察到的那样，即使随机森林的F1分数为87，它也不如决策树表现好：
- en: '[PRE9]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The final model we’ll look at is XGBoost, which generates similar results to
    the decision tree, as it outputs an F1 score of 97:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看看的最终模型是XGBoost，其产生了与决策树类似的结果，其F1分数为97：
- en: '[PRE10]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Given all the applications, here is the summary result:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 综合所有申请情况，以下是总结结果：
- en: Table 8-2\. The result of modeling fraud with undersampling
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 表8-2\. 使用欠采样对欺诈进行建模的结果
- en: '| Model | F1 score |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | F1分数 |'
- en: '| --- | --- |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Logistic regression | 0.79 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 逻辑回归 | 0.79 |'
- en: '| Decision tree | 0.96 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 决策树 | 0.96 |'
- en: '| Random forest | 0.87 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林 | 0.87 |'
- en: '| XGBoost | 0.97 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| XGBoost | 0.97 |'
- en: Cost-Based Fraud Examination
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于成本的欺诈检查
- en: Undersampling gives us a convenient tool for dealing with imbalanced data. It
    comes with costs, however, and the biggest cost is its discarding of important
    observations. Even though different sampling procedures can be applied to sensitive
    analyses such as health care, fraud, and so on, it should be noted that performance
    metrics fail to consider the extent to which different misclassifications have
    varying economic impact. Hence, if a method proposes different misclassification
    costs, it is referred to as a *cost-sensitive classifier*. Let’s consider the
    fraud case, which is a classic example of cost-sensitive analysis. In this type
    of analysis, it is evident that a false positive is less costly than a false negative.
    To be more precise, a false positive means blocking an already legitimate transaction.
    The cost of this type of classification tends to be administrative and opportunity
    cost–related, such as the time and energy spent on detection and the lost potential
    gain a financial institution can make from the transaction.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 欠采样为处理不平衡数据提供了方便的工具。然而，它也伴随着成本，其中最大的成本是丢弃重要的观察结果。尽管不同的采样程序可以应用于诸如医疗保健、欺诈等敏感分析领域，但应注意性能指标未能考虑不同误分类对经济影响的程度。因此，如果某种方法提出了不同的误分类成本，它被称为*成本敏感分类器*。让我们考虑欺诈案例，这是成本敏感分析的一个典型例子。在这种类型的分析中，明显假阳性比假阴性成本低。更准确地说，假阳性意味着阻止一个已经合法的交易。这类分类的成本往往与行政和机会成本相关，例如检测所花费的时间和精力，以及金融机构可能因交易而失去的潜在收益。
- en: However, failing to detect a fraud (i.e., having a false negative) means a lot
    for a company, as it might imply various internal weaknesses as well as poorly
    designed operational procedures. Having failed to detect a real fraud, a company
    can incur large financial costs—including the transaction amount—not to mention
    costs stemming from any damage to its reputation. The former type of cost puts
    the burden on the company’s shoulder, but the latter can be neither quantified
    nor ignored.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，未能检测到欺诈（即出现假阴性）对于一家公司意义重大，因为这可能暗示着各种内部弱点以及设计不良的运营程序。未能检测到真实欺诈的公司可能会承担大笔财务成本，包括交易金额，更不用说因声誉受损而产生的成本了。前一种成本将加重公司的负担，但后者既无法量化也不能被忽视。
- en: As you can see, the need to assign varying costs for different misclassifications
    leads us to a more pronounced, realistic solution. For the sake of simplicity,
    let’s assume the cost of false negative and true positive to be the transaction
    amount and 2, respectively. [Table 8-3](#cost_sen_mat) summarizes the results.
    Another approach for evaluating cost sensitivity would be to assume a constant
    false negative, as in other cases. However, this approach is considered unrealistic.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，为不同的误分类分配不同的成本需求使我们能够找到更为显著、现实的解决方案。为简单起见，我们假设假阴性和真阳性的成本分别为交易金额和2。[表 8-3](#cost_sen_mat)
    总结了结果。另一种评估成本敏感性的方法是假设常数假阴性，就像其他情况一样。然而，这种方法被认为是不现实的。
- en: Table 8-3\. Cost-sensitive matrix
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8-3\. 成本敏感矩阵
- en: '| Model | F1 score |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | F1 分数 |'
- en: '| --- | --- |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| True Positive = 2 | False Negative = Transaction Amount |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 真阳性 = 2 | 假阴性 = 交易金额 |'
- en: '| False Positive = 2 | True Negative = 0 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 假阳性 = 2 | 真阴性 = 0 |'
- en: 'Consequently, the total cost that an institution might face with varying false
    negative costs takes the following form:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一个机构可能面临的总成本将采取以下形式：
- en: <math alttext="Cost equals sigma-summation Underscript i equals 1 Overscript
    upper N Endscripts y Subscript i Baseline left-parenthesis c Subscript i Baseline
    upper C Subscript upper T upper P Sub Subscript i Subscript Baseline plus left-parenthesis
    1 minus c Subscript i Baseline right-parenthesis upper C Subscript upper F upper
    N Sub Subscript i Subscript Baseline right-parenthesis plus left-parenthesis 1
    minus y Subscript i Baseline right-parenthesis c Subscript i Baseline upper C
    Subscript upper F upper P Sub Subscript i Subscript Baseline" display="block"><mrow><mtext>Cost</mtext>
    <mo>=</mo> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>N</mi></munderover> <msub><mi>y</mi> <mi>i</mi></msub> <mrow><mo>(</mo> <msub><mi>c</mi>
    <mi>i</mi></msub> <msub><mi>C</mi> <mrow><mi>T</mi><msub><mi>P</mi> <mi>i</mi></msub></mrow></msub>
    <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>c</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow> <msub><mi>C</mi> <mrow><mi>F</mi><msub><mi>N</mi> <mi>i</mi></msub></mrow></msub>
    <mo>)</mo></mrow> <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>y</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow> <msub><mi>c</mi> <mi>i</mi></msub> <msub><mi>C</mi>
    <mrow><mi>F</mi><msub><mi>P</mi> <mi>i</mi></msub></mrow></msub></mrow></math>
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="Cost equals sigma-summation Underscript i equals 1 Overscript
    upper N Endscripts y Subscript i Baseline left-parenthesis c Subscript i Baseline
    upper C Subscript upper T upper P Sub Subscript i Subscript Baseline plus left-parenthesis
    1 minus c Subscript i Baseline right-parenthesis upper C Subscript upper F upper
    N Sub Subscript i Subscript Baseline right-parenthesis plus left-parenthesis 1
    minus y Subscript i Baseline right-parenthesis c Subscript i Baseline upper C
    Subscript upper F upper P Sub Subscript i Subscript Baseline" display="block"><mrow><mtext>Cost</mtext>
    <mo>=</mo> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>N</mi></munderover> <msub><mi>y</mi> <mi>i</mi></msub> <mrow><mo>(</mo> <msub><mi>c</mi>
    <mi>i</mi></msub> <msub><mi>C</mi> <mrow><mi>T</mi><msub><mi>P</mi> <mi>i</mi></msub></mrow></msub>
    <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>c</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow> <msub><mi>C</mi> <mrow><mi>F</mi><msub><mi>N</mi> <mi>i</mi></msub></mrow></msub>
    <mo>)</mo></mrow> <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>y</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow> <msub><mi>c</mi> <mi>i</mi></msub> <msub><mi>C</mi>
    <mrow><mi>F</mi><msub><mi>P</mi> <mi>i</mi></msub></mrow></msub></mrow></math>
- en: where <math alttext="c Subscript i"><msub><mi>c</mi> <mi>i</mi></msub></math>
    is the predicted label, <math alttext="y Subscript i"><msub><mi>y</mi> <mi>i</mi></msub></math>
    is the actual label, *N* is the number of observations, and <math alttext="upper
    C Subscript upper T upper P Sub Subscript i"><msub><mi>C</mi> <mrow><mi>T</mi><msub><mi>P</mi>
    <mi>i</mi></msub></mrow></msub></math> and <math alttext="upper C Subscript upper
    F upper P Sub Subscript i"><msub><mi>C</mi> <mrow><mi>F</mi><msub><mi>P</mi> <mi>i</mi></msub></mrow></msub></math>
    correspond to administrative cost, which is 2 in our case. <math alttext="upper
    C Subscript upper F upper N Sub Subscript i"><msub><mi>C</mi> <mrow><mi>F</mi><msub><mi>N</mi>
    <mi>i</mi></msub></mrow></msub></math> represents transaction amount.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 其中<math alttext="c Subscript i"><msub><mi>c</mi> <mi>i</mi></msub></math>是预测标签，<math
    alttext="y Subscript i"><msub><mi>y</mi> <mi>i</mi></msub></math>是实际标签，*N*是观察数，<math
    alttext="upper C Subscript upper T upper P Sub Subscript i"><msub><mi>C</mi> <mrow><mi>T</mi><msub><mi>P</mi>
    <mi>i</mi></msub></mrow></msub></math>和<math alttext="upper C Subscript upper
    F upper P Sub Subscript i"><msub><mi>C</mi> <mrow><mi>F</mi><msub><mi>P</mi> <mi>i</mi></msub></mrow></msub></math>对应于行政成本，在我们的案例中为2。<math
    alttext="upper C Subscript upper F upper N Sub Subscript i"><msub><mi>C</mi> <mrow><mi>F</mi><msub><mi>N</mi>
    <mi>i</mi></msub></mrow></msub></math>代表交易金额。
- en: 'Now, with this information in hand, let’s revisit the ML models considering
    the cost-sensitive approach and calculate the changing cost of these models. However,
    before we start, it is worth noting that cost-sensitive models are not fast-processing
    ones, so as we have a large number of observations, it would be wise to sample
    from them to model the data in a timely manner. A class-dependent cost measure
    is given as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，有了这些信息，让我们重新审视考虑成本敏感方法的ML模型，并计算这些模型的成本变化。然而，在我们开始之前，值得注意的是，成本敏感模型不是快速处理型的，因此由于我们有大量观察结果，从中抽样来及时地对数据建模是明智的。一个类相关的成本度量如下：
- en: '[PRE11]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[![1](assets/1.png)](#co_modeling_operational_risk_CO4-1)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_modeling_operational_risk_CO4-1)'
- en: Sampling from `fraud_df` data
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 从`fraud_df`数据中抽样
- en: '[![2](assets/2.png)](#co_modeling_operational_risk_CO4-2)'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_modeling_operational_risk_CO4-2)'
- en: Computing the cost matrix
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 计算成本矩阵
- en: '[![3](assets/3.png)](#co_modeling_operational_risk_CO4-3)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_modeling_operational_risk_CO4-3)'
- en: Computing the total cost per models employed
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 计算每个模型的总成本
- en: Calculating the total cost enables us to have different approaches in assessing
    model performance. The model with a high F1 score is expected to have low total
    cost, and this is what we have in [Table 8-4](#total_cost). Logistic regression
    has the highest total cost, and XGBoost has the lowest.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 计算总成本使我们能够采用不同的方法来评估模型性能。具有高F1分数的模型预计总成本较低，这就是我们在[表 8-4](#total_cost)中看到的情况。逻辑回归的总成本最高，而XGBoost的总成本最低。
- en: Table 8-4\. Total cost
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8-4\. 总成本
- en: '| Model | Total cost |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 总成本 |'
- en: '| --- | --- |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Logistic Regression | 5995 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 逻辑回归 | 5995 |'
- en: '| Decision Tree | 5351 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 决策树 | 5351 |'
- en: '| Random Forest | 5413 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林 | 5413 |'
- en: '| XGBoost | 5371 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| XGBoost | 5371 |'
- en: Saving Score
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 节省分数
- en: There are different metrics that can be used in cost improvement, and saving
    score is absolutely one of them. To be able to define saving, let us give the
    formula of cost.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在成本改善中可以使用不同的指标，而节省分数绝对是其中之一。为了能够定义节省，让我们给出成本的公式。
- en: 'Bahnsen, Aouada, and Ottersten (2014) clearly explain the saving score formula
    in the following manner:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: Bahnsen、Aouada和Ottersten（2014）清楚地解释了以下节省分数公式：
- en: <math alttext="Cost left-parenthesis f left-parenthesis upper S right-parenthesis
    right-parenthesis equals sigma-summation Underscript i equals 1 Overscript upper
    N Endscripts left-parenthesis y Subscript i Baseline left-parenthesis c Subscript
    i Baseline upper C Subscript upper T upper P Sub Subscript i Subscript Baseline
    plus left-parenthesis 1 minus c Subscript i Baseline right-parenthesis upper C
    Subscript upper F upper N Sub Subscript i Subscript Baseline right-parenthesis
    plus left-parenthesis 1 minus y Subscript i Baseline right-parenthesis left-parenthesis
    c Subscript i Baseline upper C Subscript upper F upper P Sub Subscript i Subscript
    Baseline plus left-parenthesis 1 minus c Subscript i Baseline right-parenthesis
    upper C Subscript upper T upper N Sub Subscript i Subscript Baseline right-parenthesis
    right-parenthesis" display="block"><mrow><mtext>Cost(f(S))</mtext> <mo>=</mo>
    <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>N</mi></munderover>
    <mfenced close=")" open="(" separators=""><msub><mi>y</mi> <mi>i</mi></msub> <mrow><mo>(</mo><msub><mi>c</mi>
    <mi>i</mi></msub> <msub><mi>C</mi> <mrow><mi>T</mi><msub><mi>P</mi> <mi>i</mi></msub></mrow></msub>
    <mo>+</mo><mrow><mo>(</mo><mn>1</mn><mo>-</mo><msub><mi>c</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow><msub><mi>C</mi> <mrow><mi>F</mi><msub><mi>N</mi> <mi>i</mi></msub></mrow></msub>
    <mo>)</mo></mrow> <mo>+</mo> <mrow><mo>(</mo><mn>1</mn><mo>-</mo><msub><mi>y</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow> <mrow><mo>(</mo><msub><mi>c</mi> <mi>i</mi></msub>
    <msub><mi>C</mi> <mrow><mi>F</mi><msub><mi>P</mi> <mi>i</mi></msub></mrow></msub>
    <mo>+</mo><mrow><mo>(</mo><mn>1</mn><mo>-</mo><msub><mi>c</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow><msub><mi>C</mi> <mrow><mi>T</mi><msub><mi>N</mi> <mi>i</mi></msub></mrow></msub>
    <mo>)</mo></mrow></mfenced></mrow></math>
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="Cost left-parenthesis f left-parenthesis upper S right-parenthesis
    right-parenthesis equals sigma-summation Underscript i equals 1 Overscript upper
    N Endscripts left-parenthesis y Subscript i Baseline left-parenthesis c Subscript
    i Baseline upper C Subscript upper T upper P Sub Subscript i Subscript Baseline
    plus left-parenthesis 1 minus c Subscript i Baseline right-parenthesis upper C
    Subscript upper F upper N Sub Subscript i Subscript Baseline right-parenthesis
    plus left-parenthesis 1 minus y Subscript i Baseline right-parenthesis left-parenthesis
    c Subscript i Baseline upper C Subscript upper F upper P Sub Subscript i Subscript
    Baseline plus left-parenthesis 1 minus c Subscript i Baseline right-parenthesis
    upper C Subscript upper T upper N Sub Subscript i Subscript Baseline right-parenthesis
    right-parenthesis" display="block"><mrow><mtext>Cost(f(S))</mtext> <mo>=</mo>
    <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>N</mi></munderover>
    <mfenced close=")" open="(" separators=""><msub><mi>y</mi> <mi>i</mi></msub> <mrow><mo>(</mo><msub><mi>c</mi>
    <mi>i</mi></msub> <msub><mi>C</mi> <mrow><mi>T</mi><msub><mi>P</mi> <mi>i</mi></msub></mrow></msub>
    <mo>+</mo><mrow><mo>(</mo><mn>1</mn><mo>-</mo><msub><mi>c</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow><msub><mi>C</mi> <mrow><mi>F</mi><msub><mi>N</mi> <mi>i</mi></msub></mrow></msub>
    <mo>)</mo></mrow> <mo>+</mo> <mrow><mo>(</mo><mn>1</mn><mo>-</mo><msub><mi>y</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow> <mrow><mo>(</mo><msub><mi>c</mi> <mi>i</mi></msub>
    <msub><mi>C</mi> <mrow><mi>F</mi><msub><mi>P</mi> <mi>i</mi></msub></mrow></msub>
    <mo>+</mo><mrow><mo>(</mo><mn>1</mn><mo>-</mo><msub><mi>c</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow><msub><mi>C</mi> <mrow><mi>T</mi><msub><mi>N</mi> <mi>i</mi></msub></mrow></msub>
    <mo>)</mo></mrow></mfenced></mrow></math>
- en: 'where *TP*, *FN*, *FP*, and *TN* are true positive, false negative, false positive,
    and true negative, respectively. <math alttext="c Subscript i"><msub><mi>c</mi>
    <mi>i</mi></msub></math> is the predicted label for each observation *i* on training
    set *S*. <math alttext="y Subscript i"><msub><mi>y</mi> <mi>i</mi></msub></math>
    is the class label and takes the value of either 1 or 0—that is, <math alttext="y
    element-of 0 comma 1"><mrow><mi>y</mi> <mo>∈</mo> <mrow><mn>0</mn> <mo>,</mo>
    <mn>1</mn></mrow></mrow></math> . Our saving formula is then:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*TP*，*FN*，*FP*和*TN*分别是真正例，假负例，假正例和真负例。对于训练集*S*上的每个观察*i*，<math alttext="c Subscript
    i"><msub><mi>c</mi> <mi>i</mi></msub></math> 是预测标签。<math alttext="y Subscript
    i"><msub><mi>y</mi> <mi>i</mi></msub></math>是类标签，取值为1或0，即<math alttext="y element-of
    0 comma 1"><mrow><mi>y</mi> <mo>∈</mo> <mrow><mn>0</mn> <mo>,</mo> <mn>1</mn></mrow></mrow></math>。我们的节省公式如下：
- en: <math alttext="Saving left-parenthesis f left-parenthesis upper S right-parenthesis
    right-parenthesis equals StartFraction Cost left-parenthesis f left-parenthesis
    upper S right-parenthesis right-parenthesis minus upper C o s t Subscript l Baseline
    left-parenthesis upper S right-parenthesis Over upper C o s t Subscript l Baseline
    left-parenthesis upper S right-parenthesis EndFraction" display="block"><mrow><mtext>Saving(f(S))</mtext>
    <mo>=</mo> <mfrac><mrow><mtext>Cost(f(S))</mtext><mo>-</mo><mi>C</mi><mi>o</mi><mi>s</mi><msub><mi>t</mi>
    <mi>l</mi></msub> <mrow><mo>(</mo><mi>S</mi><mo>)</mo></mrow></mrow> <mrow><mi>C</mi><mi>o</mi><mi>s</mi><msub><mi>t</mi>
    <mi>l</mi></msub> <mrow><mo>(</mo><mi>S</mi><mo>)</mo></mrow></mrow></mfrac></mrow></math>
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="Saving left-parenthesis f left-parenthesis upper S right-parenthesis
    right-parenthesis equals StartFraction Cost left-parenthesis f left-parenthesis
    upper S right-parenthesis right-parenthesis minus upper C o s t Subscript l Baseline
    left-parenthesis upper S right-parenthesis Over upper C o s t Subscript l Baseline
    left-parenthesis upper S right-parenthesis EndFraction" display="block"><mrow><mtext>Saving(f(S))</mtext>
    <mo>=</mo> <mfrac><mrow><mtext>Cost(f(S))</mtext><mo>-</mo><mi>C</mi><mi>o</mi><mi>s</mi><msub><mi>t</mi>
    <mi>l</mi></msub> <mrow><mo>(</mo><mi>S</mi><mo>)</mo></mrow></mrow> <mrow><mi>C</mi><mi>o</mi><mi>s</mi><msub><mi>t</mi>
    <mi>l</mi></msub> <mrow><mo>(</mo><mi>S</mi><mo>)</mo></mrow></mrow></mfrac></mrow></math>
- en: where <math alttext="upper C o s t Subscript l Baseline equals m i n upper C
    o s t left-parenthesis f 0 left-parenthesis upper S right-parenthesis right-parenthesis
    comma upper C o s t left-parenthesis f 1 left-parenthesis upper S right-parenthesis
    right-parenthesis"><mrow><mi>C</mi> <mi>o</mi> <mi>s</mi> <msub><mi>t</mi> <mi>l</mi></msub>
    <mo>=</mo> <mi>m</mi> <mi>i</mi> <mi>n</mi> <mrow><mi>C</mi> <mi>o</mi> <mi>s</mi>
    <mi>t</mi> <mrow><mo>(</mo> <msub><mi>f</mi> <mn>0</mn></msub> <mrow><mo>(</mo>
    <mi>S</mi> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>,</mo> <mi>C</mi> <mi>o</mi>
    <mi>s</mi> <mi>t</mi> <mrow><mo>(</mo> <msub><mi>f</mi> <mn>1</mn></msub> <mrow><mo>(</mo>
    <mi>S</mi> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></mrow></math> where <math
    alttext="f 0"><msub><mi>f</mi> <mn>0</mn></msub></math> predicts class 0, <math
    alttext="c 0"><msub><mi>c</mi> <mn>0</mn></msub></math> , and <math alttext="f
    1"><msub><mi>f</mi> <mn>1</mn></msub></math> predicts observations in class 1,
    <math alttext="c 1"><msub><mi>c</mi> <mn>1</mn></msub></math> .
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 <math alttext="upper C o s t Subscript l Baseline equals m i n upper C o
    s t left-parenthesis f 0 left-parenthesis upper S right-parenthesis right-parenthesis
    comma upper C o s t left-parenthesis f 1 left-parenthesis upper S right-parenthesis
    right-parenthesis"><mrow><mi>C</mi> <mi>o</mi> <mi>s</mi> <msub><mi>t</mi> <mi>l</mi></msub>
    <mo>=</mo> <mi>m</mi> <mi>i</mi> <mi>n</mi> <mrow><mi>C</mi> <mi>o</mi> <mi>s</mi>
    <mi>t</mi> <mrow><mo>(</mo> <msub><mi>f</mi> <mn>0</mn></msub> <mrow><mo>(</mo>
    <mi>S</mi> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>,</mo> <mi>C</mi> <mi>o</mi>
    <mi>s</mi> <mi>t</mi> <mrow><mo>(</mo> <msub><mi>f</mi> <mn>1</mn></msub> <mrow><mo>(</mo>
    <mi>S</mi> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></mrow></mrow></math> 其中
    <math alttext="f 0"><msub><mi>f</mi> <mn>0</mn></msub></math> 预测类别 0， <math alttext="c
    0"><msub><mi>c</mi> <mn>0</mn></msub></math> ，而 <math alttext="f 1"><msub><mi>f</mi>
    <mn>1</mn></msub></math> 预测类别 1 中的观测， <math alttext="c 1"><msub><mi>c</mi> <mn>1</mn></msub></math>
    。
- en: 'In code, we have the following:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，我们有以下内容：
- en: '[PRE12]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[![1](assets/1.png)](#co_modeling_operational_risk_CO5-1)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_modeling_operational_risk_CO5-1)'
- en: Calculating the saving score
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 计算节省分数
- en: '[![2](assets/2.png)](#co_modeling_operational_risk_CO5-2)'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_modeling_operational_risk_CO5-2)'
- en: Calculating the F1 score
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 计算 F1 分数
- en: Warning
  id: totrans-157
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Please note that, if you are using `sklearn` version 0.23 or higher, you need
    to downgrade it to 0.22 to use `costcla` library. This adjustment is required
    due to the `sklearn.external.six` package inside the `costcla` library.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果您使用的是 `sklearn` 版本 0.23 或更高版本，则需要将其降级到 0.22 版本以使用 `costcla` 库。这是由于 `costcla`
    库内部的 `sklearn.external.six` 包。
- en: '[Table 8-5](#saving_score) shows that decision tree has the highest saving
    score among the three models, and interestingly, logistic regression produces
    a negative saving score, implying that the number of false negative and false
    positive predictions is quite large, which inflates the denominator of the saving
    score formula.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 8-5](#saving_score)显示，决策树在三个模型中具有最高的节省分数，有趣的是，逻辑回归产生了负的节省分数，这意味着假阴性和假阳性预测的数量相当大，从而扩大了节省分数公式的分母。'
- en: Table 8-5\. Saving scores
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8-5\. 节省分数
- en: '| Model | Saving score | F1 score |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 节省分数 | F1分数 |'
- en: '| --- | --- | --- |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Logistic regression | -0.5602 | 0.0000 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 逻辑回归 | -0.5602 | 0.0000 |'
- en: '| Decision tree | 0.6557 | 0.7383 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 决策树 | 0.6557 | 0.7383 |'
- en: '| Random forest | 0.4789 | 0.7068 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林 | 0.4789 | 0.7068 |'
- en: Cost-Sensitive Modeling
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 成本敏感建模
- en: Thus far, we have discussed the concepts of saving score and cost sensitivity,
    and now we are ready to run cost-sensitive logistic regression, decision tree,
    and random forest. The question that we are trying to address here is what happens
    if fraud is modeled by considering varying costs of misclassification? How does
    it affect the saving score?
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了节省分数和成本敏感的概念，现在我们准备运行成本敏感的逻辑回归、决策树和随机森林。我们在这里要解决的问题是，如果考虑到误分类的不同成本来建模欺诈会发生什么？这如何影响节省分数？
- en: To undertake this investigation, we’ll use the `costcla` library. This library
    was specifically created to employ the cost-sensitive classifiers in which varying
    costs of misclassification are considered. Because, as discussed earlier, traditional
    fraud models assume that all correctly classified and misclassified examples carry
    the same cost, which is not correct due to the varying costs of misclassification
    in fraud (Bahnsen 2021).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行这项调查，我们将使用 `costcla` 库。这个库是专门为采用成本敏感分类器而创建的，其中考虑了不同的误分类成本。因为，正如之前讨论的那样，传统的欺诈模型假设所有正确分类和错误分类的示例都具有相同的成本，这在欺诈中是不正确的（Bahnsen
    2021）。
- en: 'Having applied the cost-sensitive models, the saving score is used to compare
    the models in the following code:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 应用成本敏感模型后，使用节省分数来比较以下代码中的模型：
- en: '[PRE13]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[![1](assets/1.png)](#co_modeling_operational_risk_CO6-1)'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_modeling_operational_risk_CO6-1)'
- en: Training the cost-sensitive models by iteration
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 通过迭代训练成本敏感模型
- en: 'According to [Table 8-6](#saving_score_model), the best and the worst saving
    scores are obtained in random forest and logistic regression, respectively. This
    confirms two important facts: first, it implies that random forest has a low number
    of inaccurate observations, and second, that those inaccurate observations are
    less costly. To be precise, modeling fraud with random forest generates a very
    low number of false negatives, which is the denominator of the saving score formula.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 [表 8-6](#saving_score_model)，最佳和最差的救助分数分别在随机森林和逻辑回归中获得。这证实了两个重要事实：首先，它意味着随机森林具有低数量的不准确观察结果，其次，这些不准确的观察结果成本较低。准确地说，用随机森林建模欺诈生成了极少数量的假阴性，这是救助分数公式的分母。
- en: Table 8-6\. Saving scores of cost-sensitive models
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8-6\. 成本敏感模型的救助分数
- en: '| Model | Saving score | F1 score |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 救助分数 | F1 分数 |'
- en: '| --- | --- | --- |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Logistic regression | -0.5906 | 0.0000 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 逻辑回归 | -0.5906 | 0.0000 |'
- en: '| Decision tree | 0.8414 | 0.3281 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 决策树 | 0.8414 | 0.3281 |'
- en: '| Random forest | 0.8913 | 0.4012 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林 | 0.8913 | 0.4012 |'
- en: Bayesian Minimum Risk
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 贝叶斯最小风险
- en: 'Bayesian decision can also be used to model fraud taking into account the cost
    sensitivity. The Bayesian minimum risk method rests on a decision process using
    different costs (or loss) and probabilities. Mathematically, if the transaction
    is predicted to be fraud, the overall risk is defined as follows:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯决策还可用于考虑成本敏感性的欺诈建模。贝叶斯最小风险方法基于使用不同成本（或损失）和概率的决策过程。数学上，如果预测某笔交易是欺诈，整体风险定义如下：
- en: <math alttext="upper R left-parenthesis c Subscript f Baseline vertical-bar
    upper S right-parenthesis equals upper L left-parenthesis c Subscript f Baseline
    vertical-bar y Subscript f Baseline right-parenthesis upper P left-parenthesis
    c Subscript f Baseline vertical-bar upper S right-parenthesis plus upper L left-parenthesis
    c Subscript f Baseline vertical-bar y Subscript l Baseline right-parenthesis upper
    P left-parenthesis c Subscript l Baseline vertical-bar upper S right-parenthesis"
    display="block"><mrow><mi>R</mi> <mrow><mo>(</mo> <msub><mi>c</mi> <mi>f</mi></msub>
    <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>L</mi> <mrow><mo>(</mo>
    <msub><mi>c</mi> <mi>f</mi></msub> <mo>|</mo> <msub><mi>y</mi> <mi>f</mi></msub>
    <mo>)</mo></mrow> <mi>P</mi> <mrow><mo>(</mo> <msub><mi>c</mi> <mi>f</mi></msub>
    <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow> <mo>+</mo> <mi>L</mi> <mrow><mo>(</mo>
    <msub><mi>c</mi> <mi>f</mi></msub> <mo>|</mo> <msub><mi>y</mi> <mi>l</mi></msub>
    <mo>)</mo></mrow> <mi>P</mi> <mrow><mo>(</mo> <msub><mi>c</mi> <mi>l</mi></msub>
    <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper R left-parenthesis c Subscript f Baseline vertical-bar
    upper S right-parenthesis equals upper L left-parenthesis c Subscript f Baseline
    vertical-bar y Subscript f Baseline right-parenthesis upper P left-parenthesis
    c Subscript f Baseline vertical-bar upper S right-parenthesis plus upper L left-parenthesis
    c Subscript f Baseline vertical-bar y Subscript l Baseline right-parenthesis upper
    P left-parenthesis c Subscript l Baseline vertical-bar upper S right-parenthesis"
    display="block"><mrow><mi>R</mi> <mrow><mo>(</mo> <msub><mi>c</mi> <mi>f</mi></msub>
    <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>L</mi> <mrow><mo>(</mo>
    <msub><mi>c</mi> <mi>f</mi></msub> <mo>|</mo> <msub><mi>y</mi> <mi>f</mi></msub>
    <mo>)</mo></mrow> <mi>P</mi> <mrow><mo>(</mo> <msub><mi>c</mi> <mi>f</mi></msub>
    <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow> <mo>+</mo> <mi>L</mi> <mrow><mo>(</mo>
    <msub><mi>c</mi> <mi>f</mi></msub> <mo>|</mo> <msub><mi>y</mi> <mi>l</mi></msub>
    <mo>)</mo></mrow> <mi>P</mi> <mrow><mo>(</mo> <msub><mi>c</mi> <mi>l</mi></msub>
    <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow></mrow></math>
- en: 'On the other hand, if a transaction is predicted to be legitimate, then the
    overall risk turns out to be:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果预测某笔交易合法，则整体风险如下：
- en: <math alttext="upper R left-parenthesis c Subscript l Baseline vertical-bar
    upper S right-parenthesis equals upper L left-parenthesis c Subscript l Baseline
    vertical-bar y Subscript l Baseline right-parenthesis upper P left-parenthesis
    c Subscript l Baseline vertical-bar upper S right-parenthesis plus upper L left-parenthesis
    c Subscript l Baseline vertical-bar y Subscript f Baseline right-parenthesis upper
    P left-parenthesis c Subscript f Baseline vertical-bar upper S right-parenthesis"
    display="block"><mrow><mi>R</mi> <mrow><mo>(</mo> <msub><mi>c</mi> <mi>l</mi></msub>
    <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>L</mi> <mrow><mo>(</mo>
    <msub><mi>c</mi> <mi>l</mi></msub> <mo>|</mo> <msub><mi>y</mi> <mi>l</mi></msub>
    <mo>)</mo></mrow> <mi>P</mi> <mrow><mo>(</mo> <msub><mi>c</mi> <mi>l</mi></msub>
    <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow> <mo>+</mo> <mi>L</mi> <mrow><mo>(</mo>
    <msub><mi>c</mi> <mi>l</mi></msub> <mo>|</mo> <msub><mi>y</mi> <mi>f</mi></msub>
    <mo>)</mo></mrow> <mi>P</mi> <mrow><mo>(</mo> <msub><mi>c</mi> <mi>f</mi></msub>
    <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper R left-parenthesis c Subscript l Baseline vertical-bar
    upper S right-parenthesis equals upper L left-parenthesis c Subscript l Baseline
    vertical-bar y Subscript l Baseline right-parenthesis upper P left-parenthesis
    c Subscript l Baseline vertical-bar upper S right-parenthesis plus upper L left-parenthesis
    c Subscript l Baseline vertical-bar y Subscript f Baseline right-parenthesis upper
    P left-parenthesis c Subscript f Baseline vertical-bar upper S right-parenthesis"
    display="block"><mrow><mi>R</mi> <mrow><mo>(</mo> <msub><mi>c</mi> <mi>l</mi></msub>
    <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>L</mi> <mrow><mo>(</mo>
    <msub><mi>c</mi> <mi>l</mi></msub> <mo>|</mo> <msub><mi>y</mi> <mi>l</mi></msub>
    <mo>)</mo></mrow> <mi>P</mi> <mrow><mo>(</mo> <msub><mi>c</mi> <mi>l</mi></msub>
    <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow> <mo>+</mo> <mi>L</mi> <mrow><mo>(</mo>
    <msub><mi>c</mi> <mi>l</mi></msub> <mo>|</mo> <msub><mi>y</mi> <mi>f</mi></msub>
    <mo>)</mo></mrow> <mi>P</mi> <mrow><mo>(</mo> <msub><mi>c</mi> <mi>f</mi></msub>
    <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow></mrow></math>
- en: where <math alttext="y Subscript f"><msub><mi>y</mi> <mi>f</mi></msub></math>
    and <math alttext="y Subscript l"><msub><mi>y</mi> <mi>l</mi></msub></math> are
    the actual classes for fraudulent and legitimate cases, respectively. <math alttext="upper
    L left-parenthesis c Subscript f Baseline vertical-bar y Subscript f Baseline
    right-parenthesis"><mrow><mi>L</mi> <mo>(</mo> <msub><mi>c</mi> <mi>f</mi></msub>
    <mo>|</mo> <msub><mi>y</mi> <mi>f</mi></msub> <mo>)</mo></mrow></math> represents
    the cost when fraud is detected and the real class is fraud. Similarly, <math
    alttext="upper L left-parenthesis c Subscript l Baseline vertical-bar y Subscript
    l Baseline right-parenthesis"><mrow><mi>L</mi> <mo>(</mo> <msub><mi>c</mi> <mi>l</mi></msub>
    <mo>|</mo> <msub><mi>y</mi> <mi>l</mi></msub> <mo>)</mo></mrow></math> denotes
    the cost when the transaction is predicted to be legitimate and the real class
    is legitimate. Conversely, <math alttext="upper L left-parenthesis c Subscript
    f Baseline vertical-bar y Subscript l Baseline right-parenthesis"><mrow><mi>L</mi>
    <mo>(</mo> <msub><mi>c</mi> <mi>f</mi></msub> <mo>|</mo> <msub><mi>y</mi> <mi>l</mi></msub>
    <mo>)</mo></mrow></math> and <math alttext="upper L left-parenthesis c Subscript
    l Baseline vertical-bar y Subscript f Baseline right-parenthesis"><mrow><mi>L</mi>
    <mo>(</mo> <msub><mi>c</mi> <mi>l</mi></msub> <mo>|</mo> <msub><mi>y</mi> <mi>f</mi></msub>
    <mo>)</mo></mrow></math> calculate the cost of the off-diagonal elements in [Table 8-3](#cost_sen_mat).
    The former calculates the cost when the transaction is predicted to be a fraud
    but the actual class is not, and the latter shows the cost when the transaction
    is legitimate but the actual class is fraud. <math alttext="upper P left-parenthesis
    c Subscript l Baseline vertical-bar upper S right-parenthesis"><mrow><mi>P</mi>
    <mo>(</mo> <msub><mi>c</mi> <mi>l</mi></msub> <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow></math>
    indicates the predicted probability of having a legitimate transaction given *S*
    and <math alttext="upper P left-parenthesis c Subscript f Baseline vertical-bar
    upper S right-parenthesis"><mrow><mi>P</mi> <mo>(</mo> <msub><mi>c</mi> <mi>f</mi></msub>
    <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow></math> and the predicted probability of
    having a fraudulent transaction given *S*.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 其中<math alttext="y Subscript f"><msub><mi>y</mi> <mi>f</mi></msub></math>和<math
    alttext="y Subscript l"><msub><mi>y</mi> <mi>l</mi></msub></math>分别是欺诈案例和合法案例的实际类别。<math
    alttext="upper L left-parenthesis c Subscript f Baseline vertical-bar y Subscript
    f Baseline right-parenthesis"><mrow><mi>L</mi> <mo>(</mo> <msub><mi>c</mi> <mi>f</mi></msub>
    <mo>|</mo> <msub><mi>y</mi> <mi>f</mi></msub> <mo>)</mo></mrow></math>表示检测到欺诈并且实际类别为欺诈时的成本。类似地，<math
    alttext="upper L left-parenthesis c Subscript l Baseline vertical-bar y Subscript
    l Baseline right-parenthesis"><mrow><mi>L</mi> <mo>(</mo> <msub><mi>c</mi> <mi>l</mi></msub>
    <mo>|</mo> <msub><mi>y</mi> <mi>l</mi></msub> <mo>)</mo></mrow></math>表示预测交易为合法且实际类别为合法时的成本。相反，<math
    alttext="upper L left-parenthesis c Subscript f Baseline vertical-bar y Subscript
    l Baseline right-parenthesis"><mrow><mi>L</mi> <mo>(</mo> <msub><mi>c</mi> <mi>f</mi></msub>
    <mo>|</mo> <msub><mi>y</mi> <mi>l</mi></msub> <mo>)</mo></mrow></math>和<math alttext="upper
    L left-parenthesis c Subscript l Baseline vertical-bar y Subscript f Baseline
    right-parenthesis"><mrow><mi>L</mi> <mo>(</mo> <msub><mi>c</mi> <mi>l</mi></msub>
    <mo>|</mo> <msub><mi>y</mi> <mi>f</mi></msub> <mo>)</mo></mrow></math>计算[表 8-3](#cost_sen_mat)中的非对角元素的成本。前者计算预测交易为欺诈但实际类别不是时的成本，后者显示交易为合法但实际类别为欺诈时的成本。<math
    alttext="upper P left-parenthesis c Subscript l Baseline vertical-bar upper S
    right-parenthesis"><mrow><mi>P</mi> <mo>(</mo> <msub><mi>c</mi> <mi>l</mi></msub>
    <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow></math>表示给定*S*时有合法交易的预测概率，<math alttext="upper
    P left-parenthesis c Subscript f Baseline vertical-bar upper S right-parenthesis"><mrow><mi>P</mi>
    <mo>(</mo> <msub><mi>c</mi> <mi>f</mi></msub> <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow></math>表示给定*S*时有欺诈交易的预测概率。
- en: 'Alternatively, the Bayesian minimum risk formula can be interpreted as:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，贝叶斯最小风险公式可以解释为：
- en: <math alttext="upper R left-parenthesis c Subscript f Baseline vertical-bar
    upper S right-parenthesis equals upper C Subscript a d m i n Baseline upper P
    left-parenthesis c Subscript f Baseline vertical-bar upper S right-parenthesis
    plus upper C Subscript a d m i n Baseline upper P left-parenthesis c Subscript
    l Baseline vertical-bar upper S right-parenthesis" display="block"><mrow><mi>R</mi>
    <mrow><mo>(</mo> <msub><mi>c</mi> <mi>f</mi></msub> <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow>
    <mo>=</mo> <msub><mi>C</mi> <mrow><mi>a</mi><mi>d</mi><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub>
    <mi>P</mi> <mrow><mo>(</mo> <msub><mi>c</mi> <mi>f</mi></msub> <mo>|</mo> <mi>S</mi>
    <mo>)</mo></mrow> <mo>+</mo> <msub><mi>C</mi> <mrow><mi>a</mi><mi>d</mi><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub>
    <mi>P</mi> <mrow><mo>(</mo> <msub><mi>c</mi> <mi>l</mi></msub> <mo>|</mo> <mi>S</mi>
    <mo>)</mo></mrow></mrow></math><math alttext="upper R left-parenthesis c Subscript
    l Baseline vertical-bar upper S right-parenthesis equals 0 plus upper C Subscript
    a m t Baseline upper P left-parenthesis c Subscript l Baseline vertical-bar upper
    S right-parenthesis" display="block"><mrow><mi>R</mi> <mrow><mo>(</mo> <msub><mi>c</mi>
    <mi>l</mi></msub> <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow> <mo>=</mo> <mn>0</mn>
    <mo>+</mo> <msub><mi>C</mi> <mrow><mi>a</mi><mi>m</mi><mi>t</mi></mrow></msub>
    <mi>P</mi> <mrow><mo>(</mo> <msub><mi>c</mi> <mi>l</mi></msub> <mo>|</mo> <mi>S</mi>
    <mo>)</mo></mrow></mrow></math>
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper R left-parenthesis c Subscript f Baseline vertical-bar
    upper S right-parenthesis equals upper C Subscript a d m i n Baseline upper P
    left-parenthesis c Subscript f Baseline vertical-bar upper S right-parenthesis
    plus upper C Subscript a d m i n Baseline upper P left-parenthesis c Subscript
    l Baseline vertical-bar upper S right-parenthesis" display="block"><mrow><mi>R</mi>
    <mrow><mo>(</mo> <msub><mi>c</mi> <mi>f</mi></msub> <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow>
    <mo>=</mo> <msub><mi>C</mi> <mrow><mi>a</mi><mi>d</mi><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub>
    <mi>P</mi> <mrow><mo>(</mo> <msub><mi>c</mi> <mi>f</mi></msub> <mo>|</mo> <mi>S</mi>
    <mo>)</mo></mrow> <mo>+</mo> <msub><mi>C</mi> <mrow><mi>a</mi><mi>d</mi><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub>
    <mi>P</mi> <mrow><mo>(</mo> <msub><mi>c</mi> <mi>l</mi></msub> <mo>|</mo> <mi>S</mi>
    <mo>)</mo></mrow></mrow></math><math alttext="upper R left-parenthesis c Subscript
    l Baseline vertical-bar upper S right-parenthesis equals 0 plus upper C Subscript
    a m t Baseline upper P left-parenthesis c Subscript l Baseline vertical-bar upper
    S right-parenthesis" display="block"><mrow><mi>R</mi> <mrow><mo>(</mo> <msub><mi>c</mi>
    <mi>l</mi></msub> <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow> <mo>=</mo> <mn>0</mn>
    <mo>+</mo> <msub><mi>C</mi> <mrow><mi>a</mi><mi>m</mi><mi>t</mi></mrow></msub>
    <mi>P</mi> <mrow><mo>(</mo> <msub><mi>c</mi> <mi>l</mi></msub> <mo>|</mo> <mi>S</mi>
    <mo>)</mo></mrow></mrow></math>
- en: 'with *admin* is administrative cost and *amt* is the transaction amount. With
    that being said, the transaction is labeled as fraud if:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*admin*是管理成本，*amt*是交易金额。话虽如此，如果交易是欺诈，那么它就被标记为欺诈：
- en: <math alttext="upper R left-parenthesis c Subscript f Baseline vertical-bar
    upper S right-parenthesis greater-than-or-equal-to upper R left-parenthesis c
    Subscript l Baseline vertical-bar upper S right-parenthesis" display="block"><mrow><mi>R</mi>
    <mrow><mo>(</mo> <msub><mi>c</mi> <mi>f</mi></msub> <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow>
    <mo>≥</mo> <mi>R</mi> <mrow><mo>(</mo> <msub><mi>c</mi> <mi>l</mi></msub> <mo>|</mo>
    <mi>S</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper R left-parenthesis c Subscript f Baseline vertical-bar
    upper S right-parenthesis greater-than-or-equal-to upper R left-parenthesis c
    Subscript l Baseline vertical-bar upper S right-parenthesis" display="block"><mrow><mi>R</mi>
    <mrow><mo>(</mo> <msub><mi>c</mi> <mi>f</mi></msub> <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow>
    <mo>≥</mo> <mi>R</mi> <mrow><mo>(</mo> <msub><mi>c</mi> <mi>l</mi></msub> <mo>|</mo>
    <mi>S</mi> <mo>)</mo></mrow></mrow></math>
- en: 'Alternatively:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 或者：
- en: <math alttext="upper C Subscript a d m i n Baseline upper P left-parenthesis
    c Subscript f Baseline vertical-bar upper S right-parenthesis plus upper C Subscript
    a d m i n Baseline upper P left-parenthesis c Subscript l Baseline vertical-bar
    upper S right-parenthesis greater-than-or-equal-to upper C Subscript a m t Baseline
    upper P left-parenthesis c Subscript l Baseline vertical-bar upper S right-parenthesis"
    display="block"><mrow><msub><mi>C</mi> <mrow><mi>a</mi><mi>d</mi><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub>
    <mi>P</mi> <mrow><mo>(</mo> <msub><mi>c</mi> <mi>f</mi></msub> <mo>|</mo> <mi>S</mi>
    <mo>)</mo></mrow> <mo>+</mo> <msub><mi>C</mi> <mrow><mi>a</mi><mi>d</mi><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub>
    <mi>P</mi> <mrow><mo>(</mo> <msub><mi>c</mi> <mi>l</mi></msub> <mo>|</mo> <mi>S</mi>
    <mo>)</mo></mrow> <mo>≥</mo> <msub><mi>C</mi> <mrow><mi>a</mi><mi>m</mi><mi>t</mi></mrow></msub>
    <mi>P</mi> <mrow><mo>(</mo> <msub><mi>c</mi> <mi>l</mi></msub> <mo>|</mo> <mi>S</mi>
    <mo>)</mo></mrow></mrow></math>
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper C Subscript a d m i n Baseline upper P left-parenthesis
    c Subscript f Baseline vertical-bar upper S right-parenthesis plus upper C Subscript
    a d m i n Baseline upper P left-parenthesis c Subscript l Baseline vertical-bar
    upper S right-parenthesis greater-than-or-equal-to upper C Subscript a m t Baseline
    upper P left-parenthesis c Subscript l Baseline vertical-bar upper S right-parenthesis"
    display="block"><mrow><msub><mi>C</mi> <mrow><mi>a</mi><mi>d</mi><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub>
    <mi>P</mi> <mrow><mo>(</mo> <msub><mi>c</mi> <mi>f</mi></msub> <mo>|</mo> <mi>S</mi>
    <mo>)</mo></mrow> <mo>+</mo> <msub><mi>C</mi> <mrow><mi>a</mi><mi>d</mi><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub>
    <mi>P</mi> <mrow><mo>(</mo> <msub><mi>c</mi> <mi>l</mi></msub> <mo>|</mo> <mi>S</mi>
    <mo>)</mo></mrow> <mo>≥</mo> <msub><mi>C</mi> <mrow><mi>a</mi><mi>m</mi><mi>t</mi></mrow></msub>
    <mi>P</mi> <mrow><mo>(</mo> <msub><mi>c</mi> <mi>l</mi></msub> <mo>|</mo> <mi>S</mi>
    <mo>)</mo></mrow></mrow></math>
- en: 'Well, it is time to apply the Bayesian Minimum Risk model in Python. Again,
    three models are employed and compared using F1 score: F1 score results can be
    found in [Table 8-7](#f1_score_bmr), and it turns out decision tree has the highest
    F1 score and logistic regression has the lowest one. So, the order of saving scores
    is other way around, indicating the effectiveness of the cost-sensitive approach:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候在Python中应用贝叶斯最小风险模型了。再次使用三个模型并使用F1分数进行比较：F1分数的结果可以在 [表 8-7](#f1_score_bmr)
    中找到，并且决策树具有最高的F1分数，逻辑回归具有最低的F1分数。因此，储蓄分数的顺序是相反的，显示出成本敏感方法的有效性：
- en: '[PRE14]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[![1](assets/1.png)](#co_modeling_operational_risk_CO7-1)'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_modeling_operational_risk_CO7-1)'
- en: Calling the Bayesian Minimum Risk Classifier library
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 调用贝叶斯最小风险分类器库
- en: Table 8-7\. F1 score based on BMR
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8-7\. 基于BMR的F1分数
- en: '| Model | Saving score | F1 score |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 储蓄分数 | F1 分数 |'
- en: '| --- | --- | --- |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Logistic regression | 0.8064 | 0.1709 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 逻辑回归 | 0.8064 | 0.1709 |'
- en: '| Decision tree | 0.7343 | 0.6381 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 决策树 | 0.7343 | 0.6381 |'
- en: '| Random forest | 0.9624 | 0.4367 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林 | 0.9624 | 0.4367 |'
- en: 'To create a plot of this data, we do the following (resulting in [Figure 8-6](#score_barplot_fraud)):'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建这些数据的绘图，我们执行以下操作（导致 [图 8-6](#score_barplot_fraud)）：
- en: '[PRE15]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[![1](assets/1.png)](#co_modeling_operational_risk_CO8-1)'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_modeling_operational_risk_CO8-1)'
- en: Drawing the F1 score with a line plot
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 根据使用的模型绘制F1分数的折线图
- en: '[![2](assets/2.png)](#co_modeling_operational_risk_CO8-2)'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_modeling_operational_risk_CO8-2)'
- en: Drawing the bar plot based on the models used
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 根据使用的模型绘制条形图
- en: '![f1_saving](assets/mlfr_0806.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![f1_saving](assets/mlfr_0806.png)'
- en: Figure 8-6\. F1 and saving scores
  id: totrans-209
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-6\. F1 和储蓄分数
- en: '[Figure 8-6](#score_barplot_fraud) shows the F1 and saving scores across the
    models we have employed so far. Accordingly, the cost-sensitive and Bayesian minimum
    risk model outperform the base models, as expected.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 8-6](#score_barplot_fraud) 展示了我们迄今所采用的模型在F1和储蓄分数上的表现。据此，成本敏感和贝叶斯最小风险模型超过了基础模型，符合预期。'
- en: Unsupervised Learning Modeling for Fraud Examination
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习建模用于欺诈检测
- en: Unsupervised learning models are also used to detect fraudulent activities in
    a way that extracts the hidden characteristics of the data. The most prominent
    advantage of this method over the supervised model is that there is no need to
    apply a sampling procedure to fix the imbalanced-data problem. Unsupervised models,
    by their nature, do not require any prior knowledge about the data. To see how
    unsupervised learning models perform on this type of data, we will explore the
    self-organizing map (SOM) and autoencoder models.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习模型也用于以提取数据的隐藏特征的方式检测欺诈活动。这种方法比监督模型的最显着优势是不需要应用抽样过程来解决数据不平衡问题。无监督模型本质上不需要关于数据的任何先验知识。为了了解无监督学习模型在这类数据上的表现，我们将探讨自组织映射（SOM）和自动编码器模型。
- en: Self-Organizing Map
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自组织映射
- en: SOM is an unsupervised method to obtain a low-dimensional space from a high-dimensional
    space. This is a method that was introduced by Finnish scholar Teuvo Kohonen in
    1980s and it became widespread. SOM is a type of artificial NN, and therefore
    it rests on competitive learning in the sense that output neurons compete to be
    activated. The activated neuron is referred to as the *winning neuron*, and each
    neuron has neighboring weights, so it is the spatial locations of the nodes in
    the output space that are indicative of the inherent statistical features in the
    input space (Haykin 1999).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: SOM是一种从高维空间获得低维空间的无监督方法。这是由芬兰学者Teuvo Kohonen在1980年代提出的方法，并且广泛传播开来。SOM是一种人工神经网络，因此它是基于竞争学习的，输出神经元竞争被激活。被激活的神经元称为*获胜神经元*，每个神经元都有相邻的权重，因此输出空间中节点的空间位置表明了输入空间中的固有统计特征（Haykin
    1999）。
- en: 'The most distinctive features of SOM methods are as follows (Asan and Ercan
    2012):'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: SOM方法的最显著特征如下（Asan和Ercan 2012）：
- en: No assumptions regarding the distribution of variables
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于变量分布没有任何假设
- en: Dependent structure among variables
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变量之间的依赖结构
- en: Dealing with nonlinear structure
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理非线性结构
- en: Coping with noisy and missing data
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理嘈杂和缺失数据
- en: 'Let’s walk through the important steps of the SOM technique. As you might have
    guessed, the first step is to identify the winning node, or the activated neuron.
    The winning node is identified by distance metrics—that is, Manhattan, Chebyshev,
    and Euclidean distances. Of these distance metrics, Euclidean distance is the
    most commonly used because it works well under the gradient descent process. Thus,
    given the following Euclidean formula, we can find the distance between sample
    and weight:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐步了解SOM技术的重要步骤。正如你可能猜到的那样，第一步是识别获胜节点或激活的神经元。获胜节点是通过距离度量来识别的，即曼哈顿距离、切比雪夫距离和欧氏距离。在这些距离度量中，欧氏距离是最常用的，因为它在梯度下降过程中表现良好。因此，给定以下欧几里得公式，我们可以找到样本和权重之间的距离：
- en: <math alttext="parallel-to left-parenthesis x Subscript t Baseline minus w Subscript
    i Baseline left-parenthesis t right-parenthesis right-parenthesis parallel-to
    equals StartRoot sigma-summation Underscript i Endscripts equals 1 Superscript
    n Baseline left-parenthesis x Subscript t j Baseline minus w Subscript t j i Baseline
    right-parenthesis squared EndRoot comma i equals 1 comma 2 comma period period
    period comma n" display="block"><mrow><mfenced close="∥" open="∥" separators=""><mo>(</mo>
    <msub><mi>x</mi> <mi>t</mi></msub> <mo>-</mo> <msub><mi>w</mi> <mi>i</mi></msub>
    <mrow><mo>(</mo> <mi>t</mi> <mo>)</mo></mrow> <mo>)</mo></mfenced> <mo>=</mo>
    <msqrt><mrow><msub><mo>∑</mo> <mi>i</mi></msub> <mo>=</mo> <msup><mn>1</mn> <mi>n</mi></msup>
    <msup><mrow><mo>(</mo><msub><mi>x</mi> <mrow><mi>t</mi><mi>j</mi></mrow></msub>
    <mo>-</mo><msub><mi>w</mi> <mrow><mi>t</mi><mi>j</mi><mi>i</mi></mrow></msub>
    <mo>)</mo></mrow> <mn>2</mn></msup></mrow></msqrt> <mo>,</mo> <mi>i</mi> <mo>=</mo>
    <mn>1</mn> <mo>,</mo> <mn>2</mn> <mo>,</mo> <mo>.</mo> <mo>.</mo> <mo>.</mo> <mo>,</mo>
    <mi>n</mi></mrow></math>
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="parallel-to left-parenthesis x Subscript t Baseline minus w Subscript
    i Baseline left-parenthesis t right-parenthesis right-parenthesis parallel-to
    equals StartRoot sigma-summation Underscript i Endscripts equals 1 Superscript
    n Baseline left-parenthesis x Subscript t j Baseline minus w Subscript t j i Baseline
    right-parenthesis squared EndRoot comma i equals 1 comma 2 comma period period
    period comma n" display="block"><mrow><mfenced close="∥" open="∥" separators=""><mo>(</mo>
    <msub><mi>x</mi> <mi>t</mi></msub> <mo>-</mo> <msub><mi>w</mi> <mi>i</mi></msub>
    <mrow><mo>(</mo> <mi>t</mi> <mo>)</mo></mrow> <mo>)</mo></mfenced> <mo>=</mo>
    <msqrt><mrow><msub><mo>∑</mo> <mi>i</mi></msub> <mo>=</mo> <msup><mn>1</mn> <mi>n</mi></msup>
    <msup><mrow><mo>(</mo><msub><mi>x</mi> <mrow><mi>t</mi><mi>j</mi></mrow></msub>
    <mo>-</mo><msub><mi>w</mi> <mrow><mi>t</mi><mi>j</mi><mi>i</mi></mrow></msub>
    <mo>)</mo></mrow> <mn>2</mn></msup></mrow></msqrt> <mo>,</mo> <mi>i</mi> <mo>=</mo>
    <mn>1</mn> <mo>,</mo> <mn>2</mn> <mo>,</mo> <mo>.</mo> <mo>.</mo> <mo>.</mo> <mo>,</mo>
    <mi>n</mi></mrow></math>
- en: where *x* is sample, *w* is weight, and the winning node, *k(t)*, is shown in
    [Equation 8-1](#winning_node).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*x*为样本，*w*为权重，获胜节点*k(t)*显示在[Equation 8-1](#winning_node)中。
- en: Equation 8-1\. Identifying the winning node
  id: totrans-223
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程8-1\. 识别获胜节点
- en: <math alttext="k left-parenthesis t right-parenthesis equals arg min parallel-to
    x left-parenthesis t right-parenthesis minus w right-parenthesis i left-parenthesis
    t right-parenthesis parallel-to" display="block"><mrow><mi>k</mi> <mrow><mo>(</mo>
    <mi>t</mi> <mo>)</mo></mrow> <mo>=</mo> <mtext>arg</mtext> <mtext>min</mtext>
    <mfenced close="∥" open="∥" separators=""><mi>x</mi> <mo>(</mo> <mi>t</mi> <mo>)</mo>
    <mo>-</mo> <mi>w</mi> <mo>)</mo> <mi>i</mi> <mo>(</mo> <mi>t</mi> <mo>)</mo></mfenced></mrow></math>
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="k left-parenthesis t right-parenthesis equals arg min parallel-to
    x left-parenthesis t right-parenthesis minus w right-parenthesis i left-parenthesis
    t right-parenthesis parallel-to" display="block"><mrow><mi>k</mi> <mrow><mo>(</mo>
    <mi>t</mi> <mo>)</mo></mrow> <mo>=</mo> <mtext>arg</mtext> <mtext>min</mtext>
    <mfenced close="∥" open="∥" separators=""><mi>x</mi> <mo>(</mo> <mi>t</mi> <mo>)</mo>
    <mo>-</mo> <mi>w</mi> <mo>)</mo> <mi>i</mi> <mo>(</mo> <mi>t</mi> <mo>)</mo></mfenced></mrow></math>
- en: 'The other important step is to update the weight. Given the learning rate and
    neighborhood size, the following update is applied:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的步骤是更新权重。给定学习率和邻域大小，应用以下更新：
- en: <math alttext="w Subscript i Baseline left-parenthesis t plus 1 right-parenthesis
    equals w Subscript i Baseline left-parenthesis t right-parenthesis plus lamda
    left-bracket x left-parenthesis t right-parenthesis minus w Subscript i Baseline
    left-parenthesis t right-parenthesis right-bracket" display="block"><mrow><msub><mi>w</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo> <mi>t</mi> <mo>+</mo> <mn>1</mn> <mo>)</mo></mrow>
    <mo>=</mo> <msub><mi>w</mi> <mi>i</mi></msub> <mrow><mo>(</mo> <mi>t</mi> <mo>)</mo></mrow>
    <mo>+</mo> <mi>λ</mi> <mrow><mo>[</mo> <mi>x</mi> <mrow><mo>(</mo> <mi>t</mi>
    <mo>)</mo></mrow> <mo>-</mo> <msub><mi>w</mi> <mi>i</mi></msub> <mrow><mo>(</mo>
    <mi>t</mi> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="w Subscript i Baseline left-parenthesis t plus 1 right-parenthesis
    equals w Subscript i Baseline left-parenthesis t right-parenthesis plus lamda
    left-bracket x left-parenthesis t right-parenthesis minus w Subscript i Baseline
    left-parenthesis t right-parenthesis right-bracket" display="block"><mrow><msub><mi>w</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo> <mi>t</mi> <mo>+</mo> <mn>1</mn> <mo>)</mo></mrow>
    <mo>=</mo> <msub><mi>w</mi> <mi>i</mi></msub> <mrow><mo>(</mo> <mi>t</mi> <mo>)</mo></mrow>
    <mo>+</mo> <mi>λ</mi> <mrow><mo>[</mo> <mi>x</mi> <mrow><mo>(</mo> <mi>t</mi>
    <mo>)</mo></mrow> <mo>-</mo> <msub><mi>w</mi> <mi>i</mi></msub> <mrow><mo>(</mo>
    <mi>t</mi> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>
- en: where <math alttext="w Subscript i Baseline left-parenthesis t right-parenthesis"><mrow><msub><mi>w</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo> <mi>t</mi> <mo>)</mo></mrow></mrow></math>
    is the weight of the winning neuron *i* at <math alttext="t Superscript t h"><msup><mi>t</mi>
    <mrow><mi>t</mi><mi>h</mi></mrow></msup></math> iteration, and <math alttext="lamda"><mi>λ</mi></math>
    is the learning rate.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 其中<math alttext="w Subscript i Baseline left-parenthesis t right-parenthesis"><mrow><msub><mi>w</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo> <mi>t</mi> <mo>)</mo></mrow></mrow></math>为第<math
    alttext="t Superscript t h"><msup><mi>t</mi> <mrow><mi>t</mi><mi>h</mi></mrow></msup></math>次迭代中获胜神经元*i*的权重，而<math
    alttext="lamda"><mi>λ</mi></math>为学习率。
- en: 'Richardson, Risien, and Shillington (2003) state that the rate of adaptation
    of the weights decays as it moves away from the winning node. This is defined
    by neighborhood function, <math alttext="h Subscript k i Baseline left-parenthesis
    t right-parenthesis"><mrow><msub><mi>h</mi> <mrow><mi>k</mi><mi>i</mi></mrow></msub>
    <mrow><mo>(</mo> <mi>t</mi> <mo>)</mo></mrow></mrow></math> , where *i* is index
    of the neighbor. Of the neighborhood functions, the most famous one is the Gaussian
    function with the following form:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: Richardson、Risien 和 Shillington（2003）指出，权重适应速率随着其远离获胜节点而衰减。这由邻域函数<math alttext="h
    Subscript k i Baseline left-parenthesis t right-parenthesis"><mrow><msub><mi>h</mi>
    <mrow><mi>k</mi><mi>i</mi></mrow></msub> <mrow><mo>(</mo> <mi>t</mi> <mo>)</mo></mrow></mrow></math>定义，其中*i*是邻居的索引。在邻域函数中，最著名的是具有以下形式的高斯函数：
- en: <math alttext="h Subscript k i Baseline left-parenthesis t right-parenthesis
    equals e x p left-parenthesis minus StartFraction d Subscript k i Superscript
    2 Baseline Over 2 sigma squared left-parenthesis t right-parenthesis EndFraction
    right-parenthesis" display="block"><mrow><msub><mi>h</mi> <mrow><mi>k</mi><mi>i</mi></mrow></msub>
    <mrow><mo>(</mo> <mi>t</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>e</mi> <mi>x</mi>
    <mi>p</mi> <mrow><mo>(</mo> <mo>-</mo> <mfrac><msubsup><mi>d</mi> <mrow><mi>k</mi><mi>i</mi></mrow>
    <mn>2</mn></msubsup> <mrow><mn>2</mn><msup><mi>σ</mi> <mn>2</mn></msup> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></mrow></mfrac>
    <mo>)</mo></mrow></mrow></math>
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="h Subscript k i Baseline left-parenthesis t right-parenthesis
    equals e x p left-parenthesis minus StartFraction d Subscript k i Superscript
    2 Baseline Over 2 sigma squared left-parenthesis t right-parenthesis EndFraction
    right-parenthesis" display="block"><mrow><msub><mi>h</mi> <mrow><mi>k</mi><mi>i</mi></mrow></msub>
    <mrow><mo>(</mo> <mi>t</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>e</mi> <mi>x</mi>
    <mi>p</mi> <mrow><mo>(</mo> <mo>-</mo> <mfrac><msubsup><mi>d</mi> <mrow><mi>k</mi><mi>i</mi></mrow>
    <mn>2</mn></msubsup> <mrow><mn>2</mn><msup><mi>σ</mi> <mn>2</mn></msup> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></mrow></mfrac>
    <mo>)</mo></mrow></mrow></math>
- en: where <math alttext="d Subscript k i Superscript 2"><msubsup><mi>d</mi> <mrow><mi>k</mi><mi>i</mi></mrow>
    <mn>2</mn></msubsup></math> denotes the distance between the winning neuron and
    the related neuron, and <math alttext="sigma squared left-parenthesis t right-parenthesis"><mrow><msup><mi>σ</mi>
    <mn>2</mn></msup> <mrow><mo>(</mo> <mi>t</mi> <mo>)</mo></mrow></mrow></math>
    denotes the radius at iteration *t*.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 其中<math alttext="d Subscript k i Superscript 2"><msubsup><mi>d</mi> <mrow><mi>k</mi><mi>i</mi></mrow>
    <mn>2</mn></msubsup></math>表示获胜神经元和相关神经元之间的距离，<math alttext="sigma squared left-parenthesis
    t right-parenthesis"><mrow><msup><mi>σ</mi> <mn>2</mn></msup> <mrow><mo>(</mo>
    <mi>t</mi> <mo>)</mo></mrow></mrow></math>表示在第*t*次迭代时的半径。
- en: Considering all this, the updating process becomes what’s shown in [Equation
    8-2](#weightupdate2).
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一切，更新过程如[公式 8-2](#weightupdate2)所示。
- en: Equation 8-2\. Updating the weight
  id: totrans-232
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 公式 8-2\. 更新权重
- en: <math alttext="w Subscript i Baseline left-parenthesis t plus 1 right-parenthesis
    equals w Subscript i Baseline left-parenthesis t right-parenthesis plus lamda
    h Subscript k i Baseline left-parenthesis t right-parenthesis left-bracket x left-parenthesis
    t right-parenthesis minus w Subscript i Baseline left-parenthesis t right-parenthesis
    right-bracket" display="block"><mrow><msub><mi>w</mi> <mi>i</mi></msub> <mrow><mo>(</mo>
    <mi>t</mi> <mo>+</mo> <mn>1</mn> <mo>)</mo></mrow> <mo>=</mo> <msub><mi>w</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo> <mi>t</mi> <mo>)</mo></mrow> <mo>+</mo> <mi>λ</mi>
    <msub><mi>h</mi> <mrow><mi>k</mi><mi>i</mi></mrow></msub> <mrow><mo>(</mo> <mi>t</mi>
    <mo>)</mo></mrow> <mrow><mo>[</mo> <mi>x</mi> <mrow><mo>(</mo> <mi>t</mi> <mo>)</mo></mrow>
    <mo>-</mo> <msub><mi>w</mi> <mi>i</mi></msub> <mrow><mo>(</mo> <mi>t</mi> <mo>)</mo></mrow>
    <mo>]</mo></mrow></mrow></math>
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="w Subscript i Baseline left-parenthesis t plus 1 right-parenthesis
    equals w Subscript i Baseline left-parenthesis t right-parenthesis plus lamda
    h Subscript k i Baseline left-parenthesis t right-parenthesis left-bracket x left-parenthesis
    t right-parenthesis minus w Subscript i Baseline left-parenthesis t right-parenthesis
    right-bracket" display="block"><mrow><msub><mi>w</mi> <mi>i</mi></msub> <mrow><mo>(</mo>
    <mi>t</mi> <mo>+</mo> <mn>1</mn> <mo>)</mo></mrow> <mo>=</mo> <msub><mi>w</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo> <mi>t</mi> <mo>)</mo></mrow> <mo>+</mo> <mi>λ</mi>
    <msub><mi>h</mi> <mrow><mi>k</mi><mi>i</mi></mrow></msub> <mrow><mo>(</mo> <mi>t</mi>
    <mo>)</mo></mrow> <mrow><mo>[</mo> <mi>x</mi> <mrow><mo>(</mo> <mi>t</mi> <mo>)</mo></mrow>
    <mo>-</mo> <msub><mi>w</mi> <mi>i</mi></msub> <mrow><mo>(</mo> <mi>t</mi> <mo>)</mo></mrow>
    <mo>]</mo></mrow></mrow></math>
- en: 'That’s all there is to it, but I’m aware that the process is a bit tedious.
    So let us summarize the steps:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样，但我知道这个过程有点乏味。因此，让我们总结一下步骤：
- en: 'Initialize the weights: assigning random values to weights is the most common
    approach.'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化权重：将权重赋予随机值是最常见的方法。
- en: Find the winning neuron using [Equation 8-1](#winning_node).
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用[公式 8-1](#winning_node)找到获胜神经元。
- en: Update the weights as given in [Equation 8-2](#weightupdate2).
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据[公式 8-2](#weightupdate2)给出的更新权重。
- en: Adjust the parameters based on the results of [Equation 8-2](#weightupdate2)
    by setting *t* to *t* + 1.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据[公式 8-2](#weightupdate2)的结果调整参数，通过将*t*设置为*t* + 1。
- en: 'We already know that there are two classes in the fraud data that we use, so
    the dimensions for our self organizing map should have a two-by-one structure.
    You can find the application in the following code:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经知道我们使用的欺诈数据中有两类，因此我们的自组织映射应具有两行一列的结构。您可以在以下代码中找到其应用：
- en: '[PRE16]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[![1](assets/1.png)](#co_modeling_operational_risk_CO9-1)'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_modeling_operational_risk_CO9-1)'
- en: Configuring the SOP
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 配置SOP
- en: 'Having checked the classification report, it becomes obvious that the F1 score
    is somewhat similar to what we found with the other methods. This confirms that
    the SOM is a useful model in detecting fraud when we don’t have labeled data.
    In the following code, we generate [Figure 8-7](#som_pred), which shows the actual
    and predicted classes:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 检查分类报告后，明显地，F1得分与其他方法所得相似。这证实了当我们没有标记数据时，SOM在检测欺诈方面是一个有用的模型。在以下代码中，我们生成[图 8-7](#som_pred)，显示了实际和预测类别：
- en: '[PRE17]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![som_prediction](assets/mlfr_0807.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![som_prediction](assets/mlfr_0807.png)'
- en: Figure 8-7\. SOM prediction
  id: totrans-246
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-7\. SOM 预测
- en: Autoencoders
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自编码器
- en: 'An *autoencoder* is an unsupervised deep learning model trained to transform
    inputs into outputs via a hidden layer. However, the network structure of autoencoder
    is different from other structures in the sense that autoencoder consists of two
    parts: an *encoder* and a *decoder*.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '*自编码器*是一种无监督深度学习模型，训练通过隐藏层将输入转换为输出。然而，自编码器的网络结构与其他结构不同，自编码器由两部分组成：*编码器*和*解码器*。'
- en: The encoder serves as a feature extraction function, and the decoder works as
    a reconstruction function. To illustrate, let *x* be an input and *h* be a hidden
    layer. Then, the encoder function is *h* = <math alttext="f left-parenthesis x
    right-parenthesis"><mrow><mi>f</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></math>
    , and the decoder function reconstructs by *r* = <math alttext="g left-parenthesis
    h right-parenthesis"><mrow><mi>g</mi> <mo>(</mo> <mi>h</mi> <mo>)</mo></mrow></math>
    . If an autoencoder learns by simple copying, i.e., <math alttext="g left-parenthesis
    f left-parenthesis x right-parenthesis right-parenthesis"><mrow><mi>g</mi> <mo>(</mo>
    <mi>f</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo> <mo>)</mo></mrow></math> = *x*, it
    is not an ideal situation in that the autoencoder seeks feature extraction. This
    amounts to copying only the relevant aspects of the input (Goodfellow et al. 2016).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器作为特征提取函数，解码器作为重构函数。举例来说，让*x*为输入，*h*为隐藏层。然后，编码器函数为*h* = <math alttext="f left-parenthesis
    x right-parenthesis"><mrow><mi>f</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></math>
    ，解码器函数通过 *r* = <math alttext="g left-parenthesis h right-parenthesis"><mrow><mi>g</mi>
    <mo>(</mo> <mi>h</mi> <mo>)</mo></mrow></math> 进行重构。如果自编码器通过简单复制学习，即 <math alttext="g
    left-parenthesis f left-parenthesis x right-parenthesis right-parenthesis"><mrow><mi>g</mi>
    <mo>(</mo> <mi>f</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo> <mo>)</mo></mrow></math>
    = *x*，这不是理想的情况，因为自编码器寻求特征提取。这相当于仅复制输入的相关方面（Goodfellow et al. 2016）。
- en: 'Consequently, autoencoder has a network structure such that it compresses knowledge
    in a way to have a lower-dimensional representation of the original input. Given
    the encoder and decoder functions, there are different types of autoencoders.
    Of them, we’ll discuss the three most commonly used autoencoders to keep ourselves
    on track:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，自编码器具有一种网络结构，可以以较低维度的方式压缩知识，形成原始输入的表示。给定编码器和解码器函数，有不同类型的自编码器。其中，我们将讨论三种最常用的自编码器，以保持我们的方向：
- en: Undercomplete autoencoders
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**欠完备自编码器**'
- en: Sparse autoencoders
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**稀疏自编码器**'
- en: Denoising autoencoders
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**去噪自编码器**'
- en: Undercomplete autoencoders
  id: totrans-254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**欠完备自编码器**'
- en: This is the most basic type of autoencoder, as the hidden layer, *h*, has a
    smaller dimension than training data, *x*. So the number of neurons is less than
    that of the training data. The aim of this autoencoder is to capture the latent
    attribute of the data by minimizing the loss function—that is, <math alttext="double-struck
    upper L left-parenthesis x comma g left-parenthesis f left-parenthesis x right-parenthesis
    right-parenthesis right-parenthesis"><mrow><mi>𝕃</mi> <mo>(</mo> <mi>x</mi> <mo>,</mo>
    <mi>g</mi> <mo>(</mo> <mi>f</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo> <mo>)</mo> <mo>)</mo></mrow></math>
    , where <math alttext="double-struck upper L"><mi>𝕃</mi></math> is the loss function.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最基本的自编码器类型，因为隐藏层*h*的维度比训练数据*x*小。因此，神经元数量少于训练数据的数量。这种自编码器的目标是通过最小化损失函数来捕捉数据的潜在属性，即
    <math alttext="double-struck upper L left-parenthesis x comma g left-parenthesis
    f left-parenthesis x right-parenthesis right-parenthesis right-parenthesis"><mrow><mi>𝕃</mi>
    <mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>g</mi> <mo>(</mo> <mi>f</mi> <mo>(</mo> <mi>x</mi>
    <mo>)</mo> <mo>)</mo> <mo>)</mo></mrow></math> ，其中 <math alttext="double-struck
    upper L"><mi>𝕃</mi></math> 是损失函数。
- en: Autoencoders famously face a trade-off in ML known as the bias-variance trade-off,
    in which autoencoders aim to reconstruct the input well while having low-dimensional
    representations. To remedy this issue, we’ll introduce sparse and denoising autoencoders.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器在机器学习中著名地面临着偏差-方差的权衡，即自编码器旨在良好地重构输入同时具有低维表示。为了解决这个问题，我们将介绍稀疏和去噪自编码器。
- en: Sparse autoencoder
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**稀疏自编码器**'
- en: 'Sparse autoencoders suggest a solution to this trade-off by imposing sparsity
    on the reconstruction error. There are two ways to enforce regularization in sparce
    autoencoders. The first way is to apply <math alttext="upper L 1"><msub><mi>L</mi>
    <mn>1</mn></msub></math> regularization. In this case, the autoencoders optimization
    becomes (Banks, Koenigstein, and Giryes 2020):'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏自编码器提出了一种解决这种权衡的方法，通过对重构误差施加稀疏性。在稀疏自编码器中，有两种强制正则化的方法。第一种方法是应用 <math alttext="upper
    L 1"><msub><mi>L</mi> <mn>1</mn></msub></math> 正则化。在这种情况下，自编码器的优化变为（Banks, Koenigstein,
    and Giryes 2020）：
- en: <math alttext="dollar-sign arg min Subscript g comma f Baseline double-struck
    upper L left-parenthesis x comma g left-parenthesis f left-parenthesis x right-parenthesis
    right-parenthesis right-parenthesis plus lamda left-parenthesis h right-parenthesis
    dollar-sign"><mrow><mtext>arg</mtext> <msub><mtext>min</mtext> <mrow><mi>g</mi><mo>,</mo><mi>f</mi></mrow></msub>
    <mi>𝕃</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>g</mi> <mrow><mo>(</mo>
    <mi>f</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>)</mo></mrow>
    <mo>+</mo> <mi>λ</mi> <mrow><mo>(</mo> <mi>h</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign arg min Subscript g comma f Baseline double-struck
    upper L left-parenthesis x comma g left-parenthesis f left-parenthesis x right-parenthesis
    right-parenthesis right-parenthesis plus lamda left-parenthesis h right-parenthesis
    dollar-sign"><mrow><mtext>arg</mtext> <msub><mtext>min</mtext> <mrow><mi>g</mi><mo>,</mo><mi>f</mi></mrow></msub>
    <mi>𝕃</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>g</mi> <mrow><mo>(</mo>
    <mi>f</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>)</mo></mrow>
    <mo>+</mo> <mi>λ</mi> <mrow><mo>(</mo> <mi>h</mi> <mo>)</mo></mrow></mrow></math>
- en: where <math alttext="g left-parenthesis f left-parenthesis x right-parenthesis
    right-parenthesis"><mrow><mi>g</mi> <mo>(</mo> <mi>f</mi> <mo>(</mo> <mi>x</mi>
    <mo>)</mo> <mo>)</mo></mrow></math> is the decoder, and *h* is the encoder outputs.
    [Figure 8-8](#sparse_auto) illustrates the sparse autoencoder.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 <math alttext="g left-parenthesis f left-parenthesis x right-parenthesis
    right-parenthesis"><mrow><mi>g</mi> <mo>(</mo> <mi>f</mi> <mo>(</mo> <mi>x</mi>
    <mo>)</mo> <mo>)</mo></mrow></math> 是解码器，*h* 是编码器的输出。[图 8-8](#sparse_auto)展示了稀疏自编码器。
- en: '![sparse_auto](assets/mlfr_0808.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![sparse_auto](assets/mlfr_0808.png)'
- en: Figure 8-8\. Sparse autoencoder model stucture
  id: totrans-262
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-8. 稀疏自编码器模型结构
- en: 'The second way to regularize the sparse autoencoders is with Kullback-Leibler
    (KL) divergence, which tells us the similarity of the two probability distributions
    simply by measuring the distance between them. KL divergence can be put mathematically
    as:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化稀疏自编码器的第二种方法是使用Kullback-Leibler（KL）散度，它通过测量它们之间的距离来简单地告诉我们两个概率分布的相似性。KL散度可以用数学方式表达为：
- en: <math alttext="dollar-sign double-struck upper L left-parenthesis x comma ModifyingAbove
    x With caret right-parenthesis plus sigma-summation Underscript j Endscripts upper
    K upper L left-parenthesis rho parallel-to ModifyingAbove rho With caret parallel-to
    right-parenthesis dollar-sign"><mrow><mi>𝕃</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo>
    <mover accent="true"><mi>x</mi> <mo>^</mo></mover> <mo>)</mo></mrow> <mo>+</mo>
    <msub><mo>∑</mo> <mi>j</mi></msub> <mi>K</mi> <mi>L</mi> <mrow><mo>(</mo> <mi>ρ</mi>
    <mfenced close="∥" open="∥" separators=""><mover accent="true"><mi>ρ</mi> <mo>^</mo></mover></mfenced>
    <mo>)</mo></mrow></mrow></math>
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign double-struck upper L left-parenthesis x comma ModifyingAbove
    x With caret right-parenthesis plus sigma-summation Underscript j Endscripts upper
    K upper L left-parenthesis rho parallel-to ModifyingAbove rho With caret parallel-to
    right-parenthesis dollar-sign"><mrow><mi>𝕃</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo>
    <mover accent="true"><mi>x</mi> <mo>^</mo></mover> <mo>)</mo></mrow> <mo>+</mo>
    <msub><mo>∑</mo> <mi>j</mi></msub> <mi>K</mi> <mi>L</mi> <mrow><mo>(</mo> <mi>ρ</mi>
    <mfenced close="∥" open="∥" separators=""><mover accent="true"><mi>ρ</mi> <mo>^</mo></mover></mfenced>
    <mo>)</mo></mrow></mrow></math>
- en: where <math alttext="rho"><mi>ρ</mi></math> and <math alttext="ModifyingAbove
    rho With caret"><mover accent="true"><mi>ρ</mi> <mo>^</mo></mover></math> are
    ideal and observed distributions, respectively.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 其中<math alttext="rho"><mi>ρ</mi></math>和<math alttext="ModifyingAbove rho With
    caret"><mover accent="true"><mi>ρ</mi> <mo>^</mo></mover></math>分别是理想分布和观察分布。
- en: Denoising autoencoders
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 降噪自编码器
- en: 'The idea behind denoising autoencoders is that instead of using a penalty term,
    <math alttext="lamda"><mi>λ</mi></math> , add noise to the input data and learn
    from this changed construction—that is, reconstruction. Thus, instead of minimizing
    <math alttext="double-struck upper L left-parenthesis x comma g left-parenthesis
    f left-parenthesis x right-parenthesis right-parenthesis right-parenthesis"><mrow><mi>𝕃</mi>
    <mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>g</mi> <mo>(</mo> <mi>f</mi> <mo>(</mo> <mi>x</mi>
    <mo>)</mo> <mo>)</mo> <mo>)</mo></mrow></math> , denoising autoencoders offer
    to minimize the following loss function:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 降噪自编码器的思想是，与使用惩罚项<math alttext="lamda"><mi>λ</mi></math>相反，向输入数据添加噪声，并从这种变化的构建（即重建）中学习。因此，与最小化<math
    alttext="double-struck upper L left-parenthesis x comma g left-parenthesis f left-parenthesis
    x right-parenthesis right-parenthesis right-parenthesis"><mrow><mi>𝕃</mi> <mo>(</mo>
    <mi>x</mi> <mo>,</mo> <mi>g</mi> <mo>(</mo> <mi>f</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo>
    <mo>)</mo> <mo>)</mo></mrow></math>相反，降噪自编码器提供最小化以下损失函数：
- en: <math alttext="double-struck upper L left-parenthesis x comma g left-parenthesis
    f left-parenthesis ModifyingAbove x With caret right-parenthesis right-parenthesis
    right-parenthesis" display="block"><mrow><mi>𝕃</mi> <mo>(</mo> <mi>x</mi> <mo>,</mo>
    <mi>g</mi> <mrow><mo>(</mo> <mi>f</mi> <mrow><mo>(</mo> <mover accent="true"><mi>x</mi>
    <mo>^</mo></mover> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>)</mo></mrow></math>
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="double-struck upper L left-parenthesis x comma g left-parenthesis
    f left-parenthesis ModifyingAbove x With caret right-parenthesis right-parenthesis
    right-parenthesis" display="block"><mrow><mi>𝕃</mi> <mo>(</mo> <mi>x</mi> <mo>,</mo>
    <mi>g</mi> <mrow><mo>(</mo> <mi>f</mi> <mrow><mo>(</mo> <mover accent="true"><mi>x</mi>
    <mo>^</mo></mover> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>)</mo></mrow></math>
- en: where <math alttext="ModifyingAbove x With caret"><mover accent="true"><mi>x</mi>
    <mo>^</mo></mover></math> is the corrupted input obtained by adding noise by,
    for instance, Gaussian noise. [Figure 8-9](#corrupt_noise) illustrates this process.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 其中<math alttext="ModifyingAbove x With caret"><mover accent="true"><mi>x</mi>
    <mo>^</mo></mover></math>是通过添加例如高斯噪声获得的损坏输入。[图8-9](#corrupt_noise)说明了这个过程。
- en: '![corrupt](assets/mlfr_0809.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![corrupt](assets/mlfr_0809.png)'
- en: Figure 8-9\. Denoising autoencoder model structure
  id: totrans-271
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-9. 降噪自编码器模型结构
- en: 'In the following code, we’ll use an autoencoder model with Keras. Before moving
    forward, it is scaled using Standard Scaler, and then, using a batch size of 200
    and an epoch number of 100, we are able to get a satisfactory prediction result.
    We’ll then create a reconstruction error table from the autoencoder model to compare
    with the true class, and it turns out that the means and standard deviations of
    these models are close to each other:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码中，我们将使用Keras中的自编码器模型。在继续之前，它使用标准缩放器进行了缩放，然后使用批量大小为200和100个时期，我们能够得到令人满意的预测结果。然后，我们将从自编码器模型创建一个重构误差表，以与真实类进行比较，结果表明这些模型的均值和标准差彼此接近：
- en: '[PRE18]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[![1](assets/1.png)](#co_modeling_operational_risk_CO10-1)'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_modeling_operational_risk_CO10-1)'
- en: Identifying 64 and 32 hidden layers in the encoder and decoder parts, respectively
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在编码器和解码器部分分别标识了64和32个隐藏层。
- en: '[![2](assets/2.png)](#co_modeling_operational_risk_CO10-2)'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_modeling_operational_risk_CO10-2)'
- en: Identifying 32 and 64 hidden layers in the encoder and decoder parts, respectively
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在编码器和解码器部分分别标识了32和64个隐藏层。
- en: 'After configuring the autoencoder model, the next step is to fit and predict.
    After doing the prediction, we check the quality of the model using summary statistics,
    as they are a reliable way to see whether reconstruction works well:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 配置自编码器模型后，下一步是拟合和预测。在进行预测后，我们使用摘要统计数据来检查模型的质量，因为这是查看重构是否有效的可靠方法：
- en: '[PRE19]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[![1](assets/1.png)](#co_modeling_operational_risk_CO11-1)'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_modeling_operational_risk_CO11-1)'
- en: Creating a table named `error_df` to compare the results obtained from the model
    with the real data
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个名为`error_df`的表格，以比较从模型获得的结果与真实数据
- en: 'Finally, we create our plot ([Figure 8-10](#autoencoder_fraud)):'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们创建我们的图表（[图8-10](#autoencoder_fraud)）：
- en: '[PRE20]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![autoencoder_fraud](assets/mlfr_0810.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![autoencoder_fraud](assets/mlfr_0810.png)'
- en: Figure 8-10\. Autoencoder performance
  id: totrans-285
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-10. 自编码器性能
- en: '[Figure 8-10](#autoencoder_fraud) shows the results of our autoencoder modeling
    using a line plot, and we can see that the test loss result is more volatile than
    that of train but, on average, the mean loss is similar.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 8-10](#autoencoder_fraud) 展示了我们使用线图进行的自动编码器建模的结果，我们可以看到测试损失结果比训练损失更加波动，但平均而言，平均损失是相似的。'
- en: Conclusion
  id: totrans-287
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Fraud is a hot topic in finance for several reasons. Strict regulation, reputation
    loss, and costs arising from fraud are the primary reasons to fight it. Until
    recently, fraud has been a big problem for financial institutions, as modeling
    fraud had not produced satisfactory results and, because of this, financial institutions
    had to employ more resources to handle this phenomenon. Thanks to recent advancements
    in ML, we now have various tools at our disposal for combatting fraud, and this
    chapter was dedicated to introducing these models and comparing their results.
    These models ranged from parametric approaches such as logistic regression to
    deep learning models such as autoencoders.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 欺诈因为几个原因在金融领域是一个热门话题。严格的监管、声誉损失以及欺诈带来的成本是打击它的主要原因。直到最近，欺诈对金融机构来说一直是一个大问题，因为建模欺诈没有产生令人满意的结果，因此金融机构不得不投入更多资源来处理这一现象。多亏了机器学习的最新进展，我们现在可以利用各种工具来对抗欺诈，本章专门介绍了这些模型并比较了它们的结果。这些模型涵盖了从逻辑回归这样的参数化方法到自动编码器这样的深度学习模型。
- en: In the next chapter, we’ll look at a rather different financial risk model known
    as stock price crash risk, which will enable us to gain insight about the well-being
    of corporate governance. This is an important tool for financial risk management
    because risk management is ultimately rooted in corporate management. It would
    be naive to expect low risk in a company with bad corporate governance.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨一种名为股价崩盘风险的不同财务风险模型，这将使我们能够了解公司治理的健康状况。这对于财务风险管理非常重要，因为风险管理最终根植于公司管理。希望在公司治理不善的公司中期望低风险是幼稚的。
- en: References
  id: totrans-290
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Articles cited in this chapter:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 本章引用的文章：
- en: 'Asan, Umut, and Secil Ercan. 2012\. “An Introduction to Self-Organizing Maps.”
    In *Computational Intelligence Systems in Industrial Engineering*, edited by Cengiz
    Kahraman. 295-315\. Paris: Atlantis Press'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Asan, Umut 和 Secil Ercan. 2012\. “自组织映射简介。” 在 *工业工程中的计算智能系统* 中，由Cengiz Kahraman编辑。295-315页。
    巴黎：Atlantis Press。
- en: Bahnsen, Alejandro Correa, Djamia Aouada, and Björn Ottersten. 2014\. “Example-Dependent
    Cost-Sensitive Logistic Regression for Credit Scoring.” In *The 13th International
    Conference on Machine Learning and Applications*, pp. 263-269\. IEEE.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bahnsen, Alejandro Correa, Djamia Aouada 和 Björn Ottersten. 2014\. “基于示例的成本敏感逻辑回归用于信用评分。”
    在 *第13届国际机器学习与应用会议* 中，263-269页。 IEEE。
- en: Bank, Dor, Noam Koenigstein, and Raja Giryes. 2020\. “Autoencoders.” arXiv preprint
    arXiv:2003.05991.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bank, Dor, Noam Koenigstein 和 Raja Giryes. 2020\. “自动编码器。” arXiv预印本 arXiv:2003.05991。
- en: Dunnett, Robert S., Cindy B. Levy, and Antonio P. Simoes. 2005\. “The Hidden
    Costs of Operational Risk.” McKinsey St Company.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dunnett, Robert S., Cindy B. Levy 和 Antonio P. Simoes. 2005\. “运营风险的隐藏成本。” McKinsey
    St Company.
- en: 'Richardson, Anthony J., C. Risien, and Frank Alan Shillington. 2003\. “Using
    Self-Organizing Maps to Identify Patterns in Satellite Imagery.” Progress in Oceanography
    59 (2-3): 223-239.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Richardson, Anthony J., C. Risien 和 Frank Alan Shillington. 2003\. “使用自组织映射识别卫星图像中的模式。”
    海洋学进展 59 (2-3): 223-239。'
- en: 'Books and online resources cited in this chapter:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 本章引用的书籍和在线资源：
- en: Bahnsen, Alejandro Correa. 2021\. “Introduction to Example-Dependent Cost-Sensitive
    Classification.” [*https://oreil.ly/5eCsJ*](https://oreil.ly/5eCsJ).
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bahnsen, Alejandro Correa. 2021\. “引言：基于示例的成本敏感分类。” [*https://oreil.ly/5eCsJ*](https://oreil.ly/5eCsJ)。
- en: 'Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016\. *Deep Learning*.
    Cambridge: MIT press.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow, Ian, Yoshua Bengio 和 Aaron Courville. 2016\. *深度学习*。剑桥：MIT出版社。
- en: Nilsen. 2020\. “Card Fraud Losses Reach $28.65 Billion.” Nilsen Report. [*https://oreil.ly/kSls7*](https://oreil.ly/kSls7).
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nilsen. 2020\. “卡片欺诈损失达到28.65亿美元。” Nilsen报告。 [*https://oreil.ly/kSls7*](https://oreil.ly/kSls7)。
- en: 'Office of the Comptroller of the Currency. 2019\. “Operational Risk: Fraud
    Risk Management Principles.” CC Bulletin. [*https://oreil.ly/GaQez*](https://oreil.ly/GaQez).'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Office of the Comptroller of the Currency. 2019\. “运营风险：欺诈风险管理原则。” CC公告。 [*https://oreil.ly/GaQez*](https://oreil.ly/GaQez)。
- en: 'Simon, Haykin. 1999\. *Neural Networks: A Comprehensive Foundation*, second
    edition. Englewood Cliffs, New Jersey: Prentice-Hall.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Simon, Haykin. 1999\. *神经网络：全面的基础*，第二版。新泽西州恩格尔伍德克利夫斯：Prentice-Hall。
