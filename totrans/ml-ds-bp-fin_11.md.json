["```py\nfrom sklearn.cluster import KMeans\n#Fit with k-means\nk_means = KMeans(n_clusters=nclust)\nk_means.fit(X)\n```", "```py\nfrom sklearn.cluster import AgglomerativeClustering\nmodel = AgglomerativeClustering(n_clusters=4, affinity='euclidean',\\\n  linkage='ward')\nclust_labels1 = model.fit_predict(X)\n```", "```py\nfrom sklearn.cluster import AffinityPropagation\n# Initialize the algorithm and set the number of PC's\nap = AffinityPropagation()\nap.fit(X)\n```", "```py\nfrom sklearn.cluster import KMeans, AgglomerativeClustering, AffinityPropagation\nfrom scipy.cluster.hierarchy import fcluster\nfrom scipy.cluster.hierarchy import dendrogram, linkage, cophenet\nfrom scipy.spatial.distance import pdist\nfrom sklearn.metrics import adjusted_mutual_info_score\nfrom sklearn import cluster, covariance, manifold\n```", "```py\n# Load libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pandas import read_csv, set_option\nfrom pandas.plotting import scatter_matrix\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nimport datetime\nimport pandas_datareader as dr\nimport matplotlib.ticker as ticker\nfrom itertools import cycle\n```", "```py\ndataset = read_csv('SP500Data.csv', index_col=0)\n```", "```py\n# shape\ndataset.shape\n```", "```py\n(448, 502)\n```", "```py\n#Checking for any null values and removing the null values'''\nprint('Null Values =',dataset.isnull().values.any())\n```", "```py\nNull Values = True\n```", "```py\nmissing_fractions = dataset.isnull().mean().sort_values(ascending=False)\nmissing_fractions.head(10)\ndrop_list = sorted(list(missing_fractions[missing_fractions > 0.3].index))\ndataset.drop(labels=drop_list, axis=1, inplace=True)\ndataset.shape\n```", "```py\n(448, 498)\n```", "```py\n# Fill the missing values with the last value available in the dataset.\ndataset=dataset.fillna(method='ffill')\n```", "```py\n#Calculate average annual percentage return and volatilities\nreturns = pd.DataFrame(dataset.pct_change().mean() * 252)\nreturns.columns = ['Returns']\nreturns['Volatility'] = dataset.pct_change().std() * np.sqrt(252)\ndata = returns\n```", "```py\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler().fit(data)\nrescaledDataset = pd.DataFrame(scaler.fit_transform(data),\\\n  columns = data.columns, index = data.index)\n# summarize transformed data\nrescaledDataset.head(2)\n```", "```py\ndistortions = []\nmax_loop=20\nfor k in range(2, max_loop):\n    kmeans = KMeans(n_clusters=k)\n    kmeans.fit(X)\n    distortions.append(kmeans.inertia_)\nfig = plt.figure(figsize=(15, 5))\nplt.plot(range(2, max_loop), distortions)\nplt.xticks([i for i in range(2, max_loop)], rotation=75)\nplt.grid(True)\n```", "```py\nfrom sklearn import metrics\n\nsilhouette_score = []\nfor k in range(2, max_loop):\n        kmeans = KMeans(n_clusters=k,  random_state=10, n_init=10, n_jobs=-1)\n        kmeans.fit(X)\n        silhouette_score.append(metrics.silhouette_score(X, kmeans.labels_, \\\n          random_state=10))\nfig = plt.figure(figsize=(15, 5))\nplt.plot(range(2, max_loop), silhouette_score)\nplt.xticks([i for i in range(2, max_loop)], rotation=75)\nplt.grid(True)\n```", "```py\nnclust=6\n#Fit with k-means\nk_means = cluster.KMeans(n_clusters=nclust)\nk_means.fit(X)\n#Extracting labels\ntarget_labels = k_means.predict(X)\n```", "```py\ncentroids = k_means.cluster_centers_\nfig = plt.figure(figsize=(16,10))\nax = fig.add_subplot(111)\nscatter = ax.scatter(X.iloc[:,0],X.iloc[:,1], c=k_means.labels_, \\\n  cmap=\"rainbow\", label = X.index)\nax.set_title('k-means results')\nax.set_xlabel('Mean Return')\nax.set_ylabel('Volatility')\nplt.colorbar(scatter)\n\nplt.plot(centroids[:,0],centroids[:,1],'sg',markersize=11)\n```", "```py\n# show number of stocks in each cluster\nclustered_series = pd.Series(index=X.index, data=k_means.labels_.flatten())\n# clustered stock with its cluster label\nclustered_series_all = pd.Series(index=X.index, data=k_means.labels_.flatten())\nclustered_series = clustered_series[clustered_series != -1]\n\nplt.figure(figsize=(12,7))\nplt.barh(\n    range(len(clustered_series.value_counts())), # cluster labels, y axis\n    clustered_series.value_counts()\n)\nplt.title('Cluster Member Counts')\nplt.xlabel('Stocks in Cluster')\nplt.ylabel('Cluster Number')\nplt.show()\n```", "```py\nfrom scipy.cluster.hierarchy import dendrogram, linkage, ward\n\n#Calculate linkage\nZ= linkage(X, method='ward')\nZ[0]\n```", "```py\narray([3.30000000e+01, 3.14000000e+02, 3.62580431e-03, 2.00000000e+00])\n```", "```py\n#Plot Dendrogram\nplt.figure(figsize=(10, 7))\nplt.title(\"Stocks Dendrograms\")\ndendrogram(Z,labels = X.index)\nplt.show()\n```", "```py\ndistance_threshold = 13\nclusters = fcluster(Z, distance_threshold, criterion='distance')\nchosen_clusters = pd.DataFrame(data=clusters, columns=['cluster'])\nchosen_clusters['cluster'].unique()\n```", "```py\narray([1, 4, 3, 2], dtype=int64)\n```", "```py\nnclust = 4\nhc = AgglomerativeClustering(n_clusters=nclust, affinity='euclidean', \\\nlinkage='ward')\nclust_labels1 = hc.fit_predict(X)\n```", "```py\nfig = plt.figure(figsize=(16,10))\nax = fig.add_subplot(111)\nscatter = ax.scatter(X.iloc[:,0],X.iloc[:,1], c=clust_labels1, cmap=\"rainbow\")\nax.set_title('Hierarchical Clustering')\nax.set_xlabel('Mean Return')\nax.set_ylabel('Volatility')\nplt.colorbar(scatter)\n```", "```py\nap = AffinityPropagation()\nap.fit(X)\nclust_labels2 = ap.predict(X)\n\nfig = plt.figure(figsize=(10,8))\nax = fig.add_subplot(111)\nscatter = ax.scatter(X.iloc[:,0],X.iloc[:,1], c=clust_labels2, cmap=\"rainbow\")\nax.set_title('Affinity')\nax.set_xlabel('Mean Return')\nax.set_ylabel('Volatility')\nplt.colorbar(scatter)\n```", "```py\nfrom sklearn import metrics\nprint(\"km\", metrics.silhouette_score(X, k_means.labels_, metric='euclidean'))\nprint(\"hc\", metrics.silhouette_score(X, hc.fit_predict(X), metric='euclidean'))\nprint(\"ap\", metrics.silhouette_score(X, ap.labels_, metric='euclidean'))\n```", "```py\nkm 0.3350720873411941\nhc 0.3432149515640865\nap 0.3450647315156527\n```", "```py\n# all stock with its cluster label (including -1)\nclustered_series = pd.Series(index=X.index, data=ap.fit_predict(X).flatten())\n# clustered stock with its cluster label\nclustered_series_all = pd.Series(index=X.index, data=ap.fit_predict(X).flatten())\nclustered_series = clustered_series[clustered_series != -1]\n# get the number of stocks in each cluster\ncounts = clustered_series_ap.value_counts()\n# let's visualize some clusters\ncluster_vis_list = list(counts[(counts<25) & (counts>1)].index)[::-1]\ncluster_vis_list\n# plot a handful of the smallest clusters\nplt.figure(figsize=(12, 7))\ncluster_vis_list[0:min(len(cluster_vis_list), 4)]\n\nfor clust in cluster_vis_list[0:min(len(cluster_vis_list), 4)]:\n    tickers = list(clustered_series[clustered_series==clust].index)\n    # calculate the return (lognormal) of the stocks\n    means = np.log(dataset.loc[:\"2018-02-01\", tickers].mean())\n    data = np.log(dataset.loc[:\"2018-02-01\", tickers]).sub(means)\n    data.plot(title='Stock Time Series for Cluster %d' % clust)\nplt.show()\n```", "```py\ndef find_cointegrated_pairs(data, significance=0.05):\n    # This function is from https://www.quantopian.com\n    n = data.shape[1]\n    score_matrix = np.zeros((n, n))\n    pvalue_matrix = np.ones((n, n))\n    keys = data.keys()\n    pairs = []\n    for i in range(1):\n        for j in range(i+1, n):\n            S1 = data[keys[i]]\n            S2 = data[keys[j]]\n            result = coint(S1, S2)\n            score = result[0]\n            pvalue = result[1]\n            score_matrix[i, j] = score\n            pvalue_matrix[i, j] = pvalue\n            if pvalue < significance:\n                pairs.append((keys[i], keys[j]))\n    return score_matrix, pvalue_matrix, pairs\n```", "```py\nfrom statsmodels.tsa.stattools import coint\ncluster_dict = {}\nfor i, which_clust in enumerate(ticker_count_reduced.index):\n    tickers = clustered_series[clustered_series == which_clust].index\n    score_matrix, pvalue_matrix, pairs = find_cointegrated_pairs(\n        dataset[tickers]\n    )\n    cluster_dict[which_clust] = {}\n    cluster_dict[which_clust]['score_matrix'] = score_matrix\n    cluster_dict[which_clust]['pvalue_matrix'] = pvalue_matrix\n    cluster_dict[which_clust]['pairs'] = pairs\n\npairs = []\nfor clust in cluster_dict.keys():\n    pairs.extend(cluster_dict[clust]['pairs'])\n\nprint (\"Number of pairs found : %d\" % len(pairs))\nprint (\"In those pairs, there are %d unique tickers.\" % len(np.unique(pairs)))\n```", "```py\nNumber of pairs found : 32\nIn those pairs, there are 47 unique tickers.\n```", "```py\n#Import packages for clustering techniques\nfrom sklearn.cluster import KMeans, AgglomerativeClustering,AffinityPropagation\nfrom sklearn.metrics import adjusted_mutual_info_score\nfrom sklearn import cluster, covariance, manifold\n```", "```py\n# load dataset\ndataset = pd.read_excel('ProcessedData.xlsx')\n```", "```py\ndataset.shape\n```", "```py\n(3866, 13)\n```", "```py\n# peek at data\nset_option('display.width', 100)\ndataset.head(5)\n```", "```py\nprint('Null Values =', dataset.isnull().values.any())\n```", "```py\nNull Values = False\n```", "```py\nX=X.drop(['ID'], axis=1)\n```", "```py\nnclust=7\n\n#Fit with k-means\nk_means = cluster.KMeans(n_clusters=nclust)\nk_means.fit(X)\n```", "```py\n#Extracting labels\ntarget_labels = k_means.predict(X)\n```", "```py\nap = AffinityPropagation()\nap.fit(X)\nclust_labels2 = ap.predict(X)\n\ncluster_centers_indices = ap.cluster_centers_indices_\nlabels = ap.labels_\nn_clusters_ = len(cluster_centers_indices)\nprint('Estimated number of clusters: %d' % n_clusters_)\n```", "```py\nEstimated number of clusters: 161\n```", "```py\nfrom sklearn import metrics\nprint(\"km\", metrics.silhouette_score(X, k_means.labels_))\nprint(\"ap\", metrics.silhouette_score(X, ap.labels_))\n```", "```py\nkm 0.170585217843582\nap 0.09736878398868973\n```", "```py\ncluster_output= pd.concat([pd.DataFrame(X),  pd.DataFrame(k_means.labels_, \\\n  columns = ['cluster'])],axis=1)\noutput=cluster_output.groupby('cluster').mean()\n```", "```py\noutput[['AGE','EDUC','MARRIED','KIDS','LIFECL','OCCAT']].\\\nplot.bar(rot=0, figsize=(18,5));\n```", "```py\noutput[['HHOUSES','NWCAT','INCCL','WSAVED','SPENDMOR','RISK']].\\\nplot.bar(rot=0, figsize=(18,5));\n```", "```py\n#Import Model Packages\nimport scipy.cluster.hierarchy as sch\nfrom sklearn.cluster import AgglomerativeClustering\nfrom scipy.cluster.hierarchy import fcluster\nfrom scipy.cluster.hierarchy import dendrogram, linkage, cophenet\nfrom sklearn.metrics import adjusted_mutual_info_score\nfrom sklearn import cluster, covariance, manifold\nimport ffn\n\n#Package for optimization of mean variance optimization\nimport cvxopt as opt\nfrom cvxopt import blas, solvers\n```", "```py\nX= dataset.copy('deep')\nrow= len(X)\ntrain_len = int(row*.8)\n\nX_train = X.head(train_len)\nX_test = X.tail(row-train_len)\n\n#Calculate percentage return\nreturns = X_train.to_returns().dropna()\nreturns_test=X_test.to_returns().dropna()\n```", "```py\nZ= [stock_1, stock_2, distance, sample_count]\n```", "```py\ndef correlDist(corr):\n    # A distance matrix based on correlation, where 0<=d[i,j]<=1\n    # This is a proper distance metric\n    dist = ((1 - corr) / 2.) ** .5  # distance matrix\n    return dist\n```", "```py\n#Calculate linkage\ndist = correlDist(returns.corr())\nlink = linkage(dist, 'ward')\n\n#Plot Dendrogram\nplt.figure(figsize=(20, 7))\nplt.title(\"Dendrograms\")\ndendrogram(link,labels = X.columns)\nplt.show()\n```", "```py\ndef getQuasiDiag(link):\n    # Sort clustered items by distance\n    link = link.astype(int)\n    sortIx = pd.Series([link[-1, 0], link[-1, 1]])\n    numItems = link[-1, 3]  # number of original items\n    while sortIx.max() >= numItems:\n        sortIx.index = range(0, sortIx.shape[0] * 2, 2)  # make space\n        df0 = sortIx[sortIx >= numItems]  # find clusters\n        i = df0.index\n        j = df0.values - numItems\n        sortIx[i] = link[j, 0]  # item 1\n        df0 = pd.Series(link[j, 1], index=i + 1)\n        sortIx = sortIx.append(df0)  # item 2\n        sortIx = sortIx.sort_index()  # re-sort\n        sortIx.index = range(sortIx.shape[0])  # re-index\n    return sortIx.tolist()\n```", "```py\ndef getIVP(cov, **kargs):\n# Compute the inverse-variance portfolio\nivp = 1. / np.diag(cov)\nivp /= ivp.sum()\nreturn ivp\n\ndef getClusterVar(cov,cItems):\n    # Compute variance per cluster\n    cov_=cov.loc[cItems,cItems] # matrix slice\n    w_=getIVP(cov_).reshape(-1, 1)\n    cVar=np.dot(np.dot(w_.T,cov_),w_)[0, 0]\n    return cVar\n\ndef getRecBipart(cov, sortIx):\n    # Compute HRP alloc\n    w = pd.Series(1, index=sortIx)\n    cItems = [sortIx]  # initialize all items in one cluster\n    while len(cItems) > 0:\n        cItems = [i[j:k] for i in cItems for j, k in ((0,\\\n           len(i) // 2), (len(i) // 2, len(i))) if len(i) > 1]  # bi-section\n        for i in range(0, len(cItems), 2):  # parse in pairs\n            cItems0 = cItems[i]  # cluster 1\n            cItems1 = cItems[i + 1]  # cluster 2\n            cVar0 = getClusterVar(cov, cItems0)\n            cVar1 = getClusterVar(cov, cItems1)\n            alpha = 1 - cVar0 / (cVar0 + cVar1)\n            w[cItems0] *= alpha  # weight 1\n            w[cItems1] *= 1 - alpha  # weight 2\n    return w\n```", "```py\ndef getHRP(cov, corr):\n    # Construct a hierarchical portfolio\n    dist = correlDist(corr)\n    link = sch.linkage(dist, 'single')\n    #plt.figure(figsize=(20, 10))\n    #dn = sch.dendrogram(link, labels=cov.index.values)\n    #plt.show()\n    sortIx = getQuasiDiag(link)\n    sortIx = corr.index[sortIx].tolist()\n    hrp = getRecBipart(cov, sortIx)\n    return hrp.sort_index()\n```", "```py\ndef getMVP(cov):\n    cov = cov.T.values\n    n = len(cov)\n    N = 100\n    mus = [10 ** (5.0 * t / N - 1.0) for t in range(N)]\n\n    # Convert to cvxopt matrices\n    S = opt.matrix(cov)\n    #pbar = opt.matrix(np.mean(returns, axis=1))\n    pbar = opt.matrix(np.ones(cov.shape[0]))\n\n    # Create constraint matrices\n    G = -opt.matrix(np.eye(n))  # negative n x n identity matrix\n    h = opt.matrix(0.0, (n, 1))\n    A = opt.matrix(1.0, (1, n))\n    b = opt.matrix(1.0)\n\n    # Calculate efficient frontier weights using quadratic programming\n    solvers.options['show_progress'] = False\n    portfolios = [solvers.qp(mu * S, -pbar, G, h, A, b)['x']\n                  for mu in mus]\n    ## Calculate risk and return of the frontier\n    returns = [blas.dot(pbar, x) for x in portfolios]\n    risks = [np.sqrt(blas.dot(x, S * x)) for x in portfolios]\n    ## Calculate the 2nd degree polynomial of the frontier curve.\n    m1 = np.polyfit(returns, risks, 2)\n    x1 = np.sqrt(m1[2] / m1[0])\n    # CALCULATE THE OPTIMAL PORTFOLIO\n    wt = solvers.qp(opt.matrix(x1 * S), -pbar, G, h, A, b)['x']\n\n    return list(wt)\n```", "```py\ndef get_all_portfolios(returns):\n\n    cov, corr = returns.cov(), returns.corr()\n    hrp = getHRP(cov, corr)\n    mvp = getMVP(cov)\n    mvp = pd.Series(mvp, index=cov.index)\n    portfolios = pd.DataFrame([mvp, hrp], index=['MVP', 'HRP']).T\n    return portfolios\n\n#Now getting the portfolios and plotting the pie chart\nportfolios = get_all_portfolios(returns)\n\nportfolios.plot.pie(subplots=True, figsize=(20, 10),legend = False);\nfig, (ax1, ax2) = plt.subplots(1, 2,figsize=(30,20))\nax1.pie(portfolios.iloc[:, 0], );\nax1.set_title('MVP',fontsize=30)\nax2.pie(portfolios.iloc[:, 1]);\nax2.set_title('HRP',fontsize=30)\n```", "```py\nInsample_Result=pd.DataFrame(np.dot(returns,np.array(portfolios)), \\\n'MVP','HRP'], index = returns.index)\nOutOfSample_Result=pd.DataFrame(np.dot(returns_test,np.array(portfolios)), \\\ncolumns=['MVP', 'HRP'], index = returns_test.index)\n\nInsample_Result.cumsum().plot(figsize=(10, 5), title =\"In-Sample Results\",\\\n                              style=['--','-'])\nOutOfSample_Result.cumsum().plot(figsize=(10, 5), title =\"Out Of Sample Results\",\\\n                                 style=['--','-'])\n```", "```py\n#In_sample Results\nstddev = Insample_Result.std() * np.sqrt(252)\nsharp_ratio = (Insample_Result.mean()*np.sqrt(252))/(Insample_Result).std()\nResults = pd.DataFrame(dict(stdev=stddev, sharp_ratio = sharp_ratio))\nResults\n```", "```py\n#OutOf_sample Results\nstddev_oos = OutOfSample_Result.std() * np.sqrt(252)\nsharp_ratio_oos = (OutOfSample_Result.mean()*np.sqrt(252))/(OutOfSample_Result).\\\nstd()\nResults_oos = pd.DataFrame(dict(stdev_oos=stddev_oos, sharp_ratio_oos = \\\n  sharp_ratio_oos))\nResults_oos\n```"]