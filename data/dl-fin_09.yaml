- en: Chapter 9\. Deep Learning for Time Series Prediction II
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章。时间序列预测的深度学习 II
- en: This chapter presents a few techniques and methods to complement the forecasting
    task of machine and deep learning algorithms. It is composed of different topics
    that each discuss a way to improve and optimize the process. At this point, you
    should have a sound understanding of the basics of machine and deep learning models,
    and you know how to code a basic algorithm that predicts the returns of a financial
    time series (or any stationary time series). This chapter bridges the gap between
    the basic knowledge and the advanced knowledge required to elevate the algorithms
    to a functional level.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了一些技术和方法，用于补充机器学习和深度学习算法的预测任务。它由不同的主题组成，每个主题讨论一种改进和优化流程的方法。此时，您应该对机器学习和深度学习模型的基础知识有了扎实的理解，并且知道如何编写一个基本的算法来预测金融时间序列（或任何平稳时间序列）。本章弥合了基础知识和提升算法到功能水平所需的高级知识之间的差距。
- en: Fractional Differentiation
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分数阶微分
- en: In his book *Advances in Financial Machine Learning*, Marcos López de Prado
    describes a technique to transform nonstationary data into stationary data. This
    is referred to as fractional differentiation.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在他的书籍*金融机器学习进展*中，马科斯·洛佩斯·德普拉多描述了一种将非平稳数据转换为平稳数据的技术。这称为分数阶微分。
- en: '*Fractional differentiation* is a mathematical technique used to transform
    a time series into a stationary series while preserving some of its memory. It
    extends the concept of *differencing* (or taking the returns), which is commonly
    used to remove trends and make time series stationary.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*分数阶微分*是一种数学技术，用于将时间序列转化为平稳序列，同时保留部分其记忆。它扩展了*微分*（或称为收益率），常用于去除趋势并使时间序列变得平稳的概念。'
- en: In traditional differencing, the data sequence is differenced by a whole number,
    typically 1, which involves subtracting the previous value from the current value.
    This helps eliminate trends and makes the series stationary. However, in some
    cases, the series may exhibit long-term dependencies or memory effects that are
    not effectively captured by traditional differencing. These dependencies may help
    in forecasting the time series, and if they are completely eliminated, that may
    hinder the ability of the algorithm to perform well. These dependencies are referred
    to as *memory*.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的微分中，数据序列通过一个通常为1的整数进行微分，这涉及从当前值中减去前一个值。这有助于消除趋势并使系列平稳。然而，在某些情况下，系列可能表现出传统微分无法有效捕捉的长期依赖或记忆效应。这些依赖关系可能有助于预测时间序列，如果完全消除，可能会影响算法的性能。这些依赖关系被称为*记忆*。
- en: 'Fractional differentiation addresses this limitation by allowing the differencing
    parameter to be a fractional value. The fractional differencing operator effectively
    applies a weighted sum of lagged values to each observation in the series, with
    the weights determined by the fractional differencing parameter. This allows for
    capturing long-term dependencies or memory effects in the series. Fractional differentiation
    is particularly useful in financial time series analysis, where data often exhibits
    long memory or persistent behavior. This can be implemented in Python. First,
    `pip install` the required library from the prompt:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 分数阶微分通过允许微分参数为分数值来解决这一限制。分数阶微分算子有效地对系列中的每个观测值应用滞后值的加权和，其中权重由分数阶微分参数确定。这允许捕捉系列中的长期依赖或记忆效应。分数阶微分在金融时间序列分析中特别有用，其中数据经常表现出长期记忆或持续行为。这可以在Python中实现。首先，在命令提示符中使用`pip
    install`安装所需的库：
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, import the required libraries:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，导入所需的库：
- en: '[PRE1]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Let’s use the classic example that de Prado uses in his book, the S&P 500, to
    prove that fractional differentiation transforms a nonstationary time series into
    a stationary one with visible preserved memory.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用德普拉多在他的书中经典的例子，标普500指数，来证明分数阶微分将非平稳时间序列转化为具有可见保留记忆的平稳序列。
- en: 'The following code applies fractional differentiation and compares it to traditional
    differencing:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码应用分数阶微分并与传统微分进行比较：
- en: '[PRE2]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[Figure 9-1](#figure-9-1) shows the three types of transformations. You can
    notice the trending nature in the top panel with the nontransformed S&P 500 data.
    You can also notice that in the middle panel, this trend is less visible but still
    there. This is what fractional differentiation aims to do. By keeping a hint of
    the market’s memory while rendering it stationary, this technique can help improve
    some forecasting algorithms. The bottom panel shows normal differencing of the
    price data.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9-1](#figure-9-1) 展示了三种类型的变换。您可以在顶部面板中注意到非变换的S&P 500数据的趋势性质。您还可以注意到在中间面板中，这种趋势性质减弱了但仍然存在。这就是分数阶差分的目标。通过保持市场记忆的线索同时使其稳定，这种技术可以帮助改进一些预测算法。底部面板显示了价格数据的正常差分。'
- en: '![](assets/dlff_0901.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlff_0901.png)'
- en: Figure 9-1\. Fractional differentiation on S&P 500 (order = 0.48)
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-1\. S&P 500的分数阶差分（order = 0.48）
- en: '[Figure 9-1](#figure-9-1) was generated using this code:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9-1](#figure-9-1) 是使用此代码生成的：'
- en: '[PRE3]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let’s make sure that the fractionally differentiated data is indeed stationary
    by applying the augmented Dickey—Fuller (ADF) test (you used this test in [Chapter 3](ch03.html#ch03)):'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过应用增广迪基—富勒（ADF）检验确保分数差分数据确实是稳定的（您在[第三章](ch03.html#ch03) 中使用了此检验）：
- en: '[PRE4]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The output of the previous code block is as follows (assuming a 5% significance
    level):'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 前一代码块的输出如下（假设显著性水平为5%）：
- en: '[PRE5]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'As the results show, the data is indeed stationary. Let’s look at another example.
    The following code imports the daily values of the EURUSD:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 根据结果显示，数据确实是稳定的。让我们看另一个例子。以下代码导入了EURUSD的日常数值：
- en: '[PRE6]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[Figure 9-2](#figure-9-2) compares the EURUSD with fractional differentiation
    (0.20) applied onto it, with the regular differencing shown in the bottom panel.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9-2](#figure-9-2) 将EURUSD与应用了分数阶差分（0.20）进行了比较，底部面板显示了常规差分。'
- en: '![](assets/dlff_0902.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlff_0902.png)'
- en: Figure 9-2\. Fractional differentiation on the EURUSD (order = 0.20)
  id: totrans-26
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-2\. EURUSD的分数阶差分（order = 0.20）
- en: 'The results of the ADF test are as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ADF测试的结果如下：
- en: '[PRE7]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As a comparison, [Figure 9-3](#figure-9-3) compares the same dataset with fractional
    differentiation (0.30) applied onto it, with the regular differencing shown in
    the bottom panel.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 作为比较，[图 9-3](#figure-9-3) 将同一数据集与应用了分数阶差分（0.30）进行了比较，底部面板显示了常规差分。
- en: '![](assets/dlff_0903.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlff_0903.png)'
- en: Figure 9-3\. Fractional differentiation on EURUSD (order = 0.30)
  id: totrans-31
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-3\. EURUSD的分数阶差分（order = 0.30）
- en: Note
  id: totrans-32
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Approaching an order of 1.00 intuitively makes the fractional differentiation
    approach a normal integer differencing. Similarly, approaching an order of 0.00
    makes the fractional differentiation approach the untransformed data series.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 当接近1.00的阶数时，分数阶差分方法直观上变成了正常的整数差分。同样地，当接近0.00的阶数时，分数阶差分方法则变成了未经变换的数据序列。
- en: '[Figure 9-3](#figure-9-3) shows a more stationary EURUSD series in the middle
    panel than [Figure 9-2](#figure-9-2) does, and this is because the order of fractional
    differentiation is increased. This is why the ADF test result for the fractional
    differentiation of order = 0.30 is 0.002, which is much lower than the ADF test
    result when order = 0.20 (which is at 0.043).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9-3](#figure-9-3) 在中间面板上显示的EURUSD系列比[图 9-2](#figure-9-2) 更加稳定，这是因为分数阶差分的阶数增加了。这也是为什么当阶数为0.30时，分数阶差分的ADF测试结果为0.002，远低于当阶数为0.20时的ADF测试结果（为0.043）。'
- en: In summary, fractional differentiation is a valuable tool for time series prediction
    as it captures long-term dependencies, handles nonstationarity, adapts to various
    dynamics, and preserves integral properties. Its ability to capture complex patterns
    and improve forecasting accuracy makes it a good fit for modeling and predicting
    a wide range of real-world time series data.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，分数阶差分是时间序列预测的有价值工具，因为它捕捉长期依赖关系，处理非平稳性，适应各种动态，并保留积分属性。其捕捉复杂模式和提高预测精度的能力使其非常适合对各种实际时间序列数据进行建模和预测。
- en: Forecasting Threshold
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测阈值
- en: The *forecasting threshold* is the minimum required percentage prediction to
    validate a signal. This means that the forecasting threshold technique is a filter
    that removes low conviction predictions.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*预测阈值* 是验证信号所需的最低百分比预测。这意味着预测阈值技术是一种过滤器，用于删除低信度预测。'
- en: Objectively, low conviction predictions are below a certain percentage. A hypothetical
    example is shown in [Table 9-1](#table-9-1). The threshold is ±1%.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 客观地说，低信度预测低于一定百分比。在[表 9-1](#table-9-1) 中展示了一个假设性例子。阈值为±1%。
- en: Table 9-1\. Table of forecasts
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9-1\. 预测表
- en: '| Time | Forecast | Status |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 时间 | 预测 | 状态 |'
- en: '| --- | --- | --- |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 1 | 0.09% | Dismissed |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.09% | 已取消 |'
- en: '| 2 | –0.60% | Dismissed |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 2 | –0.60% | 已取消 |'
- en: '| 3 | –1.50% | Taken |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 3 | –1.50% | 已接受 |'
- en: '| 4 | 1.00% | Taken |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 1.00% | 已接受 |'
- en: '| 5 | 2.33% | Taken |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 2.33% | 已接受 |'
- en: At time 1, the trading signal is bullish, with an expectation of a 0.09% rise
    in the hypothetical financial instrument. As this prediction is below the threshold
    of 1.00%, the trade is not taken. At time 2, the same intuition is applied, as
    the bearish signal is below the threshold.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间 1，交易信号是看涨的，预计虚拟金融工具将上涨 0.09%。由于此预测低于 1.00% 的阈值，因此不进行交易。在时间 2，应用相同的直觉，因为看跌信号低于阈值。
- en: The rest of the signals are taken since they are equal to or greater than the
    threshold (in terms of magnitude). The aim of this section is to develop a multilayer
    perceptron (MLP) model and keep only the predictions that respect a certain threshold.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 其余信号都已被接受，因为它们等于或大于阈值（在大小方面）。本节的目的是开发多层感知器（MLP）模型并仅保留符合一定阈值的预测。
- en: 'As usual, start by importing the required libraries:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，首先导入所需的库：
- en: '[PRE8]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next, set the hyperparameters and import the data using `mass_import()`:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，设置超参数并使用 `mass_import()` 导入数据：
- en: '[PRE9]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Import and preprocess the data, then design the MLP architecture:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 导入并预处理数据，然后设计 MLP 架构：
- en: '[PRE10]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The next step is to fit and predict the data and retain the predictions that
    satisfy the threshold you have defined in the hyperparameters. This is done using
    the function `forecasting_threshold()`:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是拟合和预测数据，并保留满足您在超参数中定义的阈值的预测。这是使用函数 `forecasting_threshold()` 完成的：
- en: '[PRE11]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[Figure 9-4](#figure-9-4) shows the comparison chart between the real values
    and the predicted values. Flat observations on the predictions indicate the absence
    of signals that are lower than the required threshold—in this case, 0.0015.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9-4](#figure-9-4) 显示了真实值和预测值之间的比较图表。预测中的平坦观察表示没有低于所需阈值（在本例中为 0.0015）的信号。'
- en: '![](assets/dlff_0904.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlff_0904.png)'
- en: Figure 9-4\. Predicting with the forecasting threshold
  id: totrans-59
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-4\. 使用预测阈值进行预测
- en: 'The threshold can be found in many ways, notably:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 阈值可以通过多种方式找到，特别是：
- en: The fixed numerical technique
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 固定数值技术
- en: As you saw in the previous example, this technique assumes a fixed arbitrary
    number to be used as a threshold.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在前面的示例中看到的，该技术假设一个固定的任意数字用作阈值。
- en: The volatility-based technique
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 基于波动率的技术
- en: With this technique, you use a volatility indicator such as a rolling standard
    deviation of prices to set a variable threshold at each time step. This technique
    has the benefit of using up-to-date volatility information.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种技术，您可以使用波动率指标（例如价格滚动标准差）在每个时间步长上设置可变阈值。这种技术的好处是使用最新的波动率信息。
- en: The statistical technique
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 统计技术
- en: With this technique, you look at the real values from the training set (not
    the test set) and select a certain quantile (e.g., the 75% quantile) as a minimum
    threshold to validate the signals.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此技术，您查看来自训练集（而不是测试集）的实际值，并选择某个分位数（例如，75% 分位数）作为验证信号的最低阈值。
- en: 'To summarize, using the forecasting threshold may help select the trades with
    the highest conviction and can also help minimize transaction costs since the
    algorithms assume trading all the time, which is not recommended. This assumes
    adding a new state to the algorithm, which gives a total of three:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，使用预测阈值可能有助于选择具有最高信心的交易，并且还可以帮助最小化交易成本，因为算法假定始终进行交易，这是不推荐的。这假设向算法添加一个新状态，总共有三个状态：
- en: Bullish signal
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 看涨信号
- en: The algorithm predicts a higher value.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法预测一个较高的值。
- en: Bearish signal
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 看跌信号
- en: The algorithm predicts a lower value.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法预测一个较低的值。
- en: Neutral signal
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 中性信号
- en: The algorithm does not have any directional conviction.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法没有任何方向性信念。
- en: Continuous Retraining
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 连续重新训练
- en: '*Retraining* refers to the act of training the algorithm every time new data
    comes in. This means that when dealing with a daily time series, the retraining
    is done every day while incorporating the latest daily inputs.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*重新训练* 指的是每次有新数据进来时对算法进行训练。这意味着在处理每日时间序列时，重新训练会每天进行一次，同时合并最新的每日输入。'
- en: 'The continuous retraining technique deserves to be tested, and that is the
    aim of this section. The architecture of the algorithm will follow this framework:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 连续重新训练技术值得测试，这就是本节的目的。算法的架构将遵循这个框架：
- en: Train the data on the training test.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练测试数据上训练数据。
- en: For each prediction made, rerun the algorithm and include the new real inputs
    in the training set.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每次预测，重新运行算法并将新的实际输入包含在训练集中。
- en: Note
  id: totrans-79
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: One big limitation of the continuous retraining technique is the speed of the
    algorithm, as it has to retrain at every time step. If you have 1,000 instances
    of test data where every training requires a few minutes, then the backtesting
    process becomes drastically slow. This is especially an issue with deep learning
    algorithms such as LSTM, which may take a long time to train.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 持续重新训练技术的一个重大限制是算法的速度，因为它必须在每个时间步重新训练。如果您有1,000个测试数据实例，每次训练都需要几分钟，那么回测过程将变得极其缓慢。这尤其是深度学习算法，例如LSTM，可能需要很长时间来训练。
- en: The main reason for applying continuous retraining is because of *concept drift*,
    which is the change in the data’s inner dynamics and structures that may invalidate
    the function found in the training phase. Basically, financial time series do
    not exhibit static relationships; rather, they change over time. Therefore, continuous
    retraining aims to update the models by always using the latest data to train.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 持续重新训练的主要原因是*概念漂移*，这是数据内部动态和结构的变化，可能会使训练阶段找到的函数失效。基本上，金融时间序列不表现出静态关系；相反，它们随着时间的推移而变化。因此，持续重新训练旨在通过始终使用最新数据进行训练来更新模型。
- en: Note
  id: totrans-82
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Continuous retraining does not need to be done at every time step. You can set
    *n* periods for the retraining. For example, if you select 10, then the model
    retrains after each group of 10 new values.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 持续重新训练不需要在每个时间步骤都进行。您可以设置*n*周期进行重新训练。例如，如果您选择了10，则模型在每组10个新值之后重新训练。
- en: 'To simplify things, this section shows the code for the continuous retraining
    (every day) using a linear regression model on the weekly EURUSD values at every
    time step. You can do the same thing with other models; you just have to change
    the lines of code where the model is imported and designed. First, import the
    required libraries:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化事情，本节展示了每天使用线性回归模型对每个时间步长的周EURUSD值进行连续重新训练的代码。您可以使用其他模型执行相同的操作；您只需更改导入和设计模型的代码行即可。首先，导入所需的库：
- en: '[PRE12]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Import the data and set the hyperparameters of the algorithm:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 导入数据并设置算法的超参数：
- en: '[PRE13]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Create the continuous retraining loop as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 创建持续重新训练循环如下：
- en: '[PRE14]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Plot the predicted values:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制预测值：
- en: '[PRE15]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[Figure 9-5](#figure-9-5) shows the result.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9-5](#figure-9-5)显示了结果。'
- en: As a simple comparison, the same backtest was done on the model with no retraining.
    The latter got a 48.55% test set accuracy compared to the 48.92% test set accuracy
    for the same model with retraining.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 作为简单的比较，对没有重新训练的模型进行了相同的回测。与重新训练的同一模型相比，后者在48.55%的测试集准确率上获得了48.92%的测试集准确率。
- en: Continuous retraining is not a guarantee for better results, but it makes sense
    to update the model every once in a while due to changing market dynamics. The
    frequency at which you should update the model may be subjective.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 持续重新训练并不能保证获得更好的结果，但由于市场动态的变化，定期更新模型是有意义的。您应该更新模型的频率可能是主观的。
- en: '![](assets/dlff_0905.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlff_0905.png)'
- en: Figure 9-5\. Predicting using the continuous retraining technique
  id: totrans-96
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-5\. 使用持续重新训练技术进行预测
- en: Time Series Cross Validation
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间序列交叉验证
- en: '*Cross validation* is a technique used in machine learning to assess the performance
    of a model. It involves splitting the available data into subsets for training
    and evaluation. In the case of time series data, where the order of observations
    is important (due to the sequential nature of the data), a traditional *k*-fold
    cross validation approach may not be suitable. Instead, time series cross validation
    techniques are used, such as the *rolling window* and *expanding window* methods.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '*交叉验证*是机器学习中用于评估模型性能的一种技术。它涉及将可用数据分成用于训练和评估的子集。在时间序列数据的情况下，观察值的顺序很重要（由于数据的顺序性质），传统的*k*-折叠交叉验证方法可能不适用。而是使用时间序列交叉验证技术，例如*滚动窗口*和*扩展窗口*方法。'
- en: Note
  id: totrans-99
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In traditional *k-fold cross validation*, the data is randomly split into *k*
    equally sized folds. Each fold is used as a validation set, while the remaining
    *k – 1* folds are combined for training the model. The process is repeated *k*
    times, with each fold serving as the validation set once. Finally, the performance
    metrics are averaged across the *k* iterations to assess the model’s performance.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的*k*折交叉验证中，数据被随机分成*k*个大小相等的折叠。每个折叠被用作验证集，而其余*k-1*个折叠被组合用于训练模型。该过程重复*k*次，每个折叠一次作为验证集。最后，跨*k*次迭代对性能指标进行平均以评估模型的性能。
- en: Unlike traditional *k*-fold cross validation, time series cross validation methods
    respect the temporal order of data points. Two commonly used techniques for time
    series cross validation are the rolling window and expanding window methods.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的*k*折交叉验证不同，时间序列交叉验证方法尊重数据点的时间顺序。用于时间序列交叉验证的两种常用技术是滚动窗口和扩展窗口方法。
- en: In *rolling window cross validation*, a fixed-size training window is moved
    iteratively over the time series data. At each step, the model is trained on the
    observations within the window and evaluated on the subsequent window. This process
    is repeated until the end of the data is reached. The window size can be defined
    based on a specific time duration or a fixed number of observations. [Figure 9-6](#figure-9-6)
    shows an illustration of rolling window cross validation.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在*滚动窗口交叉验证*中，固定大小的训练窗口在时间序列数据上进行迭代移动。在每个步骤中，模型在窗口内的观察结果上进行训练，并在随后的窗口上进行评估。该过程重复，直到达到数据的末尾。窗口大小可以根据特定的时间持续时间或固定数量的观察来定义。[图9-6](#figure-9-6)显示了滚动窗口交叉验证的示意图。
- en: '![](assets/dlff_0906.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图9-6](assets/dlff_0906.png)'
- en: Figure 9-6\. Rolling window cross validation
  id: totrans-104
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-6 滚动窗口交叉验证
- en: In *expanding window cross validation*, the training set starts with a small
    initial window and expands over time, incorporating additional data points at
    each step. The model is trained on the available data up to a specific point and
    evaluated on the subsequent time period. Similar to the rolling window approach,
    this process is repeated until the end of the data is reached. [Figure 9-7](#figure-9-7)
    shows an illustration of expanding window cross validation.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在*扩展窗口交叉验证*中，训练集从一个小的初始窗口开始，并随着时间的推移不断扩展，每个步骤都会纳入更多的数据点。模型根据可用数据训练到特定点，并在随后的时间段进行评估。与滚动窗口方法类似，该过程重复，直到达到数据的末尾。[图9-7](#figure-9-7)显示了扩展窗口交叉验证的示意图。
- en: '![](assets/dlff_0907.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图9-7](assets/dlff_0907.png)'
- en: Figure 9-7\. Expanding window cross validation
  id: totrans-107
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-7 扩展窗口交叉验证
- en: During each iteration of time series cross validation, the model’s performance
    is measured using appropriate evaluation metrics. The performance results obtained
    from each iteration can be aggregated and summarized to assess the model’s overall
    performance on the time series data.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间序列交叉验证的每次迭代中，使用适当的评估指标来衡量模型的性能。从每次迭代获得的性能结果可以进行汇总和总结，以评估模型在时间序列数据上的整体性能。
- en: Multiperiod Forecasting
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多期预测
- en: '*Multiperiod forecasting* (MPF) is a technique that aims to forecast more than
    just the next period. It aims to generate a path with *n* periods as defined by
    the user. There are two ways to approach MPF:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '*多期预测*（MPF）是一种旨在预测不止下一期的技术。它旨在生成一个由用户定义的*n*期路径。有两种方法可以处理MPF：'
- en: Recursive model
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 递归模型
- en: The *recursive model* uses the prediction as an input for the next prediction.
    As you may have already guessed, the recursive model may quickly get off track
    due to the exponentially rising error term from predicting while using predictions
    as inputs.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '*递归模型*将预测作为下一个预测的输入。正如你可能已经猜到的那样，递归模型可能会由于在预测时使用预测作为输入而快速偏离轨道，导致指数级增长的误差项。'
- en: Direct model
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 直接模型
- en: The *direct model* trains the model from the beginning into outputting multiple
    forecasts in their respective time periods. This model is likely to be more robust
    than the recursive model.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '*直接模型*从头开始训练模型，以在其各自的时间期间输出多个预测。该模型可能比递归模型更加健壮。'
- en: 'Let’s start with the recursive model. Mathematically speaking, its most basic
    form can be represented as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从递归模型开始。从数学上讲，它的最基本形式可以表示如下：
- en: <math alttext="upper P r e d i c t i o n Subscript i Baseline equals f x left-parenthesis
    upper P r e d i c t i o n Subscript i minus 1 Baseline comma period period period
    comma upper P r e d i c t i o n Subscript i minus n Baseline right-parenthesis"><mrow><mi>P</mi>
    <mi>r</mi> <mi>e</mi> <mi>d</mi> <mi>i</mi> <mi>c</mi> <mi>t</mi> <mi>i</mi> <mi>o</mi>
    <msub><mi>n</mi> <mi>i</mi></msub> <mo>=</mo> <mi>f</mi> <mi>x</mi> <mrow><mo>(</mo>
    <mi>P</mi> <mi>r</mi> <mi>e</mi> <mi>d</mi> <mi>i</mi> <mi>c</mi> <mi>t</mi> <mi>i</mi>
    <mi>o</mi> <msub><mi>n</mi> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>,</mo> <mo lspace="0%" rspace="0%">.</mo> <mo lspace="0%" rspace="0%">.</mo>
    <mo lspace="0%" rspace="0%">.</mo> <mo>,</mo> <mi>P</mi> <mi>r</mi> <mi>e</mi>
    <mi>d</mi> <mi>i</mi> <mi>c</mi> <mi>t</mi> <mi>i</mi> <mi>o</mi> <msub><mi>n</mi>
    <mrow><mi>i</mi><mo>-</mo><mi>n</mi></mrow></msub> <mo>)</mo></mrow></mrow></math>
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper P r e d i c t i o n Subscript i Baseline equals f x left-parenthesis
    upper P r e d i c t i o n Subscript i minus 1 Baseline comma period period period
    comma upper P r e d i c t i o n Subscript i minus n Baseline right-parenthesis"><mrow><mi>P</mi>
    <mi>r</mi> <mi>e</mi> <mi>d</mi> <mi>i</mi> <mi>c</mi> <mi>t</mi> <mi>i</mi> <mi>o</mi>
    <msub><mi>n</mi> <mi>i</mi></msub> <mo>=</mo> <mi>f</mi> <mi>x</mi> <mrow><mo>(</mo>
    <mi>P</mi> <mi>r</mi> <mi>e</mi> <mi>d</mi> <mi>i</mi> <mi>c</mi> <mi>t</mi> <mi>i</mi>
    <mi>o</mi> <msub><mi>n</mi> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>,</mo> <mo lspace="0%" rspace="0%">.</mo> <mo lspace="0%" rspace="0%">.</mo>
    <mo lspace="0%" rspace="0%">.</mo> <mo>,</mo> <mi>P</mi> <mi>r</mi> <mi>e</mi>
    <mi>d</mi> <mi>i</mi> <mi>c</mi> <mi>t</mi> <mi>i</mi> <mi>o</mi> <msub><mi>n</mi>
    <mrow><mi>i</mi><mo>-</mo><mi>n</mi></mrow></msub> <mo>)</mo></mrow></mrow></math>
- en: This section will use weather data and an economic indicator to apply the deep
    learning algorithm.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将使用天气数据和经济指标应用深度学习算法。
- en: The first step in predictive analysis is to get to know the data, so let’s see
    what the algorithm will aim to forecast. The first time series is the average
    daily temperature in Basel, Switzerland, since 2005\. [Figure 9-8](#figure-9-8)
    shows the time series.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 预测分析的第一步是了解数据，所以让我们看看算法将要预测的内容。第一个时间序列是自2005年以来瑞士巴塞尔的平均日温度。[Figure 9-8](#figure-9-8)显示了时间序列。
- en: The second time series is the Institute for Supply Management’s Purchasing Managers’
    Index (ISM PMI), a widely recognized economic indicator in the United States that
    provides insight into the health of the manufacturing sector and the overall economy.
    The index is based on a monthly survey of purchasing managers from various industries,
    including manufacturing, and assesses key factors such as new orders, production,
    employment, supplier deliveries, and inventories.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个时间序列是美国供应管理协会采购经理人指数（ISM PMI），这是一个广为认可的经济指标，提供有关制造业健康和整体经济状况的见解。该指数基于对来自各行业（包括制造业）的采购经理的月度调查，并评估新订单、生产、就业、供应商交付和库存等关键因素。
- en: '![](assets/dlff_0908.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlff_0908.png)'
- en: Figure 9-8\. A sample from the dataset showing the seasonal nature of temperature
  id: totrans-121
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-8\. 数据集样本，显示温度的季节性特征
- en: The index is reported as a percentage, with a value above 50 indicating expansion
    in the manufacturing sector and a value below 50 suggesting contraction. A higher
    PMI typically indicates positive economic growth, while a lower PMI may signal
    economic slowdown or recessionary conditions. The ISM PMI is closely monitored
    by policymakers, investors, and businesses as it can offer valuable insights into
    economic trends and potential shifts in the business cycle. [Figure 9-9](#figure-9-9)
    shows the ISM PMI historical observations.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 该指数报告为百分比，值超过50表示制造业扩张，值低于50表示收缩。更高的PMI通常表明积极的经济增长，而较低的PMI可能表示经济放缓或衰退条件。由于ISM
    PMI可以提供有关经济趋势和商业周期潜在转变的宝贵见解，因此它受到政策制定者、投资者和企业的密切关注。[Figure 9-9](#figure-9-9)显示了ISM
    PMI的历史观测。
- en: The aim of the forecast is to test the algorithm’s ability to push through the
    noise and model the original mean-reverting nature of the ISM PMI. Let’s start
    with the recursive model.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 预测的目标是测试算法穿透噪音并对ISM PMI的原始均值回归特性建模的能力。让我们从递归模型开始。
- en: 'The framework for the recursive model is as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 递归模型的框架如下：
- en: Train the data on the training set using the usual 80/20 split.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用通常的80/20分割在训练集上训练数据。
- en: Forecast the first observation using the inputs needed from the test set.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用来自测试集的所需输入预测第一个观察结果。
- en: Forecast the second observation using the last prediction in step 2 and the
    required data from the test set while dropping the first observation.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用步骤2中的最后一个预测和测试集中所需的数据预测第二个观察结果，同时放弃第一个观察结果。
- en: Repeat step 3 until reaching the desired number of predictions. At some point,
    a prediction is made by solely looking at previous predictions.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤3，直到达到所需的预测数量。在某一点上，预测是通过仅查看先前的预测来进行的。
- en: '![](assets/dlff_0909.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlff_0909.png)'
- en: Figure 9-9\. A sample from the imported dataset showing the mean-reverting nature
    of the ISM PMI
  id: totrans-130
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-9\. 导入数据集样本，显示ISM PMI的均值回归特性
- en: Note
  id: totrans-131
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Up until now, you have been evaluating accuracy using `ca⁠lc⁠ul⁠ate_​ac⁠cur⁠acy()`,
    which works when you are predicting positive or negative values (such as EURUSD
    price changes). When dealing with multiperiod forecasting of values that do not
    hover around zero, it is better to calculate the directional accuracy, which is
    basically the same calculation but does not hover around zero. For this, the function
    `calculate_directional_accuracy()` is used. Remember that the functions can be
    found in *master_function.py* in the book’s [GitHub repository](https://oreil.ly/5YGHI).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您一直在使用`ca⁠lc⁠ul⁠ate_​ac⁠cur⁠acy()`评估准确性，当您预测正负值（如EURUSD价格变动）时，这是有效的。但是，在预测不围绕零点波动的多周期值时，最好计算方向准确性，这基本上是相同的计算，但不围绕零点。为此，使用函数`calculate_directional_accuracy()`。请记住，这些函数可以在书籍的*master_function.py*中的[GitHub仓库](https://oreil.ly/5YGHI)中找到。
- en: 'Let’s start with the average temperature in Basel. Import the dataset using
    the following code (make sure you download the historical observations data from
    the [GitHub repository](https://oreil.ly/5YGHI)):'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从巴塞尔的平均温度开始。使用以下代码导入数据集（确保您从[GitHub仓库](https://oreil.ly/5YGHI)下载了历史观测数据）：
- en: '[PRE16]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Next, preprocess the data:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，预处理数据：
- en: '[PRE17]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Design the architecture of the MLP with multiple hidden layers. Then, fit and
    predict on a recursive basis:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 设计具有多个隐藏层的MLP架构。然后，基于递归方式进行拟合和预测：
- en: '[PRE18]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The `recursive_mpf()` function takes the following arguments:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 函数`recursive_mpf()`接受以下参数：
- en: The test set features that will continuously be updated. They are represented
    by the variable `x_test`.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将持续更新的测试集特征。它们由变量`x_test`表示。
- en: The test set dependent variables. They are represented by the variable `y_test`.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将持续更新的测试集依赖变量。它们由变量`y_test`表示。
- en: The number of lags. This variable is represented by `num_lags`.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 滞后数。这个变量由`num_lags`表示。
- en: The fitted model as defined by the variable `model`.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变量`model`定义的拟合模型。
- en: The type of architecture as represented by the argument `architecture`. It can
    either be `MLP` for two-dimensional arrays or `LSTM` for three-dimensional arrays.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 架构类型由参数`architecture`表示。它可以是`MLP`，适用于二维数组，或者`LSTM`，适用于三维数组。
- en: '[Figure 9-10](#figure-9-10) shows the predictions versus the real values (the
    dashed time series after the cutoff line). Notice how the deep neural network
    re-creates the seasonal characteristics of the time series (albeit with some imperfections)
    and projects it well into the future with no required knowledge along the way.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9-10](#figure-9-10)显示了预测与实际值之间的对比（虚线时间序列在截断线之后）。注意深度神经网络如何重新创建时间序列的季节特性（虽然存在一些缺陷），并且很好地将其投影到未来，而无需沿途获取任何必要的知识。'
- en: '![](assets/dlff_0910.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlff_0910.png)'
- en: Figure 9-10\. Multiperiod forecasts versus real values
  id: totrans-147
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-10\. 多期预测与实际值
- en: Note
  id: totrans-148
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Many machine and deep learning algorithms are able to model this relationship
    well. This example used MLPs, but this does not undermine other models, even simple
    ones such as linear regression. A good task for you would be to try applying the
    same example using a model of your choice (such as LSTM) and comparing the results.
    If you are using an LSTM model, make sure you set `architecture = 'LSTM'`.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习和深度学习算法能够很好地建模这种关系。本例中使用了MLP，但这并不贬低其他模型，甚至简单的模型如线性回归也同样适用。你可以尝试用你选择的模型（如LSTM）应用相同的例子并比较结果。如果你使用LSTM模型，请确保设置`architecture
    = 'LSTM'`。
- en: 'Now apply the same process on the second time series. You only need to change
    the name of the imported file and the hyperparameters (as you see fit):'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在在第二个时间序列上应用相同的过程。你只需要更改导入文件的名称和超参数（按照你的需求）：
- en: '[PRE19]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[Figure 9-11](#figure-9-11) shows the predictions (dashed line) versus the
    real values.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9-11](#figure-9-11)显示了预测（虚线）与实际值之间的对比。'
- en: '![](assets/dlff_0911.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlff_0911.png)'
- en: Figure 9-11\. Forecasting multiple periods ahead; predicted data in thin solid
    line and test data in dashed line
  id: totrans-154
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-11\. 提前多个周期预测；预测数据用细实线表示，测试数据用虚线表示
- en: 'The trained model is not too complex so as to avoid overfitting. However, it
    does manage to time turning points quite well during the first projections. Naturally,
    over time, this ability slowly fades away. Tweaking the hyperparameters is the
    key to achieving good directional accuracy. Start with the following hyperparameters:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型并不复杂，以避免过拟合。然而，在最初的预测期间，它确实能够很好地预测转折点。自然地，这种能力随着时间的推移逐渐减弱。调整超参数是实现良好方向准确性的关键。从以下超参数开始：
- en: '[PRE20]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The second MPF technique trains the model from the beginning into outputting
    multiple forecasts in their respective time periods. Mathematically, it can be
    represented as follows:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种MPF技术从头开始训练模型，输出多个在各自时间段的预测。数学上可以表示如下：
- en: <math alttext="StartLayout 1st Row  upper P r e d i c t i o n Subscript i Baseline
    equals f x left-parenthesis r e a l i n p u t Subscript i minus 1 Baseline comma
    period period period comma i n p u t Subscript i minus n Baseline right-parenthesis
    2nd Row  upper P r e d i c t i o n Subscript i plus 1 Baseline equals f x left-parenthesis
    r e a l i n p u t Subscript i minus 1 Baseline comma period period period comma
    i n p u t Subscript i minus n Baseline right-parenthesis 3rd Row  upper P r e
    d i c t i o n Subscript i plus 2 Baseline equals f x left-parenthesis r e a l
    i n p u t Subscript i minus 1 Baseline comma period period period comma i n p
    u t Subscript i minus n Baseline right-parenthesis EndLayout"><mtable><mtr><mtd
    columnalign="left"><mrow><mi>P</mi> <mi>r</mi> <mi>e</mi> <mi>d</mi> <mi>i</mi>
    <mi>c</mi> <mi>t</mi> <mi>i</mi> <mi>o</mi> <msub><mi>n</mi> <mi>i</mi></msub>
    <mo>=</mo> <mi>f</mi> <mi>x</mi> <mrow><mo>(</mo> <mi>r</mi> <mi>e</mi> <mi>a</mi>
    <mi>l</mi> <mi>i</mi> <mi>n</mi> <mi>p</mi> <mi>u</mi> <msub><mi>t</mi> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>,</mo> <mo lspace="0%" rspace="0%">.</mo> <mo lspace="0%" rspace="0%">.</mo>
    <mo lspace="0%" rspace="0%">.</mo> <mo>,</mo> <mi>i</mi> <mi>n</mi> <mi>p</mi>
    <mi>u</mi> <msub><mi>t</mi> <mrow><mi>i</mi><mo>-</mo><mi>n</mi></mrow></msub>
    <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mi>P</mi>
    <mi>r</mi> <mi>e</mi> <mi>d</mi> <mi>i</mi> <mi>c</mi> <mi>t</mi> <mi>i</mi> <mi>o</mi>
    <msub><mi>n</mi> <mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub> <mo>=</mo>
    <mi>f</mi> <mi>x</mi> <mrow><mo>(</mo> <mi>r</mi> <mi>e</mi> <mi>a</mi> <mi>l</mi>
    <mi>i</mi> <mi>n</mi> <mi>p</mi> <mi>u</mi> <msub><mi>t</mi> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>,</mo> <mo lspace="0%" rspace="0%">.</mo> <mo lspace="0%" rspace="0%">.</mo>
    <mo lspace="0%" rspace="0%">.</mo> <mo>,</mo> <mi>i</mi> <mi>n</mi> <mi>p</mi>
    <mi>u</mi> <msub><mi>t</mi> <mrow><mi>i</mi><mo>-</mo><mi>n</mi></mrow></msub>
    <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mi>P</mi>
    <mi>r</mi> <mi>e</mi> <mi>d</mi> <mi>i</mi> <mi>c</mi> <mi>t</mi> <mi>i</mi> <mi>o</mi>
    <msub><mi>n</mi> <mrow><mi>i</mi><mo>+</mo><mn>2</mn></mrow></msub> <mo>=</mo>
    <mi>f</mi> <mi>x</mi> <mrow><mo>(</mo> <mi>r</mi> <mi>e</mi> <mi>a</mi> <mi>l</mi>
    <mi>i</mi> <mi>n</mi> <mi>p</mi> <mi>u</mi> <msub><mi>t</mi> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>,</mo> <mo lspace="0%" rspace="0%">.</mo> <mo lspace="0%" rspace="0%">.</mo>
    <mo lspace="0%" rspace="0%">.</mo> <mo>,</mo> <mi>i</mi> <mi>n</mi> <mi>p</mi>
    <mi>u</mi> <msub><mi>t</mi> <mrow><mi>i</mi><mo>-</mo><mi>n</mi></mrow></msub>
    <mo>)</mo></mrow></mrow></mtd></mtr></mtable></math>
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row  upper P r e d i c t i o n Subscript i Baseline
    equals f x left-parenthesis r e a l i n p u t Subscript i minus 1 Baseline comma
    period period period comma i n p u t Subscript i minus n Baseline right-parenthesis
    2nd Row  upper P r e d i c t i o n Subscript i plus 1 Baseline equals f x left-parenthesis
    r e a l i n p u t Subscript i minus 1 Baseline comma period period period comma
    i n p u t Subscript i minus n Baseline right-parenthesis 3rd Row  upper P r e
    d i c t i o n Subscript i plus 2 Baseline equals f x left-parenthesis r e a l
    i n p u t Subscript i minus 1 Baseline comma period period period comma i n p
    u t Subscript i minus n Baseline right-parenthesis EndLayout"><mtable><mtr><mtd
    columnalign="left"><mrow><mi>P</mi> <mi>r</mi> <mi>e</mi> <mi>d</mi> <mi>i</mi>
    <mi>c</mi> <mi>t</mi> <mi>i</mi> <mi>o</mi> <msub><mi>n</mi> <mi>i</mi></msub>
    <mo>=</mo> <mi>f</mi> <mi>x</mi> <mrow><mo>(</mo> <mi>r</mi> <mi>e</mi> <mi>a</mi>
    <mi>l</mi> <mi>i</mi> <mi>n</mi> <mi>p</mi> <mi>u</mi> <msub><mi>t</mi> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>,</mo> <mo lspace="0%" rspace="0%">.</mo> <mo lspace="0%" rspace="0%">.</mo>
    <mo lspace="0%" rspace="0%">.</mo> <mo>,</mo> <mi>i</mi> <mi>n</mi> <mi>p</mi>
    <mi>u</mi> <msub><mi>t</mi> <mrow><mi>i</mi><mo>-</mo><mi>n</mi></mrow></msub>
    <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mi>P</mi>
    <mi>r</mi> <mi>e</mi> <mi>d</mi> <mi>i</mi> <mi>c</mi> <mi>t</mi> <mi>i</mi> <mi>o</mi>
    <msub><mi>n</mi> <mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub> <mo>=</mo>
    <mi>f</mi> <mi>x</mi> <mrow><mo>(</mo> <mi>r</mi> <mi>e</mi> <mi>a</mi> <mi>l</mi>
    <mi>i</mi> <mi>n</mi> <mi>p</mi> <mi>u</mi> <msub><mi>t</mi> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>,</mo> <mo lspace="0%" rspace="0%">.</mo> <mo lspace="0%" rspace="0%">.</mo>
    <mo lspace="0%" rspace="0%">.</mo> <mo>,</mo> <mi>i</mi> <mi>n</mi> <mi>p</mi>
    <mi>u</mi> <msub><mi>t</mi> <mrow><mi>i</mi><mo>-</mo><mi>n</mi></mrow></msub>
    <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mi>P</mi>
    <mi>r</mi> <mi>e</mi> <mi>d</mi> <mi>i</mi> <mi>c</mi> <mi>t</mi> <mi>i</mi> <mi>o</mi>
    <msub><mi>n</mi> <mrow><mi>i</mi><mo>+</mo><mn>2</mn></mrow></msub> <mo>=</mo>
    <mi>f</mi> <mi>x</mi> <mrow><mo>(</mo> <mi>r</mi> <mi>e</mi> <mi>a</mi> <mi>l</mi>
    <mi>i</mi> <mi>n</mi> <mi>p</mi> <mi>u</mi> <msub><mi>t</mi> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>,</mo> <mo lspace="0%" rspace="0%">.</mo> <mo lspace="0%" rspace="0%">.</mo>
    <mo lspace="0%" rspace="0%">.</mo> <mo>,</mo> <mi>i</mi> <mi>n</mi> <mi>p</mi>
    <mi>u</mi> <msub><mi>t</mi> <mrow><mi>i</mi><mo>-</mo><mi>n</mi></mrow></msub>
    <mo>)</mo></mrow></mrow></mtd></mtr></mtable></math>
- en: 'The framework for the recursive model is as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 递归模型的框架如下：
- en: Create a function that relates the desired number of inputs to the desired number
    of outputs. This means that the last layer of the neural network will contain
    a number of outputs equal to the number of forecasting periods you want to project
    into the future.
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个函数，将所需的输入数量与所需的输出数量相关联。这意味着神经网络的最后一层将包含等于你想要向未来投影的预测期数的输出数量。
- en: Train the model to predict multiple outputs at every time step based on the
    inputs from the same time step.
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型以预测每个时间步的多个输出，基于同一时间步的输入。
- en: 'Let’s continue with the ISM PMI. As usual, import the required libraries:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续使用ISM PMI。像往常一样，导入所需的库：
- en: '[PRE21]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Import and preprocess the data while setting the hyperparameters:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 导入和预处理数据时设置超参数：
- en: '[PRE22]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The `direct_mpf()` function takes the following arguments:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 函数`direct_mpf()`接受以下参数：
- en: The dataset represented by the variable `data`
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变量`data`表示的数据集
- en: The number of lags represented by the variable `num_lags`
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变量`num_lags`表示的滞后数目。
- en: The split represented by the variable `train_test_split`
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变量`train_test_split`表示的拆分。
- en: The number of observations to project represented by the variable `forecast_horizon`
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变量`forecast_horizon`表示的要预测的观察次数。
- en: 'Prepare the arrays, design the architecture, and predict the data for a horizon
    of 18 months (since the ISM PMI is a monthly indicator):'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 准备数组，设计架构，并预测数据，预测时间跨度为18个月（因为ISM PMI是一个月度指标）：
- en: '[PRE23]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[Figure 9-12](#figure-9-12) shows the predicted data and the test data at this
    point.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9-12](#figure-9-12) 在此时显示了预测数据和测试数据。'
- en: '![](assets/dlff_0912.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlff_0912.png)'
- en: Figure 9-12\. Multiperiod forecasts of model versus real values, with some optimization
  id: totrans-175
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-12。模型的多周期预测与实际值比较，带有一些优化
- en: The interpretation of the model at the time of the forecast was for a consecutive
    drop in the ISM PMI for 18 months. The model seems to have done a good job at
    predicting this direction. Note that you may get different results due to the
    random initialization of the algorithm, which may impact its convergence to a
    minimum loss function. You can use `random_state` to get the same results every
    time (you saw this in [Chapter 7](ch07.html#ch07)).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在预测时，模型解释了连续18个月ISM PMI的下降。模型似乎在预测这个趋势方面表现良好。请注意，由于算法的随机初始化可能会影响其收敛到最小损失函数，因此可能会得到不同的结果。您可以使用`random_state`以确保每次获得相同的结果（您在[第7章](ch07.html#ch07)中看到了这一点）。
- en: Note
  id: totrans-177
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The ISM PMI has a positive correlation with the US gross domestic product (GDP)
    and a slight positive correlation with the S&P 500\. To be more precise, bottoms
    in the ISM PMI have coincided with bottoms in the equity markets.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ISM PMI与美国国内生产总值（GDP）呈正相关，与标准普尔500指数略有正相关。更确切地说，ISM PMI的底部与股票市场的底部相吻合。
- en: 'Out of curiosity, let’s try running the model on very simple and basic hyperparameters:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 出于好奇，让我们尝试使用非常简单和基本的超参数来运行模型：
- en: '[PRE24]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Obviously, with one lag, the model will only take into account the previous
    value to learn how to predict the future. The hidden layers will only contain
    two neurons each and will run for only 10 epochs using a batch size of 1\. Naturally,
    you would not expect satisfying results by using these hyperparameters. [Figure 9-13](#figure-9-13)
    compares the predicted values to the real values. Notice the huge discrepancy
    as the model does not pick on the magnitude or the direction.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，对于一个滞后的情况，模型只会考虑先前的值来学习如何预测未来。隐藏层每个仅包含两个神经元，仅运行10个时期，使用批量大小为1。自然地，使用这些超参数不会得到令人满意的结果。[图 9-13](#figure-9-13)
    比较了预测值和实际值之间的巨大差异，因为模型未能捕捉到幅度或方向。
- en: '![](assets/dlff_0913.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlff_0913.png)'
- en: Figure 9-13\. Multiperiod forecasts of the model versus real values, using basic
    hyperparameters
  id: totrans-183
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-13。使用基本超参数的模型的多周期预测与实际值比较
- en: This is why hyperparameter optimization is important and a certain degree of
    complexity is needed. After all, these time series are not simple and carry a
    significant amount of noise in them.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么超参数优化很重要，需要一定程度的复杂性。毕竟，这些时间序列并不简单，其中带有相当大的噪音。
- en: 'Finally, let’s have a look at the results of running the following hyperparameters
    on Basel’s temperature data, as you saw at the beginning of this section:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们来看看在巴塞尔温度数据上运行以下超参数的结果，就像您在本节开头看到的那样：
- en: '[PRE25]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[Figure 9-14](#figure-9-14) compares the predicted values to the real values
    using the temperature time series. The number of predicted observations is 500.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9-14](#figure-9-14) 比较了使用温度时间序列的预测值与实际值，预测的观察次数为500。'
- en: '![](assets/dlff_0914.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlff_0914.png)'
- en: Figure 9-14\. Multiperiod forecasts of the model versus real values, using the
    temperature time series
  id: totrans-189
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-14。使用温度时间序列的模型的多周期预测与实际值比较
- en: Which prediction technique to use depends on your preferences and needs. It
    is worth mentioning an additional MPF technique referred to as the *multioutput
    model*, which is a one-shot forecast of a number of values. This means that the
    model is trained over the training set with the aim of producing an instant predefined
    number of outputs (predictions). Obviously, this model may be computationally
    expensive and would require a sizable amount of data.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 使用哪种预测技术取决于您的偏好和需求。值得一提的是另一种MPF技术被称为*多输出模型*，它是预测一系列值的一次性预测。这意味着模型在训练集上训练，旨在产生预先定义数量的输出（预测）。显然，这种模型可能计算成本高昂，并且需要大量的数据。
- en: Applying Regularization to MLPs
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将正则化应用于MLPs
- en: '[Chapter 8](ch08.html#ch08) discussed two regularization concepts regarding
    deep learning:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '[第8章](ch08.html#ch08)讨论了深度学习中涉及的两种正则化概念：'
- en: Dropout as a regularization technique that randomly deactivates neurons during
    training to prevent overfitting
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 丢弃作为一种正则化技术，在训练期间随机关闭神经元以防止过拟合
- en: Early stopping as a method to prevent overfitting by monitoring the model’s
    performance and stopping training when performance starts to degrade
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提前停止作为一种方法，通过监控模型性能并在性能开始下降时停止训练来防止过拟合
- en: Another regularization technique worth discussing is *batch normalization*,
    a technique used in deep learning to improve the training and generalization of
    neural networks. It normalizes the inputs of each layer within a mini batch during
    training, which helps in stabilizing and accelerating the learning process.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种值得讨论的正则化技术是*批归一化*，这是深度学习中用于改善神经网络训练和泛化的技术。它在训练期间规范化每个层的输入，有助于稳定和加速学习过程。
- en: 'The main idea behind batch normalization is to ensure that the inputs to a
    layer have zero mean and unit variance. This normalization is applied independently
    to each feature (or neuron) within the layer. The process can be summarized in
    the following steps:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 批归一化背后的主要思想是确保层的输入均值为零，方差为单位。该归一化独立应用于每个层内的每个特征（或神经元）。该过程可以总结为以下步骤：
- en: For each feature in a mini batch, calculate the mean and variance across all
    the samples in the batch.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个特征在一个小批次中，计算批次中所有样本的均值和方差。
- en: Subtract the mean and divide by the standard deviation (the square root of the
    variance) for each feature.
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个特征，减去均值并除以标准差（方差的平方根）。
- en: After normalization, the features are scaled and shifted by learnable parameters.
    These parameters allow the model to learn the optimal scale and shift for each
    normalized feature.
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 归一化后，特征通过可学习参数进行缩放和移位。这些参数允许模型为每个归一化特征学习最佳的缩放和移位。
- en: 'This section presents a simple forecasting task using LSTMs with the addition
    of the three regularization techniques. The time series is the S&P 500’s 20-day
    rolling autocorrelation data. Import the required libraries:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了使用LSTM进行简单预测任务的示例，同时添加了三种正则化技术。时间序列是标准普尔500的20天滚动自相关数据。导入所需的库：
- en: '[PRE26]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Import and preprocess the data:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 导入并预处理数据：
- en: '[PRE27]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Calculate the 20-day autocorrelation of the close prices:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 计算收盘价格的20天自相关性：
- en: '[PRE28]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Note
  id: totrans-206
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'In Python, a `lambda` function, also known as an *anonymous* function, is a
    small, unnamed function that can have any number of arguments but can only have
    one expression. These functions are often used for creating simple, inline functions
    without needing to define a full function using the `def` keyword. Here’s a simple
    example to illustrate how `lambda` works:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，`lambda`函数，也称为*匿名*函数，是一个小型的未命名函数，可以有任意数量的参数，但只能有一个表达式。这些函数通常用于创建简单的内联函数，而无需使用`def`关键字定义完整的函数。以下是一个简单的示例，说明了`lambda`的工作原理：
- en: '[PRE29]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The output will be the float 5.0 stored in `result`.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将是存储在`result`中的浮点数5.0。
- en: The `apply()` function is a method that is available in *pandas*. It is primarily
    used to apply a given function along an axis of a dataframe.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '`apply()`函数是*pandas*中可用的方法。它主要用于沿着数据帧的轴应用给定函数。'
- en: 'Before continuing, try plotting the S&P 500 price data versus its 20-day autocorrelation
    that you just calculated. Use this code to generate [Figure 9-15](#figure-9-15):'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 继续之前，请尝试绘制您刚刚计算的标准普尔500价格数据与其20天自相关性的图。使用以下代码生成[图 9-15](#figure-9-15)：
- en: '[PRE30]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '![](assets/dlff_0915.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlff_0915.png)'
- en: Figure 9-15\. The S&P 500 versus its 20-day price autocorrelation (lag = 1)
  id: totrans-214
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-15\. 标准普尔500与其20天价格自相关性（滞后 = 1）
- en: 'What you should retain from the chart and from the intuition of autocorrelation
    is that whenever autocorrelation approaches 1.00, the current trend may break,
    thus leading to a market correction. This is not a perfect assumption, but you
    can follow these basic rules to interpret the rolling autocorrelation observations:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 从图表和自相关性的直觉中应该记住的是，每当自相关性接近1.00时，当前趋势可能会中断，从而导致市场纠正。这不是一个完美的假设，但您可以遵循这些基本规则来解释滚动自相关性的观察结果：
- en: A trending market (bullish or bearish) will have its autocorrelation approach
    1.00 sooner or later. When this happens, it may signal a pause in the underlying
    trend, or in rarer occasions, a full reversal.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个趋势市场（看涨或看跌）其自相关性迟早会接近1.00。当这种情况发生时，可能表明潜在趋势暂停，或者在更少的情况下，完全反转。
- en: A sideways (ranging) market will have a low autocorrelation. If the autocorrelation
    approaches historical lows, then it may mean that the market is ready to trend.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个横向（波动）市场的自相关性会很低。如果自相关接近历史最低点，则可能意味着市场准备好趋势。
- en: 'Let’s now continue building the algorithm. The next step is to set the hyperparameters
    and prepare the arrays:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们继续构建算法。下一步是设置超参数并准备数组：
- en: '[PRE31]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Transform the input arrays into three-dimensional structures so that they are
    processed into the LSTM architecture with no issues:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 将输入数组转换为三维结构，以便在LSTM架构中无问题地处理它们：
- en: '[PRE32]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Design the LSTM architecture and add the dropout layer and batch normalization.
    Add the early stopping implementation while setting `restore_best_weights` to
    `True` so as to keep the best parameters for the prediction over the test data:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 设计LSTM架构并添加dropout层和批归一化。在设置`restore_best_weights`为`True`的同时，添加早停实现以保持对测试数据的最佳预测参数：
- en: '[PRE33]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Predict and plot the results:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 预测并绘制结果：
- en: '[PRE34]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[Figure 9-16](#figure-9-16) shows the predictions versus the real values. The
    model has stopped the training before reaching 100 epochs due to the callback
    from the early stopping mechanism.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9-16](#figure-9-16) 显示了预测与实际值的对比。由于早停机制的回调，模型在达到100个周期之前已停止训练。'
- en: '![](assets/dlff_0916.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlff_0916.png)'
- en: Figure 9-16\. Predicting correlation
  id: totrans-228
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-16\. 预测相关性
- en: 'The results are as follows:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE35]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: It’s important to note that using indicators such as rolling autocorrelation
    should be done with caution. They provide insights into historical patterns, but
    they don’t guarantee future performance. Additionally, the effectiveness of rolling
    autocorrelation as a technical indicator depends on the nature of the data and
    the context in which it’s being used. You can try applying the MPF method on the
    autocorrelation data.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，应谨慎使用诸如滚动自相关等指标。它们提供了历史模式的见解，但不能保证未来的表现。此外，滚动自相关作为技术指标的有效性取决于数据的性质和使用的上下文。您可以尝试将MPF方法应用于自相关数据。
- en: 'Other regularization techniques that exist include the following:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 其他存在的正则化技术包括以下内容：
- en: L1 and L2 regularization
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: L1和L2正则化
- en: Also known as *weight decay*, L1 and L2 regularization add a penalty term to
    the loss function based on the magnitude of the weights. *L1 regularization* adds
    the absolute values of the weights to the loss, which encourages sparsity in the
    model. *L2 regularization* adds the squared values of the weights, which discourages
    large weight values and tends to distribute the influence of features more evenly.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 也称为*权重衰减*，L1和L2正则化根据权重的大小向损失函数添加惩罚项。*L1正则化*将权重的绝对值加到损失中，促进模型的稀疏性。*L2正则化*将权重的平方值加到损失中，抑制大的权重值，并倾向于更均匀地分布特征的影响。
- en: DropConnect
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: DropConnect
- en: This technique is similar to dropout but is applied to connections rather than
    neurons. This technique randomly drops connections between layers during training.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术类似于dropout，但是应用于连接而不是神经元。这种技术在训练期间随机丢弃层之间的连接。
- en: Weight constraints
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 权重约束
- en: Limiting the magnitude of weight values can prevent the model from learning
    complex patterns from noise and helps regularize the model.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 限制权重值的大小可以防止模型从噪声中学习复杂模式，并帮助对模型进行正则化。
- en: Adversarial training
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗训练
- en: Training the model using adversarial examples can improve its robustness by
    making it more resistant to small perturbations in the input data.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 使用对抗性示例训练模型可以通过使其更抗小扰动来提高其鲁棒性。
- en: Using these regularization techniques doesn’t guarantee a better result than
    using the model without them. However, deep learning best practices encourage
    such techniques to avoid more serious problems like overfitting.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些正则化技术并不保证比不使用它们的模型结果更好。然而，深度学习的最佳实践鼓励使用这些技术来避免更严重的问题，如过拟合。
- en: Note
  id: totrans-242
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: When manually uploading an Excel file (using *pandas*, for example) that contains
    historical data, make sure that it has a shape of `(n, )` and not a shape of `(n,
    1)`. This ensures that when you use the `data_preprocessing()` function, the four
    training/test arrays will be created with the proper dimensions.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 当手动上传包含历史数据的Excel文件（例如使用*pandas*）时，请确保其形状为`(n, )`，而不是`(n, 1)`。这样做可以确保在使用`data_preprocessing()`函数时，四个训练/测试数组将以正确的维度创建。
- en: 'To transform an `(n, 1)` array to `(n, )`, use the following syntax:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 要将`(n, 1)`数组转换为`(n, )`，请使用以下语法：
- en: '[PRE36]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'To transform an `(n, )` array to `(n, 1)`, use the following syntax:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 要将`(n, )`数组转换为`(n, 1)`，请使用以下语法：
- en: '[PRE37]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Summary
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概要
- en: This chapter presented a few techniques that may improve the different machine
    and deep learning algorithms. I like to refer to such techniques as *satellites*
    since they hover around the main component, that is, neural networks. Optimizations
    and enhancements are crucial to the success of the analysis. For example, some
    markets may benefit from the forecasting threshold technique and fractional differentiation.
    Trial and error is key to understanding your data, and as you begin [Chapter 10](ch10.html#ch10)
    and learn about reinforcement learning, you will see that trial and error is not
    just a human task. It can also be a computer task.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了一些可能改进不同机器和深度学习算法的技术。我喜欢将这些技术称为*卫星*，因为它们围绕着主要组件——神经网络——盘旋。优化和增强对于分析的成功至关重要。例如，某些市场可能会从预测阈值技术和分数微分中受益。试错法是理解数据的关键，当您开始学习[第10章](ch10.html#ch10)并了解强化学习时，您会发现试错不仅仅是人类的任务，它也可以是计算机的任务。
