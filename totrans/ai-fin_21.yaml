- en: Appendix A. Interactive Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This appendix explores fundamental notions of neural networks with basic Python
    code—on the basis of both simple and shallow neural networks. The goal is to provide
    a good grasp and intuition for important concepts that often disappear behind
    high-level, abstract APIs when working with standard machine and deep learning
    packages.
  prefs: []
  type: TYPE_NORMAL
- en: 'The appendix has the following sections:'
  prefs: []
  type: TYPE_NORMAL
- en: '[“Tensors and Tensor Operations”](#app_inn_tensors) covers the basics of *tensors*
    and the operations implemented on them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[“Simple Neural Networks”](#app_inn_sinn) discusses *simple neural networks*,
    or neural networks that only have an input and an output layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[“Shallow Neural Networks”](#app_inn_shnn) focuses on *shallow neural networks*,
    or neural networks with one hidden layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tensors and Tensor Operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In addition to implementing several imports and configurations, the following
    Python code shows the four types of tensors relevant for the purposes of this
    appendix: scalar, vector, matrix, and cube tensors. Tensors are generally represented
    as potentially multidimensional `ndarray` objects in Python. For more details
    and examples, see Chollet (2017, ch. 2):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_interactive_neural_networks_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Scalar tensor
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_interactive_neural_networks_CO1-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Vector tensor
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_interactive_neural_networks_CO1-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Matrix tensor
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_interactive_neural_networks_CO1-7)'
  prefs: []
  type: TYPE_NORMAL
- en: Cube tensor
  prefs: []
  type: TYPE_NORMAL
- en: 'In a neural network context, several mathematical operations on tensors are
    of importance, such as element-wise operations or the dot product:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_interactive_neural_networks_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Broadcasting operation
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_interactive_neural_networks_CO2-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Element-wise operation
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_interactive_neural_networks_CO2-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Dot product with `NumPy` function
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_interactive_neural_networks_CO2-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Dot product in explicit notation
  prefs: []
  type: TYPE_NORMAL
- en: Simple Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Equipped with the basics of tensors, consider simple neural networks, which
    only have an input layer and an output layer.
  prefs: []
  type: TYPE_NORMAL
- en: Estimation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first problem is an *estimation problem* for which the labels are real-valued:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_interactive_neural_networks_CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Number of features
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_interactive_neural_networks_CO3-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Number of samples
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_interactive_neural_networks_CO3-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Random input layer
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_interactive_neural_networks_CO3-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Random weights
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](Images/5.png)](#co_interactive_neural_networks_CO3-7)'
  prefs: []
  type: TYPE_NORMAL
- en: Output layer via dot product
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](Images/6.png)](#co_interactive_neural_networks_CO3-9)'
  prefs: []
  type: TYPE_NORMAL
- en: Labels to be learned
  prefs: []
  type: TYPE_NORMAL
- en: 'The following Python code goes step by step through a learning episode, from
    the calculation of the errors to the calculation of the mean-squared error (MSE)
    after the weights have been updated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_interactive_neural_networks_CO4-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Errors in estimation
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_interactive_neural_networks_CO4-3)'
  prefs: []
  type: TYPE_NORMAL
- en: MSE value given the estimation
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_interactive_neural_networks_CO4-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Backward propagation (here `d = e`)^([1](app01.xhtml#idm45625236964232))
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_interactive_neural_networks_CO4-7)'
  prefs: []
  type: TYPE_NORMAL
- en: The learning rate
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](Images/5.png)](#co_interactive_neural_networks_CO4-8)'
  prefs: []
  type: TYPE_NORMAL
- en: The update values
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](Images/6.png)](#co_interactive_neural_networks_CO4-10)'
  prefs: []
  type: TYPE_NORMAL
- en: Weights before and after update
  prefs: []
  type: TYPE_NORMAL
- en: '[![7](Images/7.png)](#co_interactive_neural_networks_CO4-13)'
  prefs: []
  type: TYPE_NORMAL
- en: New output layer (estimation) after update
  prefs: []
  type: TYPE_NORMAL
- en: '[![8](Images/8.png)](#co_interactive_neural_networks_CO4-14)'
  prefs: []
  type: TYPE_NORMAL
- en: New error values after update
  prefs: []
  type: TYPE_NORMAL
- en: '[![9](Images/9.png)](#co_interactive_neural_networks_CO4-15)'
  prefs: []
  type: TYPE_NORMAL
- en: New MSE values after update
  prefs: []
  type: TYPE_NORMAL
- en: 'To improve the estimation, the same procedure needs to be repeated in general
    a larger number of times. In the following code, the learning rate is increased
    and the procedure is executed a few hundred times. The final MSE value is quite
    low and the estimation quite good:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_interactive_neural_networks_CO5-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Adjusted learning rate
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_interactive_neural_networks_CO5-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Initial random weights
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_interactive_neural_networks_CO5-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Number of learning steps
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_interactive_neural_networks_CO5-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Residual errors of the estimation
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](Images/5.png)](#co_interactive_neural_networks_CO5-6)'
  prefs: []
  type: TYPE_NORMAL
- en: Final weights of the network
  prefs: []
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The second problem is a *classification problem* for which the labels are binary
    and integer-valued. To improve the performance of the learning algorithm, a *sigmoid
    function* is used for activation (of the output layer). [Figure A-1](#figure_inn_01)
    shows the sigmoid function with its first derivative and compares it to a simple
    step function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![aiif 1601](Images/aiif_1601.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure A-1\. Step function, sigmoid function, and its first derivative
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To keep things simple, the classification problem is based on random binary
    features and binary labels data. Apart from the different features and labels
    data, only the activation for the output layer is different from the estimation
    problem. The learning algorithm for the updating of the neural networks’ weights
    is basically the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_interactive_neural_networks_CO6-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Input layer with binary features
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_interactive_neural_networks_CO6-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Sigmoid activated output layer
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_interactive_neural_networks_CO6-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Binary labels data
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_interactive_neural_networks_CO6-7)'
  prefs: []
  type: TYPE_NORMAL
- en: Backward propagation via the first derivative
  prefs: []
  type: TYPE_NORMAL
- en: 'As before, a loop with a larger number of iterations for the learning step
    is required to get to accurate classification results. Depending on the random
    numbers drawn, an accuracy of 100% is possible, as in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Shallow Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The neural network of the previous section consists only of an input layer and
    an output layer. In other words, input and output layers are directly connected.
    A *shallow neural network* has one hidden layer that is between the input and
    output layer. Given this structure, two sets of weights are required to connect
    the total of three layers in the neural network. This section analyzes shallow
    neural networks for estimation and classification.
  prefs: []
  type: TYPE_NORMAL
- en: Estimation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As in the previous section, let’s take the estimation problem first. The following
    Python code builds the neural network with the three layers and the two sets of
    weights. This first sequence of steps is usually called *forward propagation*.
    The input layer matrix in this context is in general of full rank, indicating
    that a perfect estimation result is possible:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_interactive_neural_networks_CO7-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The random input layer
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_interactive_neural_networks_CO7-3)'
  prefs: []
  type: TYPE_NORMAL
- en: The rank of the input layer matrix
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_interactive_neural_networks_CO7-4)'
  prefs: []
  type: TYPE_NORMAL
- en: The number of hidden units
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_interactive_neural_networks_CO7-5)'
  prefs: []
  type: TYPE_NORMAL
- en: The first set of random weights, given the `features` and `units` parameters
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](Images/5.png)](#co_interactive_neural_networks_CO7-7)'
  prefs: []
  type: TYPE_NORMAL
- en: The hidden layer, given the input layer and the weights
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](Images/6.png)](#co_interactive_neural_networks_CO7-9)'
  prefs: []
  type: TYPE_NORMAL
- en: The second set of random weights
  prefs: []
  type: TYPE_NORMAL
- en: '[![7](Images/7.png)](#co_interactive_neural_networks_CO7-11)'
  prefs: []
  type: TYPE_NORMAL
- en: The output layer, given the hidden layer and the weights
  prefs: []
  type: TYPE_NORMAL
- en: '[![8](Images/8.png)](#co_interactive_neural_networks_CO7-13)'
  prefs: []
  type: TYPE_NORMAL
- en: The random labels data
  prefs: []
  type: TYPE_NORMAL
- en: 'The second sequence of steps is usually called *backward propagation*—with
    respect to the estimation errors. The two sets of weights are updated, starting
    at the output layer and updating the set of weights `w1` between the hidden layer
    and the output layer. Afterwards, taking the updated weights `w1` into account,
    the set of weights `w0` between the input layer and the hidden layer is updated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_interactive_neural_networks_CO8-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Update procedure for the set of weights `w1`
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_interactive_neural_networks_CO8-10)'
  prefs: []
  type: TYPE_NORMAL
- en: Update procedure for the set of weights `w0`
  prefs: []
  type: TYPE_NORMAL
- en: 'The following Python code implements the learning (that is, the updating of
    the network weights) as a `for` loop with a larger number of iterations. By increasing
    the number of iterations, the estimation results can be made arbitrarily precise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Next is the classification problem. The implementation in this context is pretty
    close to the estimation problem. However, the sigmoid function is used again for
    activation. The following Python code generates the random sample data first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_interactive_neural_networks_CO9-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Binary features data (input layer)
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_interactive_neural_networks_CO9-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Binary labels data
  prefs: []
  type: TYPE_NORMAL
- en: 'The implementation of the learning algorithm again makes use of a `for` loop
    to repeat the weights-updating step as often as necessary. Depending on the random
    numbers generated for the features and labels data, an accuracy of 100% can be
    achieved after enough learning steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_interactive_neural_networks_CO10-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Forward propagation
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_interactive_neural_networks_CO10-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Backward propagation
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_interactive_neural_networks_CO10-11)'
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy of the classification
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Books cited in this appendix:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Chollet, Francois. 2017\. *Deep Learning with Python*. Shelter Island: Manning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ^([1](app01.xhtml#idm45625236964232-marker)) Since there is no hidden layer,
    backward propagation does take place with a factor of 1 as the value of the derivative.
    Output and input layers are directly connected.
  prefs: []
  type: TYPE_NORMAL
