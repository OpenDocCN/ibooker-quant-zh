- en: Chapter 7\. Machine Learning Models for Time Series Prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning is a subfield of AI that focuses on the development of algorithms
    and models that enable computers to learn and make predictions or decisions without
    being explicitly programmed, hence the term *learning*. Machine learning deals
    with the design and construction of systems that can automatically learn and improve
    from experience, typically by analyzing and extracting patterns from large amounts
    of data.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter presents the framework of using machine learning models for time
    series prediction and discusses a selection of known algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: The Framework
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The *framework* is very important, as it organizes the way the whole research
    process is done (from data collection to performance evaluation). Having a proper
    framework ensures harmony across the backtests, which allows for proper comparison
    among different machine learning models. The framework may follow these chronological
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Import and preprocess the historical data, which must contain a sufficient number
    of values to ensure a decent backtest and evaluation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform a *train-test* split, which splits the data into two parts where the
    first part of the data (e.g., from 2000 to 2020) is reserved for training the
    algorithm so that it understands the mathematical formula to predict the future
    values, and the second part of the data (e.g., from 2020 to 2023) is reserved
    for testing the algorithm’s performance on data that it has never seen before.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit (train) and predict (test) the data using the algorithm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run a performance evaluation algorithm to understand the model’s performance
    in the past.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The *training set* is also called the *in-sample data*, and the *test set* is
    also called the *out-of-sample data*.
  prefs: []
  type: TYPE_NORMAL
- en: The first step of the framework was discussed in [Chapter 6](ch06.html#ch06).
    You should now be able to easily import historical data using Python. The train-test
    split divides the historical data into a training (in-sample) set where the model
    is fitted (trained) so that an implied forecasting function is found, and a test
    (out-of-sample) set where the forecasting function that has been calculated on
    the training set is applied and evaluated. Theoretically, if the model does well
    on the test set, it is likely that you have a potential candidate for a trading
    strategy, but this is just a first step and the reality is much more complicated
    than that.
  prefs: []
  type: TYPE_NORMAL
- en: So that everything goes smoothly, download *master_function.py* from the [GitHub
    repository](https://oreil.ly/5YGHI), and then set the directory of the Python
    interpreter (e.g., Spyder) in the same location as the downloaded file so that
    you can import it as a library and use its functions. For example, if you download
    the file to your desktop, you may want to set the directory as shown in [Figure 6-5](ch06.html#figure-6-5)
    in [Chapter 6](ch06.html#ch06). The choice of the directory is typically found
    on the top-right corner in Spyder (above the variable explorer).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you prefer not to import *master_function.py*, you can just open it in the
    interpreter as a normal file and execute it so that Python defines the functions
    inside. However, you have to do this every time you restart the kernel.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, preprocess (transform) and split the time series into four different arrays
    (or dataframes if you wish), with each array having a utility:'
  prefs: []
  type: TYPE_NORMAL
- en: Array `x_train`
  prefs: []
  type: TYPE_NORMAL
- en: The in-sample set of features (i.e., independent variables) that explain the
    variations of the variable that you want to forecast. They are the predictors.
  prefs: []
  type: TYPE_NORMAL
- en: Array `y_train`
  prefs: []
  type: TYPE_NORMAL
- en: The in-sample set of dependent variables (i.e., the right answers) that you
    want the model to calibrate its forecasting function on.
  prefs: []
  type: TYPE_NORMAL
- en: Array `x_test`
  prefs: []
  type: TYPE_NORMAL
- en: The out-of-sample set of features that will be used as a test of the model to
    see how it performs on this never-before-seen data.
  prefs: []
  type: TYPE_NORMAL
- en: Array `y_test`
  prefs: []
  type: TYPE_NORMAL
- en: Contains the real values that the model must approach. In other words, these
    are the right answers that will be compared with the model’s forecasts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before the split, it is important to know what is being forecasted and what
    is being used to forecast it. In this chapter, lagged price differences (returns)
    will be used for the forecast. Normally, a few tests must be made before doing
    this, but for simplicity, let’s leave them out and suppose that the last 500 daily
    EURUSD returns have predictive power over the current return, which means that
    you can find a predictive formula that uses the last 500 observations to observe
    the next one:'
  prefs: []
  type: TYPE_NORMAL
- en: Dependent variable (forecast)
  prefs: []
  type: TYPE_NORMAL
- en: The t+1 return of the EURUSD in the daily time frame. This is also referred
    to as the *y* variable.
  prefs: []
  type: TYPE_NORMAL
- en: Independent variables (inputs)
  prefs: []
  type: TYPE_NORMAL
- en: The last 500 daily returns of the EURUSD. These are also referred to as the
    *x* variables.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 7-1](#figure-7-1) shows the EURUSD daily returns over a certain time
    period. Notice its stationary appearance. According to the ADF test (seen in [Chapter 3](ch03.html#ch03)),
    the returns dataset seems stationary and is valid for a regression analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/dlff_0701.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-1\. The EURUSD daily returns.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this chapter, the features (*x* values) will be the lagged daily price differences
    of the EURUSD.^([1](ch07.html#id600)) In subsequent chapters, the features used
    will be either lagged returns or values of technical indicators. Note that you
    can use whichever features you think are worthy of being considered as predictive.
  prefs: []
  type: TYPE_NORMAL
- en: The choice of the time frame (daily) is ideal for traders who want an intraday
    view that will help them trade the market and close the position before the end
    of the day.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use the dummy regression model as a first basic example. *Dummy regression*
    is a comparison machine learning algorithm that is only used as a benchmark, as
    it uses very simple rules for predictions that are unlikely to add any real forecasting
    value. The real utility of the dummy regression is to see whether your real model
    outperforms it or not. As a reminder, the process followed by the machine learning
    algorithms is composed of the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Import the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Preprocess and split the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the algorithm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Predict on test data using the training parameters. Also, predict on training
    data for comparison.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot and evaluate the results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Start by importing the libraries required for this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now import the specific library for the algorithm you will use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to import and transform the close price data. Remember, you
    are trying to forecast daily returns, which means that you must select only the
    close column and then apply a differencing function on it so that prices become
    differenced:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In finance, the term *returns* typically refers to the gain or loss generated
    by an investment or a certain asset, and it can be calculated by taking the difference
    between the current value of an asset and its value at a previous point in time.
    This is essentially a form of differencing, as you are calculating the change
    or difference in the asset’s value.
  prefs: []
  type: TYPE_NORMAL
- en: In time series analysis, differencing is a common technique used to make time
    series data stationary, which can be helpful for various analyses. Differencing
    involves subtracting consecutive observations from each other to remove trends
    or seasonality, thereby focusing on the changes in the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, set the hyperparameters of the algorithm. In the case of these basic
    algorithms, it would be the number of lags (number of predictors) and the percentage
    split of data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: A `train_test_split` of 0.80 means that 80% of the data will be used for training
    while the remaining 20% will be used for testing.
  prefs: []
  type: TYPE_NORMAL
- en: 'The function to split and define the four necessary arrays for the backtest
    can be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Call the function to create the four arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'You should now see four new arrays appearing in the variable explorer. The
    next step is to train the data using the chosen algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the dummy regression can take any of the following strategies as
    arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '`mean`'
  prefs: []
  type: TYPE_NORMAL
- en: Always predicts the mean of the training set
  prefs: []
  type: TYPE_NORMAL
- en: '`median`'
  prefs: []
  type: TYPE_NORMAL
- en: Always predicts the median of the training set
  prefs: []
  type: TYPE_NORMAL
- en: '`quantile`'
  prefs: []
  type: TYPE_NORMAL
- en: Always predicts a specified quantile of the training set, provided with the
    quantile parameter
  prefs: []
  type: TYPE_NORMAL
- en: '`constant`'
  prefs: []
  type: TYPE_NORMAL
- en: Always predicts a constant value that is provided by the user
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the previous code, the selected parameter is `mean`. Naturally,
    this signifies that all the predictions made will simply be the mean of the training
    set (`y_train`). This is why dummy regression is only used as a benchmark and
    not as a serious machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to predict on the test data, as well as on the training data
    as a means of comparison. Note that the predictions on the training data have
    no value since the algorithm has already seen the data during training, but it
    is interesting to know how worse or better the algorithm performs on data that
    has never been seen before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'To make sure your reasoning is correct with regard to using the dummy regression
    algorithm, manually calculate the mean of `y_train` and compare it to the value
    you get in every `y_predicted`. You will see that it’s the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, use the following function to plot the last training data followed
    by the first test data and the equivalent predicted data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can find the definition of the `plot_train_test_values()` function in this
    book’s [GitHub repository](https://oreil.ly/5YGHI).
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 7-2](#figure-7-2) shows the evolution of the forecasting task from
    the last values of `y_train` to the first values of `y_test` and `y_predicted`.
    Naturally, the dummy regression algorithm predicts a constant value, which is
    why the prediction line alongside the test values is a straight line.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/dlff_0702.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-2\. Training data followed by test data (dashed line) and the predicted
    data (thin line); the vertical dashed line represents the start of the test period.
    The model used is the dummy regression algorithm.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you want the figures to be plotted in a separate window, type `**%matplotlib
    qt**` in the console. If you want the figures to be inside the plots explorer,
    type `**%matplotlib inline**` in the console.
  prefs: []
  type: TYPE_NORMAL
- en: 'How can you tell whether a model is performing well or not? *Performance evaluation*
    is a key concept in trading and algorithmic development as it ensures that you
    pick the right model and take it live. However, the task is not simple, due to
    an ironically simple question: *If the past performance was good, what guarantees
    it continues to be good?*'
  prefs: []
  type: TYPE_NORMAL
- en: 'This question is painful, but it points toward the right direction. The answer
    to this question is subjective. For now, let’s talk about the different ways to
    measure the performance of a model. To simplify the task, I will split the performance
    and evaluation metrics into two: model evaluation and trading evaluation. *Model
    evaluation* deals with the algorithm’s performance in its forecasts, while *trading
    evaluation* deals with the financial performance of a system that trades using
    the algorithm (an example of a trading evaluation metric is the net profit).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with model evaluation. *Accuracy* is the first metric that comes
    to mind when comparing forecasts to real values, especially in the financial markets.
    Theoretically, if you predict the direction (up or down) and you get it right,
    you should make money (excluding transaction costs). Accuracy is also referred
    to as the *hit ratio* in financial jargon and is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="Accuracy equals StartFraction Correct predictions Over Total
    predictions EndFraction times 100"><mrow><mtext>Accuracy</mtext> <mo>=</mo> <mstyle
    displaystyle="false" scriptlevel="0"><mfrac><mrow><mtext>Correct</mtext><mtext>predictions</mtext></mrow>
    <mrow><mtext>Total</mtext><mtext>predictions</mtext></mrow></mfrac></mstyle> <mo>×</mo>
    <mn>100</mn></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: For example, if you made 100 predictions last year and 73 of them were correct,
    you would have a 73% accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Forecasting can also be evaluated by how close the predicted values (`y_predicted`)
    are to the real values (`y_test`). This is done by loss functions. A *loss function*
    is a mathematical calculation that measures the difference between the predictions
    and the real (test) values. The most basic loss function is the *mean absolute
    error* (MAE). It measures the average of the absolute differences between the
    predicted and actual values. The mathematical representation of MAE is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper M upper A upper E equals StartFraction sigma-summation
    Underscript i equals 1 Overscript n Endscripts StartAbsoluteValue ModifyingAbove
    y With caret minus y Subscript i Baseline EndAbsoluteValue Over n EndFraction"><mrow><mi>M</mi>
    <mi>A</mi> <mi>E</mi> <mo>=</mo> <mstyle displaystyle="false" scriptlevel="0"><mfrac><mrow><msubsup><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></msubsup> <mrow><mo>|</mo><mover
    accent="true"><mi>y</mi> <mo>^</mo></mover><mo>-</mo><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>|</mo></mrow></mrow> <mi>n</mi></mfrac></mstyle></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="StartLayout 1st Row  ModifyingAbove y With caret is the predicted
    value 2nd Row  y is the real value EndLayout"><mtable><mtr><mtd columnalign="left"><mrow><mover
    accent="true"><mi>y</mi> <mo>^</mo></mover> <mtext>is</mtext> <mtext>the</mtext>
    <mtext>predicted</mtext> <mtext>value</mtext></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mi>y</mi>
    <mtext>is</mtext> <mtext>the</mtext> <mtext>real</mtext> <mtext>value</mtext></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, MAE calculates the average distance (or positive difference) between
    the predicted and real values. The lower the MAE, the more accurate the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The *mean squared error* (MSE) is one of the commonly used loss functions for
    regression. It measures the average of the squared differences between the predicted
    and actual values. You can think of MSE as the equivalent of the variance metric
    seen in [Chapter 3](ch03.html#ch03). The mathematical representation of MSE is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper M upper S upper E equals StartFraction sigma-summation
    Underscript i equals 1 Overscript n Endscripts left-parenthesis ModifyingAbove
    y With caret minus y Subscript i Baseline right-parenthesis squared Over n EndFraction"><mrow><mi>M</mi>
    <mi>S</mi> <mi>E</mi> <mo>=</mo> <mstyle displaystyle="false" scriptlevel="0"><mfrac><mrow><msubsup><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></msubsup> <msup><mrow><mo>(</mo><mover
    accent="true"><mi>y</mi> <mo>^</mo></mover><mo>-</mo><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mn>2</mn></msup></mrow> <mi>n</mi></mfrac></mstyle></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Hence, MSE calculates the average squared distance between the predicted and
    the real values. Similar to the MAE, the lower the MSE, the more accurate the
    model. With this in mind, it helps to compare apples to apples (such as with variance
    and standard deviation, as seen in [Chapter 3](ch03.html#ch03)). Therefore, the
    *root mean squared error* (RMSE) has been developed to tackle this problem (hence,
    scaling the error metric back to the same units as the target variable). The mathematical
    representation of RMSE is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper R upper M upper S upper E equals StartRoot StartFraction
    sigma-summation Underscript i equals 1 Overscript n Endscripts left-parenthesis
    ModifyingAbove y With caret minus y Subscript i Baseline right-parenthesis squared
    Over n EndFraction EndRoot"><mrow><mi>R</mi> <mi>M</mi> <mi>S</mi> <mi>E</mi>
    <mo>=</mo> <msqrt><mstyle displaystyle="false" scriptlevel="0"><mfrac><mrow><msubsup><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></msubsup> <msup><mrow><mo>(</mo><mover
    accent="true"><mi>y</mi> <mo>^</mo></mover><mo>-</mo><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mn>2</mn></msup></mrow> <mi>n</mi></mfrac></mstyle></msqrt></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: The RMSE is the equivalent of the standard deviation in descriptive statistics.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: MAE is relatively less sensitive to outliers than MSE, and it is often used
    when the data contains extreme values or when the absolute magnitude of the error
    is more important than its squared value. On the other hand, as MSE gives more
    weight to larger errors, it is the go-to loss function when trying to improve
    the performance of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'When evaluating models using MAE, MSE, or RMSE, it is important to have a baseline
    for comparison:'
  prefs: []
  type: TYPE_NORMAL
- en: If you have built multiple regression models, you can compare their metrics
    to determine which model performs better. The model with the lower metric is generally
    considered to be more accurate in its predictions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Depending on the specific problem, you may have a threshold value for what is
    considered an acceptable level of prediction error. For example, in some cases,
    an RMSE below a certain threshold may be considered satisfactory, while values
    above that threshold may be considered unacceptable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can compare the loss functions of the training data with the loss functions
    of the test data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Algorithms may sometimes be directionally biased for many reasons (either structurally
    or externally). A *biased model* takes on significantly more trades in one direction
    than the other (an example would be an algorithm having 200 long positions and
    30 short positions). *Model bias* measures this as a ratio by dividing the number
    of long positions by the number of short positions. The ideal model bias is around
    1.00, which implies a balanced trading system. The mathematical representation
    of model bias is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="Model bias equals StartFraction Number of bullish signals Over
    Number of bearish signals EndFraction"><mrow><mtext>Model</mtext> <mtext>bias</mtext>
    <mo>=</mo> <mfrac><mrow><mtext>Number</mtext><mtext>of</mtext><mtext>bullish</mtext><mtext>signals</mtext></mrow>
    <mrow><mtext>Number</mtext><mtext>of</mtext><mtext>bearish</mtext><mtext>signals</mtext></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: If a model has had 934 long positions and 899 short positions this year, then
    the model bias metric is 1.038, which is acceptable. This means that the model
    is not really biased. It is worth noting that a model bias of 0.0 represents the
    absence of any bullish signals, and a model bias that has an undefined value represents
    the absence of any bearish signals (due to the division by zero).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we’ll turn our attention to trading evaluation. Finance pioneers have been
    developing metrics that measure the performance of strategies and portfolios.
    Let’s discuss the most common and most useful ones. The most basic metric is the
    *net return*, which is essentially the return over the invested capital after
    a trading period that has at least one closed trade. The mathematical representation
    of the net return is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="Net return equals left-parenthesis StartFraction Final value
    Over Initial value EndFraction minus 1 right-parenthesis times 100"><mrow><mtext>Net</mtext>
    <mtext>return</mtext> <mo>=</mo> <mo>(</mo> <mstyle displaystyle="false" scriptlevel="0"><mfrac><mrow><mtext>Final</mtext><mtext>value</mtext></mrow>
    <mrow><mtext>Initial</mtext><mtext>value</mtext></mrow></mfrac></mstyle> <mo>-</mo>
    <mn>1</mn> <mo>)</mo> <mo>×</mo> <mn>100</mn></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: The word *net* implies a result after deducting fees; otherwise, it is referred
    to as a *gross* return. For example, if you start the year with $52,000 and finish
    at $67,150, you would have made 29.13% (a net profit of $15,150).
  prefs: []
  type: TYPE_NORMAL
- en: 'Another profitability metric is the *profit factor,* which is the ratio of
    the total gross profits to the total gross losses. Intuitively, a profit factor
    above 1.00 implies a profitable strategy, and a profit factor below 1.00 implies
    a losing strategy. The mathematical representation of the profit factor is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="Profit factor equals StartFraction Gross profits Over Gross losses
    EndFraction"><mrow><mtext>Profit</mtext> <mtext>factor</mtext> <mo>=</mo> <mstyle
    displaystyle="false" scriptlevel="0"><mfrac><mrow><mtext>Gross</mtext><mtext>profits</mtext></mrow>
    <mrow><mtext>Gross</mtext><mtext>losses</mtext></mrow></mfrac></mstyle></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: The profit factor is a useful metric for evaluating the profitability of a trading
    strategy because it takes into account both the profits and losses generated by
    the strategy, rather than just looking at one side of the equation. The profit
    factor of a trading strategy that has generated $54,012 in profits and $29,988
    in losses is 1.80.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next interesting metric relates to individual trades. The *average gain*
    per trade calculates the average profits (or positive returns) per trade based
    on historical data, and the *average loss* per trade calculates losses (or negative
    returns) per trade based on historical data. These two metrics give an expected
    return depending on the outcome. Both metrics are calculated following these formulas:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="StartLayout 1st Row  Average gain equals StartFraction Total
    profit Over Number of winning trades EndFraction 2nd Row  Average loss equals
    StartFraction Total losses Over Number of losing trades EndFraction EndLayout"><mtable><mtr><mtd
    columnalign="left"><mrow><mtext>Average</mtext> <mtext>gain</mtext> <mo>=</mo>
    <mfrac><mrow><mtext>Total</mtext><mtext>profit</mtext></mrow> <mrow><mtext>Number</mtext><mtext>of</mtext><mtext>winning</mtext><mtext>trades</mtext></mrow></mfrac></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mtext>Average</mtext> <mtext>loss</mtext>
    <mo>=</mo> <mfrac><mrow><mtext>Total</mtext><mtext>losses</mtext></mrow> <mrow><mtext>Number</mtext><mtext>of</mtext><mtext>losing</mtext><mtext>trades</mtext></mrow></mfrac></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'The next metric relates to risk and is one of the most important measures of
    evaluation. *Maximum drawdown* is a metric that measures the largest percentage
    decline in the value of an investment or portfolio from its highest historical
    peak to its lowest point. It is commonly used to assess the downside risk of an
    investment or portfolio. For example, if an investment has a peak value of $100,000
    and its value subsequently drops to $50,000 before recovering, the maximum drawdown
    would be 50%, which is the percentage decline from the peak value to the trough.
    Maximum drawdown is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="Maximum drawdown equals left-parenthesis StartFraction Trough
    value minus Peak value Over Peak value EndFraction right-parenthesis times 100"><mrow><mtext>Maximum</mtext>
    <mtext>drawdown</mtext> <mo>=</mo> <mo>(</mo> <mstyle displaystyle="false" scriptlevel="0"><mfrac><mrow><mtext>Trough</mtext><mtext>value</mtext><mo>-</mo><mtext>Peak</mtext><mtext>value</mtext></mrow>
    <mrow><mtext>Peak</mtext><mtext>value</mtext></mrow></mfrac></mstyle> <mo>)</mo>
    <mo>×</mo> <mn>100</mn></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let’s discuss a well-known profitability ratio called the *Sharpe
    ratio*. It measures how much return is generated by units of excess risk. The
    formula of the ratio is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper S h a r p e equals StartFraction mu minus r Over sigma
    EndFraction"><mrow><mi>S</mi> <mi>h</mi> <mi>a</mi> <mi>r</mi> <mi>p</mi> <mi>e</mi>
    <mo>=</mo> <mfrac><mrow><mi>μ</mi><mo>-</mo><mi>r</mi></mrow> <mi>σ</mi></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: μ is the net return
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*r* is the risk-free rate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: σ is the volatility of returns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, if the net return is 5% and the risk-free rate is 2% while the volatility
    of returns is 2.5%, the Sharpe ratio is 1.20\. Anything above 1.00 is desirable
    as it implies that the strategy is generating positive excess risk-adjusted return.
  prefs: []
  type: TYPE_NORMAL
- en: The focus of this book is on developing machine and deep learning algorithms,
    so the performance evaluation step will solely focus on accuracy, RMSE, and model
    bias (with the correlation between the predicted variables as an extra metric).
    The performance functions can be found in the GitHub repository, along with the
    complete scripts.
  prefs: []
  type: TYPE_NORMAL
- en: 'The model’s results on the EURUSD after applying the performance metrics are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: With a bias of 0.0, it’s easy to see that this is a dummy regression model.
    The bias means that according to the formula, all the forecasts are bearish. Taking
    a look at the details of the predictions, you will see that they are all constant
    values.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The key takeaways from this section are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Automatic data import and creation saves you time and allows you to concentrate
    on the main issues of the algorithm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a proper backtest, the data must be split into a training set and a test
    set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The training set contains `x_train` and `y_train`, with the former containing
    the values that are supposed to have a predictive power over the latter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The test set contains `x_test` and `y_test`, with the former containing the
    values that are supposed to have a predictive power (even though the model hasn’t
    encountered them in its training) over the latter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fitting the data is when the algorithm runs on the training set; predicting
    the data is when the algorithm runs on the test set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The predictions are stored in a variable called `y_predicted` that is compared
    to `y_test` for performance evaluation purposes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The main aim of the algorithms is to have good accuracy and stable, low-volatility
    returns.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine Learning Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section presents a selection of machine learning models using the framework
    developed so far. It is important to understand every model’s strengths and weaknesses
    so that you know which model to choose depending on the forecasting task.
  prefs: []
  type: TYPE_NORMAL
- en: Linear Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *linear regression* algorithm works by finding the best-fitting line that
    minimizes the sum of squared differences between the predicted and actual target
    values. The most used optimization technique in this algorithm is the *ordinary
    least squares* (OLS) method.^([2](ch07.html#id626))
  prefs: []
  type: TYPE_NORMAL
- en: The model is trained on the training set using the OLS method, which estimates
    the coefficients that minimize the sum of squared differences between the predicted
    and actual target values to find the optimal coefficients for the independent
    variables (the coefficients represent the *y*-intercept and the slope of the best-fitting
    line, respectively). The output is a linear function that gives the expected return
    given the explanatory variables weighted by the coefficient with an adjustment
    for noise and the intercept.
  prefs: []
  type: TYPE_NORMAL
- en: 'To import the linear regression library from *sklearn*, use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s look at the algorithm’s implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The model assumes that the linear relationship that has held in the past will
    still hold in the future. This is unrealistic, and it ignores the fact that market
    dynamics and drivers are constantly shifting whether in the short term or the
    long term. They are also nonlinear.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 7-3](#figure-7-3) shows the evolution of the forecasting task from
    the last values of `y_train` to the first values of `y_test` and `y_predicted`.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/dlff_0703.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-3\. Training data followed by test data (dashed line) and the predicted
    data (thin line); the vertical dashed line represents the start of the test period.
    The model used is the linear regression algorithm.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The model’s results on the EURUSD after applying the performance metrics are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The results indicate poor performance coming from the linear regression algorithm,
    with an accuracy below 50.00%. As you can see, the accuracy generally drops after
    switching to the test set. The correlation between the in-sample predictions and
    the real in-sample values also drops from 0.373 to 0.014\. The model bias is close
    to equilibrium, which means that the number of long signals is close to the number
    of short signals.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few things to note regarding the model’s results:'
  prefs: []
  type: TYPE_NORMAL
- en: The transaction costs have not been incorporated, and therefore, these are gross
    results (not net results).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no risk management system, as this is a pure time series machine learning
    model and not a full trading algorithm that incorporates stops and targets. Therefore,
    as this is a purely directional model, the job is to try to maximize the number
    of correct forecasts. With a daily horizon, you are searching for accuracy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different FX data providers may have small differences in the historical data
    that may cause some differences between backtests.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Models are made to be optimized and tweaked. The process of optimization may
    include any of the following techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right predictors is paramount to a model’s success
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, the predictors used are the lagged returns. This has been chosen
    arbitrarily and is not necessarily the right choice. Predictors must be chosen
    based on economic and statistical intuition. For example, it may be reasonable
    to choose the returns of gold to explain (predict) the variations on the S&P 500
    index as they are economically linked. Safe haven assets like gold rise during
    periods of economic uncertainty, while the stock market tends to fall. This negative
    correlation may harbor hidden patterns between the returns of both instruments.
    Another way of choosing predictors is to use technical indicators such as the
    relative strength index (RSI) and moving averages.
  prefs: []
  type: TYPE_NORMAL
- en: Proper splitting is crucial to evaluate the model properly
  prefs: []
  type: TYPE_NORMAL
- en: Train-test splits are important as they determine the window of evaluation.
    Typically, 20/80 and 30/70 are used, which means that 20% (30%) of the data is
    used for the testing sample and 80% (70%) is used for the training sample.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization techniques can help prevent biases
  prefs: []
  type: TYPE_NORMAL
- en: Ridge regression and Lasso regression are two common regularization methods
    used in linear regression. *Ridge regression* adds a penalty term to the OLS function
    to reduce the impact of large coefficients, while *Lasso regression* can drive
    some coefficients to zero, effectively performing feature selection.
  prefs: []
  type: TYPE_NORMAL
- en: The model seen in this section is called an *autoregressive model* since the
    dependent variable depends on its past values and not on exogenous data. Also,
    since at every time step, 500 different variables (with their coefficients) have
    been used to predict the next variable, the model is referred to as a *multiple
    linear regression* model. In contrast, when the model only uses one dependent
    variable to predict the dependent variable, it is referred to as a *simple linear
    regression* model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The advantages of linear regression are:'
  prefs: []
  type: TYPE_NORMAL
- en: It is easy to implement and train. It also does not consume a lot of memory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It outperforms when the data has a linear dependency.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The disadvantages of linear regression are:'
  prefs: []
  type: TYPE_NORMAL
- en: It is sensitive to outliers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is easily biased (more on this type of bias in [“Overfitting and Underfitting”](#overfit_and_underfit)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has unrealistic assumptions, such as the independence of data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before moving on to the next section, it is important to note that some linear
    regression models do not transform the data. You may see extremely high accuracy
    and a prediction that is very close to the real data, but the reality is that
    the prediction lags by one time step. This means that at every time step, the
    prediction is simply the last real value. Let’s prove this using the previous
    example. Use the same code as before, but omit the price differencing code. You
    should see [Figure 7-4](#figure-7-4).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/dlff_0704.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-4\. Nonstationary training data followed by test data (dashed line)
    and the predicted data (thin line); the vertical dashed line represents the start
    of the test period. The model used is the linear regression algorithm.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Notice how it’s simply lagging the real values and not adding any predictive
    information. Always transform nonstationary data when dealing with such models.
    Nonstationary data cannot be forecasted using this type of algorithm (there are
    exceptions, though, which you will see later on).
  prefs: []
  type: TYPE_NORMAL
- en: 'Using linear regression on nonstationary data, such as market prices, and observing
    that the forecasts are the same as the last value might indicate an issue known
    as *naive forecasting*. This occurs when the most recent observation (in this
    case, the last value) is simply used as the forecast for the next time period.
    While this approach can sometimes work for certain types of data, it is generally
    not a sophisticated forecasting method and may not capture the underlying patterns
    or trends in the data. There are a few reasons why this might happen:'
  prefs: []
  type: TYPE_NORMAL
- en: Lack of predictive power
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression assumes that there is a linear relationship between the independent
    variable(s) and the dependent variable. If the data is highly nonstationary and
    lacks a clear linear relationship, then the linear regression model may not be
    able to capture meaningful patterns and will default to a simplistic forecast
    like naive forecasting.
  prefs: []
  type: TYPE_NORMAL
- en: Lagging indicators
  prefs: []
  type: TYPE_NORMAL
- en: Market prices often exhibit strong autocorrelation, meaning that the current
    price is highly correlated with the previous price. In such cases, if the model
    only takes into account lagged values as predictors, it might simply replicate
    the last value as the forecast.
  prefs: []
  type: TYPE_NORMAL
- en: Lack of feature engineering
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression models rely on the features (predictors) you provide to make
    forecasts. If you’re using only lagged values as predictors and not incorporating
    other relevant features, the model might struggle to generate meaningful forecasts.
  prefs: []
  type: TYPE_NORMAL
- en: Model complexity
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression is a relatively simple modeling technique. If the underlying
    relationship in the data is more complex than can be captured by a linear equation,
    the model might not be able to make accurate forecasts.
  prefs: []
  type: TYPE_NORMAL
- en: Support Vector Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Support vector regression* (SVR) is a machine learning algorithm that belongs
    to the family of *support vector machines* (SVMs). SVR is specifically designed
    for regression problems, where the goal is to predict continuous numerical values
    (such as return values).'
  prefs: []
  type: TYPE_NORMAL
- en: SVR performs regression by finding a hyperplane in a high-dimensional feature
    space that best approximates the relationship between the input features and the
    target variable. Unlike traditional regression techniques that aim to minimize
    the errors between the predicted and actual values, SVR focuses on finding a hyperplane
    that captures the majority of the data within a specified margin, known as the
    *epsilon tube* (loss function).
  prefs: []
  type: TYPE_NORMAL
- en: 'The key idea behind SVR is to transform the original input space into a higher-dimensional
    space using a kernel function. This transformation allows SVR to implicitly map
    the data into a higher-dimensional feature space, where it becomes easier to find
    a linear relationship between the features and the target variable. The kernel
    function calculates the similarity between two data points, enabling the SVR algorithm
    to work effectively in nonlinear regression problems. The steps performed in the
    SVR process are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm employs a kernel function to transform the input features into
    a higher-dimensional space. Common kernel functions include the linear kernel,
    polynomial kernel, radial basis function (RBF) kernel, and sigmoid kernel. The
    choice of kernel depends on the data and the underlying problem.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The algorithm then aims to find the hyperplane that best fits the data points
    within the epsilon tube. The training process involves solving an optimization
    problem to minimize the error (using a loss function such as MSE) while controlling
    the margin.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The RBF kernel is a popular choice for SVR because it can capture nonlinear
    relationships effectively. It is suitable when there is no prior knowledge about
    the specific form of the relationship. The RBF kernel calculates the similarity
    between feature vectors based on their distance in the input space. It uses a
    parameter called *gamma*, which determines the influence of each training example
    on the model. Higher gamma values make the model focus more on individual data
    points, potentially leading to errors.
  prefs: []
  type: TYPE_NORMAL
- en: By finding an optimal hyperplane within the epsilon tube, SVR can effectively
    capture the underlying patterns and relationships in the data, even in the presence
    of noise or outliers. It is a powerful technique for regression tasks, especially
    when dealing with nonlinear relationships between features and target variables.
  prefs: []
  type: TYPE_NORMAL
- en: As SVR is sensitive to the scale of the features, it’s important to bring all
    the features to a similar scale. Common scaling methods include *standardization*
    (mean subtraction and division by standard deviation) and *normalization* (scaling
    features to a range, e.g., [0, 1]).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at SVR in action. Once again, the aim is to predict the next
    EURUSD return given the previous returns. To import the SVR library and the scaling
    library, use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'For the SVR algorithm, a little tweaking was done to get acceptable forecasts.
    The tweak was to reduce the number of lagged values from 500 to 50:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This allows the SVR algorithm to improve its forecasts. You will see throughout
    the book that part of performing these types of backtests is tweaking and calibrating
    the models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, to implement the algorithm, use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 7-5](#figure-7-5) shows the evolution of the forecasting task from
    the last values of `y_train` to the first values of `y_test` and `y_predicted`.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/dlff_0705.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-5\. Training data followed by test data (dashed line) and the predicted
    data (thin line); the vertical dashed line represents the start of the test period.
    The model used is the SVR algorithm.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The model’s results are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The advantages of SVR are:'
  prefs: []
  type: TYPE_NORMAL
- en: It performs well even in high-dimensional feature spaces, where the number of
    features is large compared to the number of samples. It is particularly useful
    when dealing with complex datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can capture nonlinear relationships between input features and the target
    variable by using kernel functions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is robust to outliers in the training data due to the epsilon-tube formulation.
    The model focuses on fitting the majority of the data within the specified margin,
    reducing the influence of outliers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The disadvantages of SVR are:'
  prefs: []
  type: TYPE_NORMAL
- en: It has several hyperparameters that need to be tuned for optimal performance.
    Selecting appropriate hyperparameters can be a challenging task and may require
    extensive experimentation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can be computationally expensive, especially for large datasets or when using
    complex kernel functions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can be sensitive to the choice of hyperparameters. Poorly chosen hyperparameters
    can lead to fitting issues.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stochastic Gradient Descent Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Gradient descent* (GD) is a general optimization algorithm used to minimize
    the cost or loss function of a model, and it serves as the foundation for various
    optimization algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '*Gradient* simply refers to a surface’s slope or tilt. To get to the lowest
    point on the surface, one must literally descend a slope.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Stochastic gradient descent* (SGD) is an iterative optimization algorithm
    commonly used for training machine learning models, including regression models.
    It is particularly useful for large datasets and online learning scenarios. When
    applied to time series prediction, SGD can be used to train regression models
    that capture temporal patterns and make predictions based on historical data.
    SGD is therefore a type of linear regression that uses stochastic gradient descent
    optimization to find the best-fitting line.'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike ordinary least squares, SGD updates the model’s parameters iteratively,
    making it more suitable for large datasets (which are treated in small batches).
    Instead of using the entire dataset for each update step, SGD randomly selects
    a small batch of samples or individual samples from the training dataset. This
    random selection helps to introduce randomness and avoid getting stuck in local
    optima (you can refer to [Chapter 4](ch04.html#ch04) for more information on optimization).
    The main difference between GD and SGD lies in how they update the model’s parameters
    during optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: SGD does not belong to any particular family of machine learning models; it
    is essentially an optimization technique.
  prefs: []
  type: TYPE_NORMAL
- en: GD computes the gradients over the entire training dataset, updating the model’s
    parameters once per epoch, while SGD computes the gradients based on a single
    training example or mini batch, updating the parameters more frequently. SGD is
    faster but exhibits more erratic behavior, while GD is slower but has a smoother
    convergence trajectory. SGD is also more robust to local minima. The choice between
    GD and SGD depends on the specific requirements of the problem, the dataset size,
    and the trade-off between computational efficiency and convergence behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, the first step is to import the necessary libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, to implement the algorithm, use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 7-6](#figure-7-6) shows the evolution of the forecasting task from
    the last values of `y_train` to the first values of `y_test` and `y_predicted`.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/dlff_0706.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-6\. Training data followed by test data (dashed line) and the predicted
    data (thin line); the vertical dashed line represents the start of the test period.
    The model used is the SGD algorithm.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The model’s results are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The advantages of SGD are:'
  prefs: []
  type: TYPE_NORMAL
- en: It performs well with large datasets since it updates the model parameters incrementally
    based on individual or small subsets of training examples.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can escape local minima and find better global optima (due to its stochastic
    nature).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can improve generalization by exposing the model to different training examples
    in each iteration, thereby reducing overfitting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The disadvantages of SGD are:'
  prefs: []
  type: TYPE_NORMAL
- en: The convergence path can be noisy and exhibit more fluctuations compared to
    deterministic optimization algorithms. This can result in slower convergence or
    oscillations around the optimal solution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is impacted by feature scaling, which means it is sensitive to such techniques.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nearest Neighbors Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *nearest neighbors regression* algorithm, also known as *k*-nearest neighbors
    (KNN) regression, is a nonparametric^([3](ch07.html#id644)) algorithm used for
    regression tasks. It predicts the value of a target variable based on the values
    of its nearest neighbors in the feature space. The algorithm starts by determining
    *k*, which is the number of nearest neighbors to consider when making predictions.
    This is a hyperparameter that you need to choose based on the problem at hand.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A larger *k* value provides a smoother prediction, while a smaller *k* value
    captures more local variations but may be more prone to noise.
  prefs: []
  type: TYPE_NORMAL
- en: Then the model calculates the distance between the new, unseen data point and
    all the data points in the training set. The choice of distance metric depends
    on the nature of the input features. Common distance metrics include Euclidean
    distance, Manhattan distance, and Minkowski distance. Next, the algorithm selects
    the *k* data points with the shortest distances to the query point. These data
    points are the *nearest neighbors* and will be used to make predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'To import the KNN regressor, use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s look at the algorithm’s implementation. Fit the model with *k* =
    10:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 7-7](#figure-7-7) shows the evolution of the forecasting task from
    the last values of `y_train` to the first values of `y_test` and `y_predicted`.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/dlff_0707.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-7\. Training data followed by test data (dashed line) and the predicted
    data (thin line); the vertical dashed line represents the start of the test period.
    The model used is the KNN regression algorithm.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The choice of the number of neighbors in a time series prediction using the
    KNN regressor depends on several factors, including the characteristics of your
    dataset and the desired level of accuracy. There is no definitive answer as to
    how many neighbors to choose, as it is often determined through experimentation
    and validation. Typically, selecting an appropriate value for the number of neighbors
    involves a trade-off between bias and variance:'
  prefs: []
  type: TYPE_NORMAL
- en: Small *k* values are associated with a model that can capture local patterns
    in the data, but it may also be sensitive to noise or outliers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Larger *k* values are associated with a model that can become more robust to
    noise or outliers but may overlook local patterns in the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you take the limit as *k* approaches the size of the dataset, you will get
    a model that just predicts the class that appears more frequently in the dataset.
    This is known as the *Bayes error*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The model’s results are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It’s essential to consider the temporal aspect of your time series data. If
    there are clear trends or patterns that span multiple data points, a larger *k*
    value might be appropriate to capture those dependencies. However, if the time
    series exhibits rapid changes or short-term fluctuations, a smaller k value could
    be more suitable.
  prefs: []
  type: TYPE_NORMAL
- en: The size of your dataset can also influence the choice of *k*. If you have a
    small dataset, choosing a smaller value for *k* might be preferable to avoid overfitting.
    Conversely, a larger dataset can tolerate a higher value for *k*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The advantages of KNN are:'
  prefs: []
  type: TYPE_NORMAL
- en: Its nonlinearity allows it to capture complex patterns in financial data, which
    can be advantageous for predicting returns series that may exhibit nonlinear behavior.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can adapt to changing market conditions or patterns. As the algorithm is
    instance based, it does not require retraining the model when new data becomes
    available. This adaptability can be beneficial in the context of financial returns,
    where market dynamics can change over time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It provides intuitive interpretations for predictions. Since the algorithm selects
    the *k* nearest neighbors to make predictions, it can be easier to understand
    and explain compared to more complex algorithms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The disadvantages of KNN are:'
  prefs: []
  type: TYPE_NORMAL
- en: Its performance can degrade when dealing with high-dimensional data. Financial
    returns series often involve multiple predictors (such as technical indicators
    and other correlated returns), and KNN may struggle to find meaningful neighbors
    in high-dimensional spaces.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As the dataset grows in size, the computational requirements of KNN can become
    significant.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is sensitive to noisy or outlier data points since the algorithm considers
    all neighbors equally.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision Tree Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Decision trees* are versatile and intuitive machine learning models. They
    are graphical representations of a series of decisions or choices based on feature
    values that lead to different outcomes. Decision trees are structured as a hierarchical
    flowchart, where each internal node represents a decision based on a feature,
    each branch represents an outcome of that decision, and each leaf node represents
    the final prediction or class label.'
  prefs: []
  type: TYPE_NORMAL
- en: At the root of the decision tree, consider all the input features and choose
    the one that best separates the data based on a specific criterion (e.g., the
    information gain metric discussed in [Chapter 2](ch02.html#ch02)). Create a decision
    node associated with the selected feature. Split the data based on the possible
    values of the chosen feature. Repeat the preceding steps recursively for each
    subset of data created by the splits, considering the remaining features at each
    node. Stop the recursion when a stopping criterion is met, such as reaching a
    maximum depth, reaching a minimum number of samples in a node, or no further improvement
    in impurity or gain.
  prefs: []
  type: TYPE_NORMAL
- en: 'To import the decision tree regressor, use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s look at the algorithm’s implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The argument `random_state` is often used to initialize the randomization within
    algorithms that involve randomness such as initializing weights. This ensures
    that if you train a model multiple times with the same `random_state`, you’ll
    get the same results, which is important for comparing different algorithms or
    hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 7-8](#figure-7-8) shows the evolution of the forecasting task from
    the last values of `y_train` to the first values of `y_test` and `y_predicted`.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/dlff_0708.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-8\. Training data followed by test data (dashed line) and the predicted
    data (thin line); the vertical dashed line represents the start of the test period.
    The model used is the decision tree regression algorithm.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The model’s results are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Notice how the accuracy of the training set is extremely high. This is clearly
    evidence of overfitting (supported by the RMSE of the training data).
  prefs: []
  type: TYPE_NORMAL
- en: 'The advantages of decision trees are:'
  prefs: []
  type: TYPE_NORMAL
- en: They require minimal data preprocessing and can handle missing values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They can capture nonlinear relationships, interactions, and variable importance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The disadvantages of decision trees are:'
  prefs: []
  type: TYPE_NORMAL
- en: They can be sensitive to small changes in the data and can easily overfit if
    not properly regularized.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They may struggle to capture complex relationships that require deeper trees.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next section presents another breed of machine learning algorithms. These
    are called *ensemble algorithms.* Decision trees can be combined using ensemble
    methods to create more robust and accurate models. Random forest, an algorithm
    seen in the next section, combines multiple decision trees to enhance predictive
    ability and, especially, to reduce the risk of overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Random Forest Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Random forest* is a machine learning algorithm that harnesses the power of
    multiple decision trees to form a single output (prediction). It is flexible and
    does not require much tuning. It is also less prone to overfitting due to its
    ensemble learning technique. *Ensemble learning* refers to the combination of
    multiple learners (models) to improve the final prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: With random forest, the multiple learners are different decision trees that
    converge toward a single prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, one of the hyperparameters that can be tuned in random forest algorithms
    is the number of decision trees. The algorithm uses the bagging method. In the
    context of random forests, *bagging* refers to the technique of *bootstrap aggregating*
    that aims to improve the performance and robustness of machine learning models,
    such as decision trees, by reducing biases. Here’s how bagging works within the
    random forest algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Bootstrap sampling*: Random forest employs bootstrapping, which means creating
    multiple subsets of the original training data by sampling with replacement. Each
    subset has the same size as the original dataset but may contain duplicate instances
    and exclude some of them. This process is performed independently for each tree
    in the random forest.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Tree construction and feature selection*: For each bootstrap sample, a decision
    tree is constructed using a process called *recursive partitioning* where data
    is split based on features in order to create branches that optimize the separation
    of the target variables. At each node of the decision tree, a random subset of
    features is considered for splitting. This helps introduce diversity among the
    trees in the forest and prevents them from relying too heavily on a single dominant
    feature.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Ensemble prediction*: Once all the trees are constructed, predictions are
    made by aggregating the outputs of individual trees. For regression tasks, the
    predictions are averaged.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To import the random forest regressor, use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s look at the algorithm’s implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The `max_depth` hyperparameter controls the depth of each decision tree in the
    random forest. A decision tree with a larger depth can capture more intricate
    patterns in the data, but it also becomes more prone to overfitting, which means
    it might perform very well on the training data but poorly on unseen data. On
    the other hand, a shallower tree might not capture all the details of the data
    but could generalize better to new, unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 7-9](#figure-7-9) shows the evolution of the forecasting task from
    the last values of `y_train` to the first values of `y_test` and `y_predicted`.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/dlff_0709.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-9\. Training data followed by test data (dashed line) and the predicted
    data (thin line); the vertical dashed line represents the start of the test period.
    The model used is the random forest regression algorithm.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The model’s results are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The advantages of random forest regression are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: It generally has accurate forecasts on data due to its ensemble nature. With
    financial time series being highly noisy and borderline random, its results need
    to be optimized nevertheless.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It exhibits robustness to noise and outliers due to its averaging nature.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The disadvantages of random forest regression are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: It may be difficult to interpret from time to time. Since it uses an aggregating
    method, the true and final decision may be lost when a large number of trees is
    used.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As the number of trees increases, the computational time of the algorithm takes
    more time to train, resulting in a slow process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AdaBoost Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before understanding what AdaBoost is about, let’s discuss gradient boosting
    so that it becomes easier to comprehend the algorithm behind it. *Gradient boosting*
    is a technique to build models based on the idea of improving *weak learners*
    (which means models that perform only slightly better than random).
  prefs: []
  type: TYPE_NORMAL
- en: The way to improve these weak learners is to target their weak spots by creating
    other weak learners that can handle the weak spots. This gave birth to what is
    known as *Adaptive Boosting*, or *AdaBoost* for short. Hence, in layperson’s terms,
    *boosting* is all about combining weak learners to form better models.
  prefs: []
  type: TYPE_NORMAL
- en: The learners in AdaBoost (which, as discussed, are weak) are single-split decision
    trees (referred to as *stumps*). They are weighted, with more weight put on instances
    that are more difficult to classify and less weight put on the rest. At the same
    time, new learners are incorporated to be trained on the difficult parts, thus
    creating a more powerful model. Therefore, the difficult instances receive greater
    weights until they are solved by new weak learners.
  prefs: []
  type: TYPE_NORMAL
- en: 'Predictions are based on votes from the weak learners. The majority rule is
    applied in order to maximize accuracy. Gradient boosting therefore can be summarized
    in three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: It builds an ensemble of weak predictive models, typically decision trees, in
    a sequential manner.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each subsequent model is built to correct the errors or residuals of the previous
    models using gradient descent, which adjusts the predictions to minimize overall
    error.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The predictions from all the models are combined by taking a weighted average
    or sum, determined by the learning rate, to produce the final prediction.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To import the AdaBoost regressor, use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s look at the algorithm’s implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 7-10](#figure-7-10) shows the evolution of the forecasting task from
    the last values of `y_train` to the first values of `y_test` and `y_predicted`.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/dlff_0710.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-10\. Training data followed by test data (dashed line) and the predicted
    data (thin line); the vertical dashed line represents the start of the test period.
    The model used is the AdaBoost regression algorithm.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The model’s results are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The advantages of AdaBoost are:'
  prefs: []
  type: TYPE_NORMAL
- en: It generally has good accuracy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is easy to comprehend.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The disadvantages of AdaBoost are:'
  prefs: []
  type: TYPE_NORMAL
- en: It is impacted by outliers and sensitive to noise.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is slow and not optimized.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XGBoost Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*XGBoost* is a fast and performant gradient-boosted decision tree algorithm.
    The name may be complicated, but the concept is not hard to understand if you
    understood gradient boosting from the previous section on AdaBoost. XGBoost stands
    for *extreme gradient boosting* and was created by Tianqi Chen. Here’s how it
    works:'
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost starts with a simple base model, usually a decision tree.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It defines an objective function that measures the performance of the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using gradient descent optimization, it iteratively improves the model’s predictions
    by adjusting the model based on the gradient of the objective function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: New decision trees are added to the ensemble to correct errors made by previous
    models.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Regularization techniques, such as learning rate and column subsampling, are
    employed to enhance performance and prevent fitting issues.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The final prediction is obtained by combining the predictions from all the models
    in the ensemble.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The implementation of XGBoost in Python takes more steps than the previous algorithms.
    The first step is to `pip install` the required module. Type the following command
    in the prompt:^([4](ch07.html#id662))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'To import the XGBoost library, use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The implementation of the algorithm is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The argument `n_estimators` is a hyperparameter that determines the number of
    boosting rounds or trees to be built in the ensemble. As the algorithm combines
    the predictions of multiple weak learners (individual decision trees) to create
    a strong predictive model, each boosting round (iteration) adds a new decision
    tree to the ensemble, and the algorithm learns from the mistakes made by previous
    trees. The `n_estimators` hyperparameter controls the maximum number of trees
    that will be added to the ensemble during the training process.
  prefs: []
  type: TYPE_NORMAL
- en: AdaBoost and XGBoost are both boosting algorithms used to enhance the predictive
    power of weak learners, usually decision trees. AdaBoost focuses on iteratively
    emphasizing misclassified samples using exponential loss, lacks built-in regularization,
    and has limited parallelization. In contrast, XGBoost leverages gradient boosting,
    supports various loss functions, offers regularization, handles missing values,
    scales better through parallelization, provides comprehensive feature importance,
    and allows for more extensive hyperparameter tuning.
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost therefore offers more advanced features. It is often preferred for its
    overall better performance and ability to handle complex tasks. However, the choice
    between the two depends on the specific problem, dataset, and computational resources
    available.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 7-11](#figure-7-11) shows the evolution of the forecasting task from
    the last values of `y_train` to the first values of `y_test` and `y_predicted`.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/dlff_0711.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-11\. Training data followed by test data (dashed line) and the predicted
    data (thin line); the vertical dashed line represents the start of the test period.
    The model used is the XGBoost regression algorithm.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The model’s results are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Overfitting and Underfitting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Issues will arise in machine-based predictive analytics, and this is completely
    normal, since *perfection* is an impossible word in the world of data science
    (and finance). This section covers the most important issue when it comes to predicting
    data, and that is the *fitting problem*. Overfitting and underfitting are two
    terms that you must thoroughly understand so that you avoid their consequences
    when running your models.
  prefs: []
  type: TYPE_NORMAL
- en: '*Overfitting* occurs when a model performs extremely well on the training data
    but has bad results on the test data. It is a sign that the model has learned
    not only the details of the in-sample data but also the noise that occurred. Overfitting
    is generally associated with a high variance and low bias model, but what do those
    two terms mean?'
  prefs: []
  type: TYPE_NORMAL
- en: '*Bias* refers to the difference between the expected value of the model’s predictions
    and the real value of the target variable. A low bias model is one that is complex
    enough to capture the underlying patterns in the data.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Variance* refers to the variability of the model’s predictions for different
    training sets. A high variance model is one that is overly complex and can capture
    random noise and fluctuations in the training data. This can lead to overfitting,
    as the model may be fitting the noise in the data.'
  prefs: []
  type: TYPE_NORMAL
- en: To prevent overfitting, it’s important to strike a balance between bias and
    variance by selecting a model that is complex enough to capture the underlying
    patterns in the data but not so complex that it captures random noise and fluctuations
    in the data. Regularization techniques can also be used to reduce variance and
    prevent overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Overfitting occurs for a number of reasons, notably:'
  prefs: []
  type: TYPE_NORMAL
- en: Insufficient data
  prefs: []
  type: TYPE_NORMAL
- en: If the training data is not diverse enough, or if there is not enough of it,
    the model may overfit to the training data.
  prefs: []
  type: TYPE_NORMAL
- en: Overly complex model
  prefs: []
  type: TYPE_NORMAL
- en: If the model is too complex, it may learn the noise in the data rather than
    the underlying patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Feature overload
  prefs: []
  type: TYPE_NORMAL
- en: If the model is trained on too many features, it may learn irrelevant or noisy
    features that do not generalize to new data.
  prefs: []
  type: TYPE_NORMAL
- en: Lack of regularization
  prefs: []
  type: TYPE_NORMAL
- en: If the model is not regularized properly, it may overfit to the training data.
  prefs: []
  type: TYPE_NORMAL
- en: Leakage
  prefs: []
  type: TYPE_NORMAL
- en: Leakage occurs when information from the test set is inadvertently included
    in the training set. This can lead to overfitting as the model is learning from
    data that it will later see during testing.
  prefs: []
  type: TYPE_NORMAL
- en: A high bias model is one that is overly simplified and cannot capture the true
    underlying patterns in the data. This can lead to underfitting. Similarly, a low
    variance model is one that is not affected much by small changes in the training
    data and can generalize well to new, unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Underfitting occurs for a number of reasons, notably:'
  prefs: []
  type: TYPE_NORMAL
- en: Insufficient model complexity
  prefs: []
  type: TYPE_NORMAL
- en: If the model used is too simple to capture the underlying patterns in the data,
    it may result in underfitting. For example, a linear regression model might not
    be able to capture the nonlinear relationship between the features and the target
    variable.
  prefs: []
  type: TYPE_NORMAL
- en: Insufficient training
  prefs: []
  type: TYPE_NORMAL
- en: If the model is not trained for long enough, or with enough data, it may not
    be able to capture the underlying patterns in the data.
  prefs: []
  type: TYPE_NORMAL
- en: Over-regularization
  prefs: []
  type: TYPE_NORMAL
- en: Regularization is a technique used to prevent overfitting, but if it’s used
    excessively, it can lead to underfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Poor feature selection
  prefs: []
  type: TYPE_NORMAL
- en: If the features selected for the model are not informative or relevant, the
    model may underfit.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 7-12](#figure-7-12) shows a comparison between the different fits of
    a model to the data. An underfit model fails to capture the real relationship
    from the start, thus it is bad at predicting the past values and the future ones
    as well. A well-fit model captures the general tendency of the data. It is not
    an exact or a perfect model but one that generally has satisfactory predictions
    across the time period. An overfit model captures every detail of the past, even
    if it’s noise or random dislocations. The danger of an overfit model is that it
    inhibits a false promise of the future.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/dlff_0712.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-12\. Different fitting situations.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Therefore, when building machine learning models for time series prediction,
    you have to make sure you do not tune the parameters to perfectly fit the past
    values. To reduce fitting biases, make sure you incorporate the following best
    practices in your backtests:'
  prefs: []
  type: TYPE_NORMAL
- en: Increase training data
  prefs: []
  type: TYPE_NORMAL
- en: Collecting more training data helps to capture a broader range of patterns and
    variations in the data, reducing the chances of overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection
  prefs: []
  type: TYPE_NORMAL
- en: Carefully select relevant and informative features for your model. Removing
    irrelevant or redundant features reduces noise and complexity in the data, making
    it easier for the model to generalize well to unseen examples.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization techniques
  prefs: []
  type: TYPE_NORMAL
- en: Regularization methods explicitly control the complexity of the model to prevent
    overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter tuning
  prefs: []
  type: TYPE_NORMAL
- en: Optimize the hyperparameters of your model to find the best configuration. Hyperparameters
    control the behavior and complexity of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble methods
  prefs: []
  type: TYPE_NORMAL
- en: Employ ensemble methods, such as random forests, to combine predictions from
    multiple models. Ensemble methods can reduce overfitting by aggregating the predictions
    of multiple models, smoothing out individual model biases, and improving generalization.
  prefs: []
  type: TYPE_NORMAL
- en: Regular model evaluation
  prefs: []
  type: TYPE_NORMAL
- en: Regularly evaluate your model’s performance on unseen data or a dedicated validation
    set. This helps monitor the model’s generalization ability and detect any signs
    of overfitting or degradation in performance.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By properly understanding where machine learning algorithms come from, it becomes
    simpler to interpret them and understand their limitations. This chapter gave
    you the required knowledge (theory and practice) to build time series models using
    a few known machine learning algorithms in the hopes of forecasting values using
    past values.
  prefs: []
  type: TYPE_NORMAL
- en: What you must imperatively know is that past values are not necessarily indicative
    of future outcomes. Backtests are always biased somehow since a lot of tweaking
    is needed to tune the results, which may cause overfitting. Patterns do occur,
    but their results are not necessarily the same. Machine learning for financial
    time series prediction is constantly evolving, and most of the algorithms (in
    the raw form and with their basic inputs) are not very predictive, but with proper
    combination and the addition of risk management tools and filters, you may have
    a sustainable algorithm that adds value to your whole framework.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch07.html#id600-marker)) Price differences will be referred to as returns
    for simplicity. In general, returns can also represent a percentage return of
    a time series.
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch07.html#id626-marker)) The ordinary least squares method uses a mathematical
    formula to estimate the coefficients. It involves matrix algebra and calculus
    to solve for the coefficients that minimize the sum of squared residuals.
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch07.html#id644-marker)) A class of statistical methods that do not rely
    on specific assumptions about the underlying probability distribution.
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch07.html#id662-marker)) The prompt is a command-line interface that can
    generally be accessed in the Start menu. It is not the same as the area where
    you type the Python code that will later be executed.
  prefs: []
  type: TYPE_NORMAL
