- en: Chapter 1\. Artificial Intelligence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is the first time that a computer program has defeated a human professional
    player in the full-sized game of Go, a feat previously thought to be at least
    a decade away.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: David Silver et al. (2016)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This chapter introduces general notions, ideas, and definitions from the field
    of artificial intelligence (AI) for the purposes of this book. It also provides
    worked-out examples for different types of major learning algorithms. In particular,
    [“Algorithms”](#ai_algorithms) takes a broad perspective and categorizes types
    of data, types of learning, and types of problems typically encountered in an
    AI context. This chapter also presents examples for unsupervised and reinforcement
    learning. [“Neural Networks”](#ai_neural_networks) jumps right into the world
    of neural networks, which not only are central to what follows in later chapters
    of the book but also have proven to be among the most powerful algorithms AI has
    to offer nowadays. [“Importance of Data”](#ai_importance_of_data) discusses the
    importance of data volume and variety in the context of AI.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section introduces basic notions from the field of AI relevant to this
    book. It discusses the different types of data, learning, problems, and approaches
    that can be subsumed under the general term *AI*. Alpaydin (2016) provides an
    informal introduction to and overview of many of the topics covered only briefly
    in this section, along with many examples.
  prefs: []
  type: TYPE_NORMAL
- en: Types of Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Data in general has two major components:'
  prefs: []
  type: TYPE_NORMAL
- en: Features
  prefs: []
  type: TYPE_NORMAL
- en: Features data (or input data) is data that is given as input to an algorithm.
    In a financial context, this might be, for example, the income and the savings
    of a potential debtor.
  prefs: []
  type: TYPE_NORMAL
- en: Labels
  prefs: []
  type: TYPE_NORMAL
- en: Labels data (or output data) is data that is given as the relevant output to
    be learned, for example, by a supervised learning algorithm. In a financial context,
    this might be the creditworthiness of a potential debtor.
  prefs: []
  type: TYPE_NORMAL
- en: Types of Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are three major types of learning algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning (SL)
  prefs: []
  type: TYPE_NORMAL
- en: These are algorithms that learn from a given sample data set of features (input)
    and labels (output) values. The next section presents examples for such algorithms,
    like ordinary least-squares (OLS) regression and neural networks. The purpose
    of supervised learning is to learn the relationship between the input and output
    values. In finance, such algorithms might be trained to predict whether a potential
    debtor is creditworthy or not. For the purposes of this book, these are the most
    important types of algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning (UL)
  prefs: []
  type: TYPE_NORMAL
- en: These are algorithms that learn from a given sample data set of features (input)
    values only, often with the goal of finding structure in the data. They are supposed
    to learn about the input data set, given, for example, some guiding parameters.
    Clustering algorithms fall into that category. In a financial context, such algorithms
    might cluster stocks into certain groups.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning (RL)
  prefs: []
  type: TYPE_NORMAL
- en: These are algorithms that learn from trial and error by receiving a reward for
    taking an action. They update an optimal action policy according to what rewards
    and punishments they receive. Such algorithms are, for example, used for environments
    where actions need to be taken continuously and rewards are received immediately,
    such as in a computer game.
  prefs: []
  type: TYPE_NORMAL
- en: Because supervised learning is addressed in the subsequent section in some detail,
    brief examples will illustrate unsupervised learning and reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Simply speaking, a *k-means clustering algorithm* sorts <math alttext="n"><mi>n</mi></math>
    observations into <math alttext="k"><mi>k</mi></math> clusters. Each observation
    belongs to the cluster to which its mean (center) is nearest. The following Python
    code generates sample data for which the features data is clustered. [Figure 1-1](#figure_ai_08)
    visualizes the clustered sample data and also shows that the `scikit-learn` `KMeans`
    algorithm used here has identified the clusters perfectly. The coloring of the
    dots is based on what the algorithm has learned.^([1](ch01.xhtml#idm45625352526520))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_artificial_intelligence_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: A sample data set is created with clustered features data.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_artificial_intelligence_CO1-2)'
  prefs: []
  type: TYPE_NORMAL
- en: A `KMeans` model object is instantiated, fixing the number of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_artificial_intelligence_CO1-3)'
  prefs: []
  type: TYPE_NORMAL
- en: The model is fitted to the features data.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_artificial_intelligence_CO1-4)'
  prefs: []
  type: TYPE_NORMAL
- en: The predictions are generated given the fitted model.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](Images/5.png)](#co_artificial_intelligence_CO1-5)'
  prefs: []
  type: TYPE_NORMAL
- en: The predictions are numbers from 0 to 3, each representing one cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '![aiif 0101](Images/aiif_0101.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-1\. Unsupervised learning of clusters
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Once an algorithm such as `KMeans` is trained, it can, for instance, predict
    the cluster for a new (not yet seen) combination of features values. Assume that
    such an algorithm is trained on features data that describes potential and real
    debtors of a bank. It might learn about the creditworthiness of potential debtors
    by generating two clusters. New potential debtors can then be sorted into a certain
    cluster: “creditworthy” versus “not creditworthy.”'
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following example is based on a coin tossing game that is played with a
    coin that lands 80% of the time on heads and 20% of the time on tails. The coin
    tossing game is heavily biased to emphasize the benefits of learning as compared
    to an uninformed baseline algorithm. The baseline algorithm, which bets randomly
    and equally distributes on heads and tails, achieves a total reward of around
    50, on average, per epoch of 100 bets played:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_artificial_intelligence_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The state space (1 = heads, 0 = tails).
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_artificial_intelligence_CO2-2)'
  prefs: []
  type: TYPE_NORMAL
- en: The action space (1 = bet on heads, 0 = bet on tails).
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_artificial_intelligence_CO2-3)'
  prefs: []
  type: TYPE_NORMAL
- en: An action is randomly chosen from the action space.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_artificial_intelligence_CO2-4)'
  prefs: []
  type: TYPE_NORMAL
- en: A state is randomly chosen from the state space.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](Images/5.png)](#co_artificial_intelligence_CO2-5)'
  prefs: []
  type: TYPE_NORMAL
- en: The total reward `tr` is increased by one if the bet is correct.
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](Images/6.png)](#co_artificial_intelligence_CO2-6)'
  prefs: []
  type: TYPE_NORMAL
- en: The game is played for a number of epochs; each epoch is 100 bets.
  prefs: []
  type: TYPE_NORMAL
- en: '[![7](Images/7.png)](#co_artificial_intelligence_CO2-7)'
  prefs: []
  type: TYPE_NORMAL
- en: The average total reward of the epochs played is calculated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Reinforcement learning tries to learn from what is observed after an action
    is taken, usually based on a reward. To keep things simple, the following learning
    algorithm only keeps track of the states that are observed in each round insofar
    as they are appended to the action space `list` object. In this way, the algorithm
    learns the bias in the game, though maybe not perfectly. By randomly sampling
    from the updated action space, the bias is reflected because naturally the bet
    will more often be heads. Over time, heads is chosen, on average, around 80% of
    the time. The average total reward of around 65 reflects the improvement of the
    learning algorithm as compared to the uninformed baseline algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_artificial_intelligence_CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Resets the action space before starting (over)
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_artificial_intelligence_CO3-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Adds the observed state to the action space
  prefs: []
  type: TYPE_NORMAL
- en: Types of Tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Depending on the type of labels data and the problem at hand, two types of
    tasks to be learned are important:'
  prefs: []
  type: TYPE_NORMAL
- en: Estimation
  prefs: []
  type: TYPE_NORMAL
- en: Estimation (or approximation, regression) refers to the cases in which the labels
    data is real-valued (continuous); that is, it is technically represented as floating
    point numbers.
  prefs: []
  type: TYPE_NORMAL
- en: Classification
  prefs: []
  type: TYPE_NORMAL
- en: Classification refers to the cases in which the labels data consists of a finite
    number of classes or categories that are typically represented by discrete values
    (positive natural numbers), which in turn are represented technically as integers.
  prefs: []
  type: TYPE_NORMAL
- en: The following section provides examples for both types of tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Types of Approaches
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Some more definitions might be in order before finishing this section. This
    book follows the common differentiation between the following three major terms:'
  prefs: []
  type: TYPE_NORMAL
- en: Artificial intelligence (AI)
  prefs: []
  type: TYPE_NORMAL
- en: AI encompasses all types of learning (algorithms), as defined before, and some
    more (for example, expert systems).
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning (ML)
  prefs: []
  type: TYPE_NORMAL
- en: ML is the discipline of learning relationships and other information about given
    data sets based on an algorithm and a measure of success; a measure of success
    might, for example, be the mean-squared error (MSE) given labels values and output
    values to be estimated and the predicted values from the algorithm. ML is a sub-set
    of AI.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning (DL)
  prefs: []
  type: TYPE_NORMAL
- en: DL encompasses all algorithms based on neural networks. The term *deep* is usually
    only used when the neural network has more than one hidden layer. DL is a sub-set
    of machine learning and so is therefore also a sub-set of AI.
  prefs: []
  type: TYPE_NORMAL
- en: DL has proven useful for a number of broad problem areas. It is suited for estimation
    and classification tasks, as well as for RL. In many cases, DL-based approaches
    perform better than alternative algorithms, such as logistic regression or kernel-based
    ones, like support vector machines.^([2](ch01.xhtml#idm45625338167000)) That is
    why this book mainly focuses on DL. DL approaches used include dense neural networks
    (DNNs), recurrent neural networks (RNNs), and convolutional neural networks (CNNs).
    More details appear in later chapters, particularly in [Part III](part03.xhtml#part_statistical_inefficiencies).
  prefs: []
  type: TYPE_NORMAL
- en: Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous sections provide a broader overview of algorithms in AI. This section
    shows how neural networks fit in. A simple example will illustrate what characterizes
    neural networks in comparison to traditional statistical methods, such as ordinary
    least-squares (OLS) regression. The example starts with mathematics and then uses
    linear regression for *estimation* (or function approximation) and finally applies
    neural networks to accomplish the estimation. The approach taken here is a supervised
    learning approach where the task is to estimate labels data based on features
    data. This section also illustrates the use of neural networks in the context
    of *classification* problems.
  prefs: []
  type: TYPE_NORMAL
- en: OLS Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Assume that a mathematical function is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="f colon double-struck upper R right-arrow double-struck upper
    R comma y equals 2 x squared minus one-third x cubed" display="block"><mrow><mi>f</mi>
    <mo>:</mo> <mi>ℝ</mi> <mo>→</mo> <mi>ℝ</mi> <mo>,</mo> <mi>y</mi> <mo>=</mo> <mn>2</mn>
    <msup><mi>x</mi> <mn>2</mn></msup> <mo>-</mo> <mfrac><mn>1</mn> <mn>3</mn></mfrac>
    <msup><mi>x</mi> <mn>3</mn></msup></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Such a function transforms an input value <math alttext="x"><mi>x</mi></math>
    to an output value <math alttext="y"><mi>y</mi></math> . Or it transforms a series
    of input values <math alttext="x 1 comma x 2 comma ellipsis comma x Subscript
    upper N Baseline"><mrow><msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi>
    <mn>2</mn></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>x</mi> <mi>N</mi></msub></mrow></math>
    into a series of output values <math alttext="y 1 comma y 2 comma ellipsis comma
    y Subscript upper N Baseline"><mrow><msub><mi>y</mi> <mn>1</mn></msub> <mo>,</mo>
    <msub><mi>y</mi> <mn>2</mn></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>y</mi>
    <mi>N</mi></msub></mrow></math> . The following Python code implements the mathematical
    function as a Python function and creates a number of input and output values.
    [Figure 1-2](#figure_ai_01) plots the output values against the input values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_artificial_intelligence_CO4-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The mathematical function as a Python function
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_artificial_intelligence_CO4-2)'
  prefs: []
  type: TYPE_NORMAL
- en: The input values
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_artificial_intelligence_CO4-4)'
  prefs: []
  type: TYPE_NORMAL
- en: The output values
  prefs: []
  type: TYPE_NORMAL
- en: '![aiif 0102](Images/aiif_0102.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-2\. Output values against input values
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Whereas in the mathematical example the function comes first, the input data
    second, and the output data third, the sequence is different in *statistical learning*.
    Assume that the previous input values and output values are given. They represent
    the *sample* (data). The problem in *statistical regression* is to find a function
    that approximates the functional relationship between the input values (also called
    the *independent values*) and the output values (also called the *dependent values*)
    as well as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'Assume simple OLS linear regression. In this case, the functional relationship
    between the input and output values is assumed to be linear, and the problem is
    to find optimal parameters <math alttext="alpha"><mi>α</mi></math> and <math alttext="beta"><mi>β</mi></math>
    for the following linear equation:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="ModifyingAbove f With caret colon double-struck upper R right-arrow
    double-struck upper R comma ModifyingAbove y With caret equals alpha plus beta
    x" display="block"><mrow><mover accent="true"><mi>f</mi> <mo>^</mo></mover> <mo>:</mo>
    <mi>ℝ</mi> <mo>→</mo> <mi>ℝ</mi> <mo>,</mo> <mover accent="true"><mi>y</mi> <mo>^</mo></mover>
    <mo>=</mo> <mi>α</mi> <mo>+</mo> <mi>β</mi> <mi>x</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'For given input values <math alttext="x 1 comma x 2 comma ellipsis comma x
    Subscript upper N Baseline"><mrow><msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo>
    <msub><mi>x</mi> <mn>2</mn></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>x</mi>
    <mi>N</mi></msub></mrow></math> and output values <math alttext="y 1 comma y 2
    comma ellipsis comma y Subscript upper N Baseline"><mrow><msub><mi>y</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>y</mi> <mn>2</mn></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo>
    <msub><mi>y</mi> <mi>N</mi></msub></mrow></math> , *optimal* in this case means
    that they minimize the mean squared error (MSE) between the real output values
    and the approximated output values:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="min Underscript alpha comma beta Endscripts StartFraction 1 Over
    upper N EndFraction sigma-summation Underscript n Overscript upper N Endscripts
    left-parenthesis y Subscript n Baseline minus ModifyingAbove f With caret left-parenthesis
    x Subscript n Baseline right-parenthesis right-parenthesis squared" display="block"><mrow><munder><mo
    movablelimits="true" form="prefix">min</mo> <mrow><mi>α</mi><mo>,</mo><mi>β</mi></mrow></munder>
    <mfrac><mn>1</mn> <mi>N</mi></mfrac> <munderover><mo>∑</mo> <mi>n</mi> <mi>N</mi></munderover>
    <msup><mfenced separators="" open="(" close=")"><msub><mi>y</mi> <mi>n</mi></msub>
    <mo>-</mo><mover accent="true"><mi>f</mi> <mo>^</mo></mover><mrow><mo>(</mo><msub><mi>x</mi>
    <mi>n</mi></msub> <mo>)</mo></mrow></mfenced> <mn>2</mn></msup></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'For the case of simple linear regression, the solution <math alttext="left-parenthesis
    alpha Superscript asterisk Baseline comma beta Superscript asterisk Baseline right-parenthesis"><mrow><mo>(</mo>
    <msup><mi>α</mi> <mo>*</mo></msup> <mo>,</mo> <msup><mi>β</mi> <mo>*</mo></msup>
    <mo>)</mo></mrow></math> is known in closed form, as shown in the following equation.
    Bars on the variables indicate sample mean values:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="StartLayout 1st Row 1st Column beta Superscript asterisk 2nd
    Column equals 3rd Column StartFraction upper C o v left-parenthesis x comma y
    right-parenthesis Over upper V a r left-parenthesis x right-parenthesis EndFraction
    2nd Row 1st Column alpha Superscript asterisk 2nd Column equals 3rd Column y overbar
    minus beta x overbar EndLayout" display="block"><mtable><mtr><mtd><msup><mi>β</mi>
    <mo>*</mo></msup></mtd> <mtd><mo>=</mo></mtd> <mtd><mfrac><mrow><mi>C</mi><mi>o</mi><mi>v</mi><mo>(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>)</mo></mrow>
    <mrow><mi>V</mi><mi>a</mi><mi>r</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac></mtd></mtr>
    <mtr><mtd><msup><mi>α</mi> <mo>*</mo></msup></mtd> <mtd><mo>=</mo></mtd> <mtd><mrow><mover
    accent="true"><mi>y</mi> <mo>¯</mo></mover> <mo>-</mo> <mi>β</mi> <mover accent="true"><mi>x</mi>
    <mo>¯</mo></mover></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'The following Python code calculates the optimal parameter values, linearly
    estimates (approximates) the output values, and plots the linear regression line
    alongside the sample data (see [Figure 1-3](#figure_ai_02)). The linear regression
    approach does not work too well here in approximating the functional relationship.
    This is confirmed by the relatively high MSE value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_artificial_intelligence_CO5-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Calculation of optimal <math alttext="beta"><mi>β</mi></math>
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_artificial_intelligence_CO5-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Calculation of optimal <math alttext="alpha"><mi>α</mi></math>
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_artificial_intelligence_CO5-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Calculation of estimated output values
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_artificial_intelligence_CO5-6)'
  prefs: []
  type: TYPE_NORMAL
- en: Calculation of the MSE given the approximation
  prefs: []
  type: TYPE_NORMAL
- en: '![aiif 0103](Images/aiif_0103.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-3\. Sample data and linear regression line
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'How can the MSE value be improved (decreased)—maybe even to 0, that is, to
    a “perfect estimation?” Of course, OLS regression is not constrained to a simple
    linear relationship. In addition to the constant and linear terms, higher order
    monomials, for instance, can be easily added as basis functions. To this end,
    compare the regression results shown in [Figure 1-4](#figure_ai_03) and the following
    code that creates the figure. The improvements that come from using quadratic
    and cubic monomials as basis functions are obvious and also are numerically confirmed
    by the calculated MSE values. For basis functions up to and including the cubic
    monomial, the estimation is perfect, and the functional relationship is perfectly
    recovered:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_artificial_intelligence_CO6-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Regression step
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_artificial_intelligence_CO6-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Approximation step
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_artificial_intelligence_CO6-3)'
  prefs: []
  type: TYPE_NORMAL
- en: MSE calculation
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_artificial_intelligence_CO6-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Optimal (“perfect”) parameter values
  prefs: []
  type: TYPE_NORMAL
- en: '![aiif 0104](Images/aiif_0104.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-4\. Sample data and OLS regression lines
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Exploiting the knowledge of the form of the mathematical function to be approximated
    and accordingly adding more basis functions to the regression leads to a “perfect
    approximation.” That is, the OLS regression recovers the exact factors of the
    quadratic and cubic part, respectively, of the original function.
  prefs: []
  type: TYPE_NORMAL
- en: Estimation with Neural Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: However, not all relationships are of this kind. This is where, for instance,
    *neural networks* can help. Without going into the details, neural networks can
    approximate a wide range of functional relationships. Knowledge of the form of
    the relationship is generally not required.
  prefs: []
  type: TYPE_NORMAL
- en: Scikit-learn
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following Python code uses the `MLPRegressor` class of `scikit-learn`,
    which implements a DNN for estimation. DNNs are sometimes also called multi-layer
    perceptron (MLP).^([3](ch01.xhtml#idm45625340102264)) The results are not perfect,
    as [Figure 1-5](#figure_ai_04) and the MSE illustrate. However, they are quite
    good already for the simple configuration used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_artificial_intelligence_CO7-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Instantiates the `MLPRegressor` object
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_artificial_intelligence_CO7-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Implements the fitting or learning step
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_artificial_intelligence_CO7-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Implements the prediction step
  prefs: []
  type: TYPE_NORMAL
- en: Just having a look at the results in [Figure 1-4](#figure_ai_03) and [Figure 1-5](#figure_ai_04),
    one might assume that the methods and approaches are not too dissimilar after
    all. However, there is a fundamental difference worth highlighting. Although the
    OLS regression approach, as shown explicitly for the simple linear regression,
    is based on the calculation of certain well-specified quantities and parameters,
    the neural network approach relies on *incremental learning*. This in turn means
    that a set of parameters, the *weights* within the neural network, are first initialized
    randomly and then adjusted gradually given the differences between the neural
    network output and the sample output values. This approach lets you retrain (update)
    a neural network incrementally.
  prefs: []
  type: TYPE_NORMAL
- en: '![aiif 0105](Images/aiif_0105.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-5\. Sample data and neural network–based estimations
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Keras
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The next example uses a sequential model with the `Keras` deep learning package.^([4](ch01.xhtml#idm45625340095720))
    The model is fitted, or *trained*, for 100 epochs. The procedure is repeated for
    five rounds. After every such round, the approximation by the neural network is
    updated and plotted. [Figure 1-6](#figure_ai_05) shows how the approximation gradually
    improves with every round. This is also reflected in the decreasing MSE values.
    The end result is not perfect, but again, it is quite good given the simplicity
    of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_artificial_intelligence_CO8-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Instantiates the `Sequential` model object
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_artificial_intelligence_CO8-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Adds a densely connected hidden layer with rectified linear unit (ReLU) activation^([5](ch01.xhtml#idm45625339723528))
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_artificial_intelligence_CO8-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Adds the output layer with linear activation
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_artificial_intelligence_CO8-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Compiles the model for usage
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](Images/5.png)](#co_artificial_intelligence_CO8-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Trains the neural network for a fixed number of epochs
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](Images/6.png)](#co_artificial_intelligence_CO8-6)'
  prefs: []
  type: TYPE_NORMAL
- en: Implements the approximation step
  prefs: []
  type: TYPE_NORMAL
- en: '[![7](Images/7.png)](#co_artificial_intelligence_CO8-7)'
  prefs: []
  type: TYPE_NORMAL
- en: Calculates the current MSE
  prefs: []
  type: TYPE_NORMAL
- en: '[![8](Images/8.png)](#co_artificial_intelligence_CO8-8)'
  prefs: []
  type: TYPE_NORMAL
- en: Plots the current approximation results
  prefs: []
  type: TYPE_NORMAL
- en: Roughly speaking, one can say that the neural network does almost as well in
    the estimation as the OLS regression, which delivers a perfect result. Therefore,
    why use neural networks at all? A more comprehensive answer might need to come
    later in this book, but a somewhat different example might give some hint.
  prefs: []
  type: TYPE_NORMAL
- en: Consider instead the previous sample data set, as generated from a well-defined
    mathematical function, now a *random sample data set*, for which both features
    and labels are randomly chosen. Of course, this example is for illustration and
    does not allow for a deep interpretation.
  prefs: []
  type: TYPE_NORMAL
- en: '![aiif 0106](Images/aiif_0106.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-6\. Sample data and estimations after multiple training rounds
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The following code generates the random sample data set and creates the OLS
    regression estimation based on a varying number of monomial basis functions. [Figure 1-7](#figure_ai_06)
    visualizes the results. Even for the highest number of monomials in the example,
    the estimation results are still not too good. The MSE value is accordingly relatively
    high:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The results for the OLS regression are not too surprising. OLS regression in
    this case assumes that the approximation can be achieved through an appropriate
    combination of a finite number of basis functions. Since the sample data set has
    been generated randomly, the OLS regression does not perform well in this case.
  prefs: []
  type: TYPE_NORMAL
- en: '![aiif 0107](Images/aiif_0107.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-7\. Random sample data and OLS regression lines
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'What about neural networks? The application is as straightforward as before
    and yields estimations as shown in [Figure 1-8](#figure_ai_07). While the end
    result is not perfect, it is obvious that the neural network performs better than
    the OLS regression in estimating the random label values from the random features
    values. Given its architecture, however, the neural network has almost 200,000
    trainable parameters (weights), which offers relatively high flexibility, particularly
    when compared to the OLS regression, for which a maximum of 15 + 1 parameters
    are used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_artificial_intelligence_CO9-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Multiple hidden layers are added.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_artificial_intelligence_CO9-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Network architecture and number of trainable parameters are shown.
  prefs: []
  type: TYPE_NORMAL
- en: '![aiif 0108](Images/aiif_0108.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-8\. Random sample data and neural network estimations
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Classification with Neural Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another benefit of neural networks is that they can be easily used for classification
    tasks as well. Consider the following Python code that implements a classification
    using a neural network based on `Keras`. The binary features data and labels data
    are generated randomly. The major adjustment to be made modeling-wise is to change
    the activation function from the output layer to `sigmoid` from `linear`. More
    details on this appear in later chapters. The classification is not perfect. However,
    it reaches a high level of accuracy. How the accuracy, expressed as the relationship
    between correct results to all label values, changes with the number of training
    epochs is shown in [Figure 1-9](#figure_ai_09). The accuracy starts out low and
    then improves step-wise, though not necessarily with every step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_artificial_intelligence_CO10-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Creates random features data
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_artificial_intelligence_CO10-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Creates random labels data
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_artificial_intelligence_CO10-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Defines the activation function for the output layer as `sigmoid`
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_artificial_intelligence_CO10-6)'
  prefs: []
  type: TYPE_NORMAL
- en: Defines the loss function to be `binary_crossentropy`^([6](ch01.xhtml#idm45625343405032))
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](Images/5.png)](#co_artificial_intelligence_CO10-7)'
  prefs: []
  type: TYPE_NORMAL
- en: Compares the predicted values with the labels data
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](Images/6.png)](#co_artificial_intelligence_CO10-8)'
  prefs: []
  type: TYPE_NORMAL
- en: Plots the loss function and accuracy values for every training step
  prefs: []
  type: TYPE_NORMAL
- en: '![aiif 0109](Images/aiif_0109.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-9\. Classification accuracy and loss against number of epochs
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The examples in this section illustrate some fundamental characteristics of
    neural networks as compared to OLS regression:'
  prefs: []
  type: TYPE_NORMAL
- en: Problem-agnostic
  prefs: []
  type: TYPE_NORMAL
- en: The neural network approach is agnostic when it comes to estimating and classifying
    label values, given a set of feature values. Statistical methods, such as OLS
    regression, might perform well for a smaller set of problems, but not too well
    or not at all for others.
  prefs: []
  type: TYPE_NORMAL
- en: Incremental learning
  prefs: []
  type: TYPE_NORMAL
- en: The optimal weights within a neural network, given a target measure of success,
    are learned incrementally based on a random initialization and incremental improvements.
    These incremental improvements are achieved by considering the differences between
    the predicted values and the sample label values and backpropagating weights updates
    through the neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Universal approximation
  prefs: []
  type: TYPE_NORMAL
- en: There are strong mathematical theorems showing that neural networks (even with
    one hidden layer only) can approximate almost any function.^([7](ch01.xhtml#idm45625335928248))
  prefs: []
  type: TYPE_NORMAL
- en: These characteristics might justify why this book puts neural networks at the
    core with regard to the algorithms used. [Chapter 2](ch02.xhtml#superintelligence)
    discusses more good reasons.
  prefs: []
  type: TYPE_NORMAL
- en: Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Neural networks are good at learning relationships between input and output
    data. They can be applied to a number of problem types, such as estimation in
    the presence of complex relationships or classification, for which traditional
    statistical methods are not well suited.
  prefs: []
  type: TYPE_NORMAL
- en: Importance of Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The example at the end of the previous section shows that neural networks are
    capable of solving classification problems quite well. The neural network with
    one hidden layer reaches a high degree of accuracy on the given data set, or *in-sample*.
    However, what about the predictive power of a neural network? This hinges significantly
    on the volume and variety of the data available to train the neural network. Another
    numerical example, based on larger data sets, will illustrate this point.
  prefs: []
  type: TYPE_NORMAL
- en: Small Data Set
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Consider a random sample data set similar to the one used before in the classification
    example, but with more features and more samples. Most algorithms used in AI are
    about *pattern recognition*. In the following Python code, the number of binary
    features defines the number of possible patterns about which the algorithm can
    learn something. Given that the labels data is also binary, the algorithm tries
    to learn whether a `0` or `1` is more likely given a certain pattern, say `[0,
    0, 1, 1, 1, 1, 0, 0, 0, 0]`. Because all numbers are randomly chosen with equal
    probability, there is not that much to learn beyond the fact that the labels `0`
    and `1` are equally likely no matter what (random) pattern is observed. Therefore,
    a baseline prediction algorithm should be accurate about 50% of the time, no matter
    what (random) pattern it is presented with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_artificial_intelligence_CO11-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Features data
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_artificial_intelligence_CO11-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Labels data
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_artificial_intelligence_CO11-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Number of patterns
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to proceed, the raw data is put into a `pandas` `DataFrame` object,
    which simplifies certain operations and analyses:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_artificial_intelligence_CO12-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Defines column names for the features data
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_artificial_intelligence_CO12-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Puts the features data into a `DataFrame` object
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_artificial_intelligence_CO12-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Puts the labels data into the same `DataFrame` object
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_artificial_intelligence_CO12-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Shows the meta information for the data set
  prefs: []
  type: TYPE_NORMAL
- en: 'Two major problems can be identified given the results from executing the following
    Python code. First, not all patterns are in the sample data set. Second, the sample
    size is much too small per observed pattern. Even without digging deeper, it is
    clear that no classification algorithm can really learn about all the possible
    patterns in a meaningful way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_artificial_intelligence_CO13-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Groups the data along all columns
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_artificial_intelligence_CO13-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Unstacks the grouped data for the labels column
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_artificial_intelligence_CO13-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Adds up the frequency for a `0` and a `1`
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_artificial_intelligence_CO13-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Shows the frequencies for a `0` and a `1` given a certain pattern
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](Images/5.png)](#co_artificial_intelligence_CO13-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Provides statistics for the sum of the frequencies
  prefs: []
  type: TYPE_NORMAL
- en: 'The following Python code uses the `MLPClassifier` model from `scikit-learn`.^([8](ch01.xhtml#idm45625338923864))
    The model is trained on the whole data set. What about the ability of a neural
    network to learn about the relationships within a given data set? The ability
    is pretty high, as the in-sample accuracy score shows. It is in fact close to
    100%, a result driven to a large extent by the relatively high neural network
    capacity given the relatively small data set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'But what about the *predictive power* of a trained neural network? To this
    end, the given data set can be split into a training and a test data sub-set.
    The model is trained on the training data sub-set only and then tested with regard
    to its predictive power on the test data set. As before, the accuracy of the trained
    neural network is pretty high in-sample (that is, on the training data set). However,
    it is more than 10 percentage points worse than an uninformed baseline algorithm
    on the test data set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_artificial_intelligence_CO14-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Splits the data into `train` and `test` data sub-sets
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_artificial_intelligence_CO14-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Trains the model on the training data set only
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_artificial_intelligence_CO14-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Reports the accuracy in-sample (training data set)
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_artificial_intelligence_CO14-6)'
  prefs: []
  type: TYPE_NORMAL
- en: Reports the accuracy out-of-sample (test data set)
  prefs: []
  type: TYPE_NORMAL
- en: Roughly speaking, the neural network, trained on a small data set only, learns
    wrong relationships due to the identified two major problem areas. The problems
    are not really relevant in the context of learning relationships *in-sample*.
    To the contrary, the smaller a data set is, the more easily in-sample relationships
    can be learned in general. However, the problem areas are highly relevant when
    using the trained neural network to generate predictions *out-of-sample*.
  prefs: []
  type: TYPE_NORMAL
- en: Larger Data Set
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Fortunately, there is often a clear way out of this problematic situation:
    a *larger data set*. Faced with real-world problems, this theoretical insight
    might be equally correct. From a practical point of view, though, such larger
    data sets are not always available, nor can they often be generated so easily.
    However, in the context of the example of this section, a larger data set is indeed
    easily created.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following Python code increases the number of samples in the initial sample
    data set significantly. The result is that the prediction accuracy of the trained
    neural network increases by more than 10 percentage points, to a level of about
    50%, which is to be expected given the nature of the labels data. It is now in
    line with an uninformed baseline algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_artificial_intelligence_CO15-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Prediction accuracy in-sample (training data set)
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_artificial_intelligence_CO15-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Prediction accuracy out-of-sample (test data set)
  prefs: []
  type: TYPE_NORMAL
- en: 'A quick analysis of the available data, as shown next, explains the increase
    in the prediction accuracy. First, all possible patterns are now represented in
    the data set. Second, all patterns have an average frequency of above 10 in the
    data set. In other words, the neural network sees basically *all the patterns
    multiple times*. This allows the neural network to “learn” that both labels `0`
    and `1` are equally likely for all possible patterns. Of course, it is a rather
    involved way of learning this, but it is a good illustration of the fact that
    a *relatively small* data set might often be *too small* in the context of neural
    networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_artificial_intelligence_CO16-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Adds the frequency for the `0` and `1` values
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_artificial_intelligence_CO16-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Shows summary statistics for the sum values
  prefs: []
  type: TYPE_NORMAL
- en: Volume and Variety
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the context of neural networks that perform prediction tasks, the volume
    and variety of the available data used to train the neural network are decisive
    for its prediction performance. The numerical, hypothetical examples in this section
    show that the same neural network trained on a relatively small and not-as-varied
    data set underperforms its counterpart trained on a relatively large and varied
    data set by more than 10 percentage points. This difference can be considered
    huge given that AI practitioners and companies often fight for improvements as
    small as a tenth of a percentage point.
  prefs: []
  type: TYPE_NORMAL
- en: Big Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What is the difference between a *larger* data set and a *big* data set? The
    term *big data* has been used for more than a decade now to mean a number of things.
    For the purposes of this book, one might say that a *big data set* is large enough—in
    terms of volume, variety, and also maybe velocity—for an AI algorithm to be trained
    properly such that the algorithm performs better at a prediction task as compared
    to a baseline algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: The larger data set used before is still small in practical terms. However,
    it is large enough to accomplish the specified goal. The required volume and variety
    of the data set are mainly driven by the structure and characteristics of the
    features and labels data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this context, assume that a retail bank implements a neural network–based
    classification approach for credit scoring. Given in-house data, the responsible
    data scientist designs 25 categorical features, every one of which can take on
    8 different values. The resulting number of patterns is astronomically large:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: It is clear that no single data set can provide a neural network with exposure
    to every single one of these patterns.^([9](ch01.xhtml#idm45625339136664)) Fortunately,
    in practice this is not necessary for the neural network to learn about the creditworthiness
    based on data for regular, defaulting, and/or rejected debtors. It is also not
    necessary in general to generate “good” predictions with regard to the creditworthiness
    of every potential debtor.
  prefs: []
  type: TYPE_NORMAL
- en: This is due to a number of reasons. To name only a few, first, not every pattern
    will be relevant in practice—some patterns might simply not exist, might be impossible,
    and so forth. Second, not all features might be equally important, reducing the
    number of relevant features and thereby the number of possible patterns. Third,
    a value of `4` or `5` for feature number `7`, say, might not make a difference
    at all, further reducing the number of relevant patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this book, artificial intelligence, or AI, encompasses methods, techniques,
    algorithms, and so on that are able to learn relationships, rules, probabilities,
    and more from data. The focus lies on supervised learning algorithms, such as
    those for estimation and classification. With regard to algorithms, neural networks
    and deep learning approaches are at the core.
  prefs: []
  type: TYPE_NORMAL
- en: 'The central theme of this book is the application of neural networks to one
    of the core problems in finance: the prediction of future market movements. More
    specifically, the problem might be to predict the direction of movement for a
    stock index or the exchange rate for a currency pair. The prediction of the future
    market direction (that is, whether a target level or price goes up or down) is
    a problem that can be easily cast into a classification setting.'
  prefs: []
  type: TYPE_NORMAL
- en: Before diving deeper into the core theme itself, the next chapter first discusses
    selected topics related to what is called *superintelligence* and *technological
    singularity*. That discussion will provide useful background for the chapters
    that follow, which focus on finance and the application of AI to the financial
    domain.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Books and papers cited in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Alpaydin, Ethem. 2016\. *Machine Learning*. MIT Press, Cambridge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chollet, Francois. 2017\. *Deep Learning with Python*. Shelter Island: Manning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016\. *Deep Learning*.
    Cambridge: MIT Press. [*http://deeplearningbook.org*](http://deeplearningbook.org).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kratsios, Anastasis. 2019\. “Universal Approximation Theorems.” [*https://oreil.ly/COOdI*](https://oreil.ly/COOdI).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Silver, David et al. 2016\. “Mastering the Game of Go with Deep Neural Networks
    and Tree Search.” *Nature* 529 (January): 484-489.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shanahan, Murray. 2015\. *The Technological Singularity*. Cambridge: MIT Press.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tegmark, Max. 2017\. *Life 3.0: Being Human in the Age of Artificial Intelligence*.
    United Kingdom: Penguin Random House.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'VanderPlas, Jake. 2017\. *Python Data Science Handbook*. Sebastopol: O’Reilly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ^([1](ch01.xhtml#idm45625352526520-marker)) For details, see [`sklearn.cluster.KMeans`](https://oreil.ly/cRcJo)
    and VanderPlas (2017, ch. 5).
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch01.xhtml#idm45625338167000-marker)) For details, see VanderPlas (2017,
    ch. 5).
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch01.xhtml#idm45625340102264-marker)) For details, see [`sklearn.neural_network.MLPRegressor`](https://oreil.ly/Oimd8).
    For more background, see Goodfellow et al. (2016, ch. 6).
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch01.xhtml#idm45625340095720-marker)) For details, see Chollet (2017,
    ch. 3).
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch01.xhtml#idm45625339723528-marker)) For details on activation functions
    with `Keras`, see [*https://keras.io/activations*](https://keras.io/activations).
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch01.xhtml#idm45625343405032-marker)) The *loss function* calculates the
    prediction error of the neural network (or other ML algorithms). *Binary cross
    entropy* is an appropriate loss function for binary classification problems, while
    the *mean squared error* (MSE) is, for example, appropriate for estimation problems.
    For details on loss functions with `Keras`, see [*https://keras.io/losses*](https://keras.io/losses).
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch01.xhtml#idm45625335928248-marker)) See, for example, Kratsios (2019).
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch01.xhtml#idm45625338923864-marker)) For details, see [`sklearn.neural_network.MLPClassifier`](https://oreil.ly/hCR4h).
  prefs: []
  type: TYPE_NORMAL
- en: ^([9](ch01.xhtml#idm45625339136664-marker)) Nor would current compute technology
    allow one to model and train a neural network based on such a data set if it would
    be available. In this context, the next chapter discusses the importance of hardware
    for AI.
  prefs: []
  type: TYPE_NORMAL
