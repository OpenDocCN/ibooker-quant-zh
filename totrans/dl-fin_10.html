<html><head></head><body><section data-pdf-bookmark="Chapter 10. Deep Reinforcement Learning for Time Series Prediction" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch10">&#13;
<h1><span class="label">Chapter 10. </span>Deep Reinforcement Learning <span class="keep-together">for Time Series Prediction</span></h1>&#13;
&#13;
<p><a contenteditable="false" data-primary="time series prediction (deep reinforcement learning for)" data-type="indexterm" id="Chapter_10.html0"/><em>Reinforcement learning</em> is <a contenteditable="false" data-primary="reinforcement learning" data-secondary="defined" data-type="indexterm" id="id769"/>a branch of machine learning that deals with sequential decision-making problems. Algorithms in this branch learn to make optimal decisions by interacting with an environment and receiving feedback in the form of rewards. In the context of time series forecasting, it can be used to develop models that make sequential predictions based on historical data. Traditional forecasting approaches often rely on statistical methods or supervised learning techniques, which assume independence between data points. However, time series data exhibits temporal dependencies and patterns, which may be effectively captured using reinforcement learning.</p>&#13;
&#13;
<p>Reinforcement learning models for time series forecasting typically involve an agent that takes actions based on observed states and receives rewards based on the accuracy of its predictions. The agent learns through trial and error to maximize cumulative rewards over time. The key challenge is finding an optimal balance between <em>exploration </em>(trying out new actions) and <em>exploitation </em>(using learned knowledge).</p>&#13;
&#13;
<p>This chapter gives a basic overview of reinforcement learning and deep reinforcement learning with regard to predicting time series data.</p>&#13;
&#13;
<section class="pagebreak-before" data-pdf-bookmark="Intuition of Reinforcement Learning" data-type="sect1"><div class="sect1" id="id79">&#13;
<h1 class="less_space">Intuition of Reinforcement Learning</h1>&#13;
&#13;
<p><a contenteditable="false" data-primary="reinforcement learning" data-secondary="intuition of" data-type="indexterm" id="Chapter_10.html1"/>Simplification is always the right path toward understanding more advanced details. So let’s look at reinforcement learning from a simple point of view before digging deeper.</p>&#13;
&#13;
<p>Reinforcement learning deals primarily with rewards and penalties. Imagine a child who gets a reward for doing good things and a punishment for doing bad things. Over time, that child will grow and will develop their experience so that they do good things and try to avoid doing bad things as much as possible (no one is perfect). Therefore, the learning is done through experience.</p>&#13;
&#13;
<p>From a time series perspective, the main idea is the same. Imagine training a model on past data and letting it then learn by experience, rewarding it for good predictions and calibrating its parameters when it makes a mistake so that it can achieve better accuracy next time. The algorithm is greedy in nature and wants to maximize its rewards; therefore, over time it becomes better at predicting the next likely value, which is of course dependent on the quality and the signal-to-noise ratio of the analyzed time series.</p>&#13;
&#13;
<p>The term <em>reinforcement learning</em> comes from the fact that <em>positive reinforcement</em> is given to the algorithm when it makes right decisions and <em>negative reinforcement</em> is given when it makes bad decisions. <a contenteditable="false" data-primary="reinforcement learning" data-secondary="main elements of" data-type="indexterm" id="id770"/>The first three concepts you must know are states, actions, and rewards:</p>&#13;
&#13;
<dl>&#13;
	<dt>States</dt>&#13;
	<dd><p>The features at every time step. For example, at a certain time step, the current state of the market is its OHLC data and its volume data. In more familiar words, states are the explanatory variables.</p></dd>&#13;
	<dt>Actions</dt>&#13;
	<dd><p>The decisions a trader may make at every time step. They generally involve buying, selling, or holding. In more familiar words, actions are the algorithms’ decisions when faced with certain states (a simple discretionary example of this would be a trader noticing an overvalued market and deciding to initiate a <span class="keep-together">buy order).</span></p></dd>&#13;
	<dt>Rewards</dt>&#13;
	<dd><p>The results of correct actions. The simplest reward is the positive return. Note that a poorly designed reward function can lead to model issues such as a buy-and-hold strategy.<sup><a data-type="noteref" href="ch10.html#id771" id="id771-marker">1</a></sup></p></dd>&#13;
</dl>&#13;
&#13;
<p><a data-type="xref" href="#table-10-1">Table 10-1</a> shows the three main elements of reinforcement learning.</p>&#13;
&#13;
<table id="table-10-1">&#13;
	<caption><span class="label">Table 10-1. </span>A hypothetical decision table</caption>&#13;
	<thead>&#13;
		<tr>&#13;
			<th> </th>&#13;
			<th>Time</th>&#13;
			<th>Open</th>&#13;
			<th>High</th>&#13;
			<th>Low</th>&#13;
			<th>Close</th>&#13;
			<th>|</th>&#13;
			<th><strong>Action</strong></th>&#13;
			<th><strong>Reward</strong></th>&#13;
		</tr>&#13;
	</thead>&#13;
	<tbody>&#13;
		<tr>&#13;
			<td class="fakeheader"><strong>States</strong></td>&#13;
			<td>  1</td>&#13;
			<td>10</td>&#13;
			<td>14</td>&#13;
			<td>8</td>&#13;
			<td>10</td>&#13;
			<td>|</td>&#13;
			<td>BUY</td>&#13;
			<td>    0</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td class="fakeheader"><strong>States</strong></td>&#13;
			<td>  2</td>&#13;
			<td>10</td>&#13;
			<td>15</td>&#13;
			<td>6</td>&#13;
			<td>13</td>&#13;
			<td>|</td>&#13;
			<td>BUY</td>&#13;
			<td>    3</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td class="fakeheader"><strong>States</strong></td>&#13;
			<td>  3</td>&#13;
			<td>13</td>&#13;
			<td>16</td>&#13;
			<td>8</td>&#13;
			<td>14</td>&#13;
			<td>|</td>&#13;
			<td>SELL</td>&#13;
			<td>  –1</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td class="fakeheader"><strong>States</strong></td>&#13;
			<td>  4</td>&#13;
			<td>10</td>&#13;
			<td>16</td>&#13;
			<td>8</td>&#13;
			<td>14</td>&#13;
			<td>|</td>&#13;
			<td>HOLD</td>&#13;
			<td>    0</td>&#13;
		</tr>&#13;
	</tbody>&#13;
</table>&#13;
&#13;
<p>States are the rows that go from the Time column to the Close column. Actions can be categorical, as you can see from the Action column, and Rewards can either be numerical (e.g., a positive or negative profit) or categorical (e.g., profit or loss label).</p>&#13;
&#13;
<p><a contenteditable="false" data-primary="reward function" data-type="indexterm" id="id772"/>From the preceding list, it seems complicated to just design a system that looks for rewards. A <em>reward function</em> quantifies the desirability or utility of being in a particular state or taking a specific action. The reward function therefore provides feedback to the agent, indicating the immediate quality of its actions and guiding its learning process. Before we discuss reward functions in more detail, let’s look at what a state-action table is (also known as a Q-table).</p>&#13;
&#13;
<p><a contenteditable="false" data-primary="Q-table (quality table)" data-type="indexterm" id="id773"/>A <em>Q-table</em>, short for <em>quality table</em>, is a data structure to store and update the expected value (called the <em>Q-value</em>) of taking a particular action in a given state. The Q-value of a state-action pair (<em>s</em>, <em>a</em>) at time <em>t</em> represents the expected cumulative reward that an agent can achieve by taking action <em>a</em> in state <em>s</em> following a specific policy. The Q-table is therefore a table-like structure that maps each state-action pair to its corresponding Q-value.</p>&#13;
&#13;
<p>Initially, the Q-table is usually initialized with arbitrary values or set to zero. As the algorithm explores the environment (market) and receives rewards, it updates the Q-values in the table based on the observed rewards and the estimated future rewards. This process is typically done using an algorithm such as Q-learning.</p>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Over time, through repeated exploration and exploitation, the Q-table gradually converges to more accurate estimates of the optimal Q-values, representing the best actions to take in each state. By using the Q-table, the agent can make informed decisions and learn to maximize its cumulative rewards in the given environment. Remember, a reward can be a profit, Sharpe ratio, or any other performance metric.</p>&#13;
</div>&#13;
&#13;
<p class="pagebreak-before"><a contenteditable="false" data-primary="Bellman equation" data-type="indexterm" id="id774"/><a contenteditable="false" data-primary="Q-learning" data-type="indexterm" id="id775"/><em>Q-learning</em> is a popular reinforcement learning algorithm that enables an agent to learn optimal actions by iteratively updating its action-value function, known as the <em>Bellman equation</em>, defined as follows:</p>&#13;
&#13;
<div data-type="equation">&#13;
<p><math alttext="upper Q left-parenthesis s Subscript t Baseline comma a Subscript t Baseline right-parenthesis equals upper R left-parenthesis s Subscript t Baseline comma a Subscript t Baseline right-parenthesis plus gamma m a x left-bracket upper Q left-parenthesis s Subscript t plus 1 Baseline comma a Subscript t plus 1 Baseline right-parenthesis right-bracket">&#13;
  <mrow>&#13;
    <mi>Q</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <msub><mi>s</mi> <mi>t</mi> </msub>&#13;
      <mo>,</mo>&#13;
      <msub><mi>a</mi> <mi>t</mi> </msub>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <mi>R</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <msub><mi>s</mi> <mi>t</mi> </msub>&#13;
      <mo>,</mo>&#13;
      <msub><mi>a</mi> <mi>t</mi> </msub>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>+</mo>&#13;
    <mi>γ</mi>&#13;
    <mi>m</mi>&#13;
    <mi>a</mi>&#13;
    <mi>x</mi>&#13;
    <mrow>&#13;
      <mo>[</mo>&#13;
      <mi>Q</mi>&#13;
      <mrow>&#13;
        <mo>(</mo>&#13;
        <msub><mi>s</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow> </msub>&#13;
        <mo>,</mo>&#13;
        <msub><mi>a</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow> </msub>&#13;
        <mo>)</mo>&#13;
      </mrow>&#13;
      <mo>]</mo>&#13;
    </mrow>&#13;
  </mrow>&#13;
</math></p>&#13;
&#13;
<p><math alttext="StartLayout 1st Row  upper Q left-parenthesis s Subscript t Baseline comma a Subscript t Baseline right-parenthesis is the expected reward 2nd Row  upper R left-parenthesis s Subscript t Baseline comma a Subscript t Baseline right-parenthesis is the reward table 3rd Row  gamma is the learning rate left-parenthesis known as g a m m a right-parenthesis EndLayout">&#13;
  <mtable>&#13;
    <mtr>&#13;
      <mtd columnalign="left">&#13;
        <mrow>&#13;
          <mi>Q</mi>&#13;
          <mo>(</mo>&#13;
          <msub><mi>s</mi> <mi>t</mi> </msub>&#13;
          <mo>,</mo>&#13;
          <msub><mi>a</mi> <mi>t</mi> </msub>&#13;
          <mo>)</mo>&#13;
          <mspace width="4.pt"/>&#13;
          <mtext>is</mtext>&#13;
          <mspace width="4.pt"/>&#13;
          <mtext>the</mtext>&#13;
          <mspace width="4.pt"/>&#13;
          <mtext>expected</mtext>&#13;
          <mspace width="4.pt"/>&#13;
          <mtext>reward</mtext>&#13;
        </mrow>&#13;
      </mtd>&#13;
    </mtr>&#13;
    <mtr>&#13;
      <mtd columnalign="left">&#13;
        <mrow>&#13;
          <mi>R</mi>&#13;
          <mo>(</mo>&#13;
          <msub><mi>s</mi> <mi>t</mi> </msub>&#13;
          <mo>,</mo>&#13;
          <msub><mi>a</mi> <mi>t</mi> </msub>&#13;
          <mo>)</mo>&#13;
          <mspace width="4.pt"/>&#13;
          <mtext>is</mtext>&#13;
          <mspace width="4.pt"/>&#13;
          <mtext>the</mtext>&#13;
          <mspace width="4.pt"/>&#13;
          <mtext>reward</mtext>&#13;
          <mspace width="4.pt"/>&#13;
          <mtext>table</mtext>&#13;
        </mrow>&#13;
      </mtd>&#13;
    </mtr>&#13;
    <mtr>&#13;
      <mtd columnalign="left">&#13;
        <mrow>&#13;
          <mi>γ</mi>&#13;
          <mspace width="4.pt"/>&#13;
          <mtext>is</mtext>&#13;
          <mspace width="4.pt"/>&#13;
          <mtext>the</mtext>&#13;
          <mspace width="4.pt"/>&#13;
          <mtext>learning</mtext>&#13;
          <mspace width="4.pt"/>&#13;
          <mtext>rate</mtext>&#13;
          <mspace width="4.pt"/>&#13;
          <mtext>(known</mtext>&#13;
          <mspace width="4.pt"/>&#13;
          <mtext>as</mtext>&#13;
          <mspace width="4.pt"/>&#13;
          <mi>g</mi>&#13;
          <mi>a</mi>&#13;
          <mi>m</mi>&#13;
          <mi>m</mi>&#13;
          <mi>a</mi>&#13;
          <mtext>)</mtext>&#13;
        </mrow>&#13;
      </mtd>&#13;
    </mtr>&#13;
  </mtable>&#13;
</math></p>&#13;
</div>&#13;
&#13;
<p>The larger the learning rate (γ), the more the algorithm takes into account the previous experiences. Notice that if γ is equal to zero, it would be synonymous to learning nothing as the second term will cancel itself out. As a simple example, consider <a data-type="xref" href="#table-10-2">Table 10-2</a>.</p>&#13;
&#13;
<table id="table-10-2">&#13;
	<caption><span class="label">Table 10-2. </span>R-table</caption>&#13;
	<colgroup>&#13;
		<col/>&#13;
		<col/>&#13;
	</colgroup>&#13;
	<thead>&#13;
		<tr>&#13;
			<th>Time (State)</th>&#13;
			<th>Act (Action)</th>&#13;
			<th>Wait (Action)</th>&#13;
		</tr>&#13;
	</thead>&#13;
	<tbody>&#13;
		<tr>&#13;
			<td>       1</td>&#13;
			<td>2 reward units</td>&#13;
			<td>0 reward units</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>       2</td>&#13;
			<td>2 reward units</td>&#13;
			<td>0 reward units</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>       3</td>&#13;
			<td>2 reward units</td>&#13;
			<td>0 reward units</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>       4</td>&#13;
			<td>2 reward units</td>&#13;
			<td>0 reward units</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>       5</td>&#13;
			<td>2 reward units</td>&#13;
			<td>4 reward units</td>&#13;
		</tr>&#13;
	</tbody>&#13;
</table>&#13;
&#13;
<p>The table describes the results of actions through time. At every time step, acting (doing something) will give a reward of 2, while waiting to act on the fifth time step will give a reward of 4. This means that the agent can make one of the following choices:</p>&#13;
&#13;
<ul>&#13;
	<li>Act now and get 2 reward units.</li>&#13;
	<li>Wait before acting and get 4 reward units.</li>&#13;
</ul>&#13;
&#13;
<p>Let’s assume γ = 0.80. Using the Bellman equation and working backward will get you the following results:</p>&#13;
&#13;
<div data-type="equation"><math alttext="StartLayout 1st Row  upper Q left-parenthesis s 1 comma a 1 right-parenthesis equals 0 plus 0.8 left-parenthesis 2.04 right-parenthesis equals 1.63 2nd Row  upper Q left-parenthesis s 2 comma a 2 right-parenthesis equals 0 plus 0.8 left-parenthesis 2.56 right-parenthesis equals 2.04 3rd Row  upper Q left-parenthesis s 3 comma a 3 right-parenthesis equals 0 plus 0.8 left-parenthesis 3.20 right-parenthesis equals 2.56 4th Row  upper Q left-parenthesis s 4 comma a 4 right-parenthesis equals 0 plus 0.8 left-parenthesis 4.00 right-parenthesis equals 3.20 EndLayout">&#13;
  <mtable>&#13;
    <mtr>&#13;
      <mtd columnalign="left">&#13;
        <mrow>&#13;
          <mi>Q</mi>&#13;
          <mrow>&#13;
            <mo>(</mo>&#13;
            <msub><mi>s</mi> <mn>1</mn> </msub>&#13;
            <mo>,</mo>&#13;
            <msub><mi>a</mi> <mn>1</mn> </msub>&#13;
            <mo>)</mo>&#13;
          </mrow>&#13;
          <mo>=</mo>&#13;
          <mn>0</mn>&#13;
          <mo>+</mo>&#13;
          <mn>0</mn>&#13;
          <mo lspace="0%" rspace="0%">.</mo>&#13;
          <mn>8</mn>&#13;
          <mrow>&#13;
            <mo>(</mo>&#13;
            <mn>2</mn>&#13;
            <mo lspace="0%" rspace="0%">.</mo>&#13;
            <mn>04</mn>&#13;
            <mo>)</mo>&#13;
          </mrow>&#13;
          <mo>=</mo>&#13;
          <mn>1</mn>&#13;
          <mo lspace="0%" rspace="0%">.</mo>&#13;
          <mn>63</mn>&#13;
        </mrow>&#13;
      </mtd>&#13;
    </mtr>&#13;
    <mtr>&#13;
      <mtd columnalign="left">&#13;
        <mrow>&#13;
          <mi>Q</mi>&#13;
          <mrow>&#13;
            <mo>(</mo>&#13;
            <msub><mi>s</mi> <mn>2</mn> </msub>&#13;
            <mo>,</mo>&#13;
            <msub><mi>a</mi> <mn>2</mn> </msub>&#13;
            <mo>)</mo>&#13;
          </mrow>&#13;
          <mo>=</mo>&#13;
          <mn>0</mn>&#13;
          <mo>+</mo>&#13;
          <mn>0</mn>&#13;
          <mo lspace="0%" rspace="0%">.</mo>&#13;
          <mn>8</mn>&#13;
          <mrow>&#13;
            <mo>(</mo>&#13;
            <mn>2</mn>&#13;
            <mo lspace="0%" rspace="0%">.</mo>&#13;
            <mn>56</mn>&#13;
            <mo>)</mo>&#13;
          </mrow>&#13;
          <mo>=</mo>&#13;
          <mn>2</mn>&#13;
          <mo lspace="0%" rspace="0%">.</mo>&#13;
          <mn>04</mn>&#13;
        </mrow>&#13;
      </mtd>&#13;
    </mtr>&#13;
    <mtr>&#13;
      <mtd columnalign="left">&#13;
        <mrow>&#13;
          <mi>Q</mi>&#13;
          <mrow>&#13;
            <mo>(</mo>&#13;
            <msub><mi>s</mi> <mn>3</mn> </msub>&#13;
            <mo>,</mo>&#13;
            <msub><mi>a</mi> <mn>3</mn> </msub>&#13;
            <mo>)</mo>&#13;
          </mrow>&#13;
          <mo>=</mo>&#13;
          <mn>0</mn>&#13;
          <mo>+</mo>&#13;
          <mn>0</mn>&#13;
          <mo lspace="0%" rspace="0%">.</mo>&#13;
          <mn>8</mn>&#13;
          <mrow>&#13;
            <mo>(</mo>&#13;
            <mn>3</mn>&#13;
            <mo lspace="0%" rspace="0%">.</mo>&#13;
            <mn>20</mn>&#13;
            <mo>)</mo>&#13;
          </mrow>&#13;
          <mo>=</mo>&#13;
          <mn>2</mn>&#13;
          <mo lspace="0%" rspace="0%">.</mo>&#13;
          <mn>56</mn>&#13;
        </mrow>&#13;
      </mtd>&#13;
    </mtr>&#13;
    <mtr>&#13;
      <mtd columnalign="left">&#13;
        <mrow>&#13;
          <mi>Q</mi>&#13;
          <mrow>&#13;
            <mo>(</mo>&#13;
            <msub><mi>s</mi> <mn>4</mn> </msub>&#13;
            <mo>,</mo>&#13;
            <msub><mi>a</mi> <mn>4</mn> </msub>&#13;
            <mo>)</mo>&#13;
          </mrow>&#13;
          <mo>=</mo>&#13;
          <mn>0</mn>&#13;
          <mo>+</mo>&#13;
          <mn>0</mn>&#13;
          <mo lspace="0%" rspace="0%">.</mo>&#13;
          <mn>8</mn>&#13;
          <mrow>&#13;
            <mo>(</mo>&#13;
            <mn>4</mn>&#13;
            <mo lspace="0%" rspace="0%">.</mo>&#13;
            <mn>00</mn>&#13;
            <mo>)</mo>&#13;
          </mrow>&#13;
          <mo>=</mo>&#13;
          <mn>3</mn>&#13;
          <mo lspace="0%" rspace="0%">.</mo>&#13;
          <mn>20</mn>&#13;
        </mrow>&#13;
      </mtd>&#13;
    </mtr>&#13;
  </mtable>&#13;
</math></div>&#13;
&#13;
<p class="pagebreak-before"><a data-type="xref" href="#table-10-2">Table 10-2</a> may be updated to become <a data-type="xref" href="#table-10-3">Table 10-3</a> as follows.</p>&#13;
&#13;
<table id="table-10-3">&#13;
	<caption><span class="label">Table 10-3. </span>Q-table</caption>&#13;
	<thead>&#13;
		<tr>&#13;
			<th>Time (State)</th>&#13;
			<th>Act (Action)</th>&#13;
			<th>Wait (Action)</th>&#13;
		</tr>&#13;
	</thead>&#13;
	<tbody>&#13;
		<tr>&#13;
			<td>      1</td>&#13;
			<td>2 reward units</td>&#13;
			<td>1.63 reward units</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>      2</td>&#13;
			<td>2 reward units</td>&#13;
			<td>2.04 reward units</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>      3</td>&#13;
			<td>2 reward units</td>&#13;
			<td>2.56 reward units</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>      4</td>&#13;
			<td>2 reward units</td>&#13;
			<td>3.20 reward units</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>      5</td>&#13;
			<td>2 reward units</td>&#13;
			<td>4.00 reward units</td>&#13;
		</tr>&#13;
	</tbody>&#13;
</table>&#13;
&#13;
<p>Therefore, the Q-table is continuously updated with the implied rewards to help maximize the final reward. To understand why the term <em>max </em>is in the Bellman equation, consider the example in <a data-type="xref" href="#table-10-4">Table 10-4</a>.</p>&#13;
&#13;
<table id="table-10-4">&#13;
	<caption><span class="label">Table 10-4. </span>R-table</caption>&#13;
	<thead>&#13;
		<tr>&#13;
			<th>Time (State)</th>&#13;
			<th>Buy</th>&#13;
			<th>Sell</th>&#13;
			<th>Hold</th>&#13;
		</tr>&#13;
	</thead>&#13;
	<tbody>&#13;
		<tr>&#13;
			<td>      1</td>&#13;
			<td> 5</td>&#13;
			<td> 8</td>&#13;
			<td> 8</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>      2</td>&#13;
			<td> 3</td>&#13;
			<td> 2</td>&#13;
			<td> 1</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>      3</td>&#13;
			<td> 2</td>&#13;
			<td> 5</td>&#13;
			<td> 6</td>&#13;
		</tr>&#13;
	</tbody>&#13;
</table>&#13;
&#13;
<p>Calculate the would-be value of <em>x</em> in a Q-table (<a data-type="xref" href="#table-10-5">Table 10-5</a>) assuming a learning rate of 0.4.</p>&#13;
&#13;
<table id="table-10-5">&#13;
	<caption><span class="label">Table 10-5. </span>Q-table</caption>&#13;
	<thead>&#13;
		<tr>&#13;
			<th>Time (State)</th>&#13;
			<th>Buy</th>&#13;
			<th>Sell</th>&#13;
			<th>Hold</th>&#13;
		</tr>&#13;
	</thead>&#13;
	<tbody>&#13;
		<tr>&#13;
			<td>     1</td>&#13;
			<td> ?  </td>&#13;
			<td> ?</td>&#13;
			<td> ?</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>     2</td>&#13;
			<td> ?</td>&#13;
			<td> x</td>&#13;
			<td> ?</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>     3</td>&#13;
			<td> 2</td>&#13;
			<td> 5</td>&#13;
			<td> 6</td>&#13;
		</tr>&#13;
	</tbody>&#13;
</table>&#13;
&#13;
<p>Following the formula, you should get this result:</p>&#13;
&#13;
<div data-type="equation">&#13;
<p><math alttext="x equals 2 plus 0.4 left-parenthesis m a x left-parenthesis 2 comma 5 comma 6 right-parenthesis right-parenthesis equals 4.4">&#13;
  <mrow>&#13;
    <mi>x</mi>&#13;
    <mo>=</mo>&#13;
    <mn>2</mn>&#13;
    <mo>+</mo>&#13;
    <mn>0</mn>&#13;
    <mo lspace="0%" rspace="0%">.</mo>&#13;
    <mn>4</mn>&#13;
    <mo>(</mo>&#13;
    <mi>m</mi>&#13;
    <mi>a</mi>&#13;
    <mi>x</mi>&#13;
    <mo>(</mo>&#13;
    <mn>2</mn>&#13;
    <mo>,</mo>&#13;
    <mn>5</mn>&#13;
    <mo>,</mo>&#13;
    <mn>6</mn>&#13;
    <mo>)</mo>&#13;
    <mo>)</mo>&#13;
    <mo>=</mo>&#13;
    <mn>4</mn>&#13;
    <mo lspace="0%" rspace="0%">.</mo>&#13;
    <mn>4</mn>&#13;
  </mrow>&#13;
</math></p>&#13;
</div>&#13;
&#13;
<p>States (features) must be predictive in nature so that the reinforcement learning algorithm predicts the next value with an accuracy better than random. Examples of features can be the values of the relative strength index (RSI), moving averages, and lagged close prices.</p>&#13;
&#13;
<p class="pagebreak-before">It is crucial to keep in mind that the inputs’ statistical preference remains the same, that is, stationary. This begs the question: how are moving averages used as inputs if they are not stationary? The simple answer is through the usual transformation, which is to take the percentage difference.</p>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>It is possible to use fractional differencing to transform a nonstationary time series into a stationary one while retaining its <span class="keep-together">memory.</span></p>&#13;
</div>&#13;
&#13;
<p><a contenteditable="false" data-primary="policy, reinforcement learning" data-type="indexterm" id="id776"/>A <em>policy </em>defines the behavior of an agent in an environment. It is a mapping from states to actions, indicating what action the agent should take in a given state. The policy essentially guides the agent’s decision-making process by specifying the action to be executed based on the observed state.</p>&#13;
&#13;
<p>The goal of reinforcement learning is to find an optimal policy that maximizes the agent’s long-term cumulative reward. This is typically achieved through a trial-and-error process, where the agent interacts with the environment, takes actions, receives rewards, and adjusts its policy based on the observed outcomes.</p>&#13;
&#13;
<p>The exploitation policy is generally faster than the exploration policy but may be more limited as it seeks a greater and immediate reward, while there may be a path afterward that leads to an even greater reward. Ideally, the best policy to take is a combination of both. But how do you determine this optimal mix? That question is answered by epsilon (ε).</p>&#13;
&#13;
<p><a contenteditable="false" data-primary="epsilon" data-secondary="in reinforcement learning" data-type="indexterm" id="id777"/><em>Epsilon </em>is a parameter used in exploration–exploitation trade-offs. It determines the probability with which an agent selects a random action (exploration) versus selecting the action with the highest estimated value (exploitation).</p>&#13;
&#13;
<p>Commonly used exploration strategies include epsilon-greedy and softmax. In <em>epsilon-greedy</em>, the agent selects the action with the highest estimated value with a probability of (1 – ε), and then it selects a random action with a probability of ε. This allows the agent to explore different actions and potentially discover better policies. As the agent learns over time, the epsilon value is often decayed gradually to reduce exploration and focus more on exploitation.<sup><a data-type="noteref" href="ch10.html#id778" id="id778-marker">2</a></sup> <em>Softmax</em> action selection considers the estimated action values but introduces stochasticity in the decision-making process. The temperature parameter associated with softmax determines the randomness in action selection, where a higher temperature leads to more exploration.</p>&#13;
&#13;
<div data-type="warning" epub:type="warning"><h6>Warning</h6>&#13;
<p>Do not mix up epsilon and gamma:</p>&#13;
&#13;
<ul>&#13;
	<li><a contenteditable="false" data-primary="epsilon" data-secondary="gamma versus" data-type="indexterm" id="id779"/><a contenteditable="false" data-primary="gamma, epsilon versus" data-type="indexterm" id="id780"/><em>Gamma </em>is a parameter that determines the importance of future rewards. It controls the extent to which the agent values immediate rewards compared to delayed rewards (hence, it is related to a delayed gratification issue). The value of gamma is typically a number between 0 and 1, where a value closer to 1 means the agent considers future rewards more heavily, while a value closer to 0 gives less importance to future rewards. To understand this more, consider having another look at the Bellman equation.</li>&#13;
	<li><em>Epsilon </em>is a parameter used in exploration–exploitation trade-offs. It determines the probability with which an agent selects a random action (exploration) versus selecting the action with the highest estimated value (exploitation).</li>&#13;
</ul>&#13;
</div>&#13;
&#13;
<p>At this point, you may feel overwhelmed by the amount of new information presented, especially because it differs from what you have seen so far in the book. Before moving to the more complex deep reinforcement learning discussion, a quick summary of what you have seen in this chapter until now may be beneficial. Reinforcement learning is essentially giving the machine a task that it will then learn how to do on its own.</p>&#13;
&#13;
<p>With time series analysis, states represent the current situation or condition of the environment at a particular time. An example of state is a technical indicator’s value. States are represented by Q-tables. Actions are self-explanatory and can be buy, sell, or hold (or even a more complex combination such as decrease weight and increase weight). Rewards are what the algorithm is trying to maximize and can be profit per trade, Sharpe ratio, or any sort of performance evaluation metric. A reward can also be a penalty such as the number of trades or maximum drawdown (in such a case, you are aiming to minimize it). The reinforcement learning algorithm will go through many iterations and variables through different policies to try to detect hidden patterns and optimize trading decision so that profitability is maximized. This is easier said than done (or coded).</p>&#13;
&#13;
<p>One question is begging an answer: is using a Q-table to represent the different states of financial time series efficient? This question is answered in the next section.<a contenteditable="false" data-primary="" data-startref="Chapter_10.html1" data-type="indexterm" id="id781"/></p>&#13;
</div></section>&#13;
&#13;
<section class="pagebreak-before" data-pdf-bookmark="Deep Reinforcement Learning" data-type="sect1"><div class="sect1" id="id80">&#13;
<h1 class="less_space">Deep Reinforcement Learning</h1>&#13;
&#13;
<p><a contenteditable="false" data-primary="deep reinforcement learning" data-secondary="basics" data-type="indexterm" id="Chapter_10.html2"/>&#13;
<a contenteditable="false" data-primary="deep reinforcement learning" data-primary-seealso="time series prediction [deep reinforcement learning for]" data-type="indexterm" id="Chapter_10.html2a"/><a contenteditable="false" data-primary="reinforcement learning" data-secondary="deep reinforcement learning basics" data-type="indexterm" id="Chapter_10.html3"/><em>Deep reinforcement learning</em> combines reinforcement learning techniques with deep learning architectures, particularly deep neural networks. It involves training agents to learn optimal behavior and make decisions by interacting with an environment, using deep neural networks to approximate value functions or policies.</p>&#13;
&#13;
<p>The main difference between a reinforcement learning algorithm and a deep reinforcement learning algorithm is that the former estimates Q-values using the Q-table, while the latter estimates Q-values using ANNs (see <a data-type="xref" href="ch08.html#ch08">Chapter 8</a> for details on artificial neural networks).</p>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>As a reminder, <em>artificial neural networks</em> (ANNs) are a type of computational model inspired by the structure and functioning of the human brain. A neural network consists of interconnected nodes organized into layers. The three main types of layers are the input layer, hidden layers, and the output layer. The input layer receives the initial data, which is then processed through the hidden layers, and finally, the output layer produces the network’s prediction.</p>&#13;
</div>&#13;
&#13;
<p>The main objective of this section is to understand and design a deep reinforcement learning algorithm with the aim of data prediction. Keep in mind that reinforcement learning is still not heavily applied since it suffers from a few issues (discussed at the end of this section) that need to be resolved before making it one of the main trading algorithms in quantitative finance.</p>&#13;
&#13;
<p>Therefore, deep reinforcement learning will have two main elements with important tasks:</p>&#13;
&#13;
<ul>&#13;
	<li>A deep neural network architecture to recognize patterns and approximate the best function that relates dependent and independent variables</li>&#13;
	<li>A reinforcement learning architecture that allows the algorithm to learn by trial and error how to maximize a certain profit function</li>&#13;
</ul>&#13;
&#13;
<p><a contenteditable="false" data-primary="experience replay" data-type="indexterm" id="id782"/><a contenteditable="false" data-primary="replay memory" data-type="indexterm" id="id783"/>Let’s continue defining a few key concepts before putting things together. <em>Replay memory</em>, also known as <em>experience replay</em>, involves storing and reusing past experiences to enhance the learning process and improve the stability and efficiency of the training.</p>&#13;
&#13;
<p>In deep reinforcement learning, an agent interacts with an environment, observes states, takes actions, and receives rewards. Each observation, action, reward, and resulting next state is considered an experience. The replay memory serves as a buffer that stores a collection of these experiences.</p>&#13;
&#13;
<p class="pagebreak-before">The replay memory has the following key features:</p>&#13;
&#13;
<dl>&#13;
 <dt>Storage</dt>&#13;
 <dd><p>The replay memory is a data structure that can store a fixed number of experiences. Each experience typically consists of the current state, the action taken, the resulting reward, the next state, and a flag indicating whether the episode <span class="keep-together">terminated.</span></p></dd>&#13;
 &#13;
 <dt>Sampling</dt>&#13;
 <dd><p>During the training process, instead of using experiences immediately as they occur, the agent samples a batch of experiences from the replay memory. Randomly sampling experiences from a large pool of stored transitions helps in decorrelating the data and breaking the temporal dependencies that exist in <span class="keep-together">consecutive</span> experiences.</p></dd>&#13;
 &#13;
 <dt>Batch learning</dt>&#13;
 <dd><p>The sampled batch of experiences is then used to update the agent’s neural network. By learning from a batch of experiences rather than individual experiences, the agent can make more efficient use of computation and improve the learning stability. Batch learning also allows for the application of optimization techniques, such as stochastic gradient descent, to update the network weights.</p></dd>&#13;
</dl>&#13;
&#13;
&#13;
<p>The replay memory provides several benefits to deep reinforcement learning algorithms. Among those benefits is experience reuse, as the agent can learn from a more diverse set of data, reducing the bias that can arise from sequential updates. Breaking correlations is another benefit since the sequential nature of experience collection in reinforcement learning can introduce correlations between consecutive experiences. Randomly sampling experiences from the replay memory helps break these correlations, making the learning process more stable.</p>&#13;
&#13;
<p>So far, we have discussed the following steps:</p>&#13;
&#13;
<ol>&#13;
	<li>Defining and initializing the environment</li>&#13;
	<li>Designing the neural network architecture</li>&#13;
	<li>Designing the reinforcement learning architecture with experience replay to stabilize the learning process</li>&#13;
	<li>Interacting with the environment and storing experiences until the learning process is done and predictions on new data are done</li>&#13;
</ol>&#13;
&#13;
<p>One thing we have not discussed is how to reduce overestimations, which can be achieved by doubling down on the neural network architecture.</p>&#13;
&#13;
<p><a contenteditable="false" data-primary=" DDQN (Double Deep Q-Network)" data-type="indexterm" id="id784"/><a contenteditable="false" data-primary="Double Deep Q-Network (DDQN)" data-type="indexterm" id="id785"/>The <em>Double Deep Q-Network </em>(DDQN) model is an extension of the original DQN architecture introduced by DeepMind in 2015. The primary motivation behind DDQN is to address a known issue in the DQN algorithm called <a contenteditable="false" data-primary="overestimation bias" data-type="indexterm" id="id786"/><em>overestimation bias</em>, which can lead to suboptimal action selection.</p>&#13;
&#13;
<p>In the original DQN, the action values (Q-values) for each state-action pair are estimated using a single neural network. However, during the learning process, the Q-values are estimated using the maximum Q-value among all possible actions in the next state (take a look at <a data-type="xref" href="#table-10-5">Table 10-5</a>). This maximum Q-value can sometimes result in an overestimation of the true action values, leading to a suboptimal policy.</p>&#13;
&#13;
<p>The DDQN addresses this overestimation bias by utilizing two separate neural networks: the Q-network and the target-network. The <em>Q-network</em> is a deep neural network that approximates the action-value function (Q-function). In other words, it estimates the value of each possible action in a given state. The Q-network’s parameters (weights and biases) are learned through training to minimize the difference between predicted Q-values and target Q-values. The <em>target network</em> is a separate copy of the Q-network that is used to estimate the target Q-values during training. It helps stabilize the learning process and improve the convergence of the Q-network. The weights of the target network are not updated during training; instead, they are periodically updated to match the weights of the Q-network.</p>&#13;
&#13;
<p>The key idea behind the DDQN is to decouple the selection of actions from the estimation of their values.</p>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>The algorithm updates the Q-network regularly and the target network occasionally. This is done to avoid the issue of the same model being used to estimate the Q-value from the next state and then giving it to the Bellman equation to estimate the Q-value for the current state.</p>&#13;
</div>&#13;
&#13;
<p>So, to put these elements into an ordered sequence, here’s how the deep reinforcement learning architecture may look:</p>&#13;
&#13;
<ol>&#13;
	<li>The environment is initialized.</li>&#13;
	<li>The epsilon value is selected. Remember, epsilon is the exploration–exploitation trade-off parameter used to control the agent’s behavior during training.</li>&#13;
	<li>The current state is fetched. Remember, an example of the current state may be the OHLC data, the RSI, the standard deviation of the returns, or even the day of the week.</li>&#13;
	<li>In the first round, the algorithm selects the action through exploration as the model is not trained yet; therefore, the action is randomly selected (e.g., from a choice panel of buy, sell, and hold). If it’s not the first step, then exploitation may be used to select the action. Exploitation is where the action is determined by the neural network model.</li>&#13;
	<li>The action is applied.</li>&#13;
	<li>The previous elements are stored in replay memory.</li>&#13;
	<li>The inputs and the target array are fetched and the Q-network is trained.</li>&#13;
	<li>If the round is not over, repeat the process starting at step 3. Otherwise, train the target network and repeat from step 1.</li>&#13;
</ol>&#13;
&#13;
<p>To illustrate the algorithm, let’s use it on the synthetic sine wave time series. Create the time series and then apply the deep reinforcement learning algorithm with the aim of predicting the future values.</p>&#13;
&#13;
<p>The full code can be found in the <a href="https://oreil.ly/5YGHI">GitHub repository</a> (for replication purposes).</p>&#13;
&#13;
<p><a data-type="xref" href="#figure-10-1">Figure 10-1</a> shows the test data (solid line) versus the predicted data (dashed line) using 1 epoch, 5 inputs (lagged values), a batch size of 64, and 1 hidden layer with 6 neurons.</p>&#13;
&#13;
<figure><div class="figure" id="figure-10-1"><img alt="" class="iimagesepoch_1png" src="assets/dlff_1001.png"/>&#13;
<h6><span class="label">Figure 10-1. </span>Test data versus predicted data using 1 epoch, 5 inputs, a batch size of 64, and 1 hidden layer with 6 neurons</h6>&#13;
</div></figure>&#13;
&#13;
<p><a data-type="xref" href="#figure-10-2">Figure 10-2</a> shows the test data (solid line) versus the predicted data (dashed line) using 1 epoch, 5 inputs (lagged values), a batch size of 64, and 2 hidden layers with each having 6 neurons.</p>&#13;
&#13;
<figure><div class="figure" id="figure-10-2"><img alt="" class="iimagesepoch_10png" src="assets/dlff_1002.png"/>&#13;
<h6><span class="label">Figure 10-2. </span>Predicted values versus actual values using 1 epoch, 5 inputs, a batch size of 64, and 2 hidden layers with each having 6 neurons</h6>&#13;
</div></figure>&#13;
&#13;
<p><a data-type="xref" href="#figure-10-3">Figure 10-3</a> shows the predictions using 10 epochs, 5 inputs (lagged values), a batch size of 32, and 2 hidden layers with each having 6 neurons.</p>&#13;
&#13;
<figure><div class="figure" id="figure-10-3"><img alt="" class="iimagesepoch_10_32png" src="assets/dlff_1003.png"/>&#13;
<h6><span class="label">Figure 10-3. </span>Predicted values versus actual values using 10 epochs, 5 inputs, a batch&#13;
size of 32, and 2 hidden layers with each having 6 neurons</h6>&#13;
</div></figure>&#13;
&#13;
<p class="pagebreak-before"><a data-type="xref" href="#figure-10-4">Figure 10-4</a> shows the predictions using 10 epochs, 5 inputs (lagged values), a batch size of 32, and 2 hidden layers with each having 24 neurons.</p>&#13;
&#13;
<figure><div class="figure" id="figure-10-4"><img alt="" class="iimagesepoch_10_32_24png" src="assets/dlff_1004.png"/>&#13;
<h6><span class="label">Figure 10-4. </span>Predicted values versus actual values using 10 epochs, 5 inputs, a batch&#13;
size of 32, and 2 hidden layers with each having 24 neurons</h6>&#13;
</div></figure>&#13;
&#13;
<p><a data-type="xref" href="#figure-10-5">Figure 10-5</a> shows the predictions using 10 epochs, 8 inputs (lagged values), a batch size of 32, and 2 hidden layers with each having 64 neurons.</p>&#13;
&#13;
<figure><div class="figure" id="figure-10-5"><img alt="" class="iimagesepoch_10_32_64_32windowpng" src="assets/dlff_1005.png"/>&#13;
<h6><span class="label">Figure 10-5. </span>Predicted values versus actual values using 10 epochs, 8 inputs, a batch&#13;
size of 32, and 2 hidden layers with each having 64 neurons</h6>&#13;
</div></figure>&#13;
&#13;
<p>As you know, the more epochs, the better the fit—up to a certain point, where overfitting may start to become an issue. Fortunately, by now you know how to reduce that risk.</p>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Note that the sine wave example is a very basic one, and more complex data can be used with the algorithm. The choice of the sine wave time series is for illustrative purposes only, and you must use more sophisticated methods on more complex time series to be able to judge the algorithm.</p>&#13;
</div>&#13;
&#13;
<p>Reinforcement learning is easily overfit and is more likely to learn simple patterns and not hidden and complicated ones. Also, you should now be aware of the difficulty of reward function design and choice of features. Furthermore, such models are often considered mystery boxes, making it difficult to explain the reasoning behind their predictions. All of these issues are now a barrier to implementing a stable and profitable deep reinforcement learning algorithm for trading.<a contenteditable="false" data-primary="" data-startref="Chapter_10.html3" data-type="indexterm" id="id787"/><a contenteditable="false" data-primary="" data-startref="Chapter_10.html2" data-type="indexterm" id="id788"/><a contenteditable="false" data-primary="" data-startref="Chapter_10.html2a" data-type="indexterm" id="id789"/></p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="id81">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>Reinforcement learning can be applied to time series prediction tasks, where the goal is to make predictions about future values based on historical data. In this approach, an agent interacts with an environment representing the time series data. The agent receives observations of past values and takes actions to predict future values. The agent’s actions involve adjusting its internal model or parameters to make predictions. It uses reinforcement learning algorithms to learn from past experiences and improve its prediction accuracy over time.</p>&#13;
&#13;
<p>The agent receives rewards or penalties based on the accuracy of its predictions. Rewards can be designed to reflect the prediction error or the utility of the predictions for the specific application. Through a process of trial and error, the agent learns to associate certain patterns or features in the time series data with future outcomes. It learns to make predictions that maximize rewards and minimize errors.</p>&#13;
&#13;
<p class="fix_tracking">The reinforcement learning process involves a balance between exploration and exploitation. The agent explores different prediction strategies, trying to discover patterns and make accurate predictions. It also exploits its existing knowledge to make predictions based on what it has learned so far. The goal of reinforcement learning for time series prediction is to train the agent to make accurate and reliable predictions. By continually receiving feedback and updating its prediction strategies, the agent adapts to changing patterns in the time series and improves its forecasting abilities.<a contenteditable="false" data-primary="" data-startref="Chapter_10.html0" data-type="indexterm" id="id790"/></p>&#13;
&#13;
<p><a data-type="xref" href="ch11.html#ch11">Chapter 11</a> will show how to employ more deep learning techniques and applications.</p>&#13;
</div></section>&#13;
<div data-type="footnotes"><p data-type="footnote" id="id771"><sup><a href="ch10.html#id771-marker">1</a></sup> A buy-and-hold strategy is a passive action whereby the trader or the algorithm initiates one buy order and holds it for a long time in an attempt to replicate the market’s return and minimize transaction costs incurred from excessive trading.</p><p data-type="footnote" id="id778"><sup><a href="ch10.html#id778-marker">2</a></sup> Keep epsilon decay in mind as it will be used as a variable in the code later.</p></div></div></section></body></html>