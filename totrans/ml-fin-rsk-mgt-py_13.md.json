["```py\nIn [1]: from sklearn.datasets import fetch_california_housing ![1](assets/1.png)\n        import pandas as pd\n        import numpy as np\n        import matplotlib. pyplot as plt\n        import yfinance as yf\n        import datetime\n        import warnings\n        warnings.filterwarnings('ignore')\n\nIn [2]: X, y = fetch_california_housing(return_X_y=True) ![2](assets/2.png)\n\nIn [3]: import numpy as np\n        california_housing=np.column_stack([X, y]) ![3](assets/3.png)\n        california_housing_df=pd.DataFrame(california_housing)\n\nIn [4]: from ctgan import CTGANSynthesizer ![4](assets/4.png)\n\n        ctgan = CTGANSynthesizer(epochs=10) ![5](assets/5.png)\n        ctgan.fit(california_housing_df)\n        synt_sample = ctgan.sample(len(california_housing_df)) ![6](assets/6.png)\n```", "```py\nIn [5]: california_housing_df.describe()\n\nOut[5]:              0             1             2             3             4 \\\n   count  20640.000000  20640.000000  20640.000000  20640.000000  20640.000000\n   mean       3.870671     28.639486      5.429000      1.096675   1425.476744\n   std        1.899822     12.585558      2.474173      0.473911   1132.462122\n   min        0.499900      1.000000      0.846154      0.333333      3.000000\n   25%        2.563400     18.000000      4.440716      1.006079    787.000000\n   50%        3.534800     29.000000      5.229129      1.048780   1166.000000\n   75%        4.743250     37.000000      6.052381      1.099526   1725.000000\n   max       15.000100     52.000000    141.909091     34.066667  35682.000000\n\n                     5             6             7             8\n   count  20640.000000  20640.000000  20640.000000  20640.000000\n   mean       3.070655     35.631861   -119.569704      2.068558\n   std       10.386050      2.135952      2.003532      1.153956\n   min        0.692308     32.540000   -124.350000      0.149990\n   25%        2.429741     33.930000   -121.800000      1.196000\n   50%        2.818116     34.260000   -118.490000      1.797000\n   75%        3.282261     37.710000   -118.010000      2.647250\n   max     1243.333333     41.950000   -114.310000      5.000010\n\nIn [6]: synt_sample.describe()\nOut[6]:              0             1             2             3             4 \\\n   count  20640.000000  20640.000000  20640.000000  20640.000000  20640.000000\n   mean       4.819246     28.954316      6.191938      1.172562   2679.408170\n   std        3.023684     13.650675      2.237810      0.402990   2127.606868\n   min       -0.068225     -2.927976      0.877387     -0.144332   -468.985777\n   25%        2.627803     19.113346      4.779587      0.957408   1148.179104\n   50%        4.217247     29.798105      5.779768      1.062072   2021.181784\n   75%        6.254332     38.144114      7.058157      1.285233   3666.957652\n   max       19.815551     54.219486     15.639807      3.262196  12548.941245\n\n                     5             6             7             8\n   count  20640.000000  20640.000000  20640.000000  20640.000000\n   mean       3.388233     36.371957   -119.957959      2.584699\n   std        1.035668      2.411460      2.306550      1.305122\n   min        0.650323     32.234033   -125.836387      0.212203\n   25%        2.651633     34.081107   -122.010873      1.579294\n   50%        3.280092     36.677974   -119.606385      2.334144\n   75%        3.994524     38.023437   -118.080271      3.456931\n   max        7.026720     43.131795   -113.530352      5.395162\n\nIn [7]: from sdv.evaluation import evaluate ![1](assets/1.png)\n\n        evaluate(synt_sample, california_housing_df) ![2](assets/2.png)\nOut[7]: 0.4773175572768998\n\nIn [8]: from table_evaluator import TableEvaluator ![3](assets/3.png)\n\n        table_evaluator =  TableEvaluator(california_housing_df, synt_sample) ![4](assets/4.png)\n\n        table_evaluator.visual_evaluation() ![5](assets/5.png)\n```", "```py\nIn [9]: from sklearn.datasets import make_regression ![1](assets/1.png)\n        import matplotlib.pyplot as plt\n        from matplotlib import cm\n\nIn [10]: X, y = make_regression(n_samples=1000, n_features=3, noise=0.2,\n                                random_state=123) ![2](assets/2.png)\n\n         plt.scatter(X[:, 0], X[:, 1], alpha= 0.3, cmap='Greys', c=y)\n\nIn [11]: plt.figure(figsize=(18, 18))\n         k = 0\n\n         for i in range(0, 10):\n             X, y = make_regression(n_samples=100, n_features=3, noise=i,\n                                    random_state=123)\n             k+=1\n             plt.subplot(5, 2, k)\n             profit_margin_orange = np.asarray([20, 35, 40])\n             plt.scatter(X[:, 0], X[:, 1], alpha=0.3, cmap=cm.Greys, c=y)\n             plt.title('Synthetic Data with Different Noises: ' + str(i))\n         plt.show()\n```", "```py\nIn [12]: from sklearn.datasets import make_classification ![1](assets/1.png)\n\nIn [13]: plt.figure(figsize=(18, 18))\n         k = 0\n\n         for i in range(2, 6):\n             X, y = make_classification(n_samples=100,\n                                        n_features=4,\n                                        n_classes=i,\n                                        n_redundant=0,\n                                        n_informative=4,\n                                        random_state=123) ![2](assets/2.png)\n             k+=1\n             plt.subplot(2, 2, k)\n             plt.scatter(X[: ,0], X[:, 1], alpha=0.8, cmap='gray', c=y)\n             plt.title('Synthetic Data with Different Classes: ' + str(i))\n         plt.show()\n```", "```py\nIn [14]: from sklearn.datasets import make_blobs ![1](assets/1.png)\n\nIn [15]: X, y = make_blobs(n_samples=100, centers=2,\n                               n_features=2, random_state=0) ![2](assets/2.png)\n\nIn [16]: plt.figure(figsize=(18, 18))\n         k = 0\n         for i in range(2, 6):\n             X, y = make_blobs(n_samples=100, centers=i,\n                               n_features=2, random_state=0)\n             k += 1\n             plt.subplot(2, 2, k)\n             my_scatter_plot = plt.scatter(X[:, 0], X[:, 1],\n                                           alpha=0.3, cmap='gray', c=y)\n             plt.title('Synthetic Data with Different Clusters: ' + str(i))\n         plt.show()\n```", "```py\nIn [17]: ff = pd.read_csv('FF3.csv', skiprows=4)\n         ff = ff.rename(columns={'Unnamed: 0': 'Date'})\n         ff = ff.iloc[:-1]\n         ff.head()\nOut[17]:        Date  Mkt-RF   SMB   HML     RF\n         0  19260701    0.10 -0.24 -0.28  0.009\n         1  19260702    0.45 -0.32 -0.08  0.009\n         2  19260706    0.17  0.27 -0.35  0.009\n         3  19260707    0.09 -0.59  0.03  0.009\n         4  19260708    0.21 -0.36  0.15  0.009\n\nIn [18]: ff.info()\n         <class 'pandas.core.frame.DataFrame'>\n         RangeIndex: 24978 entries, 0 to 24977\n         Data columns (total 5 columns):\n          #   Column  Non-Null Count  Dtype\n         ---  ------  --------------  -----\n          0   Date    24978 non-null  object\n          1   Mkt-RF  24978 non-null  float64\n          2   SMB     24978 non-null  float64\n          3   HML     24978 non-null  float64\n          4   RF      24978 non-null  float64\n         dtypes: float64(4), object(1)\n         memory usage: 975.8+ KB\n\nIn [19]: ff['Date'] = pd.to_datetime(ff['Date'])\n         ff.set_index('Date', inplace=True)\n         ff_trim = ff.loc['2000-01-01':]\n\nIn [20]: ff_trim.head()\nOut[20]:             Mkt-RF   SMB   HML     RF\n         Date\n         2000-01-03   -0.71  0.61 -1.40  0.021\n         2000-01-04   -4.06  0.01  2.06  0.021\n         2000-01-05   -0.09  0.18  0.19  0.021\n         2000-01-06   -0.73 -0.42  1.27  0.021\n         2000-01-07    3.21 -0.49 -1.42  0.021\n```", "```py\nIn [21]: ticker = 'SPY'\n         start = datetime.datetime(2000, 1, 3)\n         end = datetime.datetime(2021, 4, 30)\n         SP_ETF = yf.download(ticker, start, end, interval='1d').Close\n         [*********************100%***********************]  1 of 1 completed\n\nIn [22]: ff_merge = pd.merge(ff_trim, SP_ETF, how='inner', on='Date')\n\nIn [23]: SP = pd.DataFrame()\n         SP['Close']= ff_merge['Close']\n\nIn [24]: SP['return'] = (SP['Close'] / SP['Close'].shift(1))-1![1](assets/1.png)\n```", "```py\nIn [25]: from hmmlearn import hmm\n\nIn [26]: hmm_model = hmm.GaussianHMM(n_components=3,\n                                     covariance_type=\"full\",\n                                     n_iter=100)\n\nIn [27]: hmm_model.fit(np.array(SP['return'].dropna()).reshape(-1, 1))![1](assets/1.png)\n         hmm_predict = hmm_model.predict(np.array(SP['return'].dropna())\n                                         .reshape(-1, 1))![2](assets/2.png)\n         df_hmm = pd.DataFrame(hmm_predict)\n\nIn [28]: ret_merged = pd.concat([df_hmm,SP['return'].dropna().reset_index()],\n                                axis=1)\n         ret_merged.drop('Date',axis=1, inplace=True)\n         ret_merged.rename(columns={0:'states'}, inplace=True)\n         ret_merged.dropna().head()\nOut[28]:    states    return\n         0       1 -0.039106\n         1       1  0.001789\n         2       1 -0.016071\n         3       1  0.058076\n         4       2  0.003431\n```", "```py\nIn [29]: ret_merged['states'].value_counts()\nOut[29]: 0    3014\n         2    2092\n         1     258\n         Name: states, dtype: int64\n\nIn [30]: state_means = []\n         state_std = []\n\n         for i in range(3):\n             state_means.append(ret_merged[ret_merged.states == i]['return']\n                                .mean())\n             state_std.append(ret_merged[ret_merged.states == i]['return']\n                              .std())\n         print('State Means are: {:.4f}'.format(state_means))\n         print('State Standard Deviations are: {:.4f}'.format(state_std))\n         State Means are: [0.0009956956923795376, -0.0018371952883552139, -0.\n         0005000714110860054]\n         State Standard Deviations are: [0.006006540155737148, 0.\n         03598912028897813, 0.01372712345328388]\n\nIn [31]: print(f'HMM means\\n {hmm_model.means_}')\n         print(f'HMM covariances\\n {hmm_model.covars_}')\n         print(f'HMM transition matrix\\n {hmm_model.transmat_}')\n         print(f'HMM initial probability\\n {hmm_model.startprob_}')\n         HMM means\n          [[ 0.00100365]\n          [-0.002317  ]\n          [-0.00036613]]\n         HMM covariances\n          [[[3.85162047e-05]]\n\n          [[1.26647594e-03]]\n\n          [[1.82565269e-04]]]\n         HMM transition matrix\n          [[9.80443302e-01 1.20922866e-06 1.95554886e-02]\n          [1.73050704e-08 9.51104459e-01 4.88955238e-02]\n          [2.67975578e-02 5.91734590e-03 9.67285096e-01]]\n         HMM initial probability\n          [0.00000000e+000 1.00000000e+000 2.98271922e-120]\n```", "```py\nIn [32]: sp_ret = SP['return'].dropna().values.reshape(-1,1)\n         n_components = np.arange(1, 10)\n         clusters = [hmm.GaussianHMM(n_components=n,\n                                     covariance_type=\"full\").fit(sp_ret)\n                    for n in n_components] ![1](assets/1.png)\n         plt.plot(n_components, [m.score(np.array(SP['return'].dropna())\\\n                                         .reshape(-1,1)) for m in clusters]) ![2](assets/2.png)\n         plt.title('Optimum Number of States')\n         plt.xlabel('n_components')\n         plt.ylabel('Log Likelihood')\nIn [33]: hmm_model = hmm.GaussianHMM(n_components=3,\n                                 covariance_type=\"full\",\n                                 random_state=123).fit(sp_ret)\n         hidden_states = hmm_model.predict(sp_ret)\n```", "```py\nIn [34]: from matplotlib.dates import YearLocator, MonthLocator\n         from matplotlib import cm\n\nIn [35]: df_sp_ret = SP['return'].dropna()\n\n         hmm_model = hmm.GaussianHMM(n_components=3,\n                                     covariance_type=\"full\",\n                                     random_state=123).fit(sp_ret)\n\n         hidden_states = hmm_model.predict(sp_ret)\n\n         fig, axs = plt.subplots(hmm_model.n_components, sharex=True,\n                                 sharey=True, figsize=(12, 9))\n         colors = cm.gray(np.linspace(0, 0.7, hmm_model.n_components))\n\n         for i, (ax, color) in enumerate(zip(axs, colors)):\n             mask = hidden_states == i\n             ax.plot_date(df_sp_ret.index.values[mask],\n                          df_sp_ret.values[mask],\n                          \".-\", c=color)\n             ax.set_title(\"Hidden state {}\".format(i + 1), fontsize=16)\n             ax.xaxis.set_minor_locator(MonthLocator())\n\n         plt.tight_layout()\n```", "```py\nIn [36]: ret_merged.groupby('states')['return'].mean()\nOut[36]: states\n         0    0.000996\n         1   -0.001837\n         2   -0.000500\n         Name: return, dtype: float64\n\nIn [37]: ff_merge['return'] = ff_merge['Close'].pct_change()\n         ff_merge.dropna(inplace=True)\n\nIn [38]: split = int(len(ff_merge) * 0.9)\n         train_ff= ff_merge.iloc[:split].dropna()\n         test_ff = ff_merge.iloc[split:].dropna()\n\nIn [39]: hmm_model = hmm.GaussianHMM(n_components=3,\n                                     covariance_type=\"full\",\n                                     n_iter=100, init_params=\"\")\n\nIn [40]: predictions = []\n\n         for i in range(len(test_ff)):\n             hmm_model.fit(train_ff)\n             adjustment = np.dot(hmm_model.transmat_, hmm_model.means_) ![1](assets/1.png)\n             predictions.append(test_ff.iloc[i] + adjustment[0])\n         predictions = pd.DataFrame(predictions)\n\nIn [41]: std_dev = predictions['return'].std()\n         sharpe = predictions['return'].mean() / std_dev\n         print('Sharpe ratio with HMM is {:.4f}'.format(sharpe))\nOut[41]: Sharpe ratio with HMM is 0.0981\n```", "```py\nIn [42]: import statsmodels.api as sm\n\nIn [43]: Y = train_ff['return']\n         X = train_ff[['Mkt-RF', 'SMB', 'HML']]\n\nIn [44]: model = sm.OLS(Y, X)\n         ff_ols = model.fit()\n         print(ff_ols.summary())\n\n                        OLS Regression Results\n================================================================================\nDep. Variable:             return   R-squared (uncentered):                0.962\nModel:                        OLS   Adj. R-squared (uncentered):           0.962\nMethod:             Least Squares   F-statistic:                       4.072e+04\nDate:            Tue, 30 Nov 2021   Prob (F-statistic):                     0.00\nTime:                    00:05:02   Log-Likelihood:                       22347.\nNo. Observations:            4827   AIC:                              -4.469e+04\nDf Residuals:                4824   BIC:                              -4.467e+04\nDf Model:                       3\nCovariance Type:        nonrobust\n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nMkt-RF         0.0098   2.82e-05    348.173      0.000       0.010       0.010\nSMB           -0.0017   5.71e-05    -29.005      0.000      -0.002      -0.002\nHML        -6.584e-05   5.21e-05     -1.264      0.206      -0.000    3.63e-05\n==============================================================================\nOmnibus:                     1326.960   Durbin-Watson:                   2.717\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            80241.345\nSkew:                           0.433   Prob(JB):                         0.00\nKurtosis:                      22.955   Cond. No.                         2.16\n==============================================================================\n\nNotes:\n[1] R² is computed without centering (uncentered) since the model does not\ncontain a constant.\n[2] Standard Errors assume that the covariance matrix of the errors is\ncorrectly specified.\n\nIn [45]: ff_pred = ff_ols.predict(test_ff[[\"Mkt-RF\", \"SMB\", \"HML\"]])\n         ff_pred.head()\nOut[45]: Date\n         2019-03-14   -0.000340\n         2019-03-15    0.005178\n         2019-03-18    0.004273\n         2019-03-19   -0.000194\n         2019-03-20   -0.003795\n         dtype: float64\n\nIn [46]: std_dev = ff_pred.std()\n         sharpe = ff_pred.mean() / std_dev\n         print('Sharpe ratio with FF 3 factor model is {:.4f}'.format(sharpe))\nOut[46]: Sharpe ratio with FF 3 factor model is 0.0589\n```", "```py\nIn [47]: split = int(len(SP['return']) * 0.9)\n         train_ret_SP = SP['return'].iloc[split:].dropna()\n         test_ret_SP = SP['return'].iloc[:split].dropna()\n\nIn [48]: hmm_model = hmm.GaussianHMM(n_components=3,\n                                     covariance_type=\"full\",\n                                     n_iter=100)\n         hmm_model.fit(np.array(train_ret_SP).reshape(-1, 1))\n         hmm_predict_vol = hmm_model.predict(np.array(test_ret_SP)\n                                             .reshape(-1, 1))\n         pd.DataFrame(hmm_predict_vol).value_counts()\nOut[48]: 0    4447\n         1     282\n         2      98\n         dtype: int64\n```", "```py\nIn [49]: startprob = hmm_model.startprob_\n         transmat = hmm_model.transmat_\n         means = hmm_model.means_\n         covars = hmm_model.covars_\n\nIn [50]: syn_hmm = hmm.GaussianHMM(n_components=3, covariance_type=\"full\")\n\nIn [51]: syn_hmm.startprob_ = startprob\n         syn_hmm.transmat_ = transmat\n         syn_hmm.means_ = means\n         syn_hmm.covars_ = covars\n\nIn [52]: syn_data, _ = syn_hmm.sample(n_samples=1000)\n\nIn [53]: plt.hist(syn_data)\n         plt.show()\nIn [54]: plt.plot(syn_data, \"--\")\n         plt.show()\n```"]