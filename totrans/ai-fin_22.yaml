- en: Appendix B. Neural Network Classes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building on the foundations from [Appendix A](app01.xhtml#app_interactive_neural_networks),
    this appendix provides simple, class-based implementations of neural networks
    that mimic the APIs of packages such as `scikit-learn`. The implementation is
    based on pure, simple Python code and is for illustration and instruction. The
    classes presented in this appendix cannot replace robust, efficient, and scalable
    implementations found in the standard Python packages, such as `scikit-learn`
    or `TensorFlow` in combination with `Keras`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The appendix comprises the following sections:'
  prefs: []
  type: TYPE_NORMAL
- en: '[“Activation Functions”](#app_nnc_activation) introduces a Python function
    with different activation functions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[“Simple Neural Networks”](#app_nnc_sinn) presents a Python class for *simple
    neural networks*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[“Shallow Neural Networks”](#app_nnc_shnn) presents a Python class for *shallow
    neural networks*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[“Predicting Market Direction”](#app_nnc_fin) applies the class for shallow
    neural networks to financial data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The implementations and examples in this appendix are simple and straightforward.
    The Python classes are not well suited to attack larger estimation or classification
    problems. The idea is rather to show easy-to-understand Python implementations
    from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Activation Functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Appendix A](app01.xhtml#app_interactive_neural_networks) uses two activation
    functions implicitly or explicitly: linear function and sigmoid function. The
    Python function `activation` adds the `relu` (rectified linear unit) and `softplus`
    functions to the set of options. For all these activation functions, the first
    derivative is also defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Simple Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section presents a class for *simple neural networks* that has an API
    similar to those of models from standard Python packages for machine or deep learning
    (in particular, `scikit-learn` and `Keras`). Consider the class `sinn` as presented
    in the following Python code. It implements a simple neural network and defines
    the two main methods `.fit()` and `.predict()`. The `.metrics()` method calculates
    typical performance metrics: the mean-squared error (MSE) for estimation and the
    accuracy for classification. The class also implements two methods for the forward
    and backward propagation steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Estimation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First is an estimation problem that can be solved by the use of regression
    techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_neural_network_classes_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Exact solution by regression
  prefs: []
  type: TYPE_NORMAL
- en: 'Applying the `sinn` class to the estimation problem requires quite some effort
    in the form of repeated learning steps. However, by increasing the number of steps,
    one can make the estimate arbitrarily precise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_neural_network_classes_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Residual errors of the neural network estimation
  prefs: []
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Second is a classification problem that can also be attacked with the `sinn`
    class. Here, standard regression techniques are in general of no use. For the
    particular set of random features and labels, the `sinn` model reaches an accuracy
    of 100%. Again, quite some effort is required in the form of repeated learning
    steps. [Figure B-1](#figure_nnc_01) shows how the prediction accuracy changes
    with the number of learning steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_neural_network_classes_CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The sigmoid function is used for activation
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_neural_network_classes_CO3-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Perfect accuracy on this particular data set
  prefs: []
  type: TYPE_NORMAL
- en: '![aiif 1601](Images/aiif_1601.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure B-1\. Prediction accuracy versus the number of learning steps
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Shallow Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section applies the class `shnn`, which implements *shallow neural networks*
    with one hidden layer, to estimation and classification problems. The class structure
    is along the lines of the `sinn` class from the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Estimation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Again, the estimation problem comes first. For 5 features and 10 samples, a
    perfect regression solution is unlikely to exist. As a result, the MSE value of
    the regression is relatively high:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'However, the shallow neural network estimate based on the `shnn` class is quite
    good and shows a relatively low MSE value compared to the regression value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The classification example takes the estimation numbers and applies rounding
    to them. The shallow neural network converges quickly to predict the labels with
    100% accuracy (see [Figure B-2](#figure_nnc_02)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![aiif 1602](Images/aiif_1602.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure B-2\. Performance metrics for the shallow neural network (classification)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Predicting Market Direction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section applies the `shnn` class to predict the future direction of the
    EUR/USD exchange rate. The analysis is in-sample only to illustrate the application
    of `shnn` to real-world data. See [Chapter 10](ch10.xhtml#vectorized_backtesting)
    for the implementation of a more realistic setup for the vectorized backtesting
    of such prediction-based strategies.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following Python code imports the financial data—10 years’ worth of EOD
    data—and creates lagged, normalized log returns used as the features. The labels
    data is the direction of the price series as a binary data set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_neural_network_classes_CO4-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Market direction as the labels data
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_neural_network_classes_CO4-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Lagged log returns as the features data
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_neural_network_classes_CO4-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Gaussian normalization of the features data
  prefs: []
  type: TYPE_NORMAL
- en: 'With the data preprocessing accomplished, the application of the shallow neural
    network class `shnn` for a supervised classification is straightforward. [Figure B-3](#figure_nnc_03)
    shows that the prediction-based strategy in-sample significantly outperforms the
    passive benchmark investment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_neural_network_classes_CO5-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Derives the position values from the prediction values
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_neural_network_classes_CO5-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Calculates the strategy returns from the position values and the log returns
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_neural_network_classes_CO5-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Calculates the gross performance of the strategy and the benchmark investment
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_neural_network_classes_CO5-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Shows the gross performance of the strategy and the benchmark investment over
    time
  prefs: []
  type: TYPE_NORMAL
- en: '![aiif 1603](Images/aiif_1603.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure B-3\. Gross performance of prediction-based strategy compared to passive
    benchmark investment (in-sample)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
