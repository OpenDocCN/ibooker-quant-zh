["```py\nIn [1]: import os\n        import numpy as np\n        import pandas as pd\n        from pylab import plt, mpl\n        plt.style.use('seaborn')\n        mpl.rcParams['savefig.dpi'] = 300\n        mpl.rcParams['font.family'] = 'serif'\n        pd.set_option('precision', 4)\n        np.set_printoptions(suppress=True, precision=4)\n        os.environ['PYTHONHASHSEED'] = '0'\n\nIn [2]: url = 'http://hilpisch.com/aiif_eikon_id_eur_usd.csv'  ![1](Images/1.png)\n\nIn [3]: symbol = 'EUR_USD'\n\nIn [4]: raw = pd.read_csv(url, index_col=0, parse_dates=True)  ![1](Images/1.png)\n\nIn [5]: raw.head()\nOut[5]:                        HIGH     LOW    OPEN   CLOSE\n        Date\n        2019-10-01 00:00:00  1.0899  1.0897  1.0897  1.0899\n        2019-10-01 00:01:00  1.0899  1.0896  1.0899  1.0898\n        2019-10-01 00:02:00  1.0898  1.0896  1.0898  1.0896\n        2019-10-01 00:03:00  1.0898  1.0896  1.0897  1.0898\n        2019-10-01 00:04:00  1.0898  1.0896  1.0897  1.0898\n\nIn [6]: raw.info()\n        <class 'pandas.core.frame.DataFrame'>\n        DatetimeIndex: 96526 entries, 2019-10-01 00:00:00 to 2019-12-31 23:06:00\n        Data columns (total 4 columns):\n         #   Column  Non-Null Count  Dtype\n        ---  ------  --------------  -----\n         0   HIGH    96526 non-null  float64\n         1   LOW     96526 non-null  float64\n         2   OPEN    96526 non-null  float64\n         3   CLOSE   96526 non-null  float64\n        dtypes: float64(4)\n        memory usage: 3.7 MB\n\nIn [7]: data = pd.DataFrame(raw['CLOSE'].loc[:])  ![2](Images/2.png)\n        data.columns = [symbol]  ![2](Images/2.png)\n\nIn [8]: data = data.resample('1h', label='right').last().ffill()  ![2](Images/2.png)\n\nIn [9]: data.info()\n        <class 'pandas.core.frame.DataFrame'>\n        DatetimeIndex: 2208 entries, 2019-10-01 01:00:00 to 2020-01-01 00:00:00\n        Freq: H\n        Data columns (total 1 columns):\n         #   Column   Non-Null Count  Dtype\n        ---  ------   --------------  -----\n         0   EUR_USD  2208 non-null   float64\n        dtypes: float64(1)\n        memory usage: 34.5 KB\n\nIn [10]: data.plot(figsize=(10, 6));  ![2](Images/2.png)\n```", "```py\nIn [11]: lags = 5\n\nIn [12]: def add_lags(data, symbol, lags, window=20):  ![1](Images/1.png)\n             cols = []\n             df = data.copy()\n             df.dropna(inplace=True)\n             df['r'] = np.log(df / df.shift())\n             df['sma'] = df[symbol].rolling(window).mean()\n             df['min'] = df[symbol].rolling(window).min()\n             df['max'] = df[symbol].rolling(window).max()\n             df['mom'] = df['r'].rolling(window).mean()\n             df['vol'] = df['r'].rolling(window).std()\n             df.dropna(inplace=True)\n             df['d'] = np.where(df['r'] > 0, 1, 0)\n             features = [symbol, 'r', 'd', 'sma', 'min', 'max', 'mom', 'vol']\n             for f in features:\n                 for lag in range(1, lags + 1):\n                     col = f'{f}_lag_{lag}'\n                     df[col] = df[f].shift(lag)\n                     cols.append(col)\n             df.dropna(inplace=True)\n             return df, cols\n\nIn [13]: data, cols = add_lags(data, symbol, lags)\n```", "```py\nIn [14]: len(data)\nOut[14]: 2183\n\nIn [15]: c = data['d'].value_counts()  ![1](Images/1.png)\n         c  ![1](Images/1.png)\nOut[15]: 0    1445\n         1     738\n         Name: d, dtype: int64\n\nIn [16]: def cw(df):  ![2](Images/2.png)\n             c0, c1 = np.bincount(df['d'])\n             w0 = (1 / c0) * (len(df)) / 2\n             w1 = (1 / c1) * (len(df)) / 2\n             return {0: w0, 1: w1}\n\nIn [17]: class_weight = cw(data)  ![2](Images/2.png)\n\nIn [18]: class_weight  ![2](Images/2.png)\nOut[18]: {0: 0.755363321799308, 1: 1.4789972899728998}\n\nIn [19]: class_weight[0] * c[0]  ![3](Images/3.png)\nOut[19]: 1091.5\n\nIn [20]: class_weight[1] * c[1]  ![3](Images/3.png)\nOut[20]: 1091.5\n```", "```py\nIn [21]: import random\n         import tensorflow as tf\n         from keras.layers import Dense\n         from keras.models import Sequential\n         from keras.optimizers import Adam\n         from sklearn.metrics import accuracy_score\n         Using TensorFlow backend.\n\nIn [22]: def set_seeds(seed=100):\n             random.seed(seed)  ![1](Images/1.png)\n             np.random.seed(seed)  ![2](Images/2.png)\n             tf.random.set_seed(seed)  ![3](Images/3.png)\n\nIn [23]: optimizer = Adam(lr=0.001)  ![4](Images/4.png)\n\nIn [24]: def create_model(hl=1, hu=128, optimizer=optimizer):\n             model = Sequential()\n             model.add(Dense(hu, input_dim=len(cols),\n                             activation='relu'))  ![5](Images/5.png)\n             for _ in range(hl):\n                 model.add(Dense(hu, activation='relu'))  ![6](Images/6.png)\n             model.add(Dense(1, activation='sigmoid'))  ![7](Images/7.png)\n             model.compile(loss='binary_crossentropy',  ![8](Images/8.png)\n                           optimizer=optimizer,  ![9](Images/9.png)\n                           metrics=['accuracy'])  ![10](Images/10.png)\n             return model\n\nIn [25]: set_seeds()\n         model = create_model(hl=1, hu=128)\n\nIn [26]: %%time\n         model.fit(data[cols], data['d'], epochs=50,\n                   verbose=False, class_weight=cw(data))\n         CPU times: user 6.44 s, sys: 939 ms, total: 7.38 s\n         Wall time: 4.07 s\n\nOut[26]: <keras.callbacks.callbacks.History at 0x7fbfc2ee6690>\n\nIn [27]: model.evaluate(data[cols], data['d'])\n         2183/2183 [==============================] - 0s 24us/step\n\nOut[27]: [0.582192026280068, 0.6087952256202698]\n\nIn [28]: data['p'] = np.where(model.predict(data[cols]) > 0.5, 1, 0)\n\nIn [29]: data['p'].value_counts()\nOut[29]: 1    1340\n         0     843\n         Name: p, dtype: int64\n```", "```py\nIn [30]: split = int(len(data) * 0.8)  ![1](Images/1.png)\n\nIn [31]: train = data.iloc[:split].copy()  ![2](Images/2.png)\n\nIn [32]: test = data.iloc[split:].copy()  ![3](Images/3.png)\n\nIn [33]: set_seeds()\n         model = create_model(hl=1, hu=128)\n\nIn [34]: %%time\n         model.fit(train[cols], train['d'],\n                   epochs=50, verbose=False,\n                   validation_split=0.2, shuffle=False,\n                   class_weight=cw(train))\n         CPU times: user 4.72 s, sys: 686 ms, total: 5.41 s\n         Wall time: 3.14 s\n\nOut[34]: <keras.callbacks.callbacks.History at 0x7fbfc3231250>\n\nIn [35]: model.evaluate(train[cols], train['d'])  ![4](Images/4.png)\n         1746/1746 [==============================] - 0s 13us/step\n\nOut[35]: [0.612861613500842, 0.5853379368782043]\n\nIn [36]: model.evaluate(test[cols], test['d'])  ![5](Images/5.png)\n         437/437 [==============================] - 0s 16us/step\n\nOut[36]: [0.5946959675858714, 0.6247139573097229]\n\nIn [37]: test['p'] = np.where(model.predict(test[cols]) > 0.5, 1, 0)\n\nIn [38]: test['p'].value_counts()\nOut[38]: 1    291\n         0    146\n         Name: p, dtype: int64\n```", "```py\nIn [39]: res = pd.DataFrame(model.history.history)\n\nIn [40]: res[['accuracy', 'val_accuracy']].plot(figsize=(10, 6), style='--');\n```", "```py\nIn [41]: mu, std = train.mean(), train.std()  ![1](Images/1.png)\n\nIn [42]: train_ = (train - mu) / std  ![2](Images/2.png)\n\nIn [43]: set_seeds()\n         model = create_model(hl=2, hu=128)\n\nIn [44]: %%time\n         model.fit(train_[cols], train['d'],\n                   epochs=50, verbose=False,\n                   validation_split=0.2, shuffle=False,\n                   class_weight=cw(train))\n         CPU times: user 5.81 s, sys: 879 ms, total: 6.69 s\n         Wall time: 3.53 s\n\nOut[44]: <keras.callbacks.callbacks.History at 0x7fbfa51353d0>\n\nIn [45]: model.evaluate(train_[cols], train['d'])  ![3](Images/3.png)\n         1746/1746 [==============================] - 0s 14us/step\n\nOut[45]: [0.4253406366728084, 0.887170672416687]\n\nIn [46]: test_ = (test - mu) / std  ![4](Images/4.png)\n\nIn [47]: model.evaluate(test_[cols], test['d'])  ![5](Images/5.png)\n         437/437 [==============================] - 0s 24us/step\n\nOut[47]: [1.1377735263422917, 0.681922197341919]\n\nIn [48]: test['p'] = np.where(model.predict(test_[cols]) > 0.5, 1, 0)\n\nIn [49]: test['p'].value_counts()\nOut[49]: 0    281\n         1    156\n         Name: p, dtype: int64\n```", "```py\nIn [50]: res = pd.DataFrame(model.history.history)\n\nIn [51]: res[['accuracy', 'val_accuracy']].plot(figsize=(10, 6), style='--');\n```", "```py\nIn [52]: from keras.layers import Dropout\n\nIn [53]: def create_model(hl=1, hu=128, dropout=True, rate=0.3,\n                          optimizer=optimizer):\n             model = Sequential()\n             model.add(Dense(hu, input_dim=len(cols),\n                             activation='relu'))\n             if dropout:\n                 model.add(Dropout(rate, seed=100))  ![1](Images/1.png)\n             for _ in range(hl):\n                 model.add(Dense(hu, activation='relu'))\n                 if dropout:\n                     model.add(Dropout(rate, seed=100))  ![1](Images/1.png)\n             model.add(Dense(1, activation='sigmoid'))\n             model.compile(loss='binary_crossentropy', optimizer=optimizer,\n                          metrics=['accuracy'])\n             return model\n\nIn [54]: set_seeds()\n         model = create_model(hl=1, hu=128, rate=0.3)\n\nIn [55]: %%time\n         model.fit(train_[cols], train['d'],\n                   epochs=50, verbose=False,\n                   validation_split=0.15, shuffle=False,\n                   class_weight=cw(train))\n         CPU times: user 5.46 s, sys: 758 ms, total: 6.21 s\n         Wall time: 3.53 s\n\nOut[55]: <keras.callbacks.callbacks.History at 0x7fbfa6386550>\n\nIn [56]: model.evaluate(train_[cols], train['d'])\n         1746/1746 [==============================] - 0s 20us/step\n\nOut[56]: [0.4423361133190911, 0.7840778827667236]\n\nIn [57]: model.evaluate(test_[cols], test['d'])\n         437/437 [==============================] - 0s 34us/step\n\nOut[57]: [0.5875822428434883, 0.6430205702781677]\n```", "```py\nIn [58]: res = pd.DataFrame(model.history.history)\n\nIn [59]: res[['accuracy', 'val_accuracy']].plot(figsize=(10, 6), style='--');\n```", "```py\nIn [60]: from keras.regularizers import l1, l2\n\nIn [61]: def create_model(hl=1, hu=128, dropout=False, rate=0.3,\n                          regularize=False, reg=l1(0.0005),\n                          optimizer=optimizer, input_dim=len(cols)):\n             if not regularize:\n                 reg = None\n             model = Sequential()\n             model.add(Dense(hu, input_dim=input_dim,\n                             activity_regularizer=reg,  ![1](Images/1.png)\n                             activation='relu'))\n             if dropout:\n                 model.add(Dropout(rate, seed=100))\n             for _ in range(hl):\n                 model.add(Dense(hu, activation='relu',\n                                 activity_regularizer=reg))  ![1](Images/1.png)\n                 if dropout:\n                     model.add(Dropout(rate, seed=100))\n             model.add(Dense(1, activation='sigmoid'))\n             model.compile(loss='binary_crossentropy', optimizer=optimizer,\n                          metrics=['accuracy'])\n             return model\n\nIn [62]: set_seeds()\n         model = create_model(hl=1, hu=128, regularize=True)\n\nIn [63]: %%time\n         model.fit(train_[cols], train['d'],\n                   epochs=50, verbose=False,\n                   validation_split=0.2, shuffle=False,\n                   class_weight=cw(train))\n         CPU times: user 5.49 s, sys: 1.05 s, total: 6.54 s\n         Wall time: 3.15 s\n\nOut[63]: <keras.callbacks.callbacks.History at 0x7fbfa6b8e110>\n\nIn [64]: model.evaluate(train_[cols], train['d'])\n         1746/1746 [==============================] - 0s 15us/step\n\nOut[64]: [0.5307255412568205, 0.7691867351531982]\n\nIn [65]: model.evaluate(test_[cols], test['d'])\n         437/437 [==============================] - 0s 22us/step\n\nOut[65]: [0.8428352184644826, 0.6590389013290405]\n```", "```py\nIn [66]: res = pd.DataFrame(model.history.history)\n\nIn [67]: res[['accuracy', 'val_accuracy']].plot(figsize=(10, 6), style='--');\n```", "```py\nIn [68]: set_seeds()\n         model = create_model(hl=2, hu=128,\n                              dropout=True, rate=0.3,  ![1](Images/1.png)\n                              regularize=True, reg=l2(0.001),  ![2](Images/2.png)\n                             )\n\nIn [69]: %%time\n         model.fit(train_[cols], train['d'],\n                   epochs=50, verbose=False,\n                   validation_split=0.2, shuffle=False,\n                   class_weight=cw(train))\n         CPU times: user 7.06 s, sys: 958 ms, total: 8.01 s\n         Wall time: 4.28 s\n\nOut[69]: <keras.callbacks.callbacks.History at 0x7fbfa701cb50>\n\nIn [70]: model.evaluate(train_[cols], train['d'])\n         1746/1746 [==============================] - 0s 18us/step\n\nOut[70]: [0.5007762827004764, 0.7691867351531982]\n\nIn [71]: model.evaluate(test_[cols], test['d'])\n         437/437 [==============================] - 0s 23us/step\n\nOut[71]: [0.6191965124699835, 0.6864988803863525]\n```", "```py\nIn [72]: res = pd.DataFrame(model.history.history)\n\nIn [73]: res[['accuracy', 'val_accuracy']].plot(figsize=(10, 6), style='--');\n```", "```py\nIn [75]: from sklearn.ensemble import BaggingClassifier\n         from keras.wrappers.scikit_learn import KerasClassifier\n\nIn [76]: max_features = 0.75\n\nIn [77]: set_seeds()\n         base_estimator = KerasClassifier(build_fn=create_model,\n                                 verbose=False, epochs=20, hl=1, hu=128,\n                                 dropout=True, regularize=False,\n                                 input_dim=int(len(cols) * max_features))  ![1](Images/1.png)\n\nIn [78]: model_bag = BaggingClassifier(base_estimator=base_estimator,\n                                   n_estimators=15,\n                                   max_samples=0.75,\n                                   max_features=max_features,\n                                   bootstrap=True,\n                                   bootstrap_features=True,\n                                   n_jobs=1,\n                                   random_state=100,\n                                  )  ![2](Images/2.png)\n\nIn [79]: %time model_bag.fit(train_[cols], train['d'])\n         CPU times: user 40 s, sys: 5.23 s, total: 45.3 s\n         Wall time: 26.3 s\n\nOut[79]: BaggingClassifier(base_estimator=<keras.wrappers.scikit_learn.KerasClassifier\n          object at 0x7fbfa7cc7b90>,\n         bootstrap_features=True, max_features=0.75, max_samples=0.75,\n                           n_estimators=15, n_jobs=1, random_state=100)\n\nIn [80]: model_bag.score(train_[cols], train['d'])\nOut[80]: 0.720504009163803\n\nIn [81]: model_bag.score(test_[cols], test['d'])\nOut[81]: 0.6704805491990846\n\nIn [82]: test['p'] = model_bag.predict(test_[cols])\n\nIn [83]: test['p'].value_counts()\nOut[83]: 0    408\n         1     29\n         Name: p, dtype: int64\n```", "```py\nIn [84]: import time\n\nIn [85]: optimizers = ['sgd', 'rmsprop', 'adagrad', 'adadelta',\n                       'adam', 'adamax', 'nadam']\n\nIn [86]: %%time\n         for optimizer in optimizers:\n             set_seeds()\n             model = create_model(hl=1, hu=128,\n                              dropout=True, rate=0.3,\n                              regularize=False, reg=l2(0.001),\n                              optimizer=optimizer\n                             )  ![1](Images/1.png)\n             t0 = time.time()\n             model.fit(train_[cols], train['d'],\n                       epochs=50, verbose=False,\n                       validation_split=0.2, shuffle=False,\n                       class_weight=cw(train))  ![2](Images/2.png)\n             t1 = time.time()\n             t = t1 - t0\n             acc_tr = model.evaluate(train_[cols], train['d'], verbose=False)[1]  ![3](Images/3.png)\n             acc_te = model.evaluate(test_[cols], test['d'], verbose=False)[1]  ![4](Images/4.png)\n             out = f'{optimizer:10s} | time[s]: {t:.4f} | in-sample={acc_tr:.4f}'\n             out += f' | out-of-sample={acc_te:.4f}'\n             print(out)\n         sgd        | time[s]: 2.8092 | in-sample=0.6363 | out-of-sample=0.6568\n         rmsprop    | time[s]: 2.9480 | in-sample=0.7600 | out-of-sample=0.6613\n         adagrad    | time[s]: 2.8472 | in-sample=0.6747 | out-of-sample=0.6499\n         adadelta   | time[s]: 3.2068 | in-sample=0.7279 | out-of-sample=0.6522\n         adam       | time[s]: 3.2364 | in-sample=0.7365 | out-of-sample=0.6545\n         adamax     | time[s]: 3.2465 | in-sample=0.6982 | out-of-sample=0.6476\n         nadam      | time[s]: 4.1275 | in-sample=0.7944 | out-of-sample=0.6590\n         CPU times: user 35.9 s, sys: 4.55 s, total: 40.4 s\n         Wall time: 23.1 s\n```"]