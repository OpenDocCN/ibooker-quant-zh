["```py\nIn [1]: import pandas as pd\n        import numpy as np\n        import matplotlib.pyplot as plt\n        import warnings\n        warnings.filterwarnings(\"ignore\")\n        plt.rcParams['figure.figsize'] = (10, 6)\n        pd.set_option('use_inf_as_na', True)\n\nIn [2]: liq_data = pd.read_csv('bid_ask.csv')\n\nIn [3]: liq_data.head()\n\nOut[3]: Unnamed: 0        Date  EXCHCD TICKER      COMNAM  BIDLO   ASKHI    PRC\n         \\\n        0     1031570  2019-01-02     3.0   INTC  INTEL CORP  45.77  47.470\n         47.08\n        1     1031571  2019-01-03     3.0   INTC  INTEL CORP  44.39  46.280\n         44.49\n        2     1031572  2019-01-04     3.0   INTC  INTEL CORP  45.54  47.570\n         47.22\n        3     1031573  2019-01-07     3.0   INTC  INTEL CORP  46.75  47.995\n         47.44\n        4     1031574  2019-01-08     3.0   INTC  INTEL CORP  46.78  48.030\n         47.74\n\n                  VOL       RET     SHROUT  OPENPRC    vwretx\n\n        0  18761673.0  0.003196  4564000.0   45.960  0.001783\n\n        1  32254097.0 -0.055013  4564000.0   46.150 -0.021219\n\n        2  35419836.0  0.061362  4564000.0   45.835  0.033399\n\n        3  22724997.0  0.004659  4564000.0   47.100  0.009191\n\n        4  22721240.0  0.006324  4564000.0   47.800  0.010240\n```", "```py\nIn [4]: rolling_five = []\n\n        for j in liq_data.TICKER.unique():\n            for i in range(len(liq_data[liq_data.TICKER == j])):\n                rolling_five.append(liq_data[i:i+5].agg({'BIDLO': 'min',\n                                                        'ASKHI': 'max',\n                                                         'VOL': 'sum',\n                                                         'SHROUT': 'mean',\n                                                         'PRC': 'mean'})) ![1](assets/1.png)\n\nIn [5]: rolling_five_df = pd.DataFrame(rolling_five)\n        rolling_five_df.columns = ['bidlo_min', 'askhi_max', 'vol_sum',\n                                   'shrout_mean', 'prc_mean']\n        liq_vol_all = pd.concat([liq_data,rolling_five_df], axis=1)\n\nIn [6]: liq_ratio = []\n\n        for j in liq_vol_all.TICKER.unique():\n            for i in range(len(liq_vol_all[liq_vol_all.TICKER == j])):\n                liq_ratio.append((liq_vol_all['PRC'][i+1:i+6] *\n                                  liq_vol_all['VOL'][i+1:i+6]).sum()/\n                                 (np.abs(liq_vol_all['PRC'][i+1:i+6].mean() -\n                                         liq_vol_all['PRC'][i:i+5].mean())))\n```", "```py\nIn [7]: Lhh = []\n\n        for j in liq_vol_all.TICKER.unique():\n            for i in range(len(liq_vol_all[liq_vol_all.TICKER == j])):\n                Lhh.append((liq_vol_all['PRC'][i:i+5].max() -\n                            liq_vol_all['PRC'][i:i+5].min()) /\n                           liq_vol_all['PRC'][i:i+5].min() /\n                           (liq_vol_all['VOL'][i:i+5].sum() /\n                            liq_vol_all['SHROUT'][i:i+5].mean() *\n                            liq_vol_all['PRC'][i:i+5].mean()))\n```", "```py\nIn [8]: turnover_ratio = []\n\n        for j in liq_vol_all.TICKER.unique():\n            for i in range(len(liq_vol_all[liq_vol_all.TICKER == j])):\n                turnover_ratio.append((1/liq_vol_all['VOL'].count()) *\n                                      (np.sum(liq_vol_all['VOL'][i:i+1]) /\n                                       np.sum(liq_vol_all['SHROUT'][i:i+1])))\n\nIn [9]: liq_vol_all['liq_ratio'] = pd.DataFrame(liq_ratio)\n        liq_vol_all['Lhh'] = pd.DataFrame(Lhh)\n        liq_vol_all['turnover_ratio'] = pd.DataFrame(turnover_ratio)\n```", "```py\nIn [10]: liq_vol_all['mid_price'] = (liq_vol_all.ASKHI + liq_vol_all.BIDLO) / 2\n         liq_vol_all['percent_quoted_ba'] = (liq_vol_all.ASKHI -\n                                             liq_vol_all.BIDLO) / \\\n                                             liq_vol_all.mid_price\n         liq_vol_all['percent_effective_ba'] = 2 * abs((liq_vol_all.PRC -\n                                                      liq_vol_all.mid_price)) / \\\n                                                      liq_vol_all.mid_price\n```", "```py\nIn [11]: liq_vol_all['price_diff'] = liq_vol_all.groupby('TICKER')['PRC']\\\n                                     .apply(lambda x:x.diff())\n         liq_vol_all.dropna(inplace=True)\n         roll = []\n\n         for j in liq_vol_all.TICKER.unique():\n              for i in range(len(liq_vol_all[liq_vol_all.TICKER == j])):\n                 roll_cov = np.cov(liq_vol_all['price_diff'][i:i+5],\n                                   liq_vol_all['price_diff'][i+1:i+6]) ![1](assets/1.png)\n                 if roll_cov[0,1] < 0: ![2](assets/2.png)\n                     roll.append(2 * np.sqrt(-roll_cov[0, 1]))\n                 else:\n                      roll.append(2 * np.sqrt(np.abs(roll_cov[0, 1]))) ![3](assets/3.png)\n```", "```py\nIn [12]: gamma = []\n\n         for j in liq_vol_all.TICKER.unique():\n             for i in range(len(liq_vol_all[liq_vol_all.TICKER == j])):\n                 gamma.append((max(liq_vol_all['ASKHI'].iloc[i+1],\n                                   liq_vol_all['ASKHI'].iloc[i]) -\n                               min(liq_vol_all['BIDLO'].iloc[i+1],\n                                   liq_vol_all['BIDLO'].iloc[i])) ** 2)\n                 gamma_array = np.array(gamma)\n\nIn [13]: beta = []\n\n         for j in liq_vol_all.TICKER.unique():\n             for i in range(len(liq_vol_all[liq_vol_all.TICKER == j])):\n                 beta.append((liq_vol_all['ASKHI'].iloc[i+1] -\n                              liq_vol_all['BIDLO'].iloc[i+1]) ** 2 +\n                             (liq_vol_all['ASKHI'].iloc[i] -\n                              liq_vol_all['BIDLO'].iloc[i]) ** 2)\n                 beta_array = np.array(beta)\n\nIn [14]: alpha = ((np.sqrt(2 * beta_array) - np.sqrt(beta_array)) /\n                (3 - (2 * np.sqrt(2)))) - np.sqrt(gamma_array /\n                                                  (3 - (2 * np.sqrt(2))))\n         CS_spread = (2 * np.exp(alpha - 1)) / (1 + np.exp(alpha))\n\nIn [15]: liq_vol_all = liq_vol_all.reset_index()\n         liq_vol_all['roll'] = pd.DataFrame(roll)\n         liq_vol_all['CS_spread'] = pd.DataFrame(CS_spread)\n```", "```py\nIn [16]: dvol = []\n\n         for j in liq_vol_all.TICKER.unique():\n             for i in range(len(liq_vol_all[liq_vol_all.TICKER == j])):\n                 dvol.append((liq_vol_all['PRC'][i:i+5] *\n                              liq_vol_all['VOL'][i:i+5]).sum())\n         liq_vol_all['dvol'] = pd.DataFrame(dvol)\n\nIn [17]: amihud = []\n\n         for j in liq_vol_all.TICKER.unique():\n             for i in range(len(liq_vol_all[liq_vol_all.TICKER == j])):\n                 amihud.append((1 / liq_vol_all['RET'].count()) *\n                               (np.sum(np.abs(liq_vol_all['RET'][i:i+1])) /\n                                       np.sum(liq_vol_all['dvol'][i:i+1])))\n```", "```py\nIn [18]: florackis = []\n\n         for j in liq_vol_all.TICKER.unique():\n             for i in range(len(liq_vol_all[liq_vol_all.TICKER == j])):\n                 florackis.append((1 / liq_vol_all['RET'].count()) *\n                                  (np.sum(np.abs(liq_vol_all['RET'][i:i+1]) /\n                                          liq_vol_all['turnover_ratio'][i:i+1])))\n```", "```py\nIn [19]: liq_vol_all['vol_diff_pct'] = liq_vol_all.groupby('TICKER')['VOL']\\\n                                       .apply(lambda x: x.diff()).pct_change() ![1](assets/1.png)\n         liq_vol_all['price_diff_pct'] = liq_vol_all.groupby('TICKER')['PRC']\\\n                                       .apply(lambda x: x.diff()).pct_change() ![2](assets/2.png)\n\nIn [20]: cet = []\n\n         for j in liq_vol_all.TICKER.unique():\n             for i in range(len(liq_vol_all[liq_vol_all.TICKER == j])):\n                 cet.append(np.sum(liq_vol_all['vol_diff_pct'][i:i+1])/\n                            np.sum(liq_vol_all['price_diff_pct'][i:i+1]))\n\nIn [21]: liq_vol_all['amihud'] = pd.DataFrame(amihud)\n         liq_vol_all['florackis'] = pd.DataFrame(florackis)\n         liq_vol_all['cet'] = pd.DataFrame(cet)\n```", "```py\nIn [22]: import statsmodels.api as sm\n\nIn [23]: liq_vol_all['VOL_pct_change'] = liq_vol_all.groupby('TICKER')['VOL']\\\n                                         .apply(lambda x: x.pct_change())\n         liq_vol_all.dropna(subset=['VOL_pct_change'], inplace=True)\n         liq_vol_all = liq_vol_all.reset_index()\n\nIn [24]: unsys_resid = []\n\n         for i in liq_vol_all.TICKER.unique():\n             X1 = liq_vol_all[liq_vol_all['TICKER'] == i]['vwretx'] ![1](assets/1.png)\n             y = liq_vol_all[liq_vol_all['TICKER'] == i]['RET'] ![2](assets/2.png)\n             ols = sm.OLS(y, X1).fit() ![3](assets/3.png)\n             unsys_resid.append(ols.resid) ![4](assets/4.png)\n```", "```py\nIn [25]: market_impact = {}\n\n         for i, j in zip(liq_vol_all.TICKER.unique(),\n                         range(len(liq_vol_all['TICKER'].unique()))):\n             X2 = liq_vol_all[liq_vol_all['TICKER'] == i]['VOL_pct_change'] ![1](assets/1.png)\n             ols = sm.OLS(unsys_resid[j] ** 2, X2).fit()\n             print('***' * 30)\n             print(f'OLS Result for {i}')\n             print(ols.summary())\n             market_impact[j] = ols.resid ![2](assets/2.png)\n********************************************************************************\nOLS Result for INTC\n                        OLS Regression Results\n================================================================================\nDep. Variable:                  y   R-squared (uncentered):                0.157\nModel:                        OLS   Adj. R-squared (uncentered)            0.154\nMethod:             Least Squares   F-statistic:                           46.31\nDate:            Thu, 02 Dec 2021   Prob (F-statistic):                 7.53e-11\nTime:                    15:33:38   Log-Likelihood:                       1444.9\nNo. Observations:             249   AIC:                                  -2888.\nDf Residuals:                 248   BIC:                                  -2884.\nDf Model:                       1\nCovariance Type:        nonrobust\n================================================================================\n                   coef    std err          t      P>|t|      [0.025      0.975]\n--------------------------------------------------------------------------------\nVOL_pct_change    0.0008     0.000      6.805      0.000       0.001       0.001\n==============================================================================\nOmnibus:                      373.849   Durbin-Watson:                   1.908\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            53506.774\nSkew:                           7.228   Prob(JB):                         0.00\nKurtosis:                      73.344   Cond. No.                         1.00\n==============================================================================\n\nNotes:\n[1] R² is computed without centering (uncentered) since the model does not\ncontain a constant.\n[2] Standard Errors assume that the covariance matrix of the errors is\ncorrectly specified.\n********************************************************************************\nOLS Result for MSFT\n                        OLS Regression Results\n================================================================================\nDep. Variable:                  y   R-squared (uncentered):                0.044\nModel:                        OLS   Adj. R-squared (uncentered):           0.040\nMethod:             Least Squares   F-statistic:                           11.45\nDate:            Thu, 02 Dec 2021   Prob (F-statistic):                 0.000833\nTime:                    15:33:38   Log-Likelihood:                       1851.0\nNo. Observations:             249   AIC:                                 -3700.\nDf Residuals:                 248   BIC:                                 -3696.\nDf Model:                       1\nCovariance Type:        nonrobust\n================================================================================\n                   coef    std err          t      P>|t|      [0.025      0.975]\n--------------------------------------------------------------------------------\nVOL_pct_change  9.641e-05  2.85e-05     3.383      0.001      4.03e-05     0.000\n==============================================================================\nOmnibus:                      285.769   Durbin-Watson:                   1.533\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            11207.666\nSkew:                           4.937   Prob(JB):                         0.00\nKurtosis:                      34.349   Cond. No.                         1.00\n==============================================================================\n\nNotes:\n[1] R² is computed without centering (uncentered) since the model does not\ncontain a constant.\n[2] Standard Errors assume that the covariance matrix of the errors is\ncorrectly specified.\n\n********************************************************************************\nOLS Result for IBM\n                         OLS Regression Results\n================================================================================\nDep. Variable:                  y   R-squared (uncentered):                0.134\nModel:                        OLS   Adj. R-squared (uncentered):           0.130\nMethod:             Least Squares   F-statistic:                           38.36\nDate:            Thu, 02 Dec 2021   Prob (F-statistic):                 2.43e-09\nTime:                    15:33:38   Log-Likelihood:                       1547.1\nNo. Observations:             249   AIC:                                  -3092.\nDf Residuals:                 248   BIC:                                  -3089.\nDf Model:                       1\nCovariance Type:        nonrobust\n================================================================================\n                  coef    std err          t      P>|t|      [0.025      0.975]\n--------------------------------------------------------------------------------\nVOL_pct_change  0.0005   7.43e-05      6.193      0.000       0.000       0.001\n==============================================================================\nOmnibus:                      446.818   Durbin-Watson:                   2.034\nProb(Omnibus):                  0.000   Jarque-Bera (JB):           156387.719\nSkew:                           9.835   Prob(JB):                         0.00\nKurtosis:                     124.188   Cond. No.                         1.00\n==============================================================================\n\nNotes:\n[1] R² is computed without centering (uncentered) since the model does not\ncontain a constant.\n[2] Standard Errors assume that the covariance matrix of the errors is\ncorrectly specified.\n```", "```py\nIn [26]: append1 = market_impact[0].append(market_impact[1])\n         liq_vol_all['market_impact'] = append1.append(market_impact[2]) ![1](assets/1.png)\n\nIn [27]: cols = ['vol_diff_pct', 'price_diff_pct', 'price_diff',\n                 'VOL_pct_change', 'dvol', 'mid_price']\n         liq_measures_all = liq_vol_all.drop(liq_vol_all[cols], axis=1)\\\n                            .iloc[:, -11:]\n         liq_measures_all.dropna(inplace=True)\n         liq_measures_all.describe().T\nOut[27]:                       count          mean           std           min \\\n         liq_ratio             738.0  7.368514e+10  2.569030e+11  8.065402e+08\n         Lhh                   738.0  3.340167e-05  5.371681e-05  3.966368e-06\n         turnover_ratio        738.0  6.491127e-03  2.842668e-03  1.916371e-03\n         percent_quoted_ba     738.0  1.565276e-02  7.562850e-03  3.779877e-03\n         percent_effective_ba  738.0  8.334177e-03  7.100304e-03  0.000000e+00\n         roll                  738.0  8.190794e-01  6.066821e-01  7.615773e-02\n         CS_spread             738.0  3.305464e-01  1.267434e-01  1.773438e-40\n         amihud                738.0  2.777021e-15  2.319450e-15  0.000000e+00\n         florackis             738.0  2.284291e-03  1.546181e-03  0.000000e+00\n         cet                   738.0 -1.113583e+00  3.333932e+01 -4.575246e+02\n         market_impact         738.0  8.614680e-05  5.087547e-04 -1.596135e-03\n\n                                   25%           50%           75%           max\n    liq_ratio             1.378496e+10  2.261858e+10  4.505784e+10  3.095986e+12\n    Lhh                   1.694354e-05  2.368095e-05  3.558960e-05  5.824148e-04\n    turnover_ratio        4.897990e-03  5.764112e-03  7.423111e-03  2.542853e-02\n    percent_quoted_ba     1.041887e-02  1.379992e-02  1.878123e-02  5.545110e-02\n    percent_effective_ba  3.032785e-03  6.851479e-03  1.152485e-02  4.656669e-02\n    roll                  4.574986e-01  6.975982e-01  1.011879e+00  4.178873e+00\n    CS_spread             2.444225e-01  3.609800e-01  4.188028e-01  5.877726e-01\n    amihud                1.117729e-15  2.220439e-15  3.766086e-15  1.320828e-14\n    florackis             1.059446e-03  2.013517e-03  3.324181e-03  7.869841e-03\n    cet                  -1.687807e-01  5.654237e-01  1.660166e+00  1.845917e+02\n    market_impact        -3.010645e-05  3.383862e-05  1.309451e-04  8.165527e-03\n```", "```py\nIn [28]: from sklearn.mixture import GaussianMixture\n         from sklearn.preprocessing import StandardScaler\n\nIn [29]: liq_measures_all2 = liq_measures_all.dropna()\n         scaled_liq = StandardScaler().fit_transform(liq_measures_all2)\n\nIn [30]: kwargs = dict(alpha=0.5, bins=50,  stacked=True)\n         plt.hist(liq_measures_all.loc[:, 'percent_quoted_ba'],\n                  **kwargs, label='TC-based')\n         plt.hist(liq_measures_all.loc[:, 'turnover_ratio'],\n                  **kwargs, label='Volume-based')\n         plt.hist(liq_measures_all.loc[:, 'market_impact'],\n                  **kwargs, label='Market-based')\n         plt.title('Multi Modality of the Liquidity Measures')\n         plt.legend()\n         plt.show()\n```", "```py\nIn [31]: n_components = np.arange(1, 10)\n         clusters = [GaussianMixture(n, covariance_type='spherical',\n                                     random_state=0).fit(scaled_liq)\n                   for n in n_components] ![1](assets/1.png)\n         plt.plot(n_components, [m.bic(scaled_liq) for m in clusters]) ![2](assets/2.png)\n         plt.title('Optimum Number of Components')\n         plt.xlabel('n_components')\n         plt.ylabel('BIC values')\n         plt.show()\n```", "```py\nIn [32]: def cluster_state(data, nstates):\n             gmm = GaussianMixture(n_components=nstates,\n                                   covariance_type='spherical',\n                                   init_params='kmeans') ![1](assets/1.png)\n             gmm_fit = gmm.fit(scaled_liq) ![2](assets/2.png)\n             labels = gmm_fit.predict(scaled_liq) ![3](assets/3.png)\n             state_probs = gmm.predict_proba(scaled_liq) ![4](assets/4.png)\n             state_probs_df = pd.DataFrame(state_probs,\n                                           columns=['state-1','state-2',\n                                                    'state-3'])\n             state_prob_means = [state_probs_df.iloc[:, i].mean()\n                                 for i in range(len(state_probs_df.columns))] ![5](assets/5.png)\n             if np.max(state_prob_means) == state_prob_means[0]:\n                 print('State-1 is likely to occur with a probability of {:4f}'\n                       .format(state_prob_means[0]))\n             elif np.max(state_prob_means) == state_prob_means[1]:\n                 print('State-2 is likely to occur with a probability of {:4f}'\n                       .format(state_prob_means[1]))\n             else:\n                 print('State-3 is likely to occur with a probability of {:4f}'\n                       .format(state_prob_means[2]))\n             return state_probs\n\nIn [33]: state_probs = cluster_state(scaled_liq, 3)\n         print(f'State probabilities are {state_probs.mean(axis=0)}')\n\n         State-3 is likely to occur with a probability of 0.550297\n         State probabilities are [0.06285593 0.38684657 0.5502975 ]\n```", "```py\nIn [34]: from sklearn.decomposition import PCA\n\nIn [35]: pca = PCA(n_components=11)\n         components = pca.fit_transform(scaled_liq)\n         plt.plot(pca.explained_variance_ratio_)\n         plt.title('Scree Plot')\n         plt.xlabel('Number of Components')\n         plt.ylabel('% of Explained Variance')\n         plt.show()\n```", "```py\nIn [36]: def gmm_pca(data, nstate):\n             pca = PCA(n_components=3)\n             components = pca.fit_transform(data)\n             mxtd = GaussianMixture(n_components=nstate,\n                                    covariance_type='spherical')\n             gmm = mxtd.fit(components)\n             labels = gmm.predict(components)\n             state_probs = gmm.predict_proba(components)\n             return state_probs,pca\n\nIn [37]: state_probs, pca = gmm_pca(scaled_liq, 3)\n         print(f'State probabilities are {state_probs.mean(axis=0)}')\n         State probabilities are [0.7329713  0.25076855 0.01626015]\n```", "```py\nIn [38]: def wpc():\n             state_probs_df = pd.DataFrame(state_probs,\n                                           columns=['state-1', 'state-2',\n                                                    'state-3'])\n             state_prob_means = [state_probs_df.iloc[:, i].mean()\n                                 for i in range(len(state_probs_df.columns))]\n             if np.max(state_prob_means) == state_prob_means[0]:\n                 print('State-1 is likely to occur with a probability of {:4f}'\n                       .format(state_prob_means[0]))\n             elif np.max(state_prob_means) == state_prob_means[1]:\n                 print('State-2 is likely to occur with a probability of {:4f}'\n                       .format(state_prob_means[1]))\n             else:\n                 print('State-3 is likely to occur with a probability of {:4f}'\n                       .format(state_prob_means[2]))\n         wpc()\n         State-1 is likely to occur with a probability of 0.732971\n```", "```py\nIn [39]: loadings = pca.components_.T * np.sqrt(pca.explained_variance_) ![1](assets/1.png)\n         loading_matrix = pd.DataFrame(loadings,\n                                       columns=['PC1', 'PC2', 'PC3'],\n                                       index=liq_measures_all.columns)\n         loading_matrix\nOut[39]:                            PC1       PC2       PC3\n         liq_ratio             0.116701 -0.115791 -0.196355\n         Lhh                  -0.211827  0.882007 -0.125890\n         turnover_ratio        0.601041 -0.006381  0.016222\n         percent_quoted_ba     0.713239  0.140103  0.551385\n         percent_effective_ba  0.641527  0.154973  0.526933\n         roll                 -0.070192  0.886080 -0.093126\n         CS_spread             0.013373 -0.299229 -0.092705\n         amihud                0.849614 -0.020623 -0.488324\n         florackis             0.710818  0.081948 -0.589693\n         cet                  -0.035736  0.101575  0.001595\n         market_impact         0.357031  0.095045  0.235266\n```", "```py\nIn [40]: from copulae.mixtures.gmc.gmc import GaussianMixtureCopula ![1](assets/1.png)\n\nIn [41]: _, dim = scaled_liq.shape\n         gmcm = GaussianMixtureCopula(n_clusters=3, ndim=dim) ![2](assets/2.png)\n\nIn [42]: gmcm_fit = gmcm.fit(scaled_liq,method='kmeans',\n                             criteria='GMCM', eps=0.0001) ![3](assets/3.png)\n         state_prob = gmcm_fit.params.prob\n         print(f'The state {np.argmax(state_prob) + 1} is likely to occur')\n         print(f'State probabilities based on GMCM are {state_prob}')\n         The state 2 is likely to occur\n         State probabilities based on GMCM are [0.3197832  0.34146341 0.\n         33875339]\n```"]