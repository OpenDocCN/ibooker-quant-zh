<html><head></head><body><section data-pdf-bookmark="Chapter 7. Machine Learning Models for Time Series Prediction" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch07">&#13;
<h1><span class="label">Chapter 7. </span>Machine Learning Models <span class="keep-together">for Time Series Prediction</span></h1>&#13;
&#13;
<p><a contenteditable="false" data-primary="machine learning (for time series prediction)" data-type="indexterm" id="Chapter_7.html0"/><a contenteditable="false" data-primary="time series prediction (machine learning for)" data-type="indexterm" id="Chapter_7.html1"/>Machine learning is a subfield of AI that focuses on the development of algorithms and models that enable computers to learn and make predictions or decisions without being explicitly programmed, hence the term <em>learning</em>. Machine learning deals with the design and construction of systems that can automatically learn and improve from experience, typically by analyzing and extracting patterns from large amounts of data.</p>&#13;
&#13;
<p>This chapter presents the framework of using machine learning models for time series prediction and discusses a selection of known algorithms.</p>&#13;
&#13;
<section data-pdf-bookmark="The Framework" data-type="sect1"><div class="sect1" id="id48">&#13;
<h1>The Framework</h1>&#13;
&#13;
<p class="fix_tracking"><a contenteditable="false" data-primary="machine learning (for time series prediction)" data-secondary="framework" data-type="indexterm" id="Chapter_7.html2"/><a contenteditable="false" data-primary="time series prediction (machine learning for)" data-secondary="framework" data-type="indexterm" id="Chapter_7.html3"/>The <em>framework </em>is very important, as it organizes the way the whole research process is done (from data collection to performance evaluation). Having a proper framework ensures harmony across the backtests, which allows for proper comparison among different machine learning models. The framework may follow these chronological steps:</p>&#13;
&#13;
<ol>&#13;
	<li>Import and preprocess the historical data, which must contain a sufficient number of values to ensure a decent backtest and evaluation.</li>&#13;
	<li>Perform a <em>train-test</em> split, which splits the data into two parts where the first part of the data (e.g., from 2000 to 2020) is reserved for training the algorithm so that it understands the mathematical formula to predict the future values, and the second part of the data (e.g., from 2020 to 2023) is reserved for testing the algorithm’s performance on data that it has never seen before.</li>&#13;
	<li class="pagebreak-before less_space">Fit (train) and predict (test) the data using the algorithm.</li>&#13;
	<li>Run a performance evaluation algorithm to understand the model’s performance in the past.</li>&#13;
</ol>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>The <em>training set </em>is also called the <em>in-sample data</em>, and the <em>test set </em>is also called the <em>out-of-sample data</em>.</p>&#13;
</div>&#13;
&#13;
<p>The first step of the framework was discussed in <a data-type="xref" href="ch06.html#ch06">Chapter 6</a>. You should now be able to easily import historical data using Python. The train-test split divides the historical data into a training (in-sample) set where the model is fitted (trained) so that an implied forecasting function is found, and a test (out-of-sample) set where the forecasting function that has been calculated on the training set is applied and evaluated. Theoretically, if the model does well on the test set, it is likely that you have a potential candidate for a trading strategy, but this is just a first step and the reality is much more complicated than that.</p>&#13;
&#13;
<p>So that everything goes smoothly, download <em>master_function.py</em> from the <a href="https://oreil.ly/5YGHI">GitHub repository</a>, and then set the directory of the Python interpreter (e.g., Spyder) in the same location as the downloaded file so that you can import it as a library and use its functions. For example, if you download the file to your desktop, you may want to set the directory as shown in <a data-type="xref" href="ch06.html#figure-6-5">Figure 6-5</a> in <a data-type="xref" href="ch06.html#ch06">Chapter 6</a>. The choice of the directory is typically found on the top-right corner in Spyder (above the variable explorer).</p>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>If you prefer not to import <em>master_function.py</em>, you can just open it in the interpreter as a normal file and execute it so that Python defines the functions inside. However, you have to do this every time you restart the kernel.</p>&#13;
</div>&#13;
&#13;
<p>Now, preprocess (transform) and split the time series into four different arrays (or dataframes if you wish), with each array having a utility:</p>&#13;
&#13;
<dl>&#13;
	<dt>Array <code>x_train</code></dt><dd><p>The in-sample set of features (i.e., independent variables) that explain the variations of the variable that you want to forecast. They are the predictors.</p></dd>&#13;
	<dt>Array <code>y_train</code></dt><dd><p>The in-sample set of dependent variables (i.e., the right answers) that you want the model to calibrate its forecasting function on.</p></dd>&#13;
	<dt>Array <code>x_test</code></dt><dd><p>The out-of-sample set of features that will be used as a test of the model to see how it performs on this never-before-seen data.</p></dd>&#13;
	<dt>Array <code>y_test</code></dt><dd><p>Contains the real values that the model must approach. In other words, these are the right answers that will be compared with the model’s forecasts.</p></dd>&#13;
</dl>&#13;
&#13;
<p>Before the split, it is important to know what is being forecasted and what is being used to forecast it. In this chapter, lagged price differences (returns) will be used for the forecast. Normally, a few tests must be made before doing this, but for simplicity, let’s leave them out and suppose that the last 500 daily EURUSD returns have predictive power over the current return, which means that you can find a predictive formula that uses the last 500 observations to observe the next one:</p>&#13;
&#13;
<dl>&#13;
	<dt>Dependent variable (forecast)</dt><dd><p>The t+1 return of the EURUSD in the daily time frame. This is also referred to as the <em>y</em> variable.</p></dd>&#13;
	<dt>Independent variables (inputs)</dt><dd><p>The last 500 daily returns of the EURUSD. These are also referred to as the <em>x</em> variables.</p></dd>&#13;
</dl>&#13;
&#13;
<p><a data-type="xref" href="#figure-7-1">Figure 7-1</a> shows the EURUSD daily returns over a certain time period. Notice its stationary appearance. According to the ADF test (seen in <a data-type="xref" href="ch03.html#ch03">Chapter 3</a>), the returns dataset seems stationary and is valid for a regression analysis.</p>&#13;
&#13;
<figure><div class="figure" id="figure-7-1"><img alt="" class="iimagesdlf_0701png" src="assets/dlff_0701.png"/>&#13;
<h6><span class="label">Figure 7-1. </span>The EURUSD daily returns.</h6>&#13;
</div></figure>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>In this chapter, the features (<em>x</em> values) will be the lagged daily price differences of the EURUSD.<sup><a data-type="noteref" href="ch07.html#id600" id="id600-marker">1</a></sup> In subsequent chapters, the features used will be either lagged returns or values of technical indicators. Note that you can use whichever features you think are worthy of being considered as predictive.</p>&#13;
&#13;
<p>The choice of the time frame (daily) is ideal for traders who want an intraday view that will help them trade the market and close the position before the end of the day.</p>&#13;
</div>&#13;
&#13;
<p><a contenteditable="false" data-primary="dummy regression" data-type="indexterm" id="Chapter_7.html4"/>Let’s use the dummy regression model as a first basic example. <em>Dummy regression</em> is a comparison machine learning algorithm that is only used as a benchmark, as it uses very simple rules for predictions that are unlikely to add any real forecasting value. The real utility of the dummy regression is to see whether your real model outperforms it or not. As a reminder, the process followed by the machine learning algorithms is composed of the following steps:</p>&#13;
&#13;
<ol>&#13;
	<li>Import the data.</li>&#13;
	<li>Preprocess and split the data.</li>&#13;
	<li>Train the algorithm.</li>&#13;
	<li>Predict on test data using the training parameters. Also, predict on training data for comparison.</li>&#13;
	<li>Plot and evaluate the results.</li>&#13;
</ol>&#13;
&#13;
<p>Start by importing the libraries required for this chapter:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="kn">import</code> <code class="nn">pandas</code> <code class="k">as</code> <code class="nn">pd</code>&#13;
<code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="k">as</code> <code class="nn">plt</code>&#13;
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>&#13;
<code class="kn">from</code> <code class="nn">master_function</code> <code class="kn">import</code> <code class="n">data_preprocessing</code><code class="p">,</code> <code class="n">mass_import</code>&#13;
<code class="kn">from</code> <code class="nn">master_function</code> <code class="kn">import</code> <code class="n">plot_train_test_values</code><code class="p">,</code> &#13;
<code class="kn">from</code> <code class="nn">master_function</code> <code class="kn">import</code> <code class="n">calculate_accuracy</code><code class="p">,</code> <code class="n">model_bias</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">mean_squared_error</code></pre>&#13;
&#13;
<p>Now import the specific library for the algorithm you will use:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">sklearn.dummy</code> <code class="kn">import</code> <code class="n">DummyRegressor</code></pre>&#13;
&#13;
<p>The next step is to import and transform the close price data. Remember, you are trying to forecast daily returns, which means that you must select only the close column and then apply a differencing function on it so that prices become differenced:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="c1"># Importing the differenced close price of EURUSD daily time frame</code>&#13;
<code class="n">data</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">diff</code><code class="p">(</code><code class="n">mass_import</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="s1">'H1'</code><code class="p">)[:,</code> <code class="mi">3</code><code class="p">])</code></pre>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p><a contenteditable="false" data-primary="returns, defined" data-type="indexterm" id="id601"/>In finance, the term <em>returns </em>typically refers to the gain or loss generated by an investment or a certain asset, and it can be calculated by taking the difference between the current value of an asset and its value at a previous point in time. This is essentially a form of differencing, as you are calculating the change or difference in the asset’s value.</p>&#13;
&#13;
<p><a contenteditable="false" data-primary="differencing" data-type="indexterm" id="id602"/>In time series analysis, differencing is a common technique used to make time series data stationary, which can be helpful for various analyses. Differencing involves subtracting consecutive observations from each other to remove trends or seasonality, thereby focusing on the changes in the data.</p>&#13;
</div>&#13;
&#13;
<p class="fix_tracking">Next, set the hyperparameters of the algorithm. In the case of these basic algorithms, it would be the number of lags (number of predictors) and the percentage split of data:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="c1"># Setting the hyperparameters</code>&#13;
<code class="n">num_lags</code> <code class="o">=</code> <code class="mi">500</code>&#13;
<code class="n">train_test_split</code> <code class="o">=</code> <code class="mf">0.80</code></pre>&#13;
&#13;
<p>A <code>train_test_split</code> of 0.80 means that 80% of the data will be used for training while the remaining 20% will be used for testing.</p>&#13;
&#13;
<p>The function to split and define the four necessary arrays for the backtest can be defined as follows:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="k">def</code> <code class="nf">data_preprocessing</code><code class="p">(</code><code class="n">data</code><code class="p">,</code> <code class="n">num_lags</code><code class="p">,</code> <code class="n">train_test_split</code><code class="p">):</code>&#13;
    <code class="c1"># Prepare the data for training</code>&#13;
    <code class="n">x</code> <code class="o">=</code> <code class="p">[]</code>&#13;
    <code class="n">y</code> <code class="o">=</code> <code class="p">[]</code>&#13;
    <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">data</code><code class="p">)</code> <code class="err">–</code> <code class="n">num_lags</code><code class="p">):</code>&#13;
        <code class="n">x</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">data</code><code class="p">[</code><code class="n">i</code><code class="p">:</code><code class="n">i</code> <code class="o">+</code> <code class="n">num_lags</code><code class="p">])</code>&#13;
        <code class="n">y</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">data</code><code class="p">[</code><code class="n">i</code><code class="o">+</code> <code class="n">num_lags</code><code class="p">])</code>&#13;
    <code class="c1"># Convert the data to numpy arrays</code>&#13;
    <code class="n">x</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>&#13;
    <code class="n">y</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">y</code><code class="p">)</code>&#13;
    <code class="c1"># Split the data into training and testing sets</code>&#13;
    <code class="n">split_index</code> <code class="o">=</code> <code class="nb">int</code><code class="p">(</code><code class="n">train_test_split</code> <code class="o">*</code> <code class="nb">len</code><code class="p">(</code><code class="n">x</code><code class="p">))</code>&#13;
    <code class="n">x_train</code> <code class="o">=</code> <code class="n">x</code><code class="p">[:</code><code class="n">split_index</code><code class="p">]</code>&#13;
    <code class="n">y_train</code> <code class="o">=</code> <code class="n">y</code><code class="p">[:</code><code class="n">split_index</code><code class="p">]</code>&#13;
    <code class="n">x_test</code> <code class="o">=</code> <code class="n">x</code><code class="p">[</code><code class="n">split_index</code><code class="p">:]</code>&#13;
    <code class="n">y_test</code> <code class="o">=</code> <code class="n">y</code><code class="p">[</code><code class="n">split_index</code><code class="p">:]</code>&#13;
    <code class="k">return</code> <code class="n">x_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">x_test</code><code class="p">,</code> <code class="n">y_test</code>&#13;
</pre>&#13;
&#13;
<p class="pagebreak-before">Call the function to create the four arrays:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="c1"># Creating the training and test sets</code>&#13;
<code class="n">x_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">x_test</code><code class="p">,</code> <code class="n">y_test</code> <code class="o">=</code> <code class="n">data_preprocessing</code><code class="p">(</code><code class="n">data</code><code class="p">,</code> &#13;
                                                      <code class="n">num_lags</code><code class="p">,</code> &#13;
                                                      <code class="n">train_test_split</code><code class="p">)</code></pre>&#13;
&#13;
<p>You should now see four new arrays appearing in the variable explorer. The next step is to train the data using the chosen algorithm:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="c1"># Fitting the model</code>&#13;
<code class="n">model</code> <code class="o">=</code> <code class="n">DummyRegressor</code><code class="p">(</code><code class="n">strategy</code> <code class="o">=</code> <code class="s1">'mean'</code><code class="p">)</code>&#13;
<code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">x_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code></pre>&#13;
&#13;
<p>Note that the dummy regression can take any of the following strategies as <span class="keep-together">arguments:</span></p>&#13;
&#13;
<dl>&#13;
	<dt><code>mean</code></dt><dd><p>Always predicts the mean of the training set</p>&#13;
	</dd>&#13;
	<dt><code>median</code></dt><dd><p>Always predicts the median of the training set</p>&#13;
	</dd>&#13;
	<dt><code>quantile</code></dt><dd><p>Always predicts a specified quantile of the training set, provided with the quantile parameter</p>&#13;
	</dd>&#13;
	<dt><code>constant</code></dt><dd><p>Always predicts a constant value that is provided by the user</p>&#13;
	</dd>&#13;
</dl>&#13;
&#13;
<p>As you can see from the previous code, the selected parameter is <code>mean</code>. Naturally, this signifies that all the predictions made will simply be the mean of the training set (<code>y_train</code>). This is why dummy regression is only used as a benchmark and not as a serious machine learning model.</p>&#13;
&#13;
<p>The next step is to predict on the test data, as well as on the training data as a means of comparison. Note that the predictions on the training data have no value since the algorithm has already seen the data during training, but it is interesting to know how worse or better the algorithm performs on data that has never been seen before:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="c1"># Predicting in-sample</code>&#13;
<code class="n">y_predicted_train</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">x_train</code><code class="p">),</code> <code class="p">(</code><code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">))</code>&#13;
<code class="c1"># Predicting out-of-sample</code>&#13;
<code class="n">y_predicted</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">x_test</code><code class="p">),</code> <code class="p">(</code><code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">))</code>&#13;
</pre>&#13;
&#13;
<p>To make sure your reasoning is correct with regard to using the dummy regression algorithm, manually calculate the mean of <code>y_train</code> and compare it to the value you get in every <code>y_predicted</code>. You will see that it’s the same:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="c1"># Comparing the mean of y_train to an arbitrary value in y_predicted</code>&#13;
<code class="n">y_train</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code> <code class="o">==</code> <code class="n">y_predicted</code><code class="p">[</code><code class="mi">123</code><code class="p">]</code>&#13;
</pre>&#13;
&#13;
<p>The output should be as follows:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="kc">True</code></pre>&#13;
&#13;
<p>Finally, use the following function to plot the last training data followed by the first test data and the equivalent predicted data:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="c1"># Plotting</code>&#13;
<code class="n">plot_train_test_values</code><code class="p">(</code><code class="mi">100</code><code class="p">,</code> <code class="mi">50</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">y_test</code><code class="p">,</code> <code class="n">y_predicted</code><code class="p">)</code></pre>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>You can find the definition of the <code>plot_train_test_values()</code> function in this book’s <a href="https://oreil.ly/5YGHI">GitHub repository</a>.</p>&#13;
</div>&#13;
&#13;
<p><a data-type="xref" href="#figure-7-2">Figure 7-2</a> shows the evolution of the forecasting task from the last values of <code>y_train</code> to the first values of <code>y_test</code> and <code>y_predicted</code>. Naturally, the dummy regression algorithm predicts a constant value, which is why the prediction line alongside the test values is a straight line.<a contenteditable="false" data-primary="" data-startref="Chapter_7.html4" data-type="indexterm" id="id603"/></p>&#13;
&#13;
<figure><div class="figure" id="figure-7-2"><img alt="" class="iimagesdlf_0702png" src="assets/dlff_0702.png"/>&#13;
<h6><span class="label">Figure 7-2. </span>Training data followed by test data (dashed line) and the predicted data (thin line); the vertical dashed line represents the start of the test period. The model used is the dummy regression algorithm.</h6>&#13;
</div></figure>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>If you want the figures to be plotted in a separate window, type <code><strong>%matplotlib qt</strong></code> in the console. If you want the figures to be inside the plots explorer, type <code><strong>%matplotlib inline</strong></code> in the console.</p>&#13;
</div>&#13;
&#13;
<p>How can you tell whether a model is performing well or not? <em>Performance evaluation</em> is a key concept in trading and algorithmic development as it ensures that you pick the right model and take it live. However, the task is not simple, due to an ironically simple question: <em>If the past performance was good, what guarantees it continues to be good?</em></p>&#13;
&#13;
<p>This question is painful, but it points toward the right direction. The answer to this question is subjective. For now, let’s talk about the different ways to measure the performance of a model. To simplify the task, I will split the performance and evaluation metrics into two: model evaluation and trading evaluation. <a contenteditable="false" data-primary="model evaluation" data-type="indexterm" id="id604"/><a contenteditable="false" data-primary="trading evaluation" data-type="indexterm" id="id605"/><em>Model evaluation</em> deals with the algorithm’s performance in its forecasts, while <em>trading evaluation</em> deals with the financial performance of a system that trades using the algorithm (an example of a trading evaluation metric is the net profit).</p>&#13;
&#13;
<p>Let’s start with model evaluation. <em>Accuracy </em>is the first metric that comes to mind when comparing forecasts to real values, especially in the financial markets. Theoretically, if you predict the direction (up or down) and you get it right, you should make money (excluding transaction costs). Accuracy is also referred to as the <em>hit ratio</em> in financial jargon and is calculated as follows:</p>&#13;
&#13;
<div data-type="equation"><p><math alttext="Accuracy equals StartFraction Correct predictions Over Total predictions EndFraction times 100">&#13;
  <mrow>&#13;
    <mtext>Accuracy</mtext>&#13;
    <mo>=</mo>&#13;
    <mstyle displaystyle="false" scriptlevel="0">&#13;
      <mfrac><mrow><mtext>Correct</mtext><mspace width="4.pt"/><mtext>predictions</mtext></mrow> <mrow><mtext>Total</mtext><mspace width="4.pt"/><mtext>predictions</mtext></mrow></mfrac>&#13;
    </mstyle>&#13;
    <mo>×</mo>&#13;
    <mn>100</mn>&#13;
  </mrow>&#13;
</math></p></div>&#13;
&#13;
<p>For example, if you made 100 predictions last year and 73 of them were correct, you would have a 73% accuracy.</p>&#13;
&#13;
<p>Forecasting can also be evaluated by how close the predicted values (<code>y_predicted</code>) are to the real values (<code>y_test</code>). This is done by loss functions. <a contenteditable="false" data-primary="loss function" data-type="indexterm" id="id606"/>A <em>loss function</em> is a mathematical calculation that measures the difference between the predictions and the real (test) values. <a contenteditable="false" data-primary="mean absolute error (MAE)" data-type="indexterm" id="id607"/><a contenteditable="false" data-primary="MAE (mean absolute error)" data-type="indexterm" id="id608"/>The most basic loss function is the <em>mean absolute error</em> (MAE). It measures the average of the absolute differences between the predicted and actual values. The mathematical representation of MAE is as follows:</p>&#13;
&#13;
<div data-type="equation"><p><math alttext="upper M upper A upper E equals StartFraction sigma-summation Underscript i equals 1 Overscript n Endscripts StartAbsoluteValue ModifyingAbove y With caret minus y Subscript i Baseline EndAbsoluteValue Over n EndFraction">&#13;
  <mrow>&#13;
    <mi>M</mi>&#13;
    <mi>A</mi>&#13;
    <mi>E</mi>&#13;
    <mo>=</mo>&#13;
    <mstyle displaystyle="false" scriptlevel="0">&#13;
      <mfrac><mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi> </msubsup><mrow><mo>|</mo><mover accent="true"><mi>y</mi> <mo>^</mo></mover><mo>-</mo><msub><mi>y</mi> <mi>i</mi> </msub><mo>|</mo></mrow></mrow> <mi>n</mi></mfrac>&#13;
    </mstyle>&#13;
  </mrow>&#13;
</math></p>&#13;
&#13;
<p><math alttext="StartLayout 1st Row  ModifyingAbove y With caret is the predicted value 2nd Row  y is the real value EndLayout">&#13;
  <mtable>&#13;
    <mtr>&#13;
      <mtd columnalign="left">&#13;
        <mrow>&#13;
          <mover accent="true"><mi>y</mi> <mo>^</mo></mover>&#13;
          <mspace width="4.pt"/>&#13;
          <mtext>is</mtext>&#13;
          <mspace width="4.pt"/>&#13;
          <mtext>the</mtext>&#13;
          <mspace width="4.pt"/>&#13;
          <mtext>predicted</mtext>&#13;
          <mspace width="4.pt"/>&#13;
          <mtext>value</mtext>&#13;
        </mrow>&#13;
      </mtd>&#13;
    </mtr>&#13;
    <mtr>&#13;
      <mtd columnalign="left">&#13;
        <mrow>&#13;
          <mi>y</mi>&#13;
          <mspace width="4.pt"/>&#13;
          <mtext>is</mtext>&#13;
          <mspace width="4.pt"/>&#13;
          <mtext>the</mtext>&#13;
          <mspace width="4.pt"/>&#13;
          <mtext>real</mtext>&#13;
          <mspace width="4.pt"/>&#13;
          <mtext>value</mtext>&#13;
        </mrow>&#13;
      </mtd>&#13;
    </mtr>&#13;
  </mtable>&#13;
</math></p></div>&#13;
&#13;
<p>Therefore, MAE calculates the average distance (or positive difference) between the predicted and real values. The lower the MAE, the more accurate the model.</p>&#13;
&#13;
<p class="pagebreak-before"><a contenteditable="false" data-primary="mean squared error (MSE)" data-type="indexterm" id="id609"/><a contenteditable="false" data-primary="MSE (mean squared error)" data-type="indexterm" id="id610"/>The <em>mean squared error</em> (MSE) is one of the commonly used loss functions for regression. It measures the average of the squared differences between the predicted and actual values. You can think of MSE as the equivalent of the variance metric seen in <a data-type="xref" href="ch03.html#ch03">Chapter 3</a>. The mathematical representation of MSE is as follows:</p>&#13;
&#13;
<div data-type="equation"><p><math alttext="upper M upper S upper E equals StartFraction sigma-summation Underscript i equals 1 Overscript n Endscripts left-parenthesis ModifyingAbove y With caret minus y Subscript i Baseline right-parenthesis squared Over n EndFraction">&#13;
  <mrow>&#13;
    <mi>M</mi>&#13;
    <mi>S</mi>&#13;
    <mi>E</mi>&#13;
    <mo>=</mo>&#13;
    <mstyle displaystyle="false" scriptlevel="0">&#13;
      <mfrac><mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi> </msubsup><msup><mrow><mo>(</mo><mover accent="true"><mi>y</mi> <mo>^</mo></mover><mo>-</mo><msub><mi>y</mi> <mi>i</mi> </msub><mo>)</mo></mrow> <mn>2</mn> </msup></mrow> <mi>n</mi></mfrac>&#13;
    </mstyle>&#13;
  </mrow>&#13;
</math></p></div>&#13;
&#13;
<p>Hence, MSE calculates the average squared distance between the predicted and the real values. Similar to the MAE, the lower the MSE, the more accurate the model. With this in mind, it helps to compare apples to apples (such as with variance and standard deviation, as seen in <a data-type="xref" href="ch03.html#ch03">Chapter 3</a>). <a contenteditable="false" data-primary="root mean squared error (RMSE)" data-type="indexterm" id="id611"/><a contenteditable="false" data-primary="RMSE (root mean squared error)" data-type="indexterm" id="id612"/>Therefore, the <em>root mean squared error</em> (RMSE) has been developed to tackle this problem (hence, scaling the error metric back to the same units as the target variable). The mathematical representation of RMSE is as follows:</p>&#13;
&#13;
<div data-type="equation"><p><math alttext="upper R upper M upper S upper E equals StartRoot StartFraction sigma-summation Underscript i equals 1 Overscript n Endscripts left-parenthesis ModifyingAbove y With caret minus y Subscript i Baseline right-parenthesis squared Over n EndFraction EndRoot">&#13;
  <mrow>&#13;
    <mi>R</mi>&#13;
    <mi>M</mi>&#13;
    <mi>S</mi>&#13;
    <mi>E</mi>&#13;
    <mo>=</mo>&#13;
    <msqrt>&#13;
      <mstyle displaystyle="false" scriptlevel="0">&#13;
        <mfrac><mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi> </msubsup><msup><mrow><mo>(</mo><mover accent="true"><mi>y</mi> <mo>^</mo></mover><mo>-</mo><msub><mi>y</mi> <mi>i</mi> </msub><mo>)</mo></mrow> <mn>2</mn> </msup></mrow> <mi>n</mi></mfrac>&#13;
      </mstyle>&#13;
    </msqrt>&#13;
  </mrow>&#13;
</math></p></div>&#13;
&#13;
<p>The RMSE is the equivalent of the standard deviation in descriptive statistics.</p>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>MAE is relatively less sensitive to outliers than MSE, and it is often used when the data contains extreme values or when the absolute magnitude of the error is more important than its squared value. On the other hand, as MSE gives more weight to larger errors, it is the go-to loss function when trying to improve the performance of the model.</p>&#13;
</div>&#13;
&#13;
<p>When evaluating models using MAE, MSE, or RMSE, it is important to have a baseline for comparison:</p>&#13;
&#13;
<ul>&#13;
	<li>If you have built multiple regression models, you can compare their metrics to determine which model performs better. The model with the lower metric is generally considered to be more accurate in its predictions.</li>&#13;
	<li>Depending on the specific problem, you may have a threshold value for what is considered an acceptable level of prediction error. For example, in some cases, an RMSE below a certain threshold may be considered satisfactory, while values above that threshold may be considered unacceptable.</li>&#13;
	<li>You can compare the loss functions of the training data with the loss functions of the test data.</li>&#13;
</ul>&#13;
&#13;
<p>Algorithms may sometimes be directionally biased for many reasons (either structurally or externally). <a contenteditable="false" data-primary="biased model" data-type="indexterm" id="id613"/>A <em>biased model</em> takes on significantly more trades in one direction than the other (an example would be an algorithm having 200 long positions and 30 short positions). <em>Model bias</em> measures this as a ratio by dividing the number of long positions by the number of short positions. The ideal model bias is around 1.00, which implies a balanced trading system. The mathematical representation of model bias is as follows:</p>&#13;
&#13;
<div data-type="equation"><p><math alttext="Model bias equals StartFraction Number of bullish signals Over Number of bearish signals EndFraction">&#13;
  <mrow>&#13;
    <mtext>Model</mtext>&#13;
    <mspace width="4.pt"/>&#13;
    <mtext>bias</mtext>&#13;
    <mo>=</mo>&#13;
    <mfrac><mrow><mtext>Number</mtext><mspace width="4.pt"/><mtext>of</mtext><mspace width="4.pt"/><mtext>bullish</mtext><mspace width="4.pt"/><mtext>signals</mtext></mrow> <mrow><mtext>Number</mtext><mspace width="4.pt"/><mtext>of</mtext><mspace width="4.pt"/><mtext>bearish</mtext><mspace width="4.pt"/><mtext>signals</mtext></mrow></mfrac>&#13;
  </mrow>&#13;
</math></p></div>&#13;
&#13;
<p>If a model has had 934 long positions and 899 short positions this year, then the model bias metric is 1.038, which is acceptable. This means that the model is not really biased. It is worth noting that a model bias of 0.0 represents the absence of any bullish signals, and a model bias that has an undefined value represents the absence of any bearish signals (due to the division by zero).</p>&#13;
&#13;
<p>Now we’ll turn our attention to trading evaluation. Finance pioneers have been developing metrics that measure the performance of strategies and portfolios. Let’s discuss the most common and most useful ones. <a contenteditable="false" data-primary="net return" data-type="indexterm" id="id614"/>The most basic metric is the <em>net return</em>, which is essentially the return over the invested capital after a trading period that has at least one closed trade. The mathematical representation of the net return is as <span class="keep-together">follows:</span></p>&#13;
&#13;
<div data-type="equation"><p><math alttext="Net return equals left-parenthesis StartFraction Final value Over Initial value EndFraction minus 1 right-parenthesis times 100">&#13;
  <mrow>&#13;
    <mtext>Net</mtext>&#13;
    <mspace width="4.pt"/>&#13;
    <mtext>return</mtext>&#13;
    <mo>=</mo>&#13;
    <mo>(</mo>&#13;
    <mstyle displaystyle="false" scriptlevel="0">&#13;
      <mfrac><mrow><mtext>Final</mtext><mspace width="4.pt"/><mtext>value</mtext></mrow> <mrow><mtext>Initial</mtext><mspace width="4.pt"/><mtext>value</mtext></mrow></mfrac>&#13;
    </mstyle>&#13;
    <mo>-</mo>&#13;
    <mn>1</mn>&#13;
    <mo>)</mo>&#13;
    <mo>×</mo>&#13;
    <mn>100</mn>&#13;
  </mrow>&#13;
</math></p></div>&#13;
&#13;
<p>The word <em>net </em>implies a result after deducting fees; otherwise, it is referred to as a <em>gross </em>return. For example, if you start the year with $52,000 and finish at $67,150, you would have made 29.13% (a net profit of $15,150).</p>&#13;
&#13;
<p><a contenteditable="false" data-primary="profit factor" data-type="indexterm" id="id615"/>Another profitability metric is the <em>profit factor,</em> which is the ratio of the total gross profits to the total gross losses. Intuitively, a profit factor above 1.00 implies a profitable strategy, and a profit factor below 1.00 implies a losing strategy. The mathematical representation of the profit factor is as follows:</p>&#13;
&#13;
<div data-type="equation"><p><math alttext="Profit factor equals StartFraction Gross profits Over Gross losses EndFraction">&#13;
  <mrow>&#13;
    <mtext>Profit</mtext>&#13;
    <mspace width="4.pt"/>&#13;
    <mtext>factor</mtext>&#13;
    <mo>=</mo>&#13;
    <mstyle displaystyle="false" scriptlevel="0">&#13;
      <mfrac><mrow><mtext>Gross</mtext><mspace width="4.pt"/><mtext>profits</mtext></mrow> <mrow><mtext>Gross</mtext><mspace width="4.pt"/><mtext>losses</mtext></mrow></mfrac>&#13;
    </mstyle>&#13;
  </mrow>&#13;
</math></p></div>&#13;
&#13;
<p>The profit factor is a useful metric for evaluating the profitability of a trading strategy because it takes into account both the profits and losses generated by the strategy, rather than just looking at one side of the equation. The profit factor of a trading strategy that has generated $54,012 in profits and $29,988 in losses is 1.80.</p>&#13;
&#13;
<p>The next interesting metric relates to individual trades. The <em>average gain </em>per trade calculates the average profits (or positive returns) per trade based on historical data, and the <em>average loss</em> per trade calculates losses (or negative returns) per trade based on historical data. These two metrics give an expected return depending on the outcome. Both metrics are calculated following these formulas:</p>&#13;
&#13;
<div data-type="equation"><p><math alttext="StartLayout 1st Row  Average gain equals StartFraction Total profit Over Number of winning trades EndFraction 2nd Row  Average loss equals StartFraction Total losses Over Number of losing trades EndFraction EndLayout">&#13;
  <mtable>&#13;
    <mtr>&#13;
      <mtd columnalign="left">&#13;
        <mrow>&#13;
          <mtext>Average</mtext>&#13;
          <mspace width="4.pt"/>&#13;
          <mtext>gain</mtext>&#13;
          <mo>=</mo>&#13;
          <mfrac><mrow><mtext>Total</mtext><mspace width="4.pt"/><mtext>profit</mtext></mrow> <mrow><mtext>Number</mtext><mspace width="4.pt"/><mtext>of</mtext><mspace width="4.pt"/><mtext>winning</mtext><mspace width="4.pt"/><mtext>trades</mtext></mrow></mfrac>&#13;
        </mrow>&#13;
      </mtd>&#13;
    </mtr>&#13;
    <mtr>&#13;
      <mtd columnalign="left">&#13;
        <mrow>&#13;
          <mtext>Average</mtext>&#13;
          <mspace width="4.pt"/>&#13;
          <mtext>loss</mtext>&#13;
          <mo>=</mo>&#13;
          <mfrac><mrow><mtext>Total</mtext><mspace width="4.pt"/><mtext>losses</mtext></mrow> <mrow><mtext>Number</mtext><mspace width="4.pt"/><mtext>of</mtext><mspace width="4.pt"/><mtext>losing</mtext><mspace width="4.pt"/><mtext>trades</mtext></mrow></mfrac>&#13;
        </mrow>&#13;
      </mtd>&#13;
    </mtr>&#13;
  </mtable>&#13;
</math></p></div>&#13;
&#13;
<p>The next metric relates to risk and is one of the most important measures of evaluation. <a contenteditable="false" data-primary="maximum drawdown" data-type="indexterm" id="id616"/><em>Maximum drawdown</em> is a metric that measures the largest percentage decline in the value of an investment or portfolio from its highest historical peak to its lowest point. It is commonly used to assess the downside risk of an investment or portfolio. For example, if an investment has a peak value of $100,000 and its value subsequently drops to $50,000 before recovering, the maximum drawdown would be 50%, which is the percentage decline from the peak value to the trough. Maximum drawdown is calculated as follows:</p>&#13;
&#13;
<div data-type="equation"><p><math alttext="Maximum drawdown equals left-parenthesis StartFraction Trough value minus Peak value Over Peak value EndFraction right-parenthesis times 100">&#13;
  <mrow>&#13;
    <mtext>Maximum</mtext>&#13;
    <mspace width="4.pt"/>&#13;
    <mtext>drawdown</mtext>&#13;
    <mo>=</mo>&#13;
    <mo>(</mo>&#13;
    <mstyle displaystyle="false" scriptlevel="0">&#13;
      <mfrac><mrow><mtext>Trough</mtext><mspace width="4.pt"/><mtext>value</mtext><mspace width="4.pt"/><mo>-</mo><mspace width="4.pt"/><mtext>Peak</mtext><mspace width="4.pt"/><mtext>value</mtext></mrow> <mrow><mtext>Peak</mtext><mspace width="4.pt"/><mtext>value</mtext></mrow></mfrac>&#13;
    </mstyle>&#13;
    <mo>)</mo>&#13;
    <mo>×</mo>&#13;
    <mn>100</mn>&#13;
  </mrow>&#13;
</math></p></div>&#13;
&#13;
<p><a contenteditable="false" data-primary="Sharpe ratio" data-type="indexterm" id="id617"/>Finally, let’s discuss a well-known profitability ratio called the <em>Sharpe ratio</em>. It measures how much return is generated by units of excess risk. The formula of the ratio is as follows:</p>&#13;
&#13;
<div data-type="equation"><p><math alttext="upper S h a r p e equals StartFraction mu minus r Over sigma EndFraction">&#13;
  <mrow>&#13;
    <mi>S</mi>&#13;
    <mi>h</mi>&#13;
    <mi>a</mi>&#13;
    <mi>r</mi>&#13;
    <mi>p</mi>&#13;
    <mi>e</mi>&#13;
    <mo>=</mo>&#13;
    <mfrac><mrow><mi>μ</mi><mo>-</mo><mi>r</mi></mrow> <mi>σ</mi></mfrac>&#13;
  </mrow>&#13;
</math></p>&#13;
&#13;
<ul class="simplelist">&#13;
 <li><p>μ is the net return</p></li>&#13;
 <li><p><em>r</em> is the risk-free rate</p></li>&#13;
 <li><p>σ is the volatility of returns</p></li>&#13;
</ul>&#13;
</div>&#13;
&#13;
<p>So, if the net return is 5% and the risk-free rate is 2% while the volatility of returns is 2.5%, the Sharpe ratio is 1.20. Anything above 1.00 is desirable as it implies that the strategy is generating positive excess risk-adjusted return.<a contenteditable="false" data-primary="" data-startref="Chapter_7.html3" data-type="indexterm" id="id618"/><a contenteditable="false" data-primary="" data-startref="Chapter_7.html2" data-type="indexterm" id="id619"/></p>&#13;
&#13;
<aside class="pagebreak-before" data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id620">&#13;
<h1 class="less_space">Risk-Free Rate Explained</h1>&#13;
&#13;
<p><a contenteditable="false" data-primary="risk-free rate" data-type="indexterm" id="id621"/>The <em>risk-free rate</em> refers to the theoretical rate of return on an investment that carries no risk. It serves as a benchmark for evaluating the potential returns of other investments that do involve risk. In practice, the risk-free rate is typically based on the yield of a government-issued bond, usually one with a short-term maturity.</p>&#13;
&#13;
<p>The specific risk-free rate can vary depending on the country and currency involved. In the United States, for example, the risk-free rate is often associated with the yield on US Treasury securities. The most commonly used benchmark is the yield on the 10-year Treasury note, as it is considered a relatively low-risk investment.</p>&#13;
</div></aside>&#13;
&#13;
<p>The focus of this book is on developing machine and deep learning algorithms, so the performance evaluation step will solely focus on accuracy, RMSE, and model bias (with the correlation between the predicted variables as an extra metric). The performance functions can be found in the GitHub repository, along with the complete scripts.</p>&#13;
&#13;
<p>The model’s results on the EURUSD after applying the performance metrics are as <span class="keep-together">follows:</span></p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="n">Accuracy</code> <code class="n">Train</code> <code class="o">=</code>  <code class="mf">49.28</code> <code class="o">%</code>&#13;
<code class="n">Accuracy</code> <code class="n">Test</code> <code class="o">=</code>  <code class="mf">49.33</code> <code class="o">%</code>&#13;
<code class="n">RMSE</code> <code class="n">Train</code> <code class="o">=</code>  <code class="mf">0.0076467838</code>&#13;
<code class="n">RMSE</code> <code class="n">Test</code> <code class="o">=</code>  <code class="mf">0.0053250347</code>&#13;
<code class="n">Model</code> <code class="n">Bias</code> <code class="o">=</code>  <code class="mf">0.0</code></pre>&#13;
&#13;
<p>With a bias of 0.0, it’s easy to see that this is a dummy regression model. The bias means that according to the formula, all the forecasts are bearish. Taking a look at the details of the predictions, you will see that they are all constant values.</p>&#13;
&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id622">&#13;
<h1>More Ways to Import Data</h1>&#13;
&#13;
<p><a contenteditable="false" data-primary="data" data-secondary="alternative methods for importing" data-type="indexterm" id="id623"/>Some readers may not have the right operating system to run the importing algorithm through MetaTrader5. Fortunately, there are alternative ways to import the data. The first method is to use the <em>pandas_datareader</em> library, which does not require third-party software installation. It does, however, require a <code>pip</code> installation. To import the EURUSD daily, use the following code (also found in the GitHub repository):</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="c1"># Importing the required libraries</code>&#13;
<code class="kn">import</code> <code class="nn">pandas_datareader</code> <code class="k">as</code> <code class="nn">pdr</code>&#13;
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>&#13;
<code class="c1"># Set the start and end dates for the data</code>&#13;
<code class="n">start_date</code> <code class="o">=</code> <code class="s1">'2000-01-01'</code>&#13;
<code class="n">end_date</code>   <code class="o">=</code> <code class="s1">'2023-06-01'</code>&#13;
<code class="c1"># Fetch EURUSD price data</code>&#13;
<code class="n">data</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">((</code><code class="n">pdr</code><code class="o">.</code><code class="n">get_data_fred</code><code class="p">(</code><code class="s1">'DEXUSEU'</code><code class="p">,</code> &#13;
                                   <code class="n">start</code> <code class="o">=</code> <code class="n">start_date</code><code class="p">,</code> &#13;
                                   <code class="n">end</code> <code class="o">=</code> <code class="n">end_date</code><code class="p">))</code><code class="o">.</code><code class="n">dropna</code><code class="p">())</code>&#13;
<code class="c1"># Difference the data and make it stationary</code>&#13;
<code class="n">data</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">diff</code><code class="p">(</code><code class="n">data</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">])</code>&#13;
</pre>&#13;
&#13;
<p>The second method is to simply refer to the <em>Historical Data</em> folder in the GitHub repository, which contains multiple Excel files with the historical data. After downloading them, you can import them into Spyder using the following code:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="c1"># Importing the required libraries</code>&#13;
<code class="kn">import</code> <code class="nn">pandas</code> <code class="k">as</code> <code class="nn">pd</code>&#13;
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>&#13;
<code class="c1"># Import the data (write the code in one line)</code>&#13;
<code class="n">data</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">pd</code><code class="o">.</code><code class="n">read_excel</code><code class="p">(</code><code class="s1">'Daily_EURUSD_Historical_Data.xlsx'</code><code class="p">)</code>&#13;
       <code class="p">[</code><code class="s1">'&lt;CLOSE&gt;'</code><code class="p">])</code>&#13;
<code class="c1"># Difference the data and make it stationary</code>&#13;
<code class="n">data</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">diff</code><code class="p">(</code><code class="n">data</code><code class="p">)</code>&#13;
</pre>&#13;
&#13;
<p>Make sure to check that the directory of the interpreter is the same as the downloaded files; otherwise, you will get an error.</p>&#13;
</div></aside>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>The key takeaways from this section are as follows:</p>&#13;
&#13;
<ul>&#13;
	<li>Automatic data import and creation saves you time and allows you to concentrate on the main issues of the algorithm.</li>&#13;
	<li>For a proper backtest, the data must be split into a training set and a test set.</li>&#13;
	<li>The training set contains <code>x_train</code> and <code>y_train</code>, with the former containing the values that are supposed to have a predictive power over the latter.</li>&#13;
	<li>The test set contains <code>x_test</code> and <code>y_test</code>, with the former containing the values that are supposed to have a predictive power (even though the model hasn’t encountered them in its <span class="keep-together">training)</span> over the latter.</li>&#13;
	<li>Fitting the data is when the algorithm runs on the training set; predicting the data is when the algorithm runs on the test set.</li>&#13;
	<li>The predictions are stored in a variable called <code>y_predicted</code> that is compared to <code>y_test</code> for performance evaluation <span class="keep-together">purposes.</span></li>&#13;
	<li>The main aim of the algorithms is to have good accuracy and stable, low-volatility returns.</li>&#13;
</ul>&#13;
</div>&#13;
&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Machine Learning Models" data-type="sect1"><div class="sect1" id="id100">&#13;
<h1>Machine Learning Models</h1>&#13;
&#13;
<p><a contenteditable="false" data-primary="machine learning (for time series prediction)" data-secondary="machine learning models for" data-type="indexterm" id="Chapter_7.html5"/><a contenteditable="false" data-primary="time series prediction (machine learning for)" data-secondary="machine learning models for" data-type="indexterm" id="Chapter_7.html6"/>This section presents a selection of machine learning models using the framework developed so far. It is important to understand every model’s strengths and weaknesses so that you know which model to choose depending on the forecasting task.</p>&#13;
&#13;
<section data-pdf-bookmark="Linear Regression" data-type="sect2"><div class="sect2" id="id49">&#13;
<h2>Linear Regression</h2>&#13;
&#13;
<p><a contenteditable="false" data-primary="linear regression algorithm, for time series prediction" data-type="indexterm" id="Chapter_7.html7"/><a contenteditable="false" data-primary="machine learning (for time series prediction)" data-secondary="linear regression algorithm" data-type="indexterm" id="Chapter_7.html8"/>The <em>linear regression </em>algorithm works by finding the best-fitting line that minimizes the sum of squared differences between the predicted and actual target values. <a contenteditable="false" data-primary="OLS (ordinary least squares) method" data-type="indexterm" id="id624"/><a contenteditable="false" data-primary="ordinary least squares (OLS) method" data-type="indexterm" id="id625"/>The most used optimization technique in this algorithm is the <em>ordinary least squares</em> (OLS) method.<sup><a data-type="noteref" href="ch07.html#id626" id="id626-marker">2</a></sup></p>&#13;
&#13;
<p>The model is trained on the training set using the OLS method, which estimates the coefficients that minimize the sum of squared differences between the predicted and actual target values to find the optimal coefficients for the independent variables (the coefficients represent the <em>y</em>-intercept and the slope of the best-fitting line, respectively). The output is a linear function that gives the expected return given the explanatory variables weighted by the coefficient with an adjustment for noise and the intercept.</p>&#13;
&#13;
<p>To import the linear regression library from <em>sklearn</em>, use the following code:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">LinearRegression</code></pre>&#13;
&#13;
<p>Now let’s look at the algorithm’s implementation:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="c1"># Fitting the model</code>&#13;
<code class="n">model</code> <code class="o">=</code> <code class="n">LinearRegression</code><code class="p">()</code>&#13;
<code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">x_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>&#13;
<code class="c1"># Predicting in-sample</code>&#13;
<code class="n">y_predicted_train</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">x_train</code><code class="p">),</code> <code class="p">(</code><code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">))</code>&#13;
<code class="c1"># Predicting out-of-sample</code>&#13;
<code class="n">y_predicted</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">x_test</code><code class="p">),</code> <code class="p">(</code><code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">))</code>&#13;
</pre>&#13;
&#13;
<p>The model assumes that the linear relationship that has held in the past will still hold in the future. This is unrealistic, and it ignores the fact that market dynamics and drivers are constantly shifting whether in the short term or the long term. They are also nonlinear.</p>&#13;
&#13;
<p><a data-type="xref" href="#figure-7-3">Figure 7-3</a> shows the evolution of the forecasting task from the last values of <code>y_train</code> to the first values of <code>y_test</code> and <code>y_predicted</code>.</p>&#13;
&#13;
<figure><div class="figure" id="figure-7-3"><img alt="" class="iimagesdlf_0703png" src="assets/dlff_0703.png"/>&#13;
<h6><span class="label">Figure 7-3. </span>Training data followed by test data (dashed line) and the predicted data (thin line); the vertical dashed line represents the start of the test period. The model used is the linear regression algorithm.</h6>&#13;
</div></figure>&#13;
&#13;
<p>The model’s results on the EURUSD after applying the performance metrics are as follows:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="n">Accuracy</code> <code class="n">Train</code> <code class="o">=</code>  <code class="mf">58.52</code> <code class="o">%</code>&#13;
<code class="n">Accuracy</code> <code class="n">Test</code> <code class="o">=</code>  <code class="mf">49.54</code> <code class="o">%</code>&#13;
<code class="n">RMSE</code> <code class="n">Train</code> <code class="o">=</code>  <code class="mf">0.007096094</code>&#13;
<code class="n">RMSE</code> <code class="n">Test</code> <code class="o">=</code>  <code class="mf">0.0055932632</code>&#13;
<code class="n">Correlation</code> <code class="n">In</code><code class="o">-</code><code class="n">Sample</code> <code class="n">Predicted</code><code class="o">/</code><code class="n">Train</code> <code class="o">=</code>  <code class="mf">0.373</code>&#13;
<code class="n">Correlation</code> <code class="n">Out</code><code class="o">-</code><code class="n">of</code><code class="o">-</code><code class="n">Sample</code> <code class="n">Predicted</code><code class="o">/</code><code class="n">Test</code> <code class="o">=</code>  <code class="mf">0.014</code>&#13;
<code class="n">Model</code> <code class="n">Bias</code> <code class="o">=</code>  <code class="mf">0.93</code>&#13;
</pre>&#13;
&#13;
<p>The results indicate poor performance coming from the linear regression algorithm, with an accuracy below 50.00%. As you can see, the accuracy generally drops after switching to the test set. The correlation between the in-sample predictions and the real in-sample values also drops from 0.373 to 0.014. The model bias is close to equilibrium, which means that the number of long signals is close to the number of short signals.</p>&#13;
&#13;
<p>There are a few things to note regarding the model’s results:</p>&#13;
&#13;
<ul>&#13;
	<li>The transaction costs have not been incorporated, and therefore, these are gross results (not net results).</li>&#13;
	<li>There is no risk management system, as this is a pure time series machine learning model and not a full trading algorithm that incorporates stops and targets. Therefore, as this is a purely directional model, the job is to try to maximize the number of correct forecasts. With a daily horizon, you are searching for accuracy.</li>&#13;
	<li>Different FX data providers may have small differences in the historical data that may cause some differences between backtests.</li>&#13;
</ul>&#13;
&#13;
<p>Models are made to be optimized and tweaked. The process of optimization may include any of the following techniques:</p>&#13;
&#13;
<dl>&#13;
	<dt>Choosing the right predictors is paramount to a model’s success</dt><dd><p>In this chapter, the predictors used are the lagged returns. This has been chosen arbitrarily and is not necessarily the right choice. Predictors must be chosen based on economic and statistical intuition. For example, it may be reasonable to choose the returns of gold to explain (predict) the variations on the S&amp;P 500 index as they are economically linked. Safe haven assets like gold rise during periods of economic uncertainty, while the stock market tends to fall. This negative correlation may harbor hidden patterns between the returns of both instruments. Another way of choosing predictors is to use technical indicators such as the relative strength index (RSI) and moving averages.</p></dd>&#13;
	&#13;
	<dt>Proper splitting is crucial to evaluate the model properly</dt><dd><p>Train-test splits are important as they determine the window of evaluation. Typically, 20/80 and 30/70 are used, which means that 20% (30%) of the data is used for the testing sample and 80% (70%) is used for the training sample.</p></dd>&#13;
	&#13;
	<dt><a contenteditable="false" data-primary="Lasso regression" data-type="indexterm" id="id627"/><a contenteditable="false" data-primary="ridge recession" data-type="indexterm" id="id628"/>Regularization techniques can help prevent biases</dt><dd><p>Ridge regression and Lasso regression are two common regularization methods used in linear regression. <em>Ridge regression</em> adds a penalty term to the OLS function to reduce the impact of large coefficients, while <em>Lasso regression</em> can drive some coefficients to zero, effectively performing feature selection.</p></dd>&#13;
</dl>&#13;
&#13;
<p><a contenteditable="false" data-primary="autoregressive models" data-type="indexterm" id="id629"/>The model seen in this section is called an <em>autoregressive model</em> since the dependent variable depends on its past values and not on exogenous data. <a contenteditable="false" data-primary="multiple linear regression model" data-type="indexterm" id="id630"/>Also, since at every time step, 500 different variables (with their coefficients) have been used to predict the next variable, the model is referred to as a <em>multiple linear regression</em> model. <a contenteditable="false" data-primary="simple linear regression model" data-type="indexterm" id="id631"/>In contrast, when the model only uses one dependent variable to predict the dependent variable, it is referred to as a <em>simple linear regression</em> model.</p>&#13;
&#13;
<p>The advantages of linear regression are:</p>&#13;
&#13;
<ul>&#13;
	<li>It is easy to implement and train. It also does not consume a lot of memory.</li>&#13;
	<li>It outperforms when the data has a linear dependency.</li>&#13;
</ul>&#13;
&#13;
<p class="pagebreak-before">The disadvantages of linear regression are:</p>&#13;
&#13;
<ul>&#13;
	<li>It is sensitive to outliers.</li>&#13;
	<li>It is easily biased (more on this type of bias in <a data-type="xref" href="#overfit_and_underfit">“Overfitting and Underfitting”</a>).</li>&#13;
	<li>It has unrealistic assumptions, such as the independence of data.</li>&#13;
</ul>&#13;
&#13;
<p>Before moving on to the next section, it is important to note that some linear regression models do not transform the data. You may see extremely high accuracy and a prediction that is very close to the real data, but the reality is that the prediction lags by one time step. This means that at every time step, the prediction is simply the last real value. Let’s prove this using the previous example. Use the same code as before, but omit the price differencing code. You should see <a data-type="xref" href="#figure-7-4">Figure 7-4</a>.</p>&#13;
&#13;
<figure><div class="figure" id="figure-7-4"><img alt="" class="iimagesdlf_0711png" src="assets/dlff_0704.png"/>&#13;
<h6><span class="label">Figure 7-4. </span>Nonstationary training data followed by test data (dashed line) and the predicted data (thin line); the vertical dashed line represents the start of the test period. The model used is the linear regression algorithm.</h6>&#13;
</div></figure>&#13;
&#13;
<p>Notice how it’s simply lagging the real values and not adding any predictive information. Always transform nonstationary data when dealing with such models. Nonstationary data cannot be forecasted using this type of algorithm (there are exceptions, though, which you will see later on).</p>&#13;
&#13;
<p><a contenteditable="false" data-primary="naive forecasting" data-type="indexterm" id="id632"/>Using linear regression on nonstationary data, such as market prices, and observing that the forecasts are the same as the last value might indicate an issue known as <em>naive forecasting</em>. This occurs when the most recent observation (in this case, the last value) is simply used as the forecast for the next time period. While this approach can <span class="keep-together">sometimes</span> work for certain types of data, it is generally not a sophisticated forecasting method and may not capture the underlying patterns or trends in the data. There are a few reasons why this might happen:</p>&#13;
&#13;
<dl>&#13;
	<dt>Lack of predictive power</dt><dd><p>Linear regression assumes that there is a linear relationship between the independent variable(s) and the dependent variable. If the data is highly nonstationary and lacks a clear linear relationship, then the linear regression model may not be able to capture meaningful patterns and will default to a simplistic forecast like naive forecasting.</p></dd>&#13;
	<dt>Lagging indicators</dt><dd><p>Market prices often exhibit strong autocorrelation, meaning that the current price is highly correlated with the previous price. In such cases, if the model only takes into account lagged values as predictors, it might simply replicate the last value as the forecast.</p></dd>&#13;
	<dt>Lack of feature engineering</dt><dd><p>Linear regression models rely on the features (predictors) you provide to make forecasts. If you’re using only lagged values as predictors and not incorporating other relevant features, the model might struggle to generate meaningful forecasts.</p></dd>&#13;
	<dt>Model complexity</dt><dd><p>Linear regression is a relatively simple modeling technique. If the underlying relationship in the data is more complex than can be captured by a linear equation, the model might not be able to make accurate forecasts.<a contenteditable="false" data-primary="" data-startref="Chapter_7.html8" data-type="indexterm" id="id633"/><a contenteditable="false" data-primary="" data-startref="Chapter_7.html7" data-type="indexterm" id="id634"/></p></dd>&#13;
</dl>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Support Vector Regression" data-type="sect2"><div class="sect2" id="id50">&#13;
<h2>Support Vector Regression</h2>&#13;
&#13;
<p><a contenteditable="false" data-primary="machine learning (for time series prediction)" data-secondary="support vector regression algorithm for" data-type="indexterm" id="Chapter_7.html9"/><a contenteditable="false" data-primary="support vector regression (SVR) algorithm" data-type="indexterm" id="Chapter_7.html10"/><a contenteditable="false" data-primary="time series prediction (machine learning for)" data-secondary="support vector regression algorithm for" data-type="indexterm" id="Chapter_7.html11"/><em>Support vector regression</em> (SVR) is a machine learning algorithm that belongs to the family of <em>support vector machines</em> (SVMs). SVR is specifically designed for regression problems, where the goal is to predict continuous numerical values (such as return values).</p>&#13;
&#13;
<p>SVR performs regression by finding a hyperplane in a high-dimensional feature space that best approximates the relationship between the input features and the target variable. Unlike traditional regression techniques that aim to minimize the errors between the predicted and actual values, SVR focuses on finding a hyperplane that captures the majority of the data within a specified margin, known as the <em>epsilon tube </em>(loss function).</p>&#13;
&#13;
<p>The key idea behind SVR is to transform the original input space into a higher-dimensional space using a kernel function. This transformation allows SVR to implicitly map the data into a higher-dimensional feature space, where it becomes easier to find a linear relationship between the features and the target variable. The kernel function calculates the similarity between two data points, enabling the SVR algorithm to work effectively in nonlinear regression problems. The steps performed in the SVR process are as follows:</p>&#13;
&#13;
<ol>&#13;
	<li>The algorithm employs a kernel function to transform the input features into a higher-dimensional space. Common kernel functions include the linear kernel, polynomial kernel, radial basis function (RBF) kernel, and sigmoid kernel. The choice of kernel depends on the data and the underlying problem.</li>&#13;
	<li>The algorithm then aims to find the hyperplane that best fits the data points within the epsilon tube. The training process involves solving an optimization problem to minimize the error (using a loss function such as MSE) while controlling the margin.</li>&#13;
</ol>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>The RBF kernel is a popular choice for SVR because it can capture nonlinear relationships effectively. It is suitable when there is no prior knowledge about the specific form of the relationship. The RBF kernel calculates the similarity between feature vectors based on their distance in the input space. It uses a parameter called <em>gamma</em>, which determines the influence of each training example on the model. Higher gamma values make the model focus more on individual data points, potentially leading to errors.</p>&#13;
</div>&#13;
&#13;
<p>By finding an optimal hyperplane within the epsilon tube, SVR can effectively capture the underlying patterns and relationships in the data, even in the presence of noise or outliers. It is a powerful technique for regression tasks, especially when dealing with nonlinear relationships between features and target variables.</p>&#13;
&#13;
<p>As SVR is sensitive to the scale of the features, it’s important to bring all the features to a similar scale. Common scaling methods include <em>standardization </em>(mean subtraction and division by standard deviation) and <em>normalization </em>(scaling features to a range, e.g., [0, 1]).</p>&#13;
&#13;
<p>Let’s take a look at SVR in action. Once again, the aim is to predict the next EURUSD return given the previous returns. To import the SVR library and the scaling library, use the following code:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">sklearn.svm</code> <code class="kn">import</code> <code class="n">SVR</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">StandardScaler</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.pipeline</code> <code class="kn">import</code> <code class="n">make_pipeline</code>&#13;
</pre>&#13;
&#13;
<p>For the SVR algorithm, a little tweaking was done to get acceptable forecasts. The tweak was to reduce the number of lagged values from 500 to 50:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="n">num_lags</code> <code class="o">=</code> <code class="mi">50</code></pre>&#13;
&#13;
<p>This allows the SVR algorithm to improve its forecasts. You will see throughout the book that part of performing these types of backtests is tweaking and calibrating the models.</p>&#13;
&#13;
<p>Next, to implement the algorithm, use the following code:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="c1"># Fitting the model</code>&#13;
<code class="n">model</code> <code class="o">=</code> <code class="n">make_pipeline</code><code class="p">(</code><code class="n">StandardScaler</code><code class="p">(),</code> &#13;
                      <code class="n">SVR</code><code class="p">(</code><code class="n">kernel</code> <code class="o">=</code> <code class="s1">'rbf'</code><code class="p">,</code> <code class="n">C</code> <code class="o">=</code> <code class="mi">1</code><code class="p">,</code> <code class="n">gamma</code> <code class="o">=</code> <code class="mf">0.04</code><code class="p">,</code> &#13;
                      <code class="n">epsilon</code> <code class="o">=</code> <code class="mf">0.01</code><code class="p">))</code>&#13;
<code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">x_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>&#13;
<code class="c1"># Predicting in-sample</code>&#13;
<code class="n">y_predicted_train</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">x_train</code><code class="p">),</code> <code class="p">(</code><code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">))</code>&#13;
<code class="c1"># Predicting out-of-sample</code>&#13;
<code class="n">y_predicted</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">x_test</code><code class="p">),</code> <code class="p">(</code><code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">))</code>&#13;
</pre>&#13;
&#13;
<p><a data-type="xref" href="#figure-7-5">Figure 7-5</a> shows the evolution of the forecasting task from the last values of <code>y_train</code> to the first values of <code>y_test</code> and <code>y_predicted</code>.</p>&#13;
&#13;
<figure><div class="figure" id="figure-7-5"><img alt="" class="iimagesdlf_0704png" src="assets/dlff_0705.png"/>&#13;
<h6><span class="label">Figure 7-5. </span>Training data followed by test data (dashed line) and the predicted data (thin line); the vertical dashed line represents the start of the test period. The model used is the SVR algorithm.</h6>&#13;
</div></figure>&#13;
&#13;
<p>The model’s results are as follows:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="n">Accuracy</code> <code class="n">Train</code> <code class="o">=</code>  <code class="mf">57.94</code> <code class="o">%</code>&#13;
<code class="n">Accuracy</code> <code class="n">Test</code> <code class="o">=</code>  <code class="mf">50.14</code> <code class="o">%</code>&#13;
<code class="n">RMSE</code> <code class="n">Train</code> <code class="o">=</code>  <code class="mf">0.0060447699</code>&#13;
<code class="n">RMSE</code> <code class="n">Test</code> <code class="o">=</code>  <code class="mf">0.0054036167</code>&#13;
<code class="n">Correlation</code> <code class="n">In</code><code class="o">-</code><code class="n">Sample</code> <code class="n">Predicted</code><code class="o">/</code><code class="n">Train</code> <code class="o">=</code>  <code class="mf">0.686</code>&#13;
<code class="n">Correlation</code> <code class="n">Out</code><code class="o">-</code><code class="n">of</code><code class="o">-</code><code class="n">Sample</code> <code class="n">Predicted</code><code class="o">/</code><code class="n">Test</code> <code class="o">=</code>  <code class="mf">0.024</code>&#13;
<code class="n">Model</code> <code class="n">Bias</code> <code class="o">=</code>  <code class="mf">0.98</code></pre>&#13;
&#13;
<p>The advantages of SVR are:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>It performs well even in high-dimensional feature spaces, where the number of features is large compared to the number of samples. It is particularly useful when dealing with complex datasets.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>It can capture nonlinear relationships between input features and the target variable by using kernel functions.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>It is robust to outliers in the training data due to the epsilon-tube formulation. The model focuses on fitting the majority of the data within the specified margin, reducing the influence of outliers.</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>The disadvantages of SVR are:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>It has several hyperparameters that need to be tuned for optimal performance. Selecting appropriate hyperparameters can be a challenging task and may require extensive experimentation.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>It can be computationally expensive, especially for large datasets or when using complex kernel functions.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>It can be sensitive to the choice of hyperparameters. Poorly chosen hyperparameters can lead to fitting issues.<a contenteditable="false" data-primary="" data-startref="Chapter_7.html11" data-type="indexterm" id="id635"/><a contenteditable="false" data-primary="" data-startref="Chapter_7.html10" data-type="indexterm" id="id636"/><a contenteditable="false" data-primary="" data-startref="Chapter_7.html9" data-type="indexterm" id="id637"/></p>&#13;
	</li>&#13;
</ul>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Stochastic Gradient Descent Regression" data-type="sect2"><div class="sect2" id="id51">&#13;
<h2>Stochastic Gradient Descent Regression</h2>&#13;
&#13;
<p><a contenteditable="false" data-primary="machine learning (for time series prediction)" data-secondary="stochastic gradient descent regression for" data-type="indexterm" id="Chapter_7.html12"/><a contenteditable="false" data-primary="stochastic gradient descent (SGD) algorithm" data-type="indexterm" id="Chapter_7.html13"/><a contenteditable="false" data-primary="SGD (stochastic gradient descent) algorithm" data-type="indexterm" id="Chapter_7.html13a"/><a contenteditable="false" data-primary="time series prediction (machine learning for)" data-secondary="stochastic gradient descent regression for" data-type="indexterm" id="Chapter_7.html14"/><em>Gradient descent</em> <a contenteditable="false" data-primary="GD (gradient descent)" data-type="indexterm" id="id638"/><a contenteditable="false" data-primary="gradient descent (GD)" data-type="indexterm" id="id639"/>(GD) is a general optimization algorithm used to minimize the cost or loss function of a model, and it serves as the foundation for various optimization algorithms.</p>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p><em>Gradient</em> simply refers to a surface’s slope or tilt. To get to the lowest point on the surface, one must literally descend a slope.</p>&#13;
</div>&#13;
&#13;
<p><em>Stochastic gradient descent</em> (SGD) is an iterative optimization algorithm commonly used for training machine learning models, including regression models. It is particularly useful for large datasets and online learning scenarios. When applied to time series prediction, SGD can be used to train regression models that capture temporal patterns and make predictions based on historical data. SGD is therefore a type of linear regression that uses stochastic gradient descent optimization to find the <span class="keep-together">best-fitting</span> line.</p>&#13;
&#13;
<p class="pagebreak-before">Unlike ordinary least squares, SGD updates the model’s parameters iteratively, making it more suitable for large datasets (which are treated in small batches). Instead of using the entire dataset for each update step, SGD randomly selects a small batch of samples or individual samples from the training dataset. This random selection helps to introduce randomness and avoid getting stuck in local optima (you can refer to <a data-type="xref" href="ch04.html#ch04">Chapter 4</a> for more information on optimization). The main difference between GD and SGD lies in how they update the model’s parameters during optimization.</p>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>SGD does not belong to any particular family of machine learning models; it is essentially an optimization technique.</p>&#13;
</div>&#13;
&#13;
<p>GD computes the gradients over the entire training dataset, updating the model’s parameters once per epoch, while SGD computes the gradients based on a single training example or mini batch, updating the parameters more frequently. SGD is faster but exhibits more erratic behavior, while GD is slower but has a smoother convergence trajectory. SGD is also more robust to local minima. The choice between GD and SGD depends on the specific requirements of the problem, the dataset size, and the trade-off between computational efficiency and convergence behavior.</p> &#13;
&#13;
<p>As usual, the first step is to import the necessary libraries:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">SGDRegressor</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">StandardScaler</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.pipeline</code> <code class="kn">import</code> <code class="n">make_pipeline</code></pre>&#13;
&#13;
<p>Next, to implement the algorithm, use the following code:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="c1"># Fitting the model</code>&#13;
<code class="n">model</code> <code class="o">=</code> <code class="n">make_pipeline</code><code class="p">(</code><code class="n">StandardScaler</code><code class="p">(),</code> <code class="n">SGDRegressor</code><code class="p">(</code><code class="n">max_iter</code> <code class="o">=</code> <code class="mi">50</code><code class="p">,</code> &#13;
                                                     <code class="n">tol</code> <code class="o">=</code> <code class="mi">1</code><code class="n">e</code><code class="err">–</code><code class="mi">3</code><code class="p">))</code>&#13;
<code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">x_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>&#13;
<code class="c1"># Predicting in-sample</code>&#13;
<code class="n">y_predicted_train</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">x_train</code><code class="p">),</code> <code class="p">(</code><code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">))</code>&#13;
<code class="c1"># Predicting out-of-sample</code>&#13;
<code class="n">y_predicted</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">x_test</code><code class="p">),</code> <code class="p">(</code><code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">))</code>&#13;
</pre>&#13;
&#13;
<p><a data-type="xref" href="#figure-7-6">Figure 7-6</a> shows the evolution of the forecasting task from the last values of <code>y_train</code> to the first values of <code>y_test</code> and <code>y_predicted</code>.</p>&#13;
&#13;
<figure><div class="figure" id="figure-7-6"><img alt="" class="iimagesdlf_0705png" src="assets/dlff_0706.png"/>&#13;
<h6><span class="label">Figure 7-6. </span>Training data followed by test data (dashed line) and the predicted data (thin line); the vertical dashed line represents the start of the test period. The model used is the SGD algorithm.</h6>&#13;
</div></figure>&#13;
&#13;
<p>The model’s results are as follows:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="n">Accuracy</code> <code class="n">Train</code> <code class="o">=</code>  <code class="mf">55.59</code> <code class="o">%</code>&#13;
<code class="n">Accuracy</code> <code class="n">Test</code> <code class="o">=</code>  <code class="mf">46.45</code> <code class="o">%</code>&#13;
<code class="n">RMSE</code> <code class="n">Train</code> <code class="o">=</code>  <code class="mf">0.007834505</code>&#13;
<code class="n">RMSE</code> <code class="n">Test</code> <code class="o">=</code>  <code class="mf">0.0059334014</code>&#13;
<code class="n">Correlation</code> <code class="n">In</code><code class="o">-</code><code class="n">Sample</code> <code class="n">Predicted</code><code class="o">/</code><code class="n">Train</code> <code class="o">=</code>  <code class="mf">0.235</code>&#13;
<code class="n">Correlation</code> <code class="n">Out</code><code class="o">-</code><code class="n">of</code><code class="o">-</code><code class="n">Sample</code> <code class="n">Predicted</code><code class="o">/</code><code class="n">Test</code> <code class="o">=</code>  <code class="err">–</code><code class="mf">0.001</code>&#13;
<code class="n">Model</code> <code class="n">Bias</code> <code class="o">=</code>  <code class="mf">0.95</code>&#13;
</pre>&#13;
&#13;
<p>The advantages of SGD are:</p>&#13;
&#13;
<ul>&#13;
	<li>It performs well with large datasets since it updates the model parameters incrementally based on individual or small subsets of training examples.</li>&#13;
	<li>It can escape local minima and find better global optima (due to its stochastic nature).</li>&#13;
	<li>It can improve generalization by exposing the model to different training examples in each iteration, thereby reducing overfitting.</li>&#13;
</ul>&#13;
&#13;
<p class="pagebreak-before">The disadvantages of SGD are:</p>&#13;
&#13;
<ul>&#13;
	<li>The convergence path can be noisy and exhibit more fluctuations compared to deterministic optimization algorithms. This can result in slower convergence or oscillations around the optimal solution.</li>&#13;
	<li>It is impacted by feature scaling, which means it is sensitive to such techniques.<a contenteditable="false" data-primary="" data-startref="Chapter_7.html14" data-type="indexterm" id="id640"/><a contenteditable="false" data-primary="" data-startref="Chapter_7.html13" data-type="indexterm" id="id641"/><a contenteditable="false" data-primary="" data-startref="Chapter_7.html13a" data-type="indexterm" id="id642"/><a contenteditable="false" data-primary="" data-startref="Chapter_7.html12" data-type="indexterm" id="id643"/></li>&#13;
</ul>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Nearest Neighbors Regression" data-type="sect2"><div class="sect2" id="id52">&#13;
<h2>Nearest Neighbors Regression</h2>&#13;
&#13;
<p><a contenteditable="false" data-primary="k-nearest neighbors (KNN) regression algorithm" data-type="indexterm" id="Chapter_7.html15"/><a contenteditable="false" data-primary="machine learning (for time series prediction)" data-secondary="k-nearest neighbors (KNN) regression algorithm for" data-type="indexterm" id="Chapter_7.html16"/><a contenteditable="false" data-primary="nearest neighbors regression algorithm" data-type="indexterm" id="Chapter_7.html17"/><a contenteditable="false" data-primary="time series prediction (machine learning for)" data-secondary="k-nearest neighbors (KNN) regression algorithm for" data-type="indexterm" id="Chapter_7.html18"/>The <em>nearest neighbors regression </em>algorithm, also known as <em>k</em>-nearest neighbors (KNN) regression, is a nonparametric<sup><a data-type="noteref" href="ch07.html#id644" id="id644-marker">3</a></sup> algorithm used for regression tasks. It predicts the value of a target variable based on the values of its nearest neighbors in the feature space. The algorithm starts by determining <em>k</em>, which is the number of nearest neighbors to consider when making predictions. This is a hyperparameter that you need to choose based on the problem at hand.</p>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>A larger <em>k</em> value provides a smoother prediction, while a smaller <em>k</em> value captures more local variations but may be more prone to noise.</p>&#13;
</div>&#13;
&#13;
<p>Then the model calculates the distance between the new, unseen data point and all the data points in the training set. The choice of distance metric depends on the nature of the input features. Common distance metrics include Euclidean distance, Manhattan distance, and Minkowski distance. Next, the algorithm selects the <em>k</em> data points with the shortest distances to the query point. These data points are the <em>nearest neighbors</em> and will be used to make predictions.</p>&#13;
&#13;
<p>To import the KNN regressor, use the following code:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">sklearn.neighbors</code> <code class="kn">import</code> <code class="n">KNeighborsRegressor</code></pre>&#13;
&#13;
<p>Now let’s look at the algorithm’s implementation. Fit the model with <em>k </em>= 10:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="c1"># Fitting the model</code>&#13;
<code class="n">model</code> <code class="o">=</code> <code class="n">KNeighborsRegressor</code><code class="p">(</code><code class="n">n_neighbors</code> <code class="o">=</code> <code class="mi">10</code><code class="p">)</code>&#13;
<code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">x_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>&#13;
<code class="c1"># Predicting in-sample</code>&#13;
<code class="n">y_predicted_train</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">x_train</code><code class="p">),</code> <code class="p">(</code><code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">))</code>&#13;
<code class="c1"># Predicting out-of-sample</code>&#13;
<code class="n">y_predicted</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">x_test</code><code class="p">),</code> <code class="p">(</code><code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">))</code>&#13;
</pre>&#13;
&#13;
<p><a data-type="xref" href="#figure-7-7">Figure 7-7</a> shows the evolution of the forecasting task from the last values of <code>y_train</code> to the first values of <code>y_test</code> and <code>y_predicted</code>.</p>&#13;
&#13;
<figure><div class="figure" id="figure-7-7"><img alt="" class="iimagesdlf_0706png" src="assets/dlff_0707.png"/>&#13;
<h6><span class="label">Figure 7-7. </span>Training data followed by test data (dashed line) and the predicted data (thin line); the vertical dashed line represents the start of the test period. The model used is the KNN regression algorithm.</h6>&#13;
</div></figure>&#13;
&#13;
<p>The choice of the number of neighbors in a time series prediction using the KNN regressor depends on several factors, including the characteristics of your dataset and the desired level of accuracy. There is no definitive answer as to how many neighbors to choose, as it is often determined through experimentation and validation. Typically, selecting an appropriate value for the number of neighbors involves a trade-off between bias and variance:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>Small <em>k</em> values are associated with a model that can capture local patterns in the data, but it may also be sensitive to noise or outliers.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Larger <em>k </em>values are associated with a model that can become more robust to noise or outliers but may overlook local patterns in the data.</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>If you take the limit as<em> k</em> approaches the size of the dataset, you will get a model that just predicts the class that appears more frequently in the dataset. This is known as the <em>Bayes error</em>.</p>&#13;
</div>&#13;
&#13;
<p class="pagebreak-before">The model’s results are as follows:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="n">Accuracy</code> <code class="n">Train</code> <code class="o">=</code>  <code class="mf">67.69</code> <code class="o">%</code>&#13;
<code class="n">Accuracy</code> <code class="n">Test</code> <code class="o">=</code>  <code class="mf">50.77</code> <code class="o">%</code>&#13;
<code class="n">RMSE</code> <code class="n">Train</code> <code class="o">=</code>  <code class="mf">0.0069584171</code>&#13;
<code class="n">RMSE</code> <code class="n">Test</code> <code class="o">=</code>  <code class="mf">0.0054027335</code>&#13;
<code class="n">Correlation</code> <code class="n">In</code><code class="o">-</code><code class="n">Sample</code> <code class="n">Predicted</code><code class="o">/</code><code class="n">Train</code> <code class="o">=</code>  <code class="mf">0.599</code>&#13;
<code class="n">Correlation</code> <code class="n">Out</code><code class="o">-</code><code class="n">of</code><code class="o">-</code><code class="n">Sample</code> <code class="n">Predicted</code><code class="o">/</code><code class="n">Test</code> <code class="o">=</code>  <code class="mf">0.002</code>&#13;
<code class="n">Model</code> <code class="n">Bias</code> <code class="o">=</code>  <code class="mf">0.76</code>&#13;
</pre>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>It’s essential to consider the temporal aspect of your time series data. If there are clear trends or patterns that span multiple data points, a larger <em>k</em> value might be appropriate to capture those dependencies. However, if the time series exhibits rapid changes or short-term fluctuations, a smaller k value could be more suitable.</p>&#13;
</div>&#13;
&#13;
<p>The size of your dataset can also influence the choice of <em>k</em>. If you have a small dataset, choosing a smaller value for <em>k</em> might be preferable to avoid overfitting. Conversely, a larger dataset can tolerate a higher value for <em>k</em>.</p>&#13;
&#13;
<p>The advantages of KNN are:</p>&#13;
&#13;
<ul>&#13;
	<li>Its nonlinearity allows it to capture complex patterns in financial data, which can be advantageous for predicting returns series that may exhibit nonlinear <span class="keep-together">behavior.</span></li>&#13;
	<li>It can adapt to changing market conditions or patterns. As the algorithm is instance based, it does not require retraining the model when new data becomes available. This adaptability can be beneficial in the context of financial returns, where market dynamics can change over time.</li>&#13;
	<li>It provides intuitive interpretations for predictions. Since the algorithm selects the <em>k</em> nearest neighbors to make predictions, it can be easier to understand and explain compared to more complex algorithms.</li>&#13;
</ul>&#13;
&#13;
<p>The disadvantages of KNN are:</p>&#13;
&#13;
<ul>&#13;
	<li>Its performance can degrade when dealing with high-dimensional data. Financial returns series often involve multiple predictors (such as technical indicators and other correlated returns), and KNN may struggle to find meaningful neighbors in high-dimensional spaces.</li>&#13;
	<li>As the dataset grows in size, the computational requirements of KNN can become significant.</li>&#13;
	<li>It is sensitive to noisy or outlier data points since the algorithm considers all neighbors equally.<a contenteditable="false" data-primary="" data-startref="Chapter_7.html18" data-type="indexterm" id="id645"/><a contenteditable="false" data-primary="" data-startref="Chapter_7.html17" data-type="indexterm" id="id646"/><a contenteditable="false" data-primary="" data-startref="Chapter_7.html16" data-type="indexterm" id="id647"/><a contenteditable="false" data-primary="" data-startref="Chapter_7.html15" data-type="indexterm" id="id648"/></li>&#13;
</ul>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Decision Tree Regression" data-type="sect2"><div class="sect2" id="id53">&#13;
<h2>Decision Tree Regression</h2>&#13;
&#13;
<p><a contenteditable="false" data-primary="decision tree regression" data-type="indexterm" id="Chapter_7.html19"/><a contenteditable="false" data-primary="machine learning (for time series prediction)" data-secondary="decision tree regression" data-type="indexterm" id="Chapter_7.html20"/><a contenteditable="false" data-primary="time series prediction (machine learning for)" data-secondary="decision tree regression" data-type="indexterm" id="Chapter_7.html21"/><em>Decision trees</em> are versatile and intuitive machine learning models. They are graphical representations of a series of decisions or choices based on feature values that lead to different outcomes. Decision trees are structured as a hierarchical flowchart, where each internal node represents a decision based on a feature, each branch represents an outcome of that decision, and each leaf node represents the final prediction or class label.</p>&#13;
&#13;
<p>At the root of the decision tree, consider all the input features and choose the one that best separates the data based on a specific criterion (e.g., the information gain metric discussed in <a data-type="xref" href="ch02.html#ch02">Chapter 2</a>). Create a decision node associated with the selected feature. Split the data based on the possible values of the chosen feature. Repeat the preceding steps recursively for each subset of data created by the splits, considering the remaining features at each node. Stop the recursion when a stopping criterion is met, such as reaching a maximum depth, reaching a minimum number of samples in a node, or no further improvement in impurity or gain.</p>&#13;
&#13;
<p>To import the decision tree regressor, use the following code:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">sklearn.tree</code> <code class="kn">import</code> <code class="n">DecisionTreeRegressor</code></pre>&#13;
&#13;
<p>Now let’s look at the algorithm’s implementation:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="c1"># Fitting the model</code>&#13;
<code class="n">model</code> <code class="o">=</code> <code class="n">DecisionTreeRegressor</code><code class="p">(</code><code class="n">random_state</code> <code class="o">=</code> <code class="mi">123</code><code class="p">)</code>&#13;
<code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">x_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>&#13;
<code class="c1"># Predicting in-sample</code>&#13;
<code class="n">y_predicted_train</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">x_train</code><code class="p">),</code> <code class="p">(</code><code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">))</code>&#13;
<code class="c1"># Predicting out-of-sample</code>&#13;
<code class="n">y_predicted</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">x_test</code><code class="p">),</code> <code class="p">(</code><code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">))</code>&#13;
</pre>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>The argument <code>random_state</code> is often used to initialize the randomization within algorithms that involve randomness such as initializing weights. This ensures that if you train a model multiple times with the same <code>random_state</code>, you’ll get the same results, which is important for comparing different algorithms or hyperparameters.</p>&#13;
</div>&#13;
&#13;
<p><a data-type="xref" href="#figure-7-8">Figure 7-8</a> shows the evolution of the forecasting task from the last values of <code>y_train</code> to the first values of <code>y_test</code> and <code>y_predicted</code>.</p>&#13;
&#13;
<figure><div class="figure" id="figure-7-8"><img alt="" class="iimagesdlf_0707png" src="assets/dlff_0708.png"/>&#13;
<h6><span class="label">Figure 7-8. </span>Training data followed by test data (dashed line) and the predicted data (thin line); the vertical dashed line represents the start of the test period. The model used is the decision tree regression algorithm.</h6>&#13;
</div></figure>&#13;
&#13;
<p>The model’s results are as follows:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="n">Accuracy</code> <code class="n">Train</code> <code class="o">=</code>  <code class="mf">100.0</code> <code class="o">%</code>&#13;
<code class="n">Accuracy</code> <code class="n">Test</code> <code class="o">=</code>  <code class="mf">47.37</code> <code class="o">%</code>&#13;
<code class="n">RMSE</code> <code class="n">Train</code> <code class="o">=</code>  <code class="mf">0.0</code>&#13;
<code class="n">RMSE</code> <code class="n">Test</code> <code class="o">=</code>  <code class="mf">0.007640736</code>&#13;
<code class="n">Correlation</code> <code class="n">In</code><code class="o">-</code><code class="n">Sample</code> <code class="n">Predicted</code><code class="o">/</code><code class="n">Train</code> <code class="o">=</code>  <code class="mf">1.0</code>&#13;
<code class="n">Correlation</code> <code class="n">Out</code><code class="o">-</code><code class="n">of</code><code class="o">-</code><code class="n">Sample</code> <code class="n">Predicted</code><code class="o">/</code><code class="n">Test</code> <code class="o">=</code>  <code class="err">–</code><code class="mf">0.079</code>&#13;
<code class="n">Model</code> <code class="n">Bias</code> <code class="o">=</code>  <code class="mf">0.94</code>&#13;
</pre>&#13;
&#13;
<p>Notice how the accuracy of the training set is extremely high. This is clearly evidence of overfitting (supported by the RMSE of the training data).</p>&#13;
&#13;
<p>The advantages of decision trees are:</p>&#13;
&#13;
<ul>&#13;
	<li>They require minimal data preprocessing and can handle missing values.</li>&#13;
	<li>They can capture nonlinear relationships, interactions, and variable importance.</li>&#13;
</ul>&#13;
&#13;
<p>The disadvantages of decision trees are:</p>&#13;
&#13;
<ul>&#13;
	<li>They can be sensitive to small changes in the data and can easily overfit if not properly regularized.</li>&#13;
	<li>They may struggle to capture complex relationships that require deeper trees.</li>&#13;
</ul>&#13;
&#13;
<p>The next section presents another breed of machine learning algorithms. These are called <em>ensemble algorithms.</em> Decision trees can be combined using ensemble methods to create more robust and accurate models. Random forest, an algorithm seen in the next section, combines multiple decision trees to enhance predictive ability and, especially, to reduce the risk of overfitting.<a contenteditable="false" data-primary="" data-startref="Chapter_7.html21" data-type="indexterm" id="id649"/><a contenteditable="false" data-primary="" data-startref="Chapter_7.html20" data-type="indexterm" id="id650"/><a contenteditable="false" data-primary="" data-startref="Chapter_7.html19" data-type="indexterm" id="id651"/></p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Random Forest Regression" data-type="sect2"><div class="sect2" id="id54">&#13;
<h2>Random Forest Regression</h2>&#13;
&#13;
<p><a contenteditable="false" data-primary="decision trees" data-secondary="random forest regression" data-type="indexterm" id="Chapter_7.html22"/><a contenteditable="false" data-primary="machine learning (for time series prediction)" data-secondary="random forest regression" data-type="indexterm" id="Chapter_7.html23"/><a contenteditable="false" data-primary="random forest regression" data-type="indexterm" id="Chapter_7.html24"/><a contenteditable="false" data-primary="time series prediction (machine learning for)" data-secondary="random forest regression" data-type="indexterm" id="Chapter_7.html25"/><em>Random forest</em> is a machine learning algorithm that harnesses the power of multiple decision trees to form a single output (prediction). <a contenteditable="false" data-primary="ensemble learning" data-type="indexterm" id="id652"/>It is flexible and does not require much tuning. It is also less prone to overfitting due to its ensemble learning <span class="keep-together">technique</span>. <em>Ensemble learning</em> refers to the combination of multiple learners (models) to improve the final prediction.</p>&#13;
&#13;
<p>With random forest, the multiple learners are different decision trees that converge toward a single prediction.</p>&#13;
&#13;
<p>Therefore, one of the hyperparameters that can be tuned in random forest algorithms is the number of decision trees. <a contenteditable="false" data-primary="bagging" data-type="indexterm" id="id653"/>The algorithm uses the bagging method. In the context of random forests, <em>bagging </em>refers to the technique of <em>bootstrap aggregating </em>that aims to improve the performance and robustness of machine learning models, such as decision trees, by reducing biases. Here’s how bagging works within the random forest algorithm:</p>&#13;
&#13;
<ol>&#13;
	<li>&#13;
	<p><em>Bootstrap sampling</em>: Random forest employs bootstrapping, which means creating multiple subsets of the original training data by sampling with replacement. Each subset has the same size as the original dataset but may contain duplicate instances and exclude some of them. This process is performed independently for each tree in the random forest.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p><em>Tree construction and feature selection</em>: For each bootstrap sample, a decision tree is constructed using a process called <em>recursive partitioning</em> where data is split based on features in order to create branches that optimize the separation of the target variables. At each node of the decision tree, a random subset of features is considered for splitting. This helps introduce diversity among the trees in the forest and prevents them from relying too heavily on a single dominant feature.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p><em>Ensemble prediction</em>: Once all the trees are constructed, predictions are made by aggregating the outputs of individual trees. For regression tasks, the predictions are averaged.</p>&#13;
	</li>&#13;
</ol>&#13;
&#13;
<p>To import the random forest regressor, use the following code:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="kn">import</code> <code class="n">RandomForestRegressor</code>&#13;
</pre>&#13;
&#13;
<p class="pagebreak-before">Now let’s look at the algorithm’s implementation:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="c1"># Fitting the model</code>&#13;
<code class="n">model</code> <code class="o">=</code> <code class="n">RandomForestRegressor</code><code class="p">(</code><code class="n">max_depth</code> <code class="o">=</code> <code class="mi">20</code><code class="p">,</code> <code class="n">random_state</code> <code class="o">=</code> <code class="mi">123</code><code class="p">)</code>&#13;
<code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">x_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>&#13;
<code class="c1"># Predicting in-sample</code>&#13;
<code class="n">y_predicted_train</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">x_train</code><code class="p">),</code> <code class="p">(</code><code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">))</code>&#13;
<code class="c1"># Predicting out-of-sample</code>&#13;
<code class="n">y_predicted</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">x_test</code><code class="p">),</code> <code class="p">(</code><code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">))</code>&#13;
</pre>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>The <code>max_depth</code> hyperparameter controls the depth of each decision tree in the random forest. A decision tree with a larger depth can capture more intricate patterns in the data, but it also becomes more prone to overfitting, which means it might perform very well on the training data but poorly on unseen data. On the other hand, a shallower tree might not capture all the details of the data but could generalize better to new, unseen data.</p>&#13;
</div>&#13;
&#13;
<p><a data-type="xref" href="#figure-7-9">Figure 7-9</a> shows the evolution of the forecasting task from the last values of <code>y_train</code> to the first values of <code>y_test</code> and <code>y_predicted</code>.</p>&#13;
&#13;
<figure><div class="figure" id="figure-7-9"><img alt="" class="iimagesdlf_0708png" src="assets/dlff_0709.png"/>&#13;
<h6><span class="label">Figure 7-9. </span>Training data followed by test data (dashed line) and the predicted data (thin line); the vertical dashed line represents the start of the test period. The model used is the random forest regression algorithm.</h6>&#13;
</div></figure>&#13;
&#13;
<p>The model’s results are as follows:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="n">Accuracy</code> <code class="n">Train</code> <code class="o">=</code>  <code class="mf">82.72</code> <code class="o">%</code>&#13;
<code class="n">Accuracy</code> <code class="n">Test</code> <code class="o">=</code>  <code class="mf">50.15</code> <code class="o">%</code>&#13;
<code class="n">RMSE</code> <code class="n">Train</code> <code class="o">=</code>  <code class="mf">0.0058106512</code>&#13;
<code class="n">RMSE</code> <code class="n">Test</code> <code class="o">=</code>  <code class="mf">0.0053452064</code>&#13;
<code class="n">Correlation</code> <code class="n">In</code><code class="o">-</code><code class="n">Sample</code> <code class="n">Predicted</code><code class="o">/</code><code class="n">Train</code> <code class="o">=</code>  <code class="mf">0.809</code>&#13;
<code class="n">Correlation</code> <code class="n">Out</code><code class="o">-</code><code class="n">of</code><code class="o">-</code><code class="n">Sample</code> <code class="n">Predicted</code><code class="o">/</code><code class="n">Test</code> <code class="o">=</code>  <code class="err">–</code><code class="mf">0.049</code>&#13;
<code class="n">Model</code> <code class="n">Bias</code> <code class="o">=</code>  <code class="mf">0.63</code>&#13;
</pre>&#13;
&#13;
<p>The advantages of random forest regression are as follows:</p>&#13;
&#13;
<ul>&#13;
	<li>It generally has accurate forecasts on data due to its ensemble nature. With financial time series being highly noisy and borderline random, its results need to be optimized nevertheless.</li>&#13;
	<li>It exhibits robustness to noise and outliers due to its averaging nature.</li>&#13;
</ul>&#13;
&#13;
<p>The disadvantages of random forest regression are as follows:</p>&#13;
&#13;
<ul>&#13;
	<li>It may be difficult to interpret from time to time. Since it uses an aggregating method, the true and final decision may be lost when a large number of trees is used.</li>&#13;
	<li>As the number of trees increases, the computational time of the algorithm takes more time to train, resulting in a slow process.<a contenteditable="false" data-primary="" data-startref="Chapter_7.html25" data-type="indexterm" id="id654"/><a contenteditable="false" data-primary="" data-startref="Chapter_7.html24" data-type="indexterm" id="id655"/><a contenteditable="false" data-primary="" data-startref="Chapter_7.html23" data-type="indexterm" id="id656"/><a contenteditable="false" data-primary="" data-startref="Chapter_7.html22" data-type="indexterm" id="id657"/></li>&#13;
</ul>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="AdaBoost Regression" data-type="sect2"><div class="sect2" id="id55">&#13;
<h2>AdaBoost Regression</h2>&#13;
&#13;
<p><a contenteditable="false" data-primary="AdaBoost regression" data-type="indexterm" id="Chapter_7.html26"/><a contenteditable="false" data-primary="boosting" data-secondary="AdaBoost regression" data-type="indexterm" id="Chapter_7.html27"/><a contenteditable="false" data-primary="machine learning (for time series prediction)" data-secondary="AdaBoost regression" data-type="indexterm" id="Chapter_7.html28"/><a contenteditable="false" data-primary="time series prediction (machine learning for)" data-secondary="AdaBoost regression" data-type="indexterm" id="Chapter_7.html29"/>Before understanding what AdaBoost is about, let’s discuss gradient boosting so that it becomes easier to comprehend the algorithm behind it. <em>Gradient boosting</em> is a technique to build models based on the idea of improving <em>weak learners</em> (which means models that perform only slightly better than random).</p>&#13;
&#13;
<p>The way to improve these weak learners is to target their weak spots by creating other weak learners that can handle the weak spots. This gave birth to what is known as <em>Adaptive Boosting</em>, or <em>AdaBoost </em>for short. Hence, in layperson’s terms, <em>boosting</em> is all about combining weak learners to form better models.</p>&#13;
&#13;
<p>The learners in AdaBoost (which, as discussed, are weak) are single-split decision trees (referred to as <em>stumps</em>). They are weighted, with more weight put on instances that are more difficult to classify and less weight put on the rest. At the same time, new learners are incorporated to be trained on the difficult parts, thus creating a more powerful model. Therefore, the difficult instances receive greater weights until they are solved by new weak learners.</p>&#13;
&#13;
<p>Predictions are based on votes from the weak learners. The majority rule is applied in order to maximize accuracy. Gradient boosting therefore can be summarized in <span class="keep-together">three steps:</span></p>&#13;
&#13;
<ol>&#13;
	<li>&#13;
	<p>It builds an ensemble of weak predictive models, typically decision trees, in a sequential manner.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Each subsequent model is built to correct the errors or residuals of the previous models using gradient descent, which adjusts the predictions to minimize overall error.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>The predictions from all the models are combined by taking a weighted average or sum, determined by the learning rate, to produce the final prediction.</p>&#13;
	</li>&#13;
</ol>&#13;
&#13;
<p>To import the AdaBoost regressor, use the following code:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="kn">import</code> <code class="n">AdaBoostRegressor</code></pre>&#13;
&#13;
<p>Now let’s look at the algorithm’s implementation:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="c1"># Fitting the model</code>&#13;
<code class="n">model</code> <code class="o">=</code> <code class="n">AdaBoostRegressor</code><code class="p">(</code><code class="n">random_state</code> <code class="o">=</code> <code class="mi">123</code><code class="p">)</code>&#13;
<code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">x_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>&#13;
<code class="c1"># Predicting in-sample</code>&#13;
<code class="n">y_predicted_train</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">x_train</code><code class="p">),</code> <code class="p">(</code><code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">))</code>&#13;
<code class="c1"># Predicting out-of-sample</code>&#13;
<code class="n">y_predicted</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">x_test</code><code class="p">),</code> <code class="p">(</code><code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">))</code>&#13;
</pre>&#13;
&#13;
<p><a data-type="xref" href="#figure-7-10">Figure 7-10</a> shows the evolution of the forecasting task from the last values of <code>y_train</code> to the first values of <code>y_test</code> and <code>y_predicted</code>.</p>&#13;
&#13;
<figure><div class="figure" id="figure-7-10"><img alt="" class="iimagesdlf_0709png" src="assets/dlff_0710.png"/>&#13;
<h6><span class="label">Figure 7-10. </span>Training data followed by test data (dashed line) and the predicted data (thin line); the vertical dashed line represents the start of the test period. The model used is the AdaBoost regression algorithm.</h6>&#13;
</div></figure>&#13;
&#13;
<p class="pagebreak-before">The model’s results are as follows:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="n">Accuracy</code> <code class="n">Train</code> <code class="o">=</code>  <code class="mf">53.27</code> <code class="o">%</code>&#13;
<code class="n">Accuracy</code> <code class="n">Test</code> <code class="o">=</code>  <code class="mf">51.7</code> <code class="o">%</code>&#13;
<code class="n">RMSE</code> <code class="n">Train</code> <code class="o">=</code>  <code class="mf">0.0070124217</code>&#13;
<code class="n">RMSE</code> <code class="n">Test</code> <code class="o">=</code>  <code class="mf">0.0053582343</code>&#13;
<code class="n">Correlation</code> <code class="n">In</code><code class="o">-</code><code class="n">Sample</code> <code class="n">Predicted</code><code class="o">/</code><code class="n">Train</code> <code class="o">=</code>  <code class="mf">0.461</code>&#13;
<code class="n">Correlation</code> <code class="n">Out</code><code class="o">-</code><code class="n">of</code><code class="o">-</code><code class="n">Sample</code> <code class="n">Predicted</code><code class="o">/</code><code class="n">Test</code> <code class="o">=</code>  <code class="mf">0.017</code>&#13;
<code class="n">Model</code> <code class="n">Bias</code> <code class="o">=</code>  <code class="mf">0.72</code>&#13;
</pre>&#13;
&#13;
<p>The advantages of AdaBoost are:</p>&#13;
&#13;
<ul>&#13;
	<li>It generally has good accuracy.</li>&#13;
	<li>It is easy to comprehend.</li>&#13;
</ul>&#13;
&#13;
<p>The disadvantages of AdaBoost are:</p>&#13;
&#13;
<ul>&#13;
	<li>It is impacted by outliers and sensitive to noise. </li>&#13;
	<li>It is slow and not optimized.<a contenteditable="false" data-primary="" data-startref="Chapter_7.html29" data-type="indexterm" id="id658"/><a contenteditable="false" data-primary="" data-startref="Chapter_7.html28" data-type="indexterm" id="id659"/><a contenteditable="false" data-primary="" data-startref="Chapter_7.html27" data-type="indexterm" id="id660"/><a contenteditable="false" data-primary="" data-startref="Chapter_7.html26" data-type="indexterm" id="id661"/></li>&#13;
</ul>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="XGBoost Regression" data-type="sect2"><div class="sect2" id="id56">&#13;
<h2>XGBoost Regression</h2>&#13;
&#13;
<p><a contenteditable="false" data-primary="boosting" data-secondary="XGBoost regression" data-type="indexterm" id="Chapter_7.html30"/><a contenteditable="false" data-primary="decision trees" data-secondary="XGBoost regression" data-type="indexterm" id="Chapter_7.html31"/><a contenteditable="false" data-primary="gradient boosting" data-type="indexterm" id="Chapter_7.html32"/><a contenteditable="false" data-primary="machine learning (for time series prediction)" data-secondary="XGBoost regression" data-type="indexterm" id="Chapter_7.html33"/><a contenteditable="false" data-primary="time series prediction (machine learning for)" data-secondary="XGBoost regression" data-type="indexterm" id="Chapter_7.html34"/><a contenteditable="false" data-primary="XGBoost regression" data-type="indexterm" id="Chapter_7.html35"/><em>XGBoost</em> is a fast and performant gradient-boosted decision tree algorithm. The name may be complicated, but the concept is not hard to understand if you understood gradient boosting from the previous section on AdaBoost. XGBoost stands for <em>extreme gradient boosting</em> and was created by Tianqi Chen. Here’s how it works:</p>&#13;
&#13;
<ol>&#13;
	<li>XGBoost starts with a simple base model, usually a decision tree.</li>&#13;
	<li>It defines an objective function that measures the performance of the model.</li>&#13;
	<li>Using gradient descent optimization, it iteratively improves the model’s predictions by adjusting the model based on the gradient of the objective function.</li>&#13;
	<li>New decision trees are added to the ensemble to correct errors made by previous models.</li>&#13;
	<li>Regularization techniques, such as learning rate and column subsampling, are employed to enhance performance and prevent fitting issues.</li>&#13;
	<li>The final prediction is obtained by combining the predictions from all the <span class="keep-together">models</span> in the ensemble.</li>&#13;
</ol>&#13;
&#13;
<p>The implementation of XGBoost in Python takes more steps than the previous algorithms. The first step is to <code>pip install</code> the required module. Type the following command in the prompt:<sup><a data-type="noteref" href="ch07.html#id662" id="id662-marker">4</a></sup></p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="n">pip</code> <code class="n">install</code> <code class="n">xgboost</code></pre>&#13;
&#13;
<p>To import the XGBoost library, use the following code:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">xgboost</code> <code class="kn">import</code> <code class="n">XGBRegressor</code></pre>&#13;
&#13;
<p>The implementation of the algorithm is as follows:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="c1"># Fitting the model</code>&#13;
<code class="n">model</code> <code class="o">=</code> <code class="n">XGBRegressor</code><code class="p">(</code><code class="n">random_state</code> <code class="o">=</code> <code class="mi">123</code><code class="p">,</code> <code class="n">n_estimators</code> <code class="o">=</code> <code class="mi">16</code><code class="p">,</code> &#13;
                     <code class="n">max_depth</code> <code class="o">=</code> <code class="mi">12</code><code class="p">)</code>&#13;
<code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">x_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>&#13;
<code class="c1"># Predicting in-sample</code>&#13;
<code class="n">y_predicted_train</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">x_train</code><code class="p">),</code> <code class="p">(</code><code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">))</code>&#13;
<code class="c1"># Predicting out-of-sample</code>&#13;
<code class="n">y_predicted</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">x_test</code><code class="p">),</code> <code class="p">(</code><code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">))</code>&#13;
</pre>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>The argument <code>n_estimators</code> is a hyperparameter that determines the number of boosting rounds or trees to be built in the ensemble. As the algorithm combines the predictions of multiple weak learners (individual decision trees) to create a strong predictive model, each boosting round (iteration) adds a new decision tree to the ensemble, and the algorithm learns from the mistakes made by previous trees. The <code>n_estimators</code> hyperparameter controls the maximum number of trees that will be added to the ensemble during the training process.</p>&#13;
</div>&#13;
&#13;
<p>AdaBoost and XGBoost are both boosting algorithms used to enhance the predictive power of weak learners, usually decision trees. AdaBoost focuses on iteratively emphasizing misclassified samples using exponential loss, lacks built-in regularization, and has limited parallelization. In contrast, XGBoost leverages gradient boosting, supports various loss functions, offers regularization, handles missing values, scales better through parallelization, provides comprehensive feature importance, and allows for more extensive hyperparameter tuning.</p>&#13;
&#13;
<p class="fix_tracking">XGBoost therefore offers more advanced features. It is often preferred for its overall better performance and ability to handle complex tasks. However, the choice between the two depends on the specific problem, dataset, and computational resources available.</p>&#13;
&#13;
<p><a data-type="xref" href="#figure-7-11">Figure 7-11</a> shows the evolution of the forecasting task from the last values of <code>y_train</code> to the first values of <code>y_test</code> and <code>y_predicted</code>.</p>&#13;
&#13;
<figure><div class="figure" id="figure-7-11"><img alt="" class="iimagesdlf_0710png" src="assets/dlff_0711.png"/>&#13;
<h6><span class="label">Figure 7-11. </span>Training data followed by test data (dashed line) and the predicted data (thin line); the vertical dashed line represents the start of the test period. The model used is the XGBoost regression algorithm.</h6>&#13;
</div></figure>&#13;
&#13;
<p>The model’s results are as follows<a contenteditable="false" data-primary="" data-startref="Chapter_7.html35" data-type="indexterm" id="id663"/><a contenteditable="false" data-primary="" data-startref="Chapter_7.html34" data-type="indexterm" id="id664"/><a contenteditable="false" data-primary="" data-startref="Chapter_7.html33" data-type="indexterm" id="id665"/><a contenteditable="false" data-primary="" data-startref="Chapter_7.html32" data-type="indexterm" id="id666"/><a contenteditable="false" data-primary="" data-startref="Chapter_7.html31" data-type="indexterm" id="id667"/><a contenteditable="false" data-primary="" data-startref="Chapter_7.html30" data-type="indexterm" id="id668"/>:<a contenteditable="false" data-primary="" data-startref="Chapter_7.html6" data-type="indexterm" id="id669"/><a contenteditable="false" data-primary="" data-startref="Chapter_7.html5" data-type="indexterm" id="id670"/></p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="n">Accuracy</code> <code class="n">Train</code> <code class="o">=</code>  <code class="mf">75.77</code> <code class="o">%</code>&#13;
<code class="n">Accuracy</code> <code class="n">Test</code> <code class="o">=</code>  <code class="mf">53.04</code> <code class="o">%</code>&#13;
<code class="n">RMSE</code> <code class="n">Train</code> <code class="o">=</code>  <code class="mf">0.0042354698</code>&#13;
<code class="n">RMSE</code> <code class="n">Test</code> <code class="o">=</code>  <code class="mf">0.0056622704</code>&#13;
<code class="n">Correlation</code> <code class="n">In</code><code class="o">-</code><code class="n">Sample</code> <code class="n">Predicted</code><code class="o">/</code><code class="n">Train</code> <code class="o">=</code>  <code class="mf">0.923</code>&#13;
<code class="n">Correlation</code> <code class="n">Out</code><code class="o">-</code><code class="n">of</code><code class="o">-</code><code class="n">Sample</code> <code class="n">Predicted</code><code class="o">/</code><code class="n">Test</code> <code class="o">=</code>  <code class="mf">0.05</code>&#13;
<code class="n">Model</code> <code class="n">Bias</code> <code class="o">=</code>  <code class="mf">6.8</code>&#13;
</pre>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Overfitting and Underfitting" data-type="sect1"><div class="sect1" id="overfit_and_underfit">&#13;
<h1>Overfitting and Underfitting</h1>&#13;
&#13;
<p><a contenteditable="false" data-primary="bias" data-secondary="in machine learning models" data-type="indexterm" id="Chapter_7.html36"/><a contenteditable="false" data-primary="fitting problem" data-type="indexterm" id="Chapter_7.html37"/><a contenteditable="false" data-primary="machine learning (for time series prediction)" data-secondary="overfitting and underfitting" data-type="indexterm" id="Chapter_7.html38"/><a contenteditable="false" data-primary="time series prediction (machine learning for)" data-secondary="overfitting and underfitting" data-type="indexterm" id="Chapter_7.html39"/><a contenteditable="false" data-primary="time series prediction (machine learning for)" data-secondary="overfitting and underfitting of machine learning model for" data-type="indexterm" id="Chapter_7.html40"/>Issues will arise in machine-based predictive analytics, and this is completely normal, since <em>perfection </em>is an impossible word in the world of data science (and finance). This section covers the most important issue when it comes to predicting data, and that is the <em>fitting problem</em>. Overfitting and underfitting are two terms that you must thoroughly understand so that you avoid their consequences when running your models.</p>&#13;
&#13;
<p><a contenteditable="false" data-primary="overfitting" data-type="indexterm" id="Chapter_7.html41"/><em>Overfitting </em>occurs when a model performs extremely well on the training data but has bad results on the test data. It is a sign that the model has learned not only the details of the in-sample data but also the noise that occurred. Overfitting is generally associated with a high variance and low bias model, but what do those two terms mean?</p>&#13;
&#13;
<p><em>Bias </em>refers to the difference between the expected value of the model’s predictions and the real value of the target variable. A low bias model is one that is complex enough to capture the underlying patterns in the data.</p>&#13;
&#13;
<p><a contenteditable="false" data-primary="variance" data-secondary="in machine learning models" data-type="indexterm" id="id671"/><em>Variance</em> refers to the variability of the model’s predictions for different training sets. A high variance model is one that is overly complex and can capture random noise and fluctuations in the training data. This can lead to overfitting, as the model may be fitting the noise in the data.</p>&#13;
&#13;
<p class="fix_tracking">To prevent overfitting, it’s important to strike a balance between bias and variance by selecting a model that is complex enough to capture the underlying patterns in the data but not so complex that it captures random noise and fluctuations in the data. Regularization techniques can also be used to reduce variance and prevent overfitting.</p>&#13;
&#13;
<p>Overfitting occurs for a number of reasons, notably:</p>&#13;
&#13;
<dl>&#13;
 <dt>Insufficient data</dt>&#13;
 <dd><p>If the training data is not diverse enough, or if there is not enough of it, the model may overfit to the training data.</p></dd>&#13;
 &#13;
 <dt>Overly complex model</dt>&#13;
 <dd><p>If the model is too complex, it may learn the noise in the data rather than the underlying patterns.</p></dd>&#13;
 &#13;
 <dt>Feature overload</dt>&#13;
 <dd><p>If the model is trained on too many features, it may learn irrelevant or noisy <span class="keep-together">features</span> that do not generalize to new data.</p></dd>&#13;
 &#13;
 <dt>Lack of regularization</dt>&#13;
 <dd><p>If the model is not regularized properly, it may overfit to the training data.</p></dd>&#13;
 &#13;
 <dt>Leakage</dt>&#13;
 <dd><p>Leakage occurs when information from the test set is inadvertently included in the training set. This can lead to overfitting as the model is learning from data that it will later see during testing.</p></dd>&#13;
</dl>&#13;
&#13;
&#13;
<p><a contenteditable="false" data-primary="underfitting" data-type="indexterm" id="id672"/>A high bias model is one that is overly simplified and cannot capture the true underlying patterns in the data. This can lead to underfitting. Similarly, a low variance model is one that is not affected much by small changes in the training data and can generalize well to new, unseen data.</p>&#13;
&#13;
<p>Underfitting occurs for a number of reasons, notably:</p>&#13;
&#13;
<dl>&#13;
 <dt>Insufficient model complexity</dt>&#13;
 <dd><p>If the model used is too simple to capture the underlying patterns in the data, it may result in underfitting. For example, a linear regression model might not be able to capture the nonlinear relationship between the features and the target variable.</p></dd>&#13;
 &#13;
 <dt>Insufficient training</dt>&#13;
 <dd><p>If the model is not trained for long enough, or with enough data, it may not be able to capture the underlying patterns in the data.</p></dd>&#13;
 &#13;
 <dt>Over-regularization</dt>&#13;
 <dd><p>Regularization is a technique used to prevent overfitting, but if it’s used excessively, it can lead to underfitting.</p></dd>&#13;
 &#13;
 <dt>Poor feature selection</dt>&#13;
 <dd><p>If the features selected for the model are not informative or relevant, the model may underfit.</p></dd>&#13;
</dl>&#13;
&#13;
&#13;
<p><a data-type="xref" href="#figure-7-12">Figure 7-12</a> shows a comparison between the different fits of a model to the data. An underfit model fails to capture the real relationship from the start, thus it is bad at predicting the past values and the future ones as well. A well-fit model captures the general tendency of the data. It is not an exact or a perfect model but one that generally has satisfactory predictions across the time period. An overfit model captures every detail of the past, even if it’s noise or random dislocations. The danger of an overfit model is that it inhibits a false promise of the future.</p>&#13;
&#13;
<figure><div class="figure" id="figure-7-12"><img alt="" class="iimagesdlf_0712png" src="assets/dlff_0712.png"/>&#13;
<h6><span class="label">Figure 7-12. </span>Different fitting situations.</h6>&#13;
</div></figure>&#13;
&#13;
<p>Therefore, when building machine learning models for time series prediction, you have to make sure you do not tune the parameters to perfectly fit the past values. To reduce fitting biases, make sure you incorporate the following best practices in <span class="keep-together">your backtests:</span></p>&#13;
&#13;
<dl>&#13;
	<dt>Increase training data</dt><dd><p>Collecting more training data helps to capture a broader range of patterns and variations in the data, reducing the chances of overfitting.</p>&#13;
	</dd>&#13;
	<dt>Feature selection</dt><dd><p>Carefully select relevant and informative features for your model. Removing irrelevant or redundant features reduces noise and complexity in the data, making it easier for the model to generalize well to unseen examples.</p>&#13;
	</dd>&#13;
	<dt>Regularization techniques</dt><dd><p>Regularization methods explicitly control the complexity of the model to prevent overfitting.</p>&#13;
	</dd>&#13;
	<dt>Hyperparameter tuning</dt><dd><p>Optimize the hyperparameters of your model to find the best configuration. Hyperparameters control the behavior and complexity of the model.</p>&#13;
	</dd>&#13;
	<dt>Ensemble methods</dt><dd><p>Employ ensemble methods, such as random forests, to combine predictions from multiple models. Ensemble methods can reduce overfitting by aggregating the predictions of multiple models, smoothing out individual model biases, and improving generalization.</p>&#13;
	</dd>&#13;
	<dt>Regular model evaluation</dt><dd><p>Regularly evaluate your model’s performance on unseen data or a dedicated validation set. This helps monitor the model’s generalization ability and detect any signs of overfitting or degradation in performance<a contenteditable="false" data-primary="" data-startref="Chapter_7.html41" data-type="indexterm" id="id673"/>.<a contenteditable="false" data-primary="" data-startref="Chapter_7.html40" data-type="indexterm" id="id674"/><a contenteditable="false" data-primary="" data-startref="Chapter_7.html39" data-type="indexterm" id="id675"/><a contenteditable="false" data-primary="" data-startref="Chapter_7.html38" data-type="indexterm" id="id676"/><a contenteditable="false" data-primary="" data-startref="Chapter_7.html37" data-type="indexterm" id="id677"/><a contenteditable="false" data-primary="" data-startref="Chapter_7.html36" data-type="indexterm" id="id678"/></p>&#13;
	</dd>&#13;
</dl>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="id58">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>By properly understanding where machine learning algorithms come from, it becomes simpler to interpret them and understand their limitations. This chapter gave you the required knowledge (theory and practice) to build time series models using a few known machine learning algorithms in the hopes of forecasting values using past values.</p>&#13;
&#13;
<p>What you must imperatively know is that past values are not necessarily indicative of future outcomes. Backtests are always biased somehow since a lot of tweaking is needed to tune the results, which may cause overfitting. Patterns do occur, but their results are not necessarily the same. Machine learning for financial time series prediction is constantly evolving, and most of the algorithms (in the raw form and with their basic inputs) are not very predictive, but with proper combination and the addition of risk management tools and filters, you may have a sustainable algorithm that adds value to your whole framework.<a contenteditable="false" data-primary="" data-startref="Chapter_7.html1" data-type="indexterm" id="id679"/><a contenteditable="false" data-primary="" data-startref="Chapter_7.html0" data-type="indexterm" id="id680"/></p>&#13;
</div></section>&#13;
<div data-type="footnotes"><p data-type="footnote" id="id600"><sup><a href="ch07.html#id600-marker">1</a></sup> Price differences will be referred to as returns for simplicity. In general, returns can also represent a <span class="keep-together">percentage</span> return of a time series.</p><p data-type="footnote" id="id626"><sup><a href="ch07.html#id626-marker">2</a></sup> The ordinary least squares method uses a mathematical formula to estimate the coefficients. It involves matrix algebra and calculus to solve for the coefficients that minimize the sum of squared residuals.</p><p data-type="footnote" id="id644"><sup><a href="ch07.html#id644-marker">3</a></sup> A class of statistical methods that do not rely on specific assumptions about the underlying <span class="keep-together">probability distribution.</span></p><p data-type="footnote" id="id662"><sup><a href="ch07.html#id662-marker">4</a></sup> The prompt is a command-line interface that can generally be accessed in the Start menu. It is not the same as the area where you type the Python code that will later be executed.</p></div></div></section></body></html>