["```py\nimport statsmodels.api as sm\nsm.tsa.seasonal_decompose(Y,freq=52).plot()\n```", "```py\nfrom statsmodels.tsa.arima_model import ARIMA\nmodel=ARIMA(endog=Y_train,order=[1,0,0])\n```", "```py\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\nfrom keras.layers import LSTM\n\ndef create_LSTMmodel(learn_rate = 0.01, momentum=0):\n       # create model\n   model = Sequential()\n   model.add(LSTM(50, input_shape=(X_train_LSTM.shape[1],\\\n      X_train_LSTM.shape[2])))\n   #More number of cells can be added if needed\n   model.add(Dense(1))\n   optimizer = SGD(lr=learn_rate, momentum=momentum)\n   model.compile(loss='mse', optimizer='adam')\n   return model\nLSTMModel = create_LSTMmodel(learn_rate = 0.01, momentum=0)\nLSTMModel_fit = LSTMModel.fit(X_train_LSTM, Y_train_LSTM, validation_data=\\\n  (X_test_LSTM, Y_test_LSTM),epochs=330, batch_size=72, verbose=0, shuffle=False)\n```", "```py\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.neural_network import MLPRegressor\n```", "```py\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2, f_regression\n```", "```py\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\nfrom keras.layers import LSTM\nfrom keras.wrappers.scikit_learn import KerasRegressor\n```", "```py\nfrom statsmodels.tsa.arima_model import ARIMA\nimport statsmodels.api as sm\n```", "```py\n# pandas, pandas_datareader, numpy and matplotlib\nimport numpy as np\nimport pandas as pd\nimport pandas_datareader.data as web\nfrom matplotlib import pyplot\nfrom pandas.plotting import scatter_matrix\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom pandas.plotting import scatter_matrix\nfrom statsmodels.graphics.tsaplots import plot_acf\n```", "```py\nstk_tickers = ['MSFT', 'IBM', 'GOOGL']\nccy_tickers = ['DEXJPUS', 'DEXUSUK']\nidx_tickers = ['SP500', 'DJIA', 'VIXCLS']\n\nstk_data = web.DataReader(stk_tickers, 'yahoo')\nccy_data = web.DataReader(ccy_tickers, 'fred')\nidx_data = web.DataReader(idx_tickers, 'fred')\n```", "```py\nreturn_period = 5\nY = np.log(stk_data.loc[:, ('Adj Close', 'MSFT')]).diff(return_period).\\\nshift(-return_period)\nY.name = Y.name[-1]+'_pred'\n\nX1 = np.log(stk_data.loc[:, ('Adj Close', ('GOOGL', 'IBM'))]).diff(return_period)\nX1.columns = X1.columns.droplevel()\nX2 = np.log(ccy_data).diff(return_period)\nX3 = np.log(idx_data).diff(return_period)\n\nX4 = pd.concat([np.log(stk_data.loc[:, ('Adj Close', 'MSFT')]).diff(i) \\\nfor i in [return_period, return_period*3,\\\nreturn_period*6, return_period*12]], axis=1).dropna()\nX4.columns = ['MSFT_DT', 'MSFT_3DT', 'MSFT_6DT', 'MSFT_12DT']\n\nX = pd.concat([X1, X2, X3, X4], axis=1)\n\ndataset = pd.concat([Y, X], axis=1).dropna().iloc[::return_period, :]\nY = dataset.loc[:, Y.name]\nX = dataset.loc[:, X.columns]\n```", "```py\ndataset.head()\n```", "```py\ncorrelation = dataset.corr()\npyplot.figure(figsize=(15,15))\npyplot.title('Correlation Matrix')\nsns.heatmap(correlation, vmax=1, square=True,annot=True,cmap='cubehelix')\n```", "```py\npyplot.figure(figsize=(15,15))\nscatter_matrix(dataset,figsize=(12,12))\npyplot.show()\n```", "```py\nres = sm.tsa.seasonal_decompose(Y,freq=52)\nfig = res.plot()\nfig.set_figheight(8)\nfig.set_figwidth(15)\npyplot.show()\n```", "```py\nvalidation_size = 0.2\ntrain_size = int(len(X) * (1-validation_size))\nX_train, X_test = X[0:train_size], X[train_size:len(X)]\nY_train, Y_test = Y[0:train_size], Y[train_size:len(X)]\n```", "```py\nnum_folds = 10\nscoring = 'neg_mean_squared_error'\n```", "```py\nmodels = []\nmodels.append(('LR', LinearRegression()))\nmodels.append(('LASSO', Lasso()))\nmodels.append(('EN', ElasticNet()))\nmodels.append(('KNN', KNeighborsRegressor()))\nmodels.append(('CART', DecisionTreeRegressor()))\nmodels.append(('SVR', SVR()))\n```", "```py\nmodels.append(('MLP', MLPRegressor()))\n```", "```py\n# Boosting methods\nmodels.append(('ABR', AdaBoostRegressor()))\nmodels.append(('GBR', GradientBoostingRegressor()))\n# Bagging methods\nmodels.append(('RFR', RandomForestRegressor()))\nmodels.append(('ETR', ExtraTreesRegressor()))\n```", "```py\nnames = []\nkfold_results = []\ntest_results = []\ntrain_results = []\nfor name, model in models:\n    names.append(name)\n    ## k-fold analysis:\n    kfold = KFold(n_splits=num_folds, random_state=seed)\n    #converted mean squared error to positive. The lower the better\n    cv_results = -1* cross_val_score(model, X_train, Y_train, cv=kfold, \\\n      scoring=scoring)\n    kfold_results.append(cv_results)\n    # Full Training period\n    res = model.fit(X_train, Y_train)\n    train_result = mean_squared_error(res.predict(X_train), Y_train)\n    train_results.append(train_result)\n    # Test results\n    test_result = mean_squared_error(res.predict(X_test), Y_test)\n    test_results.append(test_result)\n```", "```py\nfig = pyplot.figure()\nfig.suptitle('Algorithm Comparison: Kfold results')\nax = fig.add_subplot(111)\npyplot.boxplot(kfold_results)\nax.set_xticklabels(names)\nfig.set_size_inches(15,8)\npyplot.show()\n```", "```py\n# compare algorithms\nfig = pyplot.figure()\n\nind = np.arange(len(names))  # the x locations for the groups\nwidth = 0.35  # the width of the bars\n\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.bar(ind - width/2, train_results,  width=width, label='Train Error')\npyplot.bar(ind + width/2, test_results, width=width, label='Test Error')\nfig.set_size_inches(15,8)\npyplot.legend()\nax.set_xticks(ind)\nax.set_xticklabels(names)\npyplot.show()\n```", "```py\nX_train_ARIMA=X_train.loc[:, ['GOOGL', 'IBM', 'DEXJPUS', 'SP500', 'DJIA', \\\n'VIXCLS']]\nX_test_ARIMA=X_test.loc[:, ['GOOGL', 'IBM', 'DEXJPUS', 'SP500', 'DJIA', \\\n'VIXCLS']]\ntr_len = len(X_train_ARIMA)\nte_len = len(X_test_ARIMA)\nto_len = len (X)\n```", "```py\nmodelARIMA=ARIMA(endog=Y_train,exog=X_train_ARIMA,order=[1,0,0])\nmodel_fit = modelARIMA.fit()\n```", "```py\nerror_Training_ARIMA = mean_squared_error(Y_train, model_fit.fittedvalues)\npredicted = model_fit.predict(start = tr_len -1 ,end = to_len -1, \\\n  exog = X_test_ARIMA)[1:]\nerror_Test_ARIMA = mean_squared_error(Y_test,predicted)\nerror_Test_ARIMA\n```", "```py\n0.0005931919240399084\n```", "```py\nseq_len = 2 #Length of the seq for the LSTM\n\nY_train_LSTM, Y_test_LSTM = np.array(Y_train)[seq_len-1:], np.array(Y_test)\nX_train_LSTM = np.zeros((X_train.shape[0]+1-seq_len, seq_len, X_train.shape[1]))\nX_test_LSTM = np.zeros((X_test.shape[0], seq_len, X.shape[1]))\nfor i in range(seq_len):\n    X_train_LSTM[:, i, :] = np.array(X_train)[i:X_train.shape[0]+i+1-seq_len, :]\n    X_test_LSTM[:, i, :] = np.array(X)\\\n    [X_train.shape[0]+i-1:X.shape[0]+i+1-seq_len, :]\n```", "```py\n# LSTM Network\ndef create_LSTMmodel(learn_rate = 0.01, momentum=0):\n        # create model\n    model = Sequential()\n    model.add(LSTM(50, input_shape=(X_train_LSTM.shape[1],\\\n      X_train_LSTM.shape[2])))\n    #More cells can be added if needed\n    model.add(Dense(1))\n    optimizer = SGD(lr=learn_rate, momentum=momentum)\n    model.compile(loss='mse', optimizer='adam')\n    return model\nLSTMModel = create_LSTMmodel(learn_rate = 0.01, momentum=0)\nLSTMModel_fit = LSTMModel.fit(X_train_LSTM, Y_train_LSTM, \\\n  validation_data=(X_test_LSTM, Y_test_LSTM),\\\n  epochs=330, batch_size=72, verbose=0, shuffle=False)\n```", "```py\npyplot.plot(LSTMModel_fit.history['loss'], label='train', )\npyplot.plot(LSTMModel_fit.history['val_loss'], '--',label='test',)\npyplot.legend()\npyplot.show()\n```", "```py\nerror_Training_LSTM = mean_squared_error(Y_train_LSTM,\\\n  LSTMModel.predict(X_train_LSTM))\npredicted = LSTMModel.predict(X_test_LSTM)\nerror_Test_LSTM = mean_squared_error(Y_test,predicted)\n```", "```py\ntest_results.append(error_Test_ARIMA)\ntest_results.append(error_Test_LSTM)\n\ntrain_results.append(error_Training_ARIMA)\ntrain_results.append(error_Training_LSTM)\n\nnames.append(\"ARIMA\")\nnames.append(\"LSTM\")\n```", "```py\ndef evaluate_arima_model(arima_order):\n    #predicted = list()\n    modelARIMA=ARIMA(endog=Y_train,exog=X_train_ARIMA,order=arima_order)\n    model_fit = modelARIMA.fit()\n    error = mean_squared_error(Y_train, model_fit.fittedvalues)\n    return error\n\n# evaluate combinations of p, d and q values for an ARIMA model\ndef evaluate_models(p_values, d_values, q_values):\n    best_score, best_cfg = float(\"inf\"), None\n    for p in p_values:\n        for d in d_values:\n            for q in q_values:\n                order = (p,d,q)\n                try:\n                    mse = evaluate_arima_model(order)\n                    if mse < best_score:\n                        best_score, best_cfg = mse, order\n                    print('ARIMA%s MSE=%.7f' % (order,mse))\n                except:\n                    continue\n    print('Best ARIMA%s MSE=%.7f' % (best_cfg, best_score))\n\n# evaluate parameters\np_values = [0, 1, 2]\nd_values = range(0, 2)\nq_values = range(0, 2)\nwarnings.filterwarnings(\"ignore\")\nevaluate_models(p_values, d_values, q_values)\n```", "```py\nARIMA(0, 0, 0) MSE=0.0009879\nARIMA(0, 0, 1) MSE=0.0009721\nARIMA(1, 0, 0) MSE=0.0009696\nARIMA(1, 0, 1) MSE=0.0009685\nARIMA(2, 0, 0) MSE=0.0009684\nARIMA(2, 0, 1) MSE=0.0009683\nBest ARIMA(2, 0, 1) MSE=0.0009683\n```", "```py\n# prepare model\nmodelARIMA_tuned=ARIMA(endog=Y_train,exog=X_train_ARIMA,order=[2,0,1])\nmodel_fit_tuned = modelARIMA_tuned.fit()\n```", "```py\n# estimate accuracy on validation set\npredicted_tuned = model_fit.predict(start = tr_len -1 ,\\\n  end = to_len -1, exog = X_test_ARIMA)[1:]\nprint(mean_squared_error(Y_test,predicted_tuned))\n```", "```py\n0.0005970582461404503\n```", "```py\n# plotting the actual data versus predicted data\npredicted_tuned.index = Y_test.index\npyplot.plot(np.exp(Y_test).cumprod(), 'r', label='actual',)\n\n# plotting t, a separately\npyplot.plot(np.exp(predicted_tuned).cumprod(), 'b--', label='predicted')\npyplot.legend()\npyplot.rcParams[\"figure.figsize\"] = (8,5)\npyplot.show()\n```", "```py\ntrue_alpha = 0.1\ntrue_beta = 0.1\ntrue_sigma0 = 0.2\n```", "```py\nrisk_free_rate = 0.05\n```", "```py\ndef option_vol_from_surface(moneyness, time_to_maturity):\n    return true_sigma0 + true_alpha * time_to_maturity +\\\n     true_beta * np.square(moneyness - 1)\n\ndef call_option_price(moneyness, time_to_maturity, option_vol):\n    d1=(np.log(1/moneyness)+(risk_free_rate+np.square(option_vol))*\\\n    time_to_maturity)/ (option_vol*np.sqrt(time_to_maturity))\n    d2=(np.log(1/moneyness)+(risk_free_rate-np.square(option_vol))*\\\n    time_to_maturity)/(option_vol*np.sqrt(time_to_maturity))\n    N_d1 = norm.cdf(d1)\n    N_d2 = norm.cdf(d2)\n\n    return N_d1 - moneyness * np.exp(-risk_free_rate*time_to_maturity) * N_d2\n```", "```py\nN = 10000\n\nKs = 1+0.25*np.random.randn(N)\nTs = np.random.random(N)\nSigmas = np.array([option_vol_from_surface(k,t) for k,t in zip(Ks,Ts)])\nPs = np.array([call_option_price(k,t,sig) for k,t,sig in zip(Ks,Ts,Sigmas)])\n```", "```py\nY = Ps\nX = np.concatenate([Ks.reshape(-1,1), Ts.reshape(-1,1), Sigmas.reshape(-1,1)], \\\naxis=1)\n\ndataset = pd.DataFrame(np.concatenate([Y.reshape(-1,1), X], axis=1),\n                       columns=['Price', 'Moneyness', 'Time', 'Vol'])\n```", "```py\ndataset.head()\n```", "```py\npyplot.figure(figsize=(15,15))\nscatter_matrix(dataset,figsize=(12,12))\npyplot.show()\n```", "```py\nbestfeatures = SelectKBest(k='all', score_func=f_regression)\nfit = bestfeatures.fit(X,Y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(['Moneyness', 'Time', 'Vol'])\n#concat two dataframes for better visualization\nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']  #naming the dataframe columns\nfeatureScores.nlargest(10,'Score').set_index('Specs')\n```", "```py\nMoneyness : 30282.309\nVol : 2407.757\nTime : 1597.452\n```", "```py\nvalidation_size = 0.2\n\ntrain_size = int(len(X) * (1-validation_size))\nX_train, X_test = X[0:train_size], X[train_size:len(X)]\nY_train, Y_test = Y[0:train_size], Y[train_size:len(X)]\n```", "```py\nnum_folds = 10\nseed = 7\nscoring = 'neg_mean_squared_error'\n```", "```py\nmodels = []\nmodels.append(('LR', LinearRegression()))\nmodels.append(('KNN', KNeighborsRegressor()))\nmodels.append(('CART', DecisionTreeRegressor()))\nmodels.append(('SVR', SVR()))\n```", "```py\nmodels.append(('MLP', MLPRegressor()))\n```", "```py\n# Boosting methods\nmodels.append(('ABR', AdaBoostRegressor()))\nmodels.append(('GBR', GradientBoostingRegressor()))\n# Bagging methods\nmodels.append(('RFR', RandomForestRegressor()))\nmodels.append(('ETR', ExtraTreesRegressor()))\n```", "```py\n'''\nhidden_layer_sizes : tuple, length = n_layers - 2, default (100,)\n The ith element represents the number of neurons in the ith\n hidden layer.\n'''\nparam_grid={'hidden_layer_sizes': [(20,), (50,), (20,20), (20, 30, 20)]}\nmodel = MLPRegressor()\nkfold = KFold(n_splits=num_folds, random_state=seed)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, \\\n  cv=kfold)\ngrid_result = grid.fit(X_train, Y_train)\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n```", "```py\nBest: -0.000024 using {'hidden_layer_sizes': (20, 30, 20)}\n-0.000580 (0.000601) with: {'hidden_layer_sizes': (20,)}\n-0.000078 (0.000041) with: {'hidden_layer_sizes': (50,)}\n-0.000090 (0.000140) with: {'hidden_layer_sizes': (20, 20)}\n-0.000024 (0.000011) with: {'hidden_layer_sizes': (20, 30, 20)}\n```", "```py\n# prepare model\nmodel_tuned = MLPRegressor(hidden_layer_sizes=(20, 30, 20))\nmodel_tuned.fit(X_train, Y_train)\n```", "```py\n# estimate accuracy on validation set\n# transform the validation dataset\npredictions = model_tuned.predict(X_test)\nprint(mean_squared_error(Y_test, predictions))\n```", "```py\n3.08127276609567e-05\n```", "```py\nX = X[:, :2]\nvalidation_size = 0.2\ntrain_size = int(len(X) * (1-validation_size))\nX_train, X_test = X[0:train_size], X[train_size:len(X)]\nY_train, Y_test = Y[0:train_size], Y[train_size:len(X)]\n```", "```py\n# load dataset\ndataset = pd.read_excel('SCFP2009panel.xlsx')\n```", "```py\ndataset.shape\n```", "```py\n(19285, 515)\n```", "```py\n# Compute the risky assets and risk-free assets for 2007\ndataset['RiskFree07']= dataset['LIQ07'] + dataset['CDS07'] + dataset['SAVBND07']\\\n + dataset['CASHLI07']\ndataset['Risky07'] = dataset['NMMF07'] + dataset['STOCKS07'] + dataset['BOND07']\n\n# Compute the risky assets and risk-free assets for 2009\ndataset['RiskFree09']= dataset['LIQ09'] + dataset['CDS09'] + dataset['SAVBND09']\\\n+ dataset['CASHLI09']\ndataset['Risky09'] = dataset['NMMF09'] + dataset['STOCKS09'] + dataset['BOND09']\n\n# Compute the risk tolerance for 2007\ndataset['RT07'] = dataset['Risky07']/(dataset['Risky07']+dataset['RiskFree07'])\n\n#Average stock index for normalizing the risky assets in 2009\nAverage_SP500_2007=1478\nAverage_SP500_2009=948\n\n# Compute the risk tolerance for 2009\ndataset['RT09'] = dataset['Risky09']/(dataset['Risky09']+dataset['RiskFree09'])*\\\n                (Average_SP500_2009/Average_SP500_2007)\n```", "```py\ndataset.head()\n```", "```py\ndataset['PercentageChange'] = np.abs(dataset['RT09']/dataset['RT07']-1)\n```", "```py\n# Drop the rows containing NA\ndataset=dataset.dropna(axis=0)\n\ndataset=dataset[~dataset.isin([np.nan, np.inf, -np.inf]).any(1)]\n```", "```py\nsns.distplot(dataset['RT07'], hist=True, kde=False,\n             bins=int(180/5), color = 'blue',\n             hist_kws={'edgecolor':'black'})\n```", "```py\nsns.distplot(dataset['RT09'], hist=True, kde=False,\n             bins=int(180/5), color = 'blue',\n             hist_kws={'edgecolor':'black'})\n```", "```py\ndataset3 = dataset[dataset['PercentageChange']<=.1]\n```", "```py\ndataset3['TrueRiskTolerance'] = (dataset3['RT07'] + dataset3['RT09'])/2\n```", "```py\ndataset3.drop(labels=['RT07', 'RT09'], axis=1, inplace=True)\ndataset3.drop(labels=['PercentageChange'], axis=1, inplace=True)\n```", "```py\nkeep_list2 = ['AGE07','EDCL07','MARRIED07','KIDS07','OCCAT107','INCOME07',\\\n'RISK07','NETWORTH07','TrueRiskTolerance']\n\ndrop_list2 = [col for col in dataset3.columns if col not in keep_list2]\n\ndataset3.drop(labels=drop_list2, axis=1, inplace=True)\n```", "```py\n# correlation\ncorrelation = dataset3.corr()\nplt.figure(figsize=(15,15))\nplt.title('Correlation Matrix')\nsns.heatmap(correlation, vmax=1, square=True,annot=True,cmap='cubehelix')\n```", "```py\nY= dataset3[\"TrueRiskTolerance\"]\nX = dataset3.loc[:, dataset3.columns != 'TrueRiskTolerance']\nvalidation_size = 0.2\nseed = 3\nX_train, X_validation, Y_train, Y_validation = \\\ntrain_test_split(X, Y, test_size=validation_size, random_state=seed)\n```", "```py\nnum_folds = 10\nscoring = 'r2'\n```", "```py\n# spot-check the algorithms\nmodels = []\nmodels.append(('LR', LinearRegression()))\nmodels.append(('LASSO', Lasso()))\nmodels.append(('EN', ElasticNet()))\nmodels.append(('KNN', KNeighborsRegressor()))\nmodels.append(('CART', DecisionTreeRegressor()))\nmodels.append(('SVR', SVR()))\n#Ensemble Models\n# Boosting methods\nmodels.append(('ABR', AdaBoostRegressor()))\nmodels.append(('GBR', GradientBoostingRegressor()))\n# Bagging methods\nmodels.append(('RFR', RandomForestRegressor()))\nmodels.append(('ETR', ExtraTreesRegressor()))\n```", "```py\n# 8\\. Grid search : RandomForestRegressor\n'''\nn_estimators : integer, optional (default=10)\n The number of trees in the forest.\n'''\nparam_grid = {'n_estimators': [50,100,150,200,250,300,350,400]}\nmodel = RandomForestRegressor()\nkfold = KFold(n_splits=num_folds, random_state=seed)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, \\\n  cv=kfold)\ngrid_result = grid.fit(X_train, Y_train)\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\n```", "```py\nBest: 0.738632 using {'n_estimators': 250}\n```", "```py\nmodel = RandomForestRegressor(n_estimators = 250)\nmodel.fit(X_train, Y_train)\n```", "```py\nfrom sklearn.metrics import r2_score\npredictions_train = model.predict(X_train)\nprint(r2_score(Y_train, predictions_train))\n```", "```py\n0.9640632406817223\n```", "```py\npredictions = model.predict(X_validation)\nprint(mean_squared_error(Y_validation, predictions))\nprint(r2_score(Y_validation, predictions))\n```", "```py\n0.007781840953471237\n0.7614494526639909\n```", "```py\nimport pandas as pd\nimport numpy as np\nmodel = RandomForestRegressor(n_estimators= 200,n_jobs=-1)\nmodel.fit(X_train,Y_train)\n#use inbuilt class feature_importances of tree based classifiers\n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(10).plot(kind='barh')\nplt.show()\n```", "```py\n# Save Model Using Pickle\nfrom pickle import dump\nfrom pickle import load\n\n# save the model to disk\nfilename = 'finalized_model.sav'\ndump(model, open(filename, 'wb'))\n```", "```py\n# load the model from disk\nloaded_model = load(open(filename, 'rb'))\n# estimate accuracy on validation set\npredictions = loaded_model.predict(X_validation)\nresult = mean_squared_error(Y_validation, predictions)\nprint(r2_score(Y_validation, predictions))\nprint(result)\n```", "```py\n0.7683894847939692\n0.007555447734714956\n```", "```py\n# Get the data by webscraping using pandas datareader\ntsy_tickers = ['DGS1MO', 'DGS3MO', 'DGS1', 'DGS2', 'DGS5', 'DGS7', 'DGS10',\n               'DGS30',\n               'TREAST', # Treasury securities held by the Federal Reserve ($MM)\n               'FYGFDPUN', # Federal Debt Held by the Public ($MM)\n               'FDHBFIN', # Federal Debt Held by International Investors ($BN)\n               'GFDEBTN', # Federal Debt: Total Public Debt ($BN)\n               'BAA10Y', # Baa Corporate Bond Yield Relative to Yield on 10-Year\n              ]\ntsy_data = web.DataReader(tsy_tickers, 'fred').dropna(how='all').ffill()\ntsy_data['FDHBFIN'] = tsy_data['FDHBFIN'] * 1000\ntsy_data['GOV_PCT'] = tsy_data['TREAST'] / tsy_data['GFDEBTN']\ntsy_data['HOM_PCT'] = tsy_data['FYGFDPUN'] / tsy_data['GFDEBTN']\ntsy_data['FOR_PCT'] = tsy_data['FDHBFIN'] / tsy_data['GFDEBTN']\n```", "```py\ndataset.shape\n```", "```py\n(505, 15)\n```", "```py\nY.plot(style=['-','--',':'])\n```", "```py\n# Scatterplot Matrix\npyplot.figure(figsize=(15,15))\nscatter_matrix(dataset,figsize=(15,16))\npyplot.show()\n```", "```py\n'''\nhidden_layer_sizes : tuple, length = n_layers - 2, default (100,)\n The ith element represents the number of neurons in the ith\n hidden layer.\n'''\nparam_grid={'hidden_layer_sizes': [(20,), (50,), (20,20), (20, 30, 20)]}\nmodel = MLPRegressor()\nkfold = KFold(n_splits=num_folds, random_state=seed)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, \\\n  cv=kfold)\ngrid_result = grid.fit(X_train, Y_train)\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n```", "```py\nBest: -0.018006 using {'hidden_layer_sizes': (20, 30, 20)}\n-0.036433 (0.019326) with: {'hidden_layer_sizes': (20,)}\n-0.020793 (0.007075) with: {'hidden_layer_sizes': (50,)}\n-0.026638 (0.010154) with: {'hidden_layer_sizes': (20, 20)}\n-0.018006 (0.005637) with: {'hidden_layer_sizes': (20, 30, 20)}\n```"]