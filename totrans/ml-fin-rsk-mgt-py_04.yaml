- en: Chapter 3\. Deep Learning for Time Series Modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '...Yes, it is true that a Turing machine can compute any computable function
    given enough memory and enough time, but nature had to solve problems in real
    time. To do this, it made use of the brain’s neural networks that, like the most
    powerful computers on the planet, have massively parallel processors. Algorithms
    that run efficiently on them will eventually win out.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Terrence J. Sejnowski (2018)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Deep learning* has recently become a buzzword for some good reasons, although
    recent attempts to improve deep learning practices are not the first of their
    kind. However, it is quite understandable why deep learning has been appreciated
    for nearly two decades. Deep learning is an abstract concept, which makes it hard
    to define in few of words. Unlike a neural network (NN), deep learning has a more
    complex structure, and hidden layers define the complexity. Therefore, some researchers
    use the number of hidden layers as a comparison benchmark to distinguish a neural
    network from deep learning, a useful but not particularly rigorous way to make
    this distinction. A better definition can clarify the difference.'
  prefs: []
  type: TYPE_NORMAL
- en: 'At a high level, deep learning can be defined:'
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning methods are representation-learning^([1](ch03.html#idm45737247845584))
    methods with multiple levels of representation, obtained by composing simple but
    nonlinear modules that each transform the representation at one level (starting
    with the raw input) into a representation at a higher, slightly more abstract
    level.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Le Cunn et al. (2015)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Applications of deep learning date back to the 1940s, when *Cybernetics* by
    Norbert Wiener was published. Connectivist thinking then dominated between the
    1980s and 1990s. Recent developments in deep learning, such as backpropagation
    and neural networks, have created the field as we know it. Basically, there have
    been three waves of deep learning, so we might wonder why deep learning is in
    its heyday *now*? Goodfellow et al. (2016) list some plausible reasons, including:'
  prefs: []
  type: TYPE_NORMAL
- en: Increasing data sizes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increasing model sizes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increasing accuracy, complexity, and real-world impact
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It seems like modern technology and data availability have paved the way for
    an era of deep learning in which new data-driven methods are proposed so that
    we are able to model time series using unconventional models. This development
    has given rise to a new wave of deep learning. Two methods stand out in their
    ability to include longer time periods: the *recurrent neural network* (RNN) and
    *long short-term memory* (LSTM). In this section, we will concentrate on the practicality
    of these models in Python after briefly discussing the theoretical background.'
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An RNN has a neural network structure with at least one feedback connection
    so that the network can learn sequences. A feedback connection results in a loop,
    enabling us to unveil the nonlinear characteristics. This type of connection brings
    us a new and quite useful property: *memory*. Thus, an RNN can make use not only
    of the input data but also the previous outputs, which sounds compelling when
    it comes to time series modeling.'
  prefs: []
  type: TYPE_NORMAL
- en: 'RNNs come in many forms, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: One-to-one
  prefs: []
  type: TYPE_NORMAL
- en: A one-to-one RNN consists of a single input and a single output, which makes
    it the most basic type of RNN.
  prefs: []
  type: TYPE_NORMAL
- en: One-to-many
  prefs: []
  type: TYPE_NORMAL
- en: In this form, an RNN produces multiple outputs for a single input.
  prefs: []
  type: TYPE_NORMAL
- en: Many-to-one
  prefs: []
  type: TYPE_NORMAL
- en: As opposed to the one-to-many structure, many-to-one has multiple inputs for
    a single output.
  prefs: []
  type: TYPE_NORMAL
- en: Many-to-many
  prefs: []
  type: TYPE_NORMAL
- en: This structure has multiple inputs and outputs and is known as the most complicated
    structure for an RNN.
  prefs: []
  type: TYPE_NORMAL
- en: A hidden unit in an RNN feeds itself back into the neural network so that the
    RNN has recurrent layers (unlike a feed-forward neural network) making it a suitable
    method for modeling time series data. Therefore, in RNNs, activation of a neuron
    comes from a previous time-step indication that the RNN represents as an accumulating
    state of the network instance (Buduma and Locascio 2017).
  prefs: []
  type: TYPE_NORMAL
- en: 'As summarized by Nielsen (2019):'
  prefs: []
  type: TYPE_NORMAL
- en: RNNs have time steps one at a time in an orderly fashion.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The state of the network stays as it is from one time step to another.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An RNN updates its state based on the time step.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These dimensions are illustrated in [Figure 3-1](#RNN_dimensions). As can be
    seen, the RNN structure on the right-hand side has a time step, which is the main
    difference between it and the feed-forward network.
  prefs: []
  type: TYPE_NORMAL
- en: '![RNN structure](assets/mlfr_0301.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-1\. RNN structure^([2](ch03.html#idm45737247426656))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'RNNs have a three-dimensional input, comprised of:'
  prefs: []
  type: TYPE_NORMAL
- en: Batch size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time steps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Batch size* denotes the number of observations or number of rows of data.
    *Time steps* are the number of times to feed the model. Finally, *number of features*
    is the number of columns of each sample.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll start with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_deep_learning_for_time_series_modeling_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Defining the number of steps for prediction
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_deep_learning_for_time_series_modeling_CO1-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Defining the number of features as 1
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_deep_learning_for_time_series_modeling_CO1-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Calling a sequential model to run the RNN
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_deep_learning_for_time_series_modeling_CO1-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Identifying the number of hidden neurons, activation function, and input shape
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_deep_learning_for_time_series_modeling_CO1-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Adding a dropout layer to prevent overfitting
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](assets/6.png)](#co_deep_learning_for_time_series_modeling_CO1-6)'
  prefs: []
  type: TYPE_NORMAL
- en: Adding one more hidden layer with 256 neurons with the `relu` activation function
  prefs: []
  type: TYPE_NORMAL
- en: '[![7](assets/7.png)](#co_deep_learning_for_time_series_modeling_CO1-7)'
  prefs: []
  type: TYPE_NORMAL
- en: Flattening the model to transform the three-dimensional matrix into a vector
  prefs: []
  type: TYPE_NORMAL
- en: '[![8](assets/8.png)](#co_deep_learning_for_time_series_modeling_CO1-8)'
  prefs: []
  type: TYPE_NORMAL
- en: Adding an output layer with `linear` activation function
  prefs: []
  type: TYPE_NORMAL
- en: '[![9](assets/9.png)](#co_deep_learning_for_time_series_modeling_CO1-9)'
  prefs: []
  type: TYPE_NORMAL
- en: Compiling the RNN model
  prefs: []
  type: TYPE_NORMAL
- en: '[![10](assets/10.png)](#co_deep_learning_for_time_series_modeling_CO1-10)'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a dependent variable `y`
  prefs: []
  type: TYPE_NORMAL
- en: 'After configuring the model and generating a dependent variable, let’s extract
    the data and run the prediction for the stock prices for both Apple and Microsoft:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_deep_learning_for_time_series_modeling_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Calling the `split_sequence` function to define the lookback period
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_deep_learning_for_time_series_modeling_CO2-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Reshaping training data into a three-dimensional case
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_deep_learning_for_time_series_modeling_CO2-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Fitting the RNN model to Apple’s stock price
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_deep_learning_for_time_series_modeling_CO2-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Defining the starting point of the prediction for Apple
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_deep_learning_for_time_series_modeling_CO2-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Renaming the variable
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](assets/6.png)](#co_deep_learning_for_time_series_modeling_CO2-6)'
  prefs: []
  type: TYPE_NORMAL
- en: Creating an empty list to store predictions
  prefs: []
  type: TYPE_NORMAL
- en: '[![7](assets/7.png)](#co_deep_learning_for_time_series_modeling_CO2-7)'
  prefs: []
  type: TYPE_NORMAL
- en: Reshaping the `x_input`, which is used for prediction
  prefs: []
  type: TYPE_NORMAL
- en: '[![8](assets/8.png)](#co_deep_learning_for_time_series_modeling_CO2-8)'
  prefs: []
  type: TYPE_NORMAL
- en: Running prediction for Apple stock
  prefs: []
  type: TYPE_NORMAL
- en: '[![9](assets/9.png)](#co_deep_learning_for_time_series_modeling_CO2-9)'
  prefs: []
  type: TYPE_NORMAL
- en: Storing `yhat` into `tempList_aapl`
  prefs: []
  type: TYPE_NORMAL
- en: 'For the sake of visualization, the following code block is used, resulting
    in [Figure 3-2](#rnn):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 3-2](#rnn) shows the stock price prediction results for Apple and Microsoft.
    Simply eyeballing this, we can readily observe that there is room for improvement
    in terms of predictive performance of the model in both cases.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Even if we can have satisfactory predictive performance, the drawbacks of the
    RNN model should not be overlooked. The main drawbacks of the model are:'
  prefs: []
  type: TYPE_NORMAL
- en: The vanishing or exploding gradient problem (please see the following note for
    a detailed explanation).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training an RNN is a very difficult task as it requires a considerable amount
    of data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An RNN is unable to process very long sequences when the *tanh* activation function
    is used.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A vanishing gradient is a commonplace problem in deep learning scenarios that
    are not properly designed. The vanishing gradient problem arises if the gradient
    tends to get smaller as we conduct the backpropagation. It implies that neurons
    are learning so slowly that optimization grinds to a halt.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike the vanishing gradient problem, the exploding gradient problem occurs
    when small changes in the backpropagation results in huge updates to the weights
    during the optimization process.
  prefs: []
  type: TYPE_NORMAL
- en: '![RNN predictions](assets/mlfr_0302.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-2\. RNN prediction results
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The drawbacks of RNNs are well stated by Haviv et al. (2019):'
  prefs: []
  type: TYPE_NORMAL
- en: This is due to the dependency of the network on its past states, and through
    them on the entire input history. This ability comes with a cost—RNNs are known
    to be hard to train (Pascanu et al. 2013a). This difficulty is commonly associated
    with the vanishing gradient that appears when trying to propagate errors over
    long times (Hochreiter 1998). When training is successful, the network’s hidden
    state represents these memories. Understanding how such representation forms throughout
    training can open new avenues for improving learning of memory-related tasks.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Long-Short Term Memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The LSTM deep learning approach was developed by Hochreiter and Schmidhuber
    (1997) and is mainly based on the *gated recurrent unit* (GRU).
  prefs: []
  type: TYPE_NORMAL
- en: 'GRU was proposed to deal with the vanishing gradient problem, which is common
    in neural network structures and occurs when the weight update becomes too small
    to create a significant change in the network. GRU consists of two gates: *update*
    and *reset*. When an early observation is detected as highly important, then we
    do not update the hidden state. Similarly, when early observations are not significant,
    that leads to resetting the state.'
  prefs: []
  type: TYPE_NORMAL
- en: As previously discussed, one of the most appealing features of an RNN is its
    ability to connect past and present information. However, this ability turns out
    to be a failure when *long-term dependencies* comes into the picture. Long-term
    dependencies mean that the model learns from early observations.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, let’s examine the following sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Countries have their own currencies as in the USA, where people transact with
    dollars…*'
  prefs: []
  type: TYPE_NORMAL
- en: In the case of short-term dependencies, it is known that the next predicted
    word is about a currency, but what if it is asked *which* currency it’s about?
    Things get complicated because we might have mentioned various currencies earlier
    on in the text, implying long-term dependencies. It is necessary to go way back
    to find something relevant about the countries using dollars.
  prefs: []
  type: TYPE_NORMAL
- en: 'LSTM tries to attack the weakness of RNN regarding long-term dependencies.
    LSTM has a quite useful tool to get rid of the unnecessary information so that
    it works more efficiently. LSTM works with gates, enabling it to forget irrelevant
    data. These gates are:'
  prefs: []
  type: TYPE_NORMAL
- en: Forget gates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Input gates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Output gates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Forget gates are created to sort out the necessary and unnecessary information
    so that LSTM performs more efficiently than RNN. In doing so, the value of the
    activation function, *sigmoid*, becomes zero if the information is irrelevant.
    Forget gates can be formulated as:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi>F</mi> <mi>t</mi></msub> <mo>=</mo> <mi>σ</mi>
    <mrow><mo>(</mo> <msub><mi>X</mi> <mi>t</mi></msub> <msub><mi>W</mi> <mi>I</mi></msub>
    <mo>+</mo> <msub><mi>h</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <msub><mi>W</mi> <mi>f</mi></msub> <mo>+</mo> <msub><mi>b</mi> <mi>f</mi></msub>
    <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where <math alttext="sigma"><mi>σ</mi></math> is the activation function, <math
    alttext="h Subscript t minus 1"><msub><mi>h</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></math>
    is the previous hidden state, <math alttext="upper W Subscript upper I"><msub><mi>W</mi>
    <mi>I</mi></msub></math> and <math alttext="upper W Subscript f"><msub><mi>W</mi>
    <mi>f</mi></msub></math> are weights, and finally, <math alttext="b Subscript
    f"><msub><mi>b</mi> <mi>f</mi></msub></math> is the bias parameter in the forget
    cell.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input gates are fed by the current timestep, <math alttext="upper X Subscript
    t"><msub><mi>X</mi> <mi>t</mi></msub></math> , and the hidden state of the previous
    timestep, <math alttext="t minus 1"><mrow><mi>t</mi> <mo>-</mo> <mn>1</mn></mrow></math>
    . The goal of input gates is to determine the extent that information should be
    added to the long-term state. The input gate can be formulated like this:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi>I</mi> <mi>t</mi></msub> <mo>=</mo> <mi>σ</mi>
    <mrow><mo>(</mo> <msub><mi>X</mi> <mi>t</mi></msub> <msub><mi>W</mi> <mi>I</mi></msub>
    <mo>+</mo> <msub><mi>h</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <msub><mi>W</mi> <mi>f</mi></msub> <mo>+</mo> <msub><mi>b</mi> <mi>I</mi></msub>
    <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Output gates basically determine the extent of the output that should be read,
    and work as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi>O</mi> <mi>t</mi></msub> <mo>=</mo> <mi>σ</mi>
    <mrow><mo>(</mo> <msub><mi>X</mi> <mi>t</mi></msub> <msub><mi>W</mi> <mi>o</mi></msub>
    <mo>+</mo> <msub><mi>h</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <msub><mi>W</mi> <mi>o</mi></msub> <mo>+</mo> <msub><mi>b</mi> <mi>I</mi></msub>
    <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'These gates are not the sole components of LSTM. The other components are:'
  prefs: []
  type: TYPE_NORMAL
- en: Candidate memory cell
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory cell
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden state
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Candidate memory cell determines the extent to which information passes to
    the cell state. Differently, the activation function in the candidate cell is
    tanh and takes the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mover accent="true"><msub><mi>C</mi> <mi>t</mi></msub>
    <mo>^</mo></mover> <mo>=</mo> <mi>ϕ</mi> <mrow><mo>(</mo> <msub><mi>X</mi> <mi>t</mi></msub>
    <msub><mi>W</mi> <mi>c</mi></msub> <mo>+</mo> <msub><mi>h</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <msub><mi>W</mi> <mi>c</mi></msub> <mo>+</mo> <msub><mi>b</mi> <mi>c</mi></msub>
    <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Memory cell allows LSTM to remember or to forget the information:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi>C</mi> <mi>t</mi></msub> <mo>=</mo> <msub><mi>F</mi>
    <mi>t</mi></msub> <mo>⊙</mo> <mi>C</mi> <mo>+</mo> <mrow><mi>t</mi> <mo>-</mo>
    <mn>1</mn></mrow> <mo>+</mo> <msub><mi>I</mi> <mi>t</mi></msub> <mo>⊙</mo> <mover
    accent="true"><msub><mi>C</mi> <mi>t</mi></msub> <mo>^</mo></mover></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where <math alttext="circled-dot"><mo>⊙</mo></math> is Hadamard product.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this recurrent network, hidden state is a tool to circulate information.
    Memory cell relates output gate to hidden state:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi>h</mi> <mi>t</mi></msub> <mo>=</mo> <mi>ϕ</mi>
    <mrow><mo>(</mo> <msub><mi>c</mi> <mi>t</mi></msub> <mo>)</mo></mrow> <mo>⊙</mo>
    <msub><mi>O</mi> <mi>t</mi></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 3-3](#LSTM_structure) exhibits the LSTM structure.'
  prefs: []
  type: TYPE_NORMAL
- en: '![LSTM_structure](assets/mlfr_0303.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-3\. LSTM structure
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now, let’s predict the stock prices using LSTM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_deep_learning_for_time_series_modeling_CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Defining the number of steps for prediction
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_deep_learning_for_time_series_modeling_CO3-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Defining the number of feature as 1
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_deep_learning_for_time_series_modeling_CO3-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Identifying the number of hidden neurons, the activation function, which is
    `relu`, and input shape
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_deep_learning_for_time_series_modeling_CO3-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Adding a dropout layer to prevent overfitting
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_deep_learning_for_time_series_modeling_CO3-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Adding one more hidden layer with 256 neurons, with a `relu` activation function
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](assets/6.png)](#co_deep_learning_for_time_series_modeling_CO3-6)'
  prefs: []
  type: TYPE_NORMAL
- en: Flattening the model to vectorize the three-dimensional matrix
  prefs: []
  type: TYPE_NORMAL
- en: '[![7](assets/7.png)](#co_deep_learning_for_time_series_modeling_CO3-7)'
  prefs: []
  type: TYPE_NORMAL
- en: Adding an output layer with a `linear` activation function
  prefs: []
  type: TYPE_NORMAL
- en: '[![8](assets/8.png)](#co_deep_learning_for_time_series_modeling_CO3-8)'
  prefs: []
  type: TYPE_NORMAL
- en: Compiling LSTM with Root Mean Square Propagation, `rmsprop`, and mean squared
    error (MSE), `mean_squared_error`
  prefs: []
  type: TYPE_NORMAL
- en: '[![9](assets/9.png)](#co_deep_learning_for_time_series_modeling_CO3-9)'
  prefs: []
  type: TYPE_NORMAL
- en: Fitting the LSTM model to Apple’s stock price
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Root Mean Square Propagation (`RMSProp`) is an optimization method in which
    we calculate the moving average of the squared gradients for each weight. We then
    find the difference of weight, which is to be used to compute the new weight:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi>v</mi> <mi>t</mi></msub> <mo>=</mo> <msub><mi>ρ</mi>
    <msub><mi>v</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></msub> <mo>+</mo>
    <mrow><mn>1</mn> <mo>-</mo> <mi>ρ</mi></mrow> <msubsup><mi>g</mi> <mi>t</mi> <mn>2</mn></msubsup></mrow></math><math
    display="block"><mrow><mi>Δ</mi> <msub><mi>w</mi> <mi>t</mi></msub> <mo>=</mo>
    <mo>-</mo> <mfrac><mi>ν</mi> <msqrt><mrow><mi>η</mi><mo>+</mo><mi>ϵ</mi></mrow></msqrt></mfrac>
    <msub><mi>g</mi> <mi>t</mi></msub></mrow></math><math display="block"><mrow><msub><mi>w</mi>
    <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub> <mo>=</mo> <msub><mi>w</mi>
    <mi>t</mi></msub> <mo>+</mo> <mi>Δ</mi> <msub><mi>w</mi> <mi>t</mi></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Pursuing the same procedure and given the Microsoft stock price, a prediction
    analysis is carried out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code creates the plot ([Figure 3-4](#LSTM)) that shows the prediction
    results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: LSTM seems to outperform the RNN, particularly in the way it captures the extreme
    values better.
  prefs: []
  type: TYPE_NORMAL
- en: '![LSTM predictions](assets/mlfr_0304.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-4\. LSTM prediction results
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter was about predicting stock prices based on deep learning. The models
    used are RNN and LSTM, which have the ability to process longer time periods.
    These models do not suggest remarkable improvement but still can be employed to
    model time series data. LSTM considers, in our case, a 13-step lookback period
    for prediction. For an extension, it would be a wise approach to include multiple
    features in the models based on deep learning, which is not allowed in parametric
    time series models.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss volatility predictions based on parametric
    and ML models so that we can compare their performance.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Articles cited in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Ding, Daizong, et al. 2019\. “Modeling Extreme Events in Time Series Prediction.”
    *Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery
    and Data Mining*. 1114-1122.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Haviv, Doron, Alexander Rivkind, and Omri Barak. 2019\. “Understanding and Controlling
    Memory in Recurrent Neural Networks.” arXiv preprint. arXiv:1902.07275.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hochreiter, Sepp, and Jürgen Schmidhuber. 1997\. “Long Short-term Memory.”
    *Neural Computation* 9 (8): 1735-1780.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. 2015\. “Deep Learning.” *Nature*
    521, (7553): 436-444.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Books cited in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Buduma, N., and N. Locascio. 2017\. *Fundamentals of Deep Learning: Designing
    Next-generation Machine Intelligence Algorithms*. Sebastopol: O’Reilly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goodfellow, I., Y. Bengio, and A. Courville. 2016\. *Deep Learning*. Cambridge,
    MA: MIT Press.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nielsen, A. 2019\. *Practical Time Series Analysis: Prediction with Statistics
    and Machine Learning*. Sebastopol: O’Reilly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Patterson, Josh, and Adam Gibson. 2017\. *Deep Learning: A Practitioner’S Approach*.
    Sebastopol: O’Reilly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sejnowski, Terrence J. 2018\. *The Deep Learning Revolution*. Cambridge, MA:
    MIT Press.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ^([1](ch03.html#idm45737247845584-marker)) Representation learning helps us
    define a concept in a unique way. For instance, if the task is to detect whether
    something is a circle, then edges play a key role, as a circle has no edge. So
    using color, shape, and size, we can create a representation for an object. In
    essence, this is how the human brain works, and we know that deep learning structures
    are inspired by the brain’s functioning.
  prefs: []
  type: TYPE_NORMAL
- en: '^([2](ch03.html#idm45737247426656-marker)) Patterson et. al, 2017\. “Deep learning:
    A practitioner’s approach.”'
  prefs: []
  type: TYPE_NORMAL
