["```py\nimport numpy as np\nnan=np.nan # represents impossible actions\n#Array for transition probability\nP = np.array([ # shape=[s, a, s']\n[[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],\n[[0.0, 1.0, 0.0], [nan, nan, nan], [0.0, 0.0, 1.0]],\n[[nan, nan, nan], [0.8, 0.1, 0.1], [nan, nan, nan]],\n])\n\n# Array for the return\nR = np.array([ # shape=[s, a, s']\n[[50., 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]],\n[[50., 0.0, 0.0], [nan, nan, nan], [0.0, 0.0, -250.]],\n[[nan, nan, nan], [200., 0.0, 0.0], [nan, nan, nan]],\n])\n#Actions\nA = [[0, 1, 2], [0, 2], [1]]\n#The data already obtained from yahoo finance is imported.\n\n#Now let's run the Q-Value Iteration algorithm:\nQ = np.full((3, 3), -np.inf) # -inf for impossible actions\nfor state, actions in enumerate(A):\n    Q[state, actions] = 0.0 # Initial value = 0.0, for all possible actions\ndiscount_rate = 0.95\nn_iterations = 100\nfor iteration in range(n_iterations):\n    Q_prev = Q.copy()\n    for s in range(3):\n        for a in A[s]:\n            Q[s, a] = np.sum([\n                T[s, a, sp] * (R[s, a, sp] + discount_rate * np.max(Q_prev[sp]))\n        for sp in range(3)])\nprint(Q)\n```", "```py\n[[109.43230584 103.95749333  84.274035  ]\n [  5.5402017          -inf   5.83515676]\n [        -inf 269.30353051         -inf]]\n```", "```py\nimport keras\nfrom keras import layers, models, optimizers from keras import backend as K\nfrom collections import namedtuple, deque\nfrom keras.models import Sequential\nfrom keras.models import load_model\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam\n```", "```py\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pandas import read_csv, set_option\nimport datetime\nimport math\nfrom numpy.random import choice\nimport random\nfrom collections import deque\n```", "```py\ndataset = read_csv('data/SP500.csv', index_col=0)\n```", "```py\n# shape\ndataset.shape\n```", "```py\n(2515, 6)\n```", "```py\n# peek at data\nset_option('display.width', 100)\ndataset.head(5)\n```", "```py\n#Checking for any null values and removing the null values'''\nprint('Null Values =', dataset.isnull().values.any())\n```", "```py\nNull Values = False\n```", "```py\nX=list(dataset[\"Close\"])\nX=[float(x) for x in X]\nvalidation_size = 0.2\ntrain_size = int(len(X) * (1-validation_size))\nX_train, X_test = X[0:train_size], X[train_size:len(X)]\n```", "```py\nclass Agent:\n    def __init__(self, state_size, is_eval=False, model_name=\"\"):\n        self.state_size = state_size # normalized previous days\n        self.action_size = 3 # hold, buy, sell\n        self.memory = deque(maxlen=1000)\n        self.inventory = []\n        self.model_name = model_name\n        self.is_eval = is_eval\n\n        self.gamma = 0.95\n        self.epsilon = 1.0\n        self.epsilon_min = 0.01\n        self.epsilon_decay = 0.995\n\n        self.model = load_model(\"models/\" + model_name) if is_eval \\\n         else self._model()\n```", "```py\n    def _model(self):\n        model = Sequential()\n        model.add(Dense(units=64, input_dim=self.state_size, activation=\"relu\"))\n        model.add(Dense(units=32, activation=\"relu\"))\n        model.add(Dense(units=8, activation=\"relu\"))\n        model.add(Dense(self.action_size, activation=\"linear\"))\n        model.compile(loss=\"mse\", optimizer=Adam(lr=0.001))\n\n        return model\n```", "```py\n    def act(self, state):\n        if not self.is_eval and random.random() <= self.epsilon:\n            return random.randrange(self.action_size)\n\n        options = self.model.predict(state)\n        return np.argmax(options[0])\n```", "```py\n    def expReplay(self, batch_size):\n        mini_batch = []\n        l = len(self.memory)\n        #1: prepare replay memory\n        for i in range(l - batch_size + 1, l):\n            mini_batch.append(self.memory[i])\n\n        #2: Loop across the replay memory batch.\n        for state, action, reward, next_state, done in mini_batch:\n            target = reward # reward or Q at time t\n            #3: update the target for Q table. table equation\n            if not done:\n                target = reward + self.gamma * \\\n                 np.amax(self.model.predict(next_state)[0])\n            #set_trace()\n\n            # 4: Q-value of the state currently from the table\n            target_f = self.model.predict(state)\n            # 5: Update the output Q table for the given action in the table\n            target_f[0][action] = target\n            # 6\\. train and fit the model.\n            self.model.fit(state, target_f, epochs=1, verbose=0)\n\n        #7\\. Implement epsilon greedy algorithm\n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay\n```", "```py\ndef getState(data, t, n):\n    d = t - n + 1\n    block = data[d:t + 1] if d >= 0 else -d * [data[0]] + data[0:t + 1]\n    res = []\n    for i in range(n - 1):\n        res.append(sigmoid(block[i + 1] - block[i]))\n    return np.array([res])\n```", "```py\ndef plot_behavior(data_input, states_buy, states_sell, profit):\n    fig = plt.figure(figsize = (15, 5))\n    plt.plot(data_input, color='r', lw=2.)\n    plt.plot(data_input, '^', markersize=10, color='m', label='Buying signal',\\\n     markevery=states_buy)\n    plt.plot(data_input, 'v', markersize=10, color='k', label='Selling signal',\\\n     markevery = states_sell)\n    plt.title('Total gains: %f'%(profit))\n    plt.legend()\n    plt.show()\n```", "```py\nwindow_size = 1\nagent = Agent(window_size)\nl = len(data) - 1\nbatch_size = 10\nstates_sell = []\nstates_buy = []\nepisode_count = 3\n\nfor e in range(episode_count + 1):\n    print(\"Episode \" + str(e) + \"/\" + str(episode_count))\n    # 1-get state\n    state = getState(data, 0, window_size + 1)\n\n    total_profit = 0\n    agent.inventory = []\n\n    for t in range(l):\n        # 2-apply best action\n        action = agent.act(state)\n\n        # sit\n        next_state = getState(data, t + 1, window_size + 1)\n        reward = 0\n\n        if action == 1: # buy\n            agent.inventory.append(data[t])\n            states_buy.append(t)\n            print(\"Buy: \" + formatPrice(data[t]))\n\n        elif action == 2 and len(agent.inventory) > 0: # sell\n            bought_price = agent.inventory.pop(0)\n             #3: Get Reward\n\n            reward = max(data[t] - bought_price, 0)\n            total_profit += data[t] - bought_price\n            states_sell.append(t)\n            print(\"Sell: \" + formatPrice(data[t]) + \" | Profit: \" \\\n            + formatPrice(data[t] - bought_price))\n\n        done = True if t == l - 1 else False\n        # 4: get next state to be used in bellman's equation\n        next_state = getState(data, t + 1, window_size + 1)\n\n        # 5: add to the memory\n        agent.memory.append((state, action, reward, next_state, done))\n        state = next_state\n\n        if done:\n\n            print(\"--------------------------------\")\n            print(\"Total Profit: \" + formatPrice(total_profit))\n            print(\"--------------------------------\")\n\n        # 6: Run replay buffer function\n        if len(agent.memory) > batch_size:\n            agent.expReplay(batch_size)\n\n    if e % 10 == 0:\n        agent.model.save(\"models/model_ep\" + str(e))\n```", "```py\nRunning episode 0/10\nTotal Profit: $6738.87\n```", "```py\nRunning episode 1/10\nTotal Profit: â€“$45.07\n```", "```py\nRunning episode 9/10\nTotal Profit: $1971.54\n```", "```py\nRunning episode 10/10\nTotal Profit: $1926.84\n```", "```py\n#agent is already defined in the training set above.\ntest_data = X_test\nl_test = len(test_data) - 1\nstate = getState(test_data, 0, window_size + 1)\ntotal_profit = 0\nis_eval = True\ndone = False\nstates_sell_test = []\nstates_buy_test = []\nmodel_name = \"model_ep10\"\nagent = Agent(window_size, is_eval, model_name)\nstate = getState(data, 0, window_size + 1)\ntotal_profit = 0\nagent.inventory = []\n\nfor t in range(l_test):\n    action = agent.act(state)\n\n    next_state = getState(test_data, t + 1, window_size + 1)\n    reward = 0\n\n    if action == 1:\n\n        agent.inventory.append(test_data[t])\n        print(\"Buy: \" + formatPrice(test_data[t]))\n\n    elif action == 2 and len(agent.inventory) > 0:\n        bought_price = agent.inventory.pop(0)\n        reward = max(test_data[t] - bought_price, 0)\n        total_profit += test_data[t] - bought_price\n        print(\"Sell: \" + formatPrice(test_data[t]) + \" | profit: \" +\\\n         formatPrice(test_data[t] - bought_price))\n\n    if t == l_test - 1:\n        done = True\n    agent.memory.append((state, action, reward, next_state, done))\n    state = next_state\n\n    if done:\n        print(\"------------------------------------------\")\n        print(\"Total Profit: \" + formatPrice(total_profit))\n        print(\"------------------------------------------\")\n```", "```py\nTotal Profit: $1280.40\n```", "```py\ndef monte_carlo_paths(S_0, time_to_expiry, sigma, drift, seed, n_sims, \\\n  n_timesteps):\n    \"\"\"\n Create random paths of a stock price following a brownian geometric motion\n return:\n\n a (n_timesteps x n_sims x 1) matrix\n \"\"\"\n    if seed > 0:\n            np.random.seed(seed)\n    stdnorm_random_variates = np.random.randn(n_sims, n_timesteps)\n    S = S_0\n    dt = time_to_expiry / stdnorm_random_variates.shape[1]\n    r = drift\n    S_T = S * np.cumprod(np.exp((r-sigma**2/2)*dt+sigma*np.sqrt(dt)*\\\n    stdnorm_random_variates), axis=1)\n    return np.reshape(np.transpose(np.c_[np.ones(n_sims)*S_0, S_T]), \\\n    (n_timesteps+1, n_sims, 1))\n```", "```py\nS_0 = 100; K = 100; r = 0; vol = 0.2; T = 1/12\ntimesteps = 30; seed = 42; n_sims = 5000\n\n# Generate the monte carlo paths\npaths_train = monte_carlo_paths(S_0, T, vol, r, seed, n_sims, timesteps)\n```", "```py\n#Plot Paths for one simulation\nplt.figure(figsize=(20, 10))\nplt.plot(paths_train[1])\nplt.xlabel('Time Steps')\nplt.title('Stock Price Sample Paths')\nplt.show()\n```", "```py\nclass Agent(object):\n    def __init__(self, time_steps, batch_size, features,\\\n       nodes = [62, 46, 46, 1], name='model'):\n\n        #1\\. Initialize the variables\n        tf.reset_default_graph()\n        self.batch_size = batch_size # Number of options in a batch\n        self.S_t_input = tf.placeholder(tf.float32, [time_steps, batch_size, \\\n          features]) #Spot\n        self.K = tf.placeholder(tf.float32, batch_size) #Strike\n        self.alpha = tf.placeholder(tf.float32) #alpha for cVaR\n\n        S_T = self.S_t_input[-1,:,0] #Spot at time T\n        # Change in the Spot\n        dS = self.S_t_input[1:, :, 0] - self.S_t_input[0:-1, :, 0]\n        #dS = tf.reshape(dS, (time_steps, batch_size))\n\n        #2\\. Prepare S_t for use in the RNN remove the \\\n        #last time step (at T the portfolio is zero)\n        S_t = tf.unstack(self.S_t_input[:-1, :,:], axis=0)\n\n        # Build the lstm\n        lstm = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.LSTMCell(n) \\\n        for n in nodes])\n\n        #3\\. So the state is a convenient tensor that holds the last\n        #actual RNN state,ignoring the zeros.\n        #The strategy tensor holds the outputs of all cells.\n        self.strategy, state = tf.nn.static_rnn(lstm, S_t, initial_state=\\\n          lstm.zero_state(batch_size, tf.float32), dtype=tf.float32)\n\n        self.strategy = tf.reshape(self.strategy, (time_steps-1, batch_size))\n\n        #4\\. Option Price\n        self.option = tf.maximum(S_T-self.K, 0)\n\n        self.Hedging_PnL = - self.option + tf.reduce_sum(dS*self.strategy, \\\n          axis=0)\n\n        #5\\. Total Hedged PnL of each path\n        self.Hedging_PnL_Paths = - self.option + dS*self.strategy\n\n        # 6\\. Calculate the CVaR for a given confidence level alpha\n        # Take the 1-alpha largest losses (top 1-alpha negative PnLs)\n        #and calculate the mean\n        CVaR, idx = tf.nn.top_k(-self.Hedging_PnL, tf.cast((1-self.alpha)*\\\n        batch_size, tf.int32))\n        CVaR = tf.reduce_mean(CVaR)\n        #7\\. Minimize the CVaR\n        self.train = tf.train.AdamOptimizer().minimize(CVaR)\n        self.saver = tf.train.Saver()\n        self.modelname = name\n```", "```py\n    def _execute_graph_batchwise(self, paths, strikes, riskaversion, sess, \\\n      epochs=1, train_flag=False):\n        #1: Initialize the variables.\n        sample_size = paths.shape[1]\n        batch_size=self.batch_size\n        idx = np.arange(sample_size)\n        start = dt.datetime.now()\n        #2:Loop across all the epochs\n        for epoch in range(epochs):\n            # Save the hedging Pnl for each batch\n            pnls = []\n            strategies = []\n            if train_flag:\n                np.random.shuffle(idx)\n            #3\\. Loop across the observations\n            for i in range(int(sample_size/batch_size)):\n                indices = idx[i*batch_size : (i+1)*batch_size]\n                batch = paths[:,indices,:]\n\n                #4\\. Train the LSTM\n                if train_flag:#runs the train, hedging PnL and strategy.\n                    _, pnl, strategy = sess.run([self.train, self.Hedging_PnL, \\\n                      self.strategy], {self.S_t_input: batch,\\\n                        self.K : strikes[indices],\\\n                        self.alpha: riskaversion})\n                        #5\\. Evaluation and no training\n                else:\n                    pnl, strategy = sess.run([self.Hedging_PnL, self.strategy], \\\n                      {self.S_t_input: batch,\\\n                      self.K : strikes[indices],\n                      self.alpha: riskaversion})\\\n\n                pnls.append(pnl)\n                strategies.append(strategy)\n            #6\\. Calculate the option price # given the risk aversion level alpha\n\n            CVaR = np.mean(-np.sort(np.concatenate(pnls))\\\n            [:int((1-riskaversion)*sample_size)])\n            #7\\. Return training metrics, \\\n            #if it is in the training phase\n            if train_flag:\n                if epoch % 10 == 0:\n                    print('Time elapsed:', dt.datetime.now()-start)\n                    print('Epoch', epoch, 'CVaR', CVaR)\n                    #Saving the model\n                    self.saver.save(sess, \"model.ckpt\")\n        self.saver.save(sess, \"model.ckpt\")\n\n        #8\\. return CVaR and other parameters\n        return CVaR, np.concatenate(pnls), np.concatenate(strategies,axis=1)\n```", "```py\n    def training(self, paths, strikes, riskaversion, epochs, session, init=True):\n        if init:\n            sess.run(tf.global_variables_initializer())\n        self._execute_graph_batchwise(paths, strikes, riskaversion, session, \\\n          epochs, train_flag=True)\n\n    def predict(self, paths, strikes, riskaversion, session):\n        return self._execute_graph_batchwise(paths, strikes, riskaversion,\\\n          session,1, train_flag=False)\n\n    def restore(self, session, checkpoint):\n        self.saver.restore(session, checkpoint)\n```", "```py\nbatch_size = 1000\nfeatures = 1\nK = 100\nalpha = 0.50 #risk aversion parameter for CVaR\nepoch = 101 #It is set to 11, but should ideally be a high number\nmodel_1 = Agent(paths_train.shape[0], batch_size, features, name='rnn_final')\n# Training the model takes a few minutes\nstart = dt.datetime.now()\nwith tf.Session() as sess:\n    # Train Model\n    model_1.training(paths_train, np.ones(paths_train.shape[1])*K, alpha,\\\n     epoch, sess)\nprint('Training finished, Time elapsed:', dt.datetime.now()-start)\n```", "```py\nTime elapsed: 0:00:03.326560\nEpoch 0 CVaR 4.0718956\nEpoch 100 CVaR 2.853285\nTraining finished, Time elapsed: 0:01:56.299444\n```", "```py\ndef BS_d1(S, dt, r, sigma, K):\n    return (np.log(S/K) + (r+sigma**2/2)*dt) / (sigma*np.sqrt(dt))\n\ndef BlackScholes_price(S, T, r, sigma, K, t=0):\n    dt = T-t\n    Phi = stats.norm(loc=0, scale=1).cdf\n    d1 = BS_d1(S, dt, r, sigma, K)\n    d2 = d1 - sigma*np.sqrt(dt)\n    return S*Phi(d1) - K*np.exp(-r*dt)*Phi(d2)\n\ndef BS_delta(S, T, r, sigma, K, t=0):\n    dt = T-t\n    d1 = BS_d1(S, dt, r, sigma, K)\n    Phi = stats.norm(loc=0, scale=1).cdf\n    return Phi(d1)\n```", "```py\ndef test_hedging_strategy(deltas, paths, K, price, alpha, output=True):\n    S_returns = paths[1:,:,0]-paths[:-1,:,0]\n    hedge_pnl = np.sum(deltas * S_returns, axis=0)\n    option_payoff = np.maximum(paths[-1,:,0] - K, 0)\n    replication_portfolio_pnls = -option_payoff + hedge_pnl + price\n    mean_pnl = np.mean(replication_portfolio_pnls)\n    cvar_pnl = -np.mean(np.sort(replication_portfolio_pnls)\\\n    [:int((1-alpha)*replication_portfolio_pnls.shape[0])])\n    if output:\n        plt.hist(replication_portfolio_pnls)\n        print('BS price at t0:', price)\n        print('Mean Hedging PnL:', mean_pnl)\n        print('CVaR Hedging PnL:', cvar_pnl)\n    return (mean_pnl, cvar_pnl, hedge_pnl, replication_portfolio_pnls, deltas)\n\ndef plot_deltas(paths, deltas_bs, deltas_rnn, times=[0, 1, 5, 10, 15, 29]):\n    fig = plt.figure(figsize=(10,6))\n    for i, t in enumerate(times):\n        plt.subplot(2,3,i+1)\n        xs =  paths[t,:,0]\n        ys_bs = deltas_bs[t,:]\n        ys_rnn = deltas_rnn[t,:]\n        df = pd.DataFrame([xs, ys_bs, ys_rnn]).T\n\n        plt.plot(df[0], df[1], df[0], df[2], linestyle='', marker='x' )\n        plt.legend(['BS delta', 'RNN Delta'])\n        plt.title('Delta at Time %i' % t)\n        plt.xlabel('Spot')\n        plt.ylabel('$\\Delta$')\n    plt.tight_layout()\n\ndef plot_strategy_pnl(portfolio_pnl_bs, portfolio_pnl_rnn):\n    fig = plt.figure(figsize=(10,6))\n    sns.boxplot(x=['Black-Scholes', 'RNN-LSTM-v1 '], y=[portfolio_pnl_bs, \\\n    portfolio_pnl_rnn])\n    plt.title('Compare PnL Replication Strategy')\n    plt.ylabel('PnL')\n```", "```py\ndef black_scholes_hedge_strategy(S_0, K, r, vol, T, paths, alpha, output):\n    bs_price = BlackScholes_price(S_0, T, r, vol, K, 0)\n    times = np.zeros(paths.shape[0])\n    times[1:] = T / (paths.shape[0]-1)\n    times = np.cumsum(times)\n    bs_deltas = np.zeros((paths.shape[0]-1, paths.shape[1]))\n    for i in range(paths.shape[0]-1):\n        t = times[i]\n        bs_deltas[i,:] = BS_delta(paths[i,:,0], T, r, vol, K, t)\n    return test_hedging_strategy(bs_deltas, paths, K, bs_price, alpha, output)\n```", "```py\nn_sims_test = 1000\n# Monte Carlo Path for the test set\nalpha = 0.99\npaths_test =  monte_carlo_paths(S_0, T, vol, r, seed_test, n_sims_test, \\\n  timesteps)\n```", "```py\nwith tf.Session() as sess:\n    model_1.restore(sess, 'model.ckpt')\n    #Using the model_1 trained in the section above\n    test1_results = model_1.predict(paths_test, np.ones(paths_test.shape[1])*K, \\\n    alpha, sess)\n\n_,_,_,portfolio_pnl_bs, deltas_bs = black_scholes_hedge_strategy\\\n(S_0,K, r, vol, T, paths_test, alpha, True)\nplt.figure()\n_,_,_,portfolio_pnl_rnn, deltas_rnn = test_hedging_strategy\\\n(test1_results[2], paths_test, K, 2.302974467802428, alpha, True)\nplot_deltas(paths_test, deltas_bs, deltas_rnn)\nplot_strategy_pnl(portfolio_pnl_bs, portfolio_pnl_rnn)\n```", "```py\nBS price at t0: 2.3029744678024286\nMean Hedging PnL: -0.0010458505607415178\nCVaR Hedging PnL: 1.2447953011695538\nRL based BS price at t0: 2.302974467802428\nRL based Mean Hedging PnL: -0.0019250998451393934\nRL based CVaR Hedging PnL: 1.3832611348053374\n```", "```py\nBS price at t0: 10.07339936955367\nMean Hedging PnL: 0.0007508571761945107\nCVaR Hedging PnL: 0.6977526775080665\nRL based BS price at t0: 10.073\nRL based Mean Hedging PnL: -0.038571546628968216\nRL based CVaR Hedging PnL: 3.4732447615593975\n```", "```py\nBS price at t0: 2.3029744678024286\nMean Hedging PnL: -0.01723902964827388\nCVaR Hedging PnL: 1.2141220199385756\nRL based BS price at t0: 2.3029\nRL based Mean Hedging PnL: -0.037668804359885316\nRL based CVaR Hedging PnL: 1.357201635552361\n```", "```py\nBS price at t0: 2.3029744678024286\nMean Hedging PnL: -0.5787493248269506\nCVaR Hedging PnL: 2.5583922824407566\nRL based BS price at t0: 2.309\nRL based Mean Hedging PnL: -0.5735181045192523\nRL based CVaR Hedging PnL: 2.835487824499669\n```", "```py\ndataset = read_csv('data/crypto_portfolio.csv',index_col=0)\n```", "```py\n# shape\ndataset.shape\n```", "```py\n(375, 15)\n```", "```py\n# peek at data\nset_option('display.width', 100)\ndataset.head(5)\n```", "```py\nclass CryptoEnvironment:\n\n    def __init__(self, prices = './data/crypto_portfolio.csv', capital = 1e6):\n        self.prices = prices\n        self.capital = capital\n        self.data = self.load_data()\n\n    def load_data(self):\n        data =  pd.read_csv(self.prices)\n        try:\n            data.index = data['Date']\n            data = data.drop(columns = ['Date'])\n        except:\n            data.index = data['date']\n            data = data.drop(columns = ['date'])\n        return data\n\n    def preprocess_state(self, state):\n        return state\n\n    def get_state(self, t, lookback, is_cov_matrix=True\\\n       is_raw_time_series=False):\n\n        assert lookback <= t\n\n        decision_making_state = self.data.iloc[t-lookback:t]\n        decision_making_state = decision_making_state.pct_change().dropna()\n\n        if is_cov_matrix:\n            x = decision_making_state.cov()\n            return x\n        else:\n            if is_raw_time_series:\n                decision_making_state = self.data.iloc[t-lookback:t]\n            return self.preprocess_state(decision_making_state)\n\n    def get_reward(self, action, action_t, reward_t, alpha = 0.01):\n\n        def local_portfolio(returns, weights):\n            weights = np.array(weights)\n            rets = returns.mean() # * 252\n            covs = returns.cov() # * 252\n            P_ret = np.sum(rets * weights)\n            P_vol = np.sqrt(np.dot(weights.T, np.dot(covs, weights)))\n            P_sharpe = P_ret / P_vol\n            return np.array([P_ret, P_vol, P_sharpe])\n\n        data_period = self.data[action_t:reward_t]\n        weights = action\n        returns = data_period.pct_change().dropna()\n\n        sharpe = local_portfolio(returns, weights)[-1]\n        sharpe = np.array([sharpe] * len(self.data.columns))\n        ret = (data_period.values[-1] - data_period.values[0]) / \\\n        data_period.values[0]\n\n        return np.dot(returns, weights), ret\n```", "```py\nN_ASSETS = 15\nagent = Agent(N_ASSETS)\nenv = CryptoEnvironment()\nwindow_size = 180\nepisode_count = 50\nbatch_size = 32\nrebalance_period = 90\n```", "```py\nagent.is_eval = True\n\nactions_equal, actions_rl = [], []\nresult_equal, result_rl = [], []\n\nfor t in range(window_size, len(env.data), rebalance_period):\n\n    date1 = t-rebalance_period\n    s_ = env.get_state(t, window_size)\n    action = agent.act(s_)\n\n    weighted_returns, reward = env.get_reward(action[0], date1, t)\n    weighted_returns_equal, reward_equal = env.get_reward(\n        np.ones(agent.portfolio_size) / agent.portfolio_size, date1, t)\n\n    result_equal.append(weighted_returns_equal.tolist())\n    actions_equal.append(np.ones(agent.portfolio_size) / agent.portfolio_size)\n\n    result_rl.append(weighted_returns.tolist())\n    actions_rl.append(action[0])\n\nresult_equal_vis = [item for sublist in result_equal for item in sublist]\nresult_rl_vis = [item for sublist in result_rl for item in sublist]\n\nplt.figure()\nplt.plot(np.array(result_equal_vis).cumsum(), label = 'Benchmark', \\\ncolor = 'grey',ls = '--')\nplt.plot(np.array(result_rl_vis).cumsum(), label = 'Deep RL portfolio', \\\ncolor = 'black',ls = '-')\nplt.xlabel('Time Period')\nplt.ylabel('Cumulative Returnimage::images\\Chapter9-b82b2.png[]')\nplt.show()\n```", "```py\nimport statsmodels.api as sm\nfrom statsmodels import regression\ndef sharpe(R):\n    r = np.diff(R)\n    sr = r.mean()/r.std() * np.sqrt(252)\n    return sr\n\ndef print_stats(result, benchmark):\n\n    sharpe_ratio = sharpe(np.array(result).cumsum())\n    returns = np.mean(np.array(result))\n    volatility = np.std(np.array(result))\n\n    X = benchmark\n    y = result\n    x = sm.add_constant(X)\n    model = regression.linear_model.OLS(y, x).fit()\n    alpha = model.params[0]\n    beta = model.params[1]\n\n    return np.round(np.array([returns, volatility, sharpe_ratio, \\\n      alpha, beta]), 4).tolist()\n```", "```py\nprint('EQUAL', print_stats(result_equal_vis, result_equal_vis))\nprint('RL AGENT', print_stats(result_rl_vis, result_equal_vis))\n```", "```py\nEQUAL [-0.0013, 0.0468, -0.5016, 0.0, 1.0]\nRL AGENT [0.0004, 0.0231, 0.4445, 0.0002, -0.1202]\n```"]