<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 10. Natural Language Processing"><div class="chapter" id="Chapter10">
<h1><span class="label">Chapter 10. </span>Natural Language Processing</h1>


<p><a data-type="indexterm" data-primary="natural language processing (NLP)" id="ix_Chapter10-asciidoc0"/>Natural language processing (NLP) is a subfield of artificial intelligence used to aid computers in understanding natural human language. Most NLP techniques rely on machine learning to derive meaning from human languages. When text has been provided, the computer utilizes algorithms to extract meaning associated with every sentence and collect essential data from them. NLP manifests itself in different forms across many disciplines under various aliases, including (but not limited to) textual analysis, text mining, computational linguistics, and content analysis.</p>

<p><a data-type="indexterm" data-primary="Securities and Exchange Commission (SEC)" id="idm45174900305848"/>In the financial landscape, one of the earliest applications of NLP was implemented by the US Securities and Exchange Commission (SEC). <a data-type="indexterm" data-primary="accounting fraud, early use of NLP to detect" id="idm45174900304824"/>The group used text mining and natural language processing to detect accounting fraud. The ability of NLP algorithms to scan and analyze legal and other documents at a high speed provides banks and other financial institutions with enormous efficiency gains to help them meet compliance regulations and combat fraud.</p>

<p>In the investment process, uncovering investment insights requires not only domain knowledge of finance but also a strong grasp of data science and machine learning principles. NLP tools may help detect, measure, predict, and anticipate important market characteristics and indicators, such as market volatility, liquidity risks, financial stress, housing prices, and unemployment.</p>

<p>News has always been a key factor in investment decisions. It is well established that company-specific, macroeconomic, and political news strongly influence the financial markets. As technology advances, and market participants become more connected, the volume and frequency of news will continue to grow rapidly. Even today, the volume of daily text data being produced presents an untenable task for even a large team of fundamental researchers to navigate. Fundamental analysis assisted by NLP techniques is now critical to unlock the complete picture of how experts and the masses feel about the market.</p>

<p><a data-type="indexterm" data-primary="qualitative data" id="idm45174900280616"/>In banks and other organizations, teams of analysts are dedicated to poring over, analyzing, and attempting to quantify qualitative data from news and SEC-mandated reporting. Automation using NLP is well suited in this context. NLP can provide in-depth support in the analysis and interpretation of various reports and documents. This reduces the strain that repetitive, low-value tasks put on human employees. It also provides a level of objectivity and consistency to otherwise subjective interpretations; mistakes from human error are lessened. NLP can also allow a company to garner insights that can be used to assess a creditor’s risk or gauge brand-related sentiment from content across the web.</p>

<p>With the rise in popularity of live chat software in banking and finance businesses, NLP-based chatbots are a natural evolution. The combination of robo-advisors with chatbots is expected to automate the entire process of wealth and portfolio 
<span class="keep-together">management</span>.</p>

<p>In this chapter, we present three NLP-based case studies that cover applications of NLP in algorithmic trading, chatbot creation, and document interpretation and automation. The case studies follow a standardized seven-step model development process presented in <a data-type="xref" href="ch02.xhtml#Chapter2">Chapter 2</a>. Key model steps for NLP-based problems are data preprocessing, feature representation, and inference. As such, these areas, along with the related concepts and Python-based examples, are outlined in this chapter.</p>

<p><a data-type="xref" href="#CaseStudy1NLP">“Case Study 1: NLP and Sentiment Analysis–Based Trading Strategies”</a> demonstrates the usage of sentiment analysis and word embedding for a trading strategy. This case study highlights key focus areas for implementing an NLP-based trading strategy.</p>

<p>In <a data-type="xref" href="#CaseStudy2NLP">“Case Study 2: Chatbot Digital Assistant”</a>, we create a chatbot and demonstrate how NLP enables chatbots to understand messages and respond appropriately. We leverage Python-based packages and modules to develop a chatbot in a few lines of code.</p>

<p><a data-type="xref" href="#CaseStudy3NLP">“Case Study 3: Document Summarization”</a> illustrates the use of an NLP-based <em>topic modeling</em> technique to discover hidden topics or themes across documents. The purpose of this case study is to demonstrate the usage of NLP to automatically summarize large collections of documents to facilitate organization and management, as well as search and recommendations.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45174900271432">
<h5/>
<p>In addition to the points mentioned above, this chapter will cover:</p>

<ul>
<li>
<p>How to perform NLP data preprocessing, including steps such as tokenization, part-of-speech (PoS) tagging, or named entity recognition, in a few lines of code.</p>
</li>
<li>
<p>How to use different supervised techniques, including LSTM, for sentiment 
<span class="keep-together">analysis</span>.</p>
</li>
<li>
<p>Understanding the main Python packages (i.e., NLTK, spaCy and TextBlob) and how to use them for several NLP-related tasks.</p>
</li>
<li>
<p>How to build a data preprocessing pipeline using the spaCy package.</p>
</li>
<li>
<p>How to use pretrained models, such as word2vec, for feature representation.</p>
</li>
<li>
<p>How to use models such as LDA for topic modeling.</p>
</li>
</ul>
</div></aside>
<div data-type="note" epub:type="note"><h1>This Chapter’s Code Repository</h1>
<p>The Python code for this chapter is included under the <a href="https://oreil.ly/J2FFn">Chapter 10 - Natural Language Processing</a> folder of the online GitHub repository for this chapter. For any new NLP-based case study, use the common template from the code repository and modify the elements specific to the case study. The templates are designed to run on the cloud (i.e., Kaggle, Google Colab, and AWS).</p>
</div>






<section data-type="sect1" data-pdf-bookmark="Natural Language Processing: Python Packages"><div class="sect1" id="idm45174900260744">
<h1>Natural Language Processing: Python Packages</h1>

<p><a data-type="indexterm" data-primary="natural language processing (NLP)" data-secondary="Python packages for" id="idm45174900259368"/>Python is one of the best options to build an NLP-based expert system, and a large variety of open source NLP libraries are available for Python programmers. These libraries and packages contain ready-to-use modules and functions to incorporate complex NLP steps and algorithms, making implementation fast, easy, and efficient.</p>

<p>In this section, we will describe three Python-based NLP libraries we’ve found to be the most useful and that we will be using in this chapter.</p>








<section data-type="sect2" data-pdf-bookmark="NLTK"><div class="sect2" id="idm45174900257288">
<h2>NLTK</h2>

<p><a data-type="indexterm" data-primary="natural language processing (NLP)" data-secondary="NLTK library" id="idm45174900256088"/><a data-type="indexterm" data-primary="NLTK (Natural Language Took Kit)" id="idm45174900255144"/><a href="https://www.nltk.org">NLTK</a> is the most famous Python NLP library, and it has led to incredible breakthroughs across several areas. Its modularized structure makes it excellent for learning and exploring NLP concepts. However, it has heavy functionality with a steep learning curve.</p>

<p>NLTK can be installed using the typical installation procedure. After installing NLTK, NLTK Data needs to be downloaded. The NLTK Data package includes a pretrained tokenizer <code>punkt</code> for English, which can be downloaded as well:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">import</code> <code class="nn">nltk</code>
<code class="kn">import</code> <code class="nn">nltk.data</code>
<code class="n">nltk</code><code class="o">.</code><code class="n">download</code><code class="p">(</code><code class="s">'punkt'</code><code class="p">)</code></pre>
</div></section>













<section data-type="sect2" data-pdf-bookmark="TextBlob"><div class="sect2" id="idm45174900248056">
<h2>TextBlob</h2>

<p><a data-type="indexterm" data-primary="natural language processing (NLP)" data-secondary="TextBlob library" id="idm45174900246248"/><a data-type="indexterm" data-primary="TextBlob" id="idm45174900245304"/><a href="https://oreil.ly/tABh4">TextBlob</a> is built on top of NLTK. This is one of the best libraries for fast prototyping or building applications with minimal performance 
<span class="keep-together">requirements</span>. TextBlob makes text processing simple by providing an intuitive interface to NLTK. TextBlob can be imported using the following command:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">from</code> <code class="nn">textblob</code> <code class="k">import</code> <code class="n">TextBlob</code></pre>
</div></section>













<section data-type="sect2" data-pdf-bookmark="spaCy"><div class="sect2" id="idm45174900219688">
<h2>spaCy</h2>

<p><a data-type="indexterm" data-primary="natural language processing (NLP)" data-secondary="spaCy library" id="idm45174900232008"/><a data-type="indexterm" data-primary="spaCy" id="idm45174900231176"/><a href="https://spacy.io">spaCy</a> is an NLP library designed to be fast, streamlined, and production-ready. Its philosophy is to present only one algorithm (the best one) for each purpose. We don’t have to make choices and can focus on being productive. spaCy uses its own pipeline to perform multiple preprocessing steps at the same time. We will demonstrate it in a subsequent section.</p>

<p>spaCy’s models can be installed as Python packages, just like any other module. To load a model, use <code>spacy.load</code> with the model’s shortcut link or package name or a path to the data directory:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">import</code> <code class="nn">spacy</code>
<code class="n">nlp</code> <code class="o">=</code> <code class="n">spacy</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="s">"en_core_web_lg"</code><code class="p">)</code></pre>

<p>In addition to these, there are a few other libraries, such as gensim, that we will explore for some of the examples in this chapter.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Natural Language Processing: Theory and Concepts"><div class="sect1" id="idm45174900133576">
<h1>Natural Language Processing: Theory and Concepts</h1>

<p><a data-type="indexterm" data-primary="natural language processing (NLP)" data-secondary="theory and concepts" id="ix_Chapter10-asciidoc1"/>As we have already established, NLP is a subfield of artificial intelligence concerned with programming computers to process textual data in order to gain useful insights. All NLP applications go through common sequential steps, which include some combination of preprocessing textual data and representing the text as predictive features before feeding them into a statistical inference algorithm.
<a data-type="xref" href="#StepsNLP">Figure 10-1</a> outlines the major steps in an NLP-based application.</p>

<figure><div id="StepsNLP" class="figure">
<img src="Images/mlbf_1001.png" alt="mlbf 1001" width="1390" height="493"/>
<h6><span class="label">Figure 10-1. </span>Natural language processing pipeline</h6>
</div></figure>

<p>The next section reviews these steps. For a thorough coverage of the topic, the reader is referred to <a class="orm:hideurl" href="https://www.oreilly.com/library/view/natural-language-processing/9780596803346"><em>Natural Language Processing with Python</em></a> by Steven Bird, Ewan Klein, and Edward Loper (O’Reilly).</p>








<section data-type="sect2" data-pdf-bookmark="1. Preprocessing"><div class="sect2" id="idm45174900104536">
<h2>1. Preprocessing</h2>

<p><a data-type="indexterm" data-primary="natural language processing (NLP)" data-secondary="preprocessing" id="ix_Chapter10-asciidoc2"/>There are usually multiple steps involved in preprocessing textual data for NLP. <a data-type="xref" href="#StepsNLP">Figure 10-1</a> shows the key components of the preprocessing steps for NLP. These are tokenization, stop words removal, stemming, lemmatization, PoS (part-of-speech) tagging, and NER (Name Entity Recognition).</p>










<section data-type="sect3" data-pdf-bookmark="1.1. Tokenization"><div class="sect3" id="idm45174900100680">
<h3>1.1. Tokenization</h3>

<p><a data-type="indexterm" data-primary="natural language processing (NLP)" data-secondary="tokenization" id="idm45174900099112"/><a data-type="indexterm" data-primary="tokenization" id="idm45174900098168"/><em>Tokenization</em> is the task of splitting a text into meaningful segments, called tokens. These segments could be words, punctuation, numbers, or other special characters that are the building blocks of a sentence. A set of predetermined rules allows us to effectively convert a sentence into a list of tokens. The following code snippets show sample word tokenization using the NLTK and TextBlob packages:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c">#Text to tokenize</code>
<code class="n">text</code> <code class="o">=</code> <code class="s">"This is a tokenize test"</code></pre>

<p><a data-type="indexterm" data-primary="NLTK (Natural Language Took Kit)" data-secondary="tokenizer" id="idm45174900094120"/>The NLTK data package includes a pretrained <code>Punkt</code> tokenizer for English, which was previously loaded:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">from</code> <code class="nn">nltk.tokenize</code> <code class="k">import</code> <code class="n">word_tokenize</code>
<code class="n">word_tokenize</code><code class="p">(</code><code class="n">text</code><code class="p">)</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">['This', 'is', 'a', 'tokenize', 'test']</pre>

<p>Let’s look at tokenization using TextBlob:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">TextBlob</code><code class="p">(</code><code class="n">text</code><code class="p">)</code><code class="o">.</code><code class="n">words</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">WordList(['This', 'is', 'a', 'tokenize', 'test'])</pre>
</div></section>













<section data-type="sect3" data-pdf-bookmark="1.2. Stop words removal"><div class="sect3" id="idm45174900100216">
<h3>1.2. Stop words removal</h3>

<p><a data-type="indexterm" data-primary="natural language processing (NLP)" data-secondary="stop words removal" id="idm45174900022264"/><a data-type="indexterm" data-primary="NLTK (Natural Language Took Kit)" data-secondary="stop words removal" id="idm45174900019640"/>At times, extremely common words that offer little value in modeling are excluded from the vocabulary. These words are called stop words. The code for removing stop words using the NLTK library is shown below:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">text</code> <code class="o">=</code> <code class="s">"S&amp;P and NASDAQ are the two most popular indices in US"</code>

<code class="kn">from</code> <code class="nn">nltk.corpus</code> <code class="k">import</code> <code class="n">stopwords</code>
<code class="kn">from</code> <code class="nn">nltk.tokenize</code> <code class="k">import</code> <code class="n">word_tokenize</code>
<code class="n">stop_words</code> <code class="o">=</code> <code class="nb">set</code><code class="p">(</code><code class="n">stopwords</code><code class="o">.</code><code class="n">words</code><code class="p">(</code><code class="s">'english'</code><code class="p">))</code>
<code class="n">text_tokens</code> <code class="o">=</code> <code class="n">word_tokenize</code><code class="p">(</code><code class="n">text</code><code class="p">)</code>
<code class="n">tokens_without_sw</code><code class="o">=</code> <code class="p">[</code><code class="n">word</code> <code class="k">for</code> <code class="n">word</code> <code class="ow">in</code> <code class="n">text_tokens</code> <code class="k">if</code> <code class="ow">not</code> <code class="n">word</code> <code class="ow">in</code> <code class="n">stop_words</code><code class="p">]</code>

<code class="nb">print</code><code class="p">(</code><code class="n">tokens_without_sw</code><code class="p">)</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">['S', '&amp;', 'P', 'NASDAQ', 'two', 'popular', 'indices', 'US']</pre>

<p>We first load the language model and store it in the stop words variable. The <code>stopwords.words('english')</code> is a set of default stop words for the English language model in NLTK. Next, we simply iterate through each word in the input text, and if the word exists in the stop word set of the NLTK language model, the word is removed. As we can see, stop words, such as <em>are</em> and <em>most</em>, are removed from the sentence.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="1.3. Stemming"><div class="sect3" id="idm45174899932312">
<h3>1.3. Stemming</h3>

<p><a data-type="indexterm" data-primary="natural language processing (NLP)" data-secondary="stemming" id="idm45174899930872"/><a data-type="indexterm" data-primary="stemming" id="idm45174899929704"/><em>Stemming</em> is the process of reducing inflected (or sometimes derived) words to their stem, base, or root form (generally a written word form). For example, if we were to stem the words <em>Stems</em>, <em>Stemming</em>, <em>Stemmed</em>, and <em>Stemitization</em>, the result would be a single word: <em>Stem</em>. <a data-type="indexterm" data-primary="NLTK (Natural Language Took Kit)" data-secondary="stemming code" id="idm45174899926280"/>The code for stemming using the NLTK library is shown here:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">text</code> <code class="o">=</code> <code class="s">"It's a Stemming testing"</code>

<code class="n">parsed_text</code> <code class="o">=</code> <code class="n">word_tokenize</code><code class="p">(</code><code class="n">text</code><code class="p">)</code>

<code class="c"># Initialize stemmer.</code>
<code class="kn">from</code> <code class="nn">nltk.stem.snowball</code> <code class="k">import</code> <code class="n">SnowballStemmer</code>
<code class="n">stemmer</code> <code class="o">=</code> <code class="n">SnowballStemmer</code><code class="p">(</code><code class="s">'english'</code><code class="p">)</code>

<code class="c"># Stem each word.</code>
<code class="p">[(</code><code class="n">word</code><code class="p">,</code> <code class="n">stemmer</code><code class="o">.</code><code class="n">stem</code><code class="p">(</code><code class="n">word</code><code class="p">))</code> <code class="k">for</code> <code class="n">i</code><code class="p">,</code> <code class="n">word</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">parsed_text</code><code class="p">)</code>
 <code class="k">if</code> <code class="n">word</code><code class="o">.</code><code class="n">lower</code><code class="p">()</code> <code class="o">!=</code> <code class="n">stemmer</code><code class="o">.</code><code class="n">stem</code><code class="p">(</code><code class="n">parsed_text</code><code class="p">[</code><code class="n">i</code><code class="p">])]</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">[('Stemming', 'stem'), ('testing', 'test')]</pre>
</div></section>













<section data-type="sect3" data-pdf-bookmark="1.4. Lemmatization"><div class="sect3" id="idm45174899836008">
<h3>1.4. Lemmatization</h3>

<p><a data-type="indexterm" data-primary="lemmatization" id="idm45174899834632"/><a data-type="indexterm" data-primary="natural language processing (NLP)" data-secondary="lemmatization" id="idm45174899833704"/>A slight variant of stemming is <em>lemmatization</em>. The major difference between the two processes is that stemming can often create nonexistent words, whereas lemmas are actual words. An example of lemmatization is <em>run</em> as a base form for words like <em>running</em> and <em>ran</em>, or that the words <em>better</em> and <em>good</em> are considered the same lemma. The code for lemmatization using the TextBlob library is shown below:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">text</code> <code class="o">=</code> <code class="s">"This world has a lot of faces "</code>

<code class="kn">from</code> <code class="nn">textblob</code> <code class="k">import</code> <code class="n">Word</code>
<code class="n">parsed_data</code><code class="o">=</code> <code class="n">TextBlob</code><code class="p">(</code><code class="n">text</code><code class="p">)</code><code class="o">.</code><code class="n">words</code>
<code class="p">[(</code><code class="n">word</code><code class="p">,</code> <code class="n">word</code><code class="o">.</code><code class="n">lemmatize</code><code class="p">())</code> <code class="k">for</code> <code class="n">i</code><code class="p">,</code> <code class="n">word</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">parsed_data</code><code class="p">)</code>
 <code class="k">if</code> <code class="n">word</code> <code class="o">!=</code> <code class="n">parsed_data</code><code class="p">[</code><code class="n">i</code><code class="p">]</code><code class="o">.</code><code class="n">lemmatize</code><code class="p">()]</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">[('has', 'ha'), ('faces', 'face')]</pre>
</div></section>













<section data-type="sect3" data-pdf-bookmark="1.5. PoS tagging"><div class="sect3" id="idm45174899757608">
<h3>1.5. PoS tagging</h3>

<p><a data-type="indexterm" data-primary="natural language processing (NLP)" data-secondary="PoS tagging" id="idm45174899756232"/><a data-type="indexterm" data-primary="part-of-speech (PoS) tagging" id="idm45174899755288"/><a data-type="indexterm" data-primary="PoS (part-of-speech) tagging" id="idm45174899754648"/><em>Part-of-speech (PoS) tagging</em> is the process of assigning a token to its grammatical category (e.g., verb, noun, etc.) in order to understand its role within a sentence. PoS tags have been used for a variety of NLP tasks and are extremely useful since they provide a linguistic signal of how a word is being used within the scope of a phrase, sentence, or document.</p>

<p>After a sentence is split into tokens, a tagger, or PoS tagger, is used to assign each token to a part-of-speech category. Historically, <a href="https://oreil.ly/OpuRm">hidden Markov models (HMM)</a> were used to create such taggers. More recently, artificial neural networks have been leveraged. The code for PoS tagging using the TextBlob library is shown here:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">text</code> <code class="o">=</code> <code class="s">'Google is looking at buying U.K. startup for $1 billion'</code>
<code class="n">TextBlob</code><code class="p">(</code><code class="n">text</code><code class="p">)</code><code class="o">.</code><code class="n">tags</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">[('Google', 'NNP'),
 ('is', 'VBZ'),
 ('looking', 'VBG'),
 ('at', 'IN'),
 ('buying', 'VBG'),
 ('U.K.', 'NNP'),
 ('startup', 'NN'),
 ('for', 'IN'),
 ('1', 'CD'),
 ('billion', 'CD')]</pre>
</div></section>













<section data-type="sect3" data-pdf-bookmark="1.6. Named entity recognition"><div class="sect3" id="idm45174899743288">
<h3>1.6. Named entity recognition</h3>

<p><a data-type="indexterm" data-primary="named entity recognition (NER)" id="idm45174899741944"/><a data-type="indexterm" data-primary="natural language processing (NLP)" data-secondary="named entity recognition (NER)" id="idm45174899741272"/><a data-type="indexterm" data-primary="NER (named entity recognition)" id="idm45174899740392"/><em>Named entity recognition</em> (NER) is an optional next step in data preprocessing that seeks to locate and classify named entities in text into predefined categories. These categories can include names of persons, organizations, locations, expressions of times, quantities, monetary values, or percentages. The NER performed using spaCy is shown below:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">text</code> <code class="o">=</code> <code class="s">'Google is looking at buying U.K. startup for $1 billion'</code>

<code class="k">for</code> <code class="n">entity</code> <code class="ow">in</code> <code class="n">nlp</code><code class="p">(</code><code class="n">text</code><code class="p">)</code><code class="o">.</code><code class="n">ents</code><code class="p">:</code>
    <code class="nb">print</code><code class="p">(</code><code class="s">"Entity: "</code><code class="p">,</code> <code class="n">entity</code><code class="o">.</code><code class="n">text</code><code class="p">)</code></pre>

<p class="pagebreak-before"><code>Output</code></p>

<pre data-type="programlisting">Entity:  Google
Entity:  U.K.
Entity:  $1 billion</pre>

<p>Visualizing named entities in text using the <code>displacy</code> module, as shown in <a data-type="xref" href="#NER">Figure 10-2</a>, can also be incredibly helpful in speeding up development and debugging the code and training process:</p>

<pre data-type="programlisting">from spacy import displacy
displacy.render(nlp(text), style="ent", jupyter = True)</pre>

<figure><div id="NER" class="figure">
<img src="Images/mlbf_1002.png" alt="mlbf 1002" width="1004" height="57"/>
<h6><span class="label">Figure 10-2. </span>NER output</h6>
</div></figure>












<section data-type="sect4" data-pdf-bookmark="1.7. spaCy: All of the above steps in one go"><div class="sect4" id="idm45174899657256">
<h4>1.7. spaCy: All of the above steps in one go</h4>

<p><a data-type="indexterm" data-primary="natural language processing (NLP)" data-secondary="spaCy library" id="ix_Chapter10-asciidoc3"/><a data-type="indexterm" data-primary="spaCy" id="ix_Chapter10-asciidoc4"/>All the preprocessing steps shown above can be performed in one step using spaCy. When we call <em>nlp</em> on a text, spaCy first tokenizes the text to produce a <em>Doc</em> object. The <em>Doc</em> is then processed in several different steps. <a data-type="indexterm" data-primary="processing pipeline" id="idm45174899651608"/>This is also referred to as the <em>processing pipeline</em>. The pipeline used by the default models consists of a <em>tagger</em>, a <em>parser</em>, and an <em>entity recognizer</em>. Each pipeline component returns the processed <em>Doc</em>, which is then passed on to the next component, as demonstrated in <a data-type="xref" href="#PreprocessingPipeline">Figure 10-3</a>.</p>

<figure><div id="PreprocessingPipeline" class="figure">
<img src="Images/mlbf_1003.png" alt="mlbf 1003" width="1311" height="228"/>
<h6><span class="label">Figure 10-3. </span>spaCy pipeline (based on an image from <a href="https://oreil.ly/ZhMlp">the spaCy website</a>.</h6>
</div></figure>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">Python</code> <code class="n">code</code> <code class="n">text</code> <code class="o">=</code> <code class="s">'Google is looking at buying U.K. startup for $1 billion'</code>
<code class="n">doc</code> <code class="o">=</code> <code class="n">nlp</code><code class="p">(</code><code class="n">text</code><code class="p">)</code>
<code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">([[</code><code class="n">t</code><code class="o">.</code><code class="n">text</code><code class="p">,</code> <code class="n">t</code><code class="o">.</code><code class="n">is_stop</code><code class="p">,</code> <code class="n">t</code><code class="o">.</code><code class="n">lemma_</code><code class="p">,</code> <code class="n">t</code><code class="o">.</code><code class="n">pos_</code><code class="p">]</code>
              <code class="k">for</code> <code class="n">t</code> <code class="ow">in</code> <code class="n">doc</code><code class="p">],</code>
             <code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s">'Token'</code><code class="p">,</code> <code class="s">'is_stop_word'</code><code class="p">,</code> <code class="s">'lemma'</code><code class="p">,</code> <code class="s">'POS'</code><code class="p">])</code></pre>

<p><code>Output</code></p>
<table>

<thead>
<tr>
<th/>
<th>Token</th>
<th>is_stop_word</th>
<th>lemma</th>
<th>POS</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>0</p></td>
<td><p>Google</p></td>
<td><p>False</p></td>
<td><p>Google</p></td>
<td><p>PROPN</p></td>
</tr>
<tr>
<td><p>1</p></td>
<td><p>is</p></td>
<td><p>True</p></td>
<td><p>be</p></td>
<td><p>VERB</p></td>
</tr>
<tr>
<td><p>2</p></td>
<td><p>looking</p></td>
<td><p>False</p></td>
<td><p>look</p></td>
<td><p>VERB</p></td>
</tr>
<tr>
<td><p>3</p></td>
<td><p>at</p></td>
<td><p>True</p></td>
<td><p>at</p></td>
<td><p>ADP</p></td>
</tr>
<tr>
<td><p>4</p></td>
<td><p>buying</p></td>
<td><p>False</p></td>
<td><p>buy</p></td>
<td><p>VERB</p></td>
</tr>
<tr>
<td><p>5</p></td>
<td><p>U.K.</p></td>
<td><p>False</p></td>
<td><p>U.K.</p></td>
<td><p>PROPN</p></td>
</tr>
<tr>
<td><p>6</p></td>
<td><p>startup</p></td>
<td><p>False</p></td>
<td><p>startup</p></td>
<td><p>NOUN</p></td>
</tr>
<tr>
<td><p>7</p></td>
<td><p>for</p></td>
<td><p>True</p></td>
<td><p>for</p></td>
<td><p>ADP</p></td>
</tr>
<tr>
<td><p>8</p></td>
<td><p>$</p></td>
<td><p>False</p></td>
<td><p>$</p></td>
<td><p>SYM</p></td>
</tr>
<tr>
<td><p>9</p></td>
<td><p>1</p></td>
<td><p>False</p></td>
<td><p>1</p></td>
<td><p>NUM</p></td>
</tr>
<tr>
<td><p>10</p></td>
<td><p>billion</p></td>
<td><p>False</p></td>
<td><p>billion</p></td>
<td><p>NUM</p></td>
</tr>
</tbody>
</table>

<p>The output for each of the preprocessing steps is shown in the preceding table. Given that spaCy performs a wide range of NLP-related tasks in a single step, it is a highly recommended package. As such, we will be using spaCy extensively in our case 
<span class="keep-together">studies</span>.</p>

<p>In addition to the above preprocessing steps, there are other frequently used preprocessing steps, such as <em>lower casing</em> or <em>nonalphanumeric data removing</em>, that we can perform depending on the type of data. For example, data scraped from a website has to be cleansed further, including the removal of HTML tags. Data from a PDF report must be converted into a text format.</p>

<p>Other optional preprocessing steps include dependency parsing, coreference resolution, triplet extraction, and relation extraction:</p>
<dl>
<dt>Dependency parsing</dt>
<dd>
<p>Assigns a syntactic structure to sentences to make sense of how the words in the sentence relate to each other.</p>
</dd>
<dt>Coreference resolution</dt>
<dd>
<p>The process of connecting tokens that represent the same entity. It is common in languages to introduce a subject with their name in one sentence and then refer to them as him/her/it in subsequent sentences.</p>
</dd>
<dt>Triplet extraction</dt>
<dd>
<p>The process of recording subject, verb, and object triplets when available in the sentence structure.</p>
</dd>
<dt>Relation extraction</dt>
<dd>
<p>A broader form of triplet extraction in which entities can have multiple 
<span class="keep-together">interactions</span>.</p>
</dd>
</dl>

<p>These additional steps should be performed only if they will help with the task at hand. We will demonstrate examples of these preprocessing steps in the case studies in this chapter<a data-type="indexterm" data-startref="ix_Chapter10-asciidoc4" id="idm45174899495544"/><a data-type="indexterm" data-startref="ix_Chapter10-asciidoc3" id="idm45174899494936"/>.<a data-type="indexterm" data-startref="ix_Chapter10-asciidoc2" id="idm45174899494200"/></p>
</div></section>

</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="2. Feature Representation"><div class="sect2" id="idm45174899656600">
<h2>2. Feature Representation</h2>

<p><a data-type="indexterm" data-primary="feature representation in NLP" id="ix_Chapter10-asciidoc5"/><a data-type="indexterm" data-primary="natural language processing (NLP)" data-secondary="feature representation" id="ix_Chapter10-asciidoc6"/>The vast majority of NLP-related data, such as news feed articles, PDF reports, social media posts, and audio files, is created for human consumption. As such, it is often stored in an unstructured format, which cannot be readily processed by computers. In order for the preprocessed information to be conveyed to the statistical inference algorithm, the tokens need to be translated into predictive features. A model is used to embed raw text into a <em>vector space</em>.</p>

<p>Feature representation involves two things:</p>

<ul>
<li>
<p>A vocabulary of known words.</p>
</li>
<li>
<p>A measure of the presence of known words.</p>
</li>
</ul>

<p>Some of the feature representation methods are:</p>

<ul>
<li>
<p>Bag of words</p>
</li>
<li>
<p>TF-IDF</p>
</li>
<li>
<p>Word embedding</p>

<ul>
<li>
<p>Pretrained models (e.g., word2vec, <a href="https://oreil.ly/u9SZG">GloVe</a>, spaCy’s word embedding model)</p>
</li>
<li>
<p>Customized deep learning–based feature representation<sup><a data-type="noteref" id="idm45174899480680-marker" href="ch10.xhtml#idm45174899480680">1</a></sup></p>
</li>
</ul>
</li>
</ul>

<p>Let’s learn more about each of these methods.</p>










<section data-type="sect3" data-pdf-bookmark="2.1. Bag of words—word count"><div class="sect3" id="idm45174899478840">
<h3>2.1. Bag of words—word count</h3>

<p><a data-type="indexterm" data-primary="bag of words model" id="idm45174899477496"/><a data-type="indexterm" data-primary="feature representation in NLP" data-secondary="bag of words model" id="idm45174899476792"/>In natural language processing, a common technique for extracting features from text is to place all words that occur in the text in a bucket. This approach is called a <em>bag of words</em> model. It’s referred to as a bag of words because any information about the structure of the sentence is lost. In this technique, we build a single matrix from a collection of texts, as shown in <a data-type="xref" href="#BagOfWords">Figure 10-4</a>, in which each row represents a token and each column represents a document or sentence in our corpus. The values of the matrix represent the count of the number of instances of the token appearing.</p>

<figure><div id="BagOfWords" class="figure">
<img src="Images/mlbf_1004.png" alt="mlbf 1004" width="1259" height="770"/>
<h6><span class="label">Figure 10-4. </span>Bag of words</h6>
</div></figure>

<p>The <code>CountVectorizer</code> from sklearn provides a simple way to both tokenize a collection of text documents and encode new documents using that vocabulary. The <code>fit_transform</code> function learns the vocabulary from one or more documents and encodes each document in the word as a vector:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">sentences</code> <code class="o">=</code> <code class="p">[</code>
<code class="s">'The stock price of google jumps on the earning data today'</code><code class="p">,</code>
<code class="s">'Google plunge on China Data!'</code>
<code class="p">]</code>
<code class="kn">from</code> <code class="nn">sklearn.feature_extraction.text</code> <code class="k">import</code> <code class="n">CountVectorizer</code>
<code class="n">vectorizer</code> <code class="o">=</code> <code class="n">CountVectorizer</code><code class="p">()</code>
<code class="nb">print</code><code class="p">(</code> <code class="n">vectorizer</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">sentences</code><code class="p">)</code><code class="o">.</code><code class="n">todense</code><code class="p">()</code> <code class="p">)</code>
<code class="nb">print</code><code class="p">(</code> <code class="n">vectorizer</code><code class="o">.</code><code class="n">vocabulary_</code> <code class="p">)</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">[[0 1 1 1 1 1 1 0 1 1 2 1]
 [1 1 0 1 0 0 1 1 0 0 0 0]]
{'the': 10, 'stock': 9, 'price': 8, 'of': 5, 'google': 3, 'jumps':\
 4, 'on': 6, 'earning': 2, 'data': 1, 'today': 11, 'plunge': 7,\
 'china': 0}</pre>

<p>We can see an array version of the encoded vector showing a count of one occurrence for each word except <em>the</em> (index 10), which has an occurrence of two. Word counts are a good starting point, but they are very basic. One issue with simple counts is that some words like <em>the</em> will appear many times, and their large counts will not be very meaningful in the encoded vectors. These bag of words representations are sparse because the vocabularies are vast, and a given word or document would be represented by a large vector comprised mostly of zero values.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="2.2. TF-IDF"><div class="sect3" id="idm45174899431912">
<h3>2.2. TF-IDF</h3>

<p><a data-type="indexterm" data-primary="feature representation in NLP" data-secondary="TF-IDF" id="idm45174899430472"/><a data-type="indexterm" data-primary="TF–IDF (term frequency–inverse document frequency)" id="idm45174899429528"/>An alternative is to calculate word frequencies, and by far the most popular method for that is <em>TF-IDF</em>, which stands for <em>Term Frequency–Inverse Document Frequency</em>:</p>
<dl>
<dt>Term Frequency</dt>
<dd>
<p>This summarizes how often a given word appears within a document.</p>
</dd>
<dt>Inverse Document Frequency</dt>
<dd>
<p>This downscales words that appear a lot across documents.</p>
</dd>
</dl>

<p>Put simply, TF-IDF is a word frequency score that tries to highlight words that are more interesting (i.e., frequent <em>within</em> a document, but not <em>across</em> documents). The <em>TfidfVectorizer</em> will tokenize documents, learn the vocabulary and the inverse document frequency weightings, and allow you to encode new documents:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">from</code> <code class="nn">sklearn.feature_extraction.text</code> <code class="k">import</code> <code class="n">TfidfVectorizer</code>
<code class="n">vectorizer</code> <code class="o">=</code> <code class="n">TfidfVectorizer</code><code class="p">(</code><code class="n">max_features</code><code class="o">=</code><code class="mi">1000</code><code class="p">,</code> <code class="n">stop_words</code><code class="o">=</code><code class="s">'english'</code><code class="p">)</code>
<code class="n">TFIDF</code> <code class="o">=</code> <code class="n">vectorizer</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">sentences</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">vectorizer</code><code class="o">.</code><code class="n">get_feature_names</code><code class="p">()[</code><code class="o">-</code><code class="mi">10</code><code class="p">:])</code>
<code class="nb">print</code><code class="p">(</code><code class="n">TFIDF</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">TFIDF</code><code class="o">.</code><code class="n">toarray</code><code class="p">())</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">['china', 'data', 'earning', 'google', 'jumps', 'plunge', 'price', 'stock', \
'today']
(2, 9)
[[0.         0.29017021 0.4078241  0.29017021 0.4078241  0.
  0.4078241  0.4078241  0.4078241 ]
 [0.57615236 0.40993715 0.         0.40993715 0.         0.57615236
  0.         0.         0.        ]]</pre>

<p>In the provided code snippet, a vocabulary of nine words is learned from the documents. Each word is assigned a unique integer index in the output vector. The sentences are encoded as a nine-element sparse array, and we can review the final scorings of each word with different values from the other words in the vocabulary.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="2.3. Word embedding"><div class="sect3" id="idm45174899348024">
<h3>2.3. Word embedding</h3>

<p><a data-type="indexterm" data-primary="feature representation in NLP" data-secondary="word embedding" id="idm45174899346856"/><a data-type="indexterm" data-primary="word embedding" id="idm45174899345912"/>A <em>word embedding</em> represents words and documents using a dense vector representation. In an embedding, words are represented by dense vectors in which a vector represents the projection of the word into a continuous vector space. The position of a word within the vector space is learned from text and is based on the words that surround the word when it is used. The position of a word in the learned vector space is referred to as its <em>embedding</em>.</p>

<p>Some of the models of learning word embeddings from text include word2Vec, spaCy’s pretrained word embedding model, and GloVe. In addition to these carefully designed methods, a word embedding can be learned as part of a deep learning model. This can be a slower approach, but it tailors the model to a specific training dataset.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="2.3.1. Pretrained model: Via spaCy"><div class="sect3" id="idm45174899342792">
<h3>2.3.1. Pretrained model: Via spaCy</h3>

<p><a data-type="indexterm" data-primary="spaCy" data-secondary="word embedding via" id="idm45174899341384"/><a data-type="indexterm" data-primary="word embedding" data-secondary="via spaCy" id="idm45174899340408"/>spaCy comes with built-in representation of text as vectors at different levels of word, sentence, and document. The underlying vector representations come from a word embedding model, which generally produces a dense, multidimensional semantic representation of words (as shown in the following example). The word embedding model includes 20,000 unique vectors with 300 dimensions. Using this vector representation, we can calculate similarities and dissimilarities between tokens, named entities, noun phrases, sentences, and documents.</p>

<p>The word embedding in spaCy is performed by first loading the model and then processing text. The vectors can be accessed directly using the <code>.vector</code> attribute of each processed token (i.e., word). The mean vector for the entire sentence is also calculated simply by using the vector, providing a very convenient input for machine learning models based on sentences:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">doc</code> <code class="o">=</code> <code class="n">nlp</code><code class="p">(</code><code class="s">"Apple orange cats dogs"</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="s">"Vector representation of the sentence for first 10 features: </code><code class="se">\n</code><code class="s">"</code><code class="p">,</code> \
<code class="n">doc</code><code class="o">.</code><code class="n">vector</code><code class="p">[</code><code class="mi">0</code><code class="p">:</code><code class="mi">10</code><code class="p">])</code></pre>

<p><code>Output:\</code></p>

<pre data-type="programlisting">Vector representation of the sentence for first 10 features:
 [ -0.30732775 0.22351399 -0.110111   -0.367025   -0.13430001
   0.13790375 -0.24379876 -0.10736975  0.2715925   1.3117325 ]</pre>

<p>The vector representation of the sentence for the first 10 features of the pretrained model is shown in the output.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="2.3.2. Pretrained model: Word2Vec using gensim package"><div class="sect3" id="idm45174899326568">
<h3>2.3.2. Pretrained model: Word2Vec using gensim package</h3>

<p><a data-type="indexterm" data-primary="gensim" id="idm45174899325192"/><a data-type="indexterm" data-primary="word embedding" data-secondary="via word2vec model" id="idm45174899324488"/>The Python-based implementation of the word2vec model using the <a href="https://oreil.ly/p9hOJ">gensim package</a> is demonstrated here:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">from</code> <code class="nn">gensim.models</code> <code class="k">import</code> <code class="n">Word2Vec</code>

<code class="n">sentences</code> <code class="o">=</code> <code class="p">[</code>
<code class="p">[</code><code class="s">'The'</code><code class="p">,</code><code class="s">'stock'</code><code class="p">,</code><code class="s">'price'</code><code class="p">,</code> <code class="s">'of'</code><code class="p">,</code> <code class="s">'Google'</code><code class="p">,</code> <code class="s">'increases'</code><code class="p">],</code>
<code class="p">[</code><code class="s">'Google'</code><code class="p">,</code><code class="s">'plunge'</code><code class="p">,</code><code class="s">' on'</code><code class="p">,</code><code class="s">'China'</code><code class="p">,</code><code class="s">' Data!'</code><code class="p">]]</code>

<code class="c"># train model</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">Word2Vec</code><code class="p">(</code><code class="n">sentences</code><code class="p">,</code> <code class="n">min_count</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>

<code class="c"># summarize the loaded model</code>
<code class="n">words</code> <code class="o">=</code> <code class="nb">list</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">wv</code><code class="o">.</code><code class="n">vocab</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">words</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">model</code><code class="p">[</code><code class="s">'Google'</code><code class="p">][</code><code class="mi">1</code><code class="p">:</code><code class="mi">5</code><code class="p">])</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">['The', 'stock', 'price', 'of', 'Google', 'increases', 'plunge', ' on', 'China',\
' Data!']
[-1.7868265e-03 -7.6242397e-04  6.0105987e-05  3.5568199e-03
]</pre>

<p>The vector representation of the sentence for the first five features of the pretrained word2vec model is shown above.<a data-type="indexterm" data-startref="ix_Chapter10-asciidoc6" id="idm45174899171480"/><a data-type="indexterm" data-startref="ix_Chapter10-asciidoc5" id="idm45174899170920"/></p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="3. Inference"><div class="sect2" id="idm45174899493000">
<h2>3. Inference</h2>

<p><a data-type="indexterm" data-primary="inference (natural language processing)" id="ix_Chapter10-asciidoc7"/><a data-type="indexterm" data-primary="natural language processing (NLP)" data-secondary="inference" id="ix_Chapter10-asciidoc8"/>As with other artificial intelligence tasks, an inference generated by an NLP application usually needs to be translated into a decision in order to be actionable. Inference falls under three machine learning categories covered in the previous chapters (i.e., supervised, unsupervised, and reinforcement learning). While the type of inference required depends on the business problem and the type of training data, the most commonly used algorithms are supervised and unsupervised.</p>

<p>One of the most frequently used supervised methodologies in NLP is the <em>Naive Bayes</em> model, as it can produce reasonable accuracy using simple assumptions. A more complex supervised methodology is using artificial neural network architectures. In past years, these architectures, such as recurrent neural networks (RNNs), have dominated NLP-based inference.</p>

<p>Most of the existing literature in NLP focuses on supervised learning. As such, unsupervised learning applications constitute a relatively less developed subdomain in which measuring <em>document similarity</em> is among the most common tasks. <a data-type="indexterm" data-primary="latent semantic analysis (LSA)" id="idm45174899163752"/><a data-type="indexterm" data-primary="LSA (latent semantic analysis)" id="idm45174899163080"/>A popular unsupervised technique applied in NLP is <em>Latent Semantic Analysis</em> (LSA). LSA looks at relationships between a set of documents and the words they contain by producing a set of latent concepts related to the documents and terms. <a data-type="indexterm" data-primary="latent Dirichlet allocation (LDA)" id="ix_Chapter10-asciidoc9"/><a data-type="indexterm" data-primary="LDA (latent Dirichlet allocation)" id="ix_Chapter10-asciidoc10"/>LSA has paved the way for a more sophisticated approach called <em>Latent Dirichlet Allocation</em> (LDA), under which documents are modeled as a finite mixture of topics. These topics in turn are modeled as a finite mixture over words in the vocabulary. LDA has been extensively used for <em>topic modeling</em>—a growing area of research in which NLP practitioners build probabilistic generative models to reveal likely topic attributions for words.</p>

<p>Since we have reviewed many supervised and unsupervised learning models in the previous chapters, we will provide details only on Naive Bayes and LDA models in the next sections. These are used extensively in NLP and were not covered in the previous chapters.</p>










<section data-type="sect3" data-pdf-bookmark="3.1. Supervised learning example—Naive Bayes"><div class="sect3" id="idm45174899157784">
<h3>3.1. Supervised learning example—Naive Bayes</h3>

<p><a data-type="indexterm" data-primary="inference (natural language processing)" data-secondary="Naive Bayes approach" id="idm45174899156376"/><a data-type="indexterm" data-primary="naive Bayes" data-secondary="NLP and" id="idm45174899155384"/>Naive Bayes is a family of algorithms based on applying <a href="https://oreil.ly/bVeZK"><em>Bayes’s theorem</em></a> with a strong (naive) assumption that every feature used to predict the category of a given sample is independent of the others. They are probabilistic classifiers and therefore will 
<span class="keep-together">calculate</span> the probability of each category using Bayes’s theorem. The category with the highest probability will be output.</p>

<p>In NLP, a Naive Bayes approach assumes that all word features are independent of each other given the class labels. Due to this simplifying assumption, Naive Bayes is very compatible with a bag-of-words word representation, and it has been demonstrated to be fast, reliable, and accurate in a number of NLP applications. Moreover, despite its simplifying assumptions, it is competitive with (and at times even outperforms) more complicated classifiers.</p>

<p>Let us look at the usage of Naive Bayes for the inference in a sentiment analysis problem. We take a dataframe in which there are two sentences with sentiments assigned to each. In the next step, we convert the sentences into a feature representation using <code>CountVectorizer</code>. The features and sentiments are used to train and test the model using Naive Bayes:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">sentences</code> <code class="o">=</code> <code class="p">[</code>
<code class="s">'The stock price of google jumps on the earning data today'</code><code class="p">,</code>
<code class="s">'Google plunge on China Data!'</code><code class="p">]</code>
<code class="n">sentiment</code> <code class="o">=</code> <code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">)</code>
<code class="n">data</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">({</code><code class="s">'Sentence'</code><code class="p">:</code><code class="n">sentences</code><code class="p">,</code>
        <code class="s">'sentiment'</code><code class="p">:</code><code class="n">sentiment</code><code class="p">})</code>

<code class="c"># feature extraction</code>
<code class="kn">from</code> <code class="nn">sklearn.feature_extraction.text</code> <code class="k">import</code> <code class="n">CountVectorizer</code>
<code class="n">vect</code> <code class="o">=</code> <code class="n">CountVectorizer</code><code class="p">()</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">data</code><code class="p">[</code><code class="s">'Sentence'</code><code class="p">])</code>
<code class="n">X_train_vectorized</code> <code class="o">=</code> <code class="n">vect</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">data</code><code class="p">[</code><code class="s">'Sentence'</code><code class="p">])</code>

<code class="c"># Running naive bayes model</code>
<code class="kn">from</code> <code class="nn">sklearn.naive_bayes</code> <code class="k">import</code> <code class="n">MultinomialNB</code>
<code class="n">clfrNB</code> <code class="o">=</code> <code class="n">MultinomialNB</code><code class="p">(</code><code class="n">alpha</code><code class="o">=</code><code class="mf">0.1</code><code class="p">)</code>
<code class="n">clfrNB</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train_vectorized</code><code class="p">,</code> <code class="n">data</code><code class="p">[</code><code class="s">'sentiment'</code><code class="p">])</code>

<code class="c">#Testing the model</code>
<code class="n">preds</code> <code class="o">=</code> <code class="n">clfrNB</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">vect</code><code class="o">.</code><code class="n">transform</code><code class="p">([</code><code class="s">'Apple price plunge'</code><code class="p">,</code>\
 <code class="s">'Amazon price jumps'</code><code class="p">]))</code>
<code class="n">preds</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">array([0, 1])</pre>

<p>As we can see, the Naive Bayes trains the model fairly well from the two sentences. The model gives a sentiment of zero and one for the test sentences “Apple price plunge” and “Amazon price jumps,” respectively, given the sentences used for training also had the keywords “plunge” and “jumps,” with corresponding sentiment assignments.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="3.2. Unsupervised learning example: LDA"><div class="sect3" id="idm45174899032104">
<h3>3.2. Unsupervised learning example: LDA</h3>

<p><a data-type="indexterm" data-primary="inference (natural language processing)" data-secondary="LDA implementation" id="idm45174899030728"/><a data-type="indexterm" data-primary="topic modeling, LDA for" id="idm45174899029784"/><a data-type="indexterm" data-primary="unsupervised learning" data-secondary="LDA for topic modeling" id="idm45174899029112"/>LDA is extensively used for <em>topic modeling</em> because it tends to produce meaningful topics that humans can interpret, assigns topics to new documents, and is extensible. It works by first making a key assumption: documents are generated by first selecting <em>topics</em>, and then, for each topic, a set of <em>words</em>. The algorithm then reverse engineers this process to find the topics in a document.</p>

<p>In the following code snippet, we show an implementation of LDA for topic modeling. We take two sentences and convert the sentences into a feature representation using <code>CountVectorizer</code>. These features and the sentiments are used to train the model and produce two smaller matrices representing the topics:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">sentences</code> <code class="o">=</code> <code class="p">[</code>
<code class="s">'The stock price of google jumps on the earning data today'</code><code class="p">,</code>
<code class="s">'Google plunge on China Data!'</code>
<code class="p">]</code>

<code class="c">#Getting the bag of words</code>
<code class="kn">from</code> <code class="nn">sklearn.decomposition</code> <code class="k">import</code> <code class="n">LatentDirichletAllocation</code>
<code class="n">vect</code><code class="o">=</code><code class="n">CountVectorizer</code><code class="p">(</code><code class="n">ngram_range</code><code class="o">=</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">),</code><code class="n">stop_words</code><code class="o">=</code><code class="s">'english'</code><code class="p">)</code>
<code class="n">sentences_vec</code><code class="o">=</code><code class="n">vect</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">sentences</code><code class="p">)</code>

<code class="c">#Running LDA on the bag of words.</code>
<code class="kn">from</code> <code class="nn">sklearn.feature_extraction.text</code> <code class="k">import</code> <code class="n">CountVectorizer</code>
<code class="n">lda</code><code class="o">=</code><code class="n">LatentDirichletAllocation</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mi">3</code><code class="p">)</code>
<code class="n">lda</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">sentences_vec</code><code class="p">)</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">array([[0.04283242, 0.91209846, 0.04506912],
       [0.06793339, 0.07059533, 0.86147128]])</pre>

<p>We will be using LDA for topic modeling in the third case study of this chapter and will discuss the concepts and interpretation in detail.<a data-type="indexterm" data-startref="ix_Chapter10-asciidoc10" id="idm45174898897160"/><a data-type="indexterm" data-startref="ix_Chapter10-asciidoc9" id="idm45174898896488"/></p>

<p>To review, in order to approach any NLP-based problem, we need to follow the preprocessing, feature extraction, and inference steps<a data-type="indexterm" data-startref="ix_Chapter10-asciidoc8" id="idm45174898895288"/><a data-type="indexterm" data-startref="ix_Chapter10-asciidoc7" id="idm45174898894584"/>.<a data-type="indexterm" data-startref="ix_Chapter10-asciidoc1" id="idm45174898893784"/> Now, let’s dive into the case 
<span class="keep-together">studies</span>.</p>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Case Study 1: NLP and Sentiment Analysis–Based Trading Strategies"><div class="sect1" id="CaseStudy1NLP">
<h1>Case Study 1: NLP and Sentiment Analysis–Based Trading Strategies</h1>

<p><a data-type="indexterm" data-primary="sentiment analysis-based trading strategies" id="ix_Chapter10-asciidoc11"/>Natural language processing offers the ability to quantify text. One can begin to ask questions such as: How positive or negative is this news? and How can we quantify words?</p>

<p>Perhaps the most notable application of NLP is its use in algorithmic trading. NLP provides an efficient means of monitoring market sentiments. By applying 
<span class="keep-together">NLP-based</span> sentiment analysis techniques to news articles, reports, social media, or other web content, one can effectively determine whether those sources have a positive or negative senitment score. Sentiment scores can be used as a directional signal to buy stocks with positive scores and sell stocks with negative ones.</p>

<p>Trading strategies based on text data are becoming more popular as the amount of unstructured data increases. In this case study we are going to look at how one can use NLP-based sentiments to build a trading 
<span class="keep-together">strategy</span>.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45174898886040">
<h5/>
<p>In this case study, we will focus on:</p>

<ul>
<li>
<p>Producing news sentiments using supervised and unsupervised algorithms.</p>
</li>
<li>
<p>Enhancing sentiment analysis by using a deep learning model, such as LSTM.</p>
</li>
<li>
<p>Comparison of different sentiment generation methodologies for the purpose of building a trading strategy.</p>
</li>
<li>
<p>Using sentiments and word vectors effectively as features in a trading strategy.</p>
</li>
<li>
<p>Collecting data from different sources and preprocessing it for sentiment analysis.</p>
</li>
<li>
<p>Using NLP Python packages for sentiment analysis.</p>
</li>
<li>
<p>Building a framework for backtesting results of a trading strategy using available Python packages.</p>
</li>
</ul>
</div></aside>

<p>This case study combines concepts presented in previous chapters. The overall model development steps of this case study are similar to the seven-step model development in prior case studies, with slight modifications.</p>
<img class="blueprint top" src="Images/bracket_top.png" width="1625" height="32"/>








<section data-type="sect2" class="blueprint" data-pdf-bookmark="Blueprint for Building a Trading Strategy Based on Sentiment Analysis"><div class="sect2" id="idm45174898994264">
<h2>Blueprint for Building a Trading Strategy Based on Sentiment Analysis</h2>










<section data-type="sect3" data-pdf-bookmark="1. Problem definition"><div class="sect3" id="idm45174898992584">
<h3>1. Problem definition</h3>

<p>Our goal is to (1) use NLP to extract information from news headlines, (2) assign a sentiment to that information, and (3) use sentiment analysis to build a trading strategy.</p>

<p class="pagebreak-before">The data used for this case study will be from the following sources:</p>
<dl>
<dt>News headlines data compiled from the RSS feeds of several news websites</dt>
<dd>
<p>For the purpose of this study, we will look only at the headlines, not at the full text of the stories. Our dataset contains around 82,000 headlines from May 2011 through December 2018.<sup><a data-type="noteref" id="idm45174898988232-marker" href="ch10.xhtml#idm45174898988232">2</a></sup></p>
</dd>
<dt>Yahoo Finance website for stock data</dt>
<dd>
<p>The return data for stocks used in this case study is derived from Yahoo Finance price data.</p>
</dd>
<dt><a href="https://www.kaggle.com">Kaggle</a></dt>
<dd>
<p>We will use the labeled data of news sentiments for a classification-based sentiment analysis model. Note that this data may not be fully applicable to the case at hand and is used here for demonstration purposes.</p>
</dd>
<dt>Stock market lexicon</dt>
<dd>
<p><em>Lexicon</em> refers to the component of an NLP system that contains information (semantic, grammatical) about individual words or word strings. This is created based on stock market conversations in microblogging services.<sup><a data-type="noteref" id="idm45174898982312-marker" href="ch10.xhtml#idm45174898982312">3</a></sup></p>
</dd>
</dl>

<p>The key steps of this case study are outlined in <a data-type="xref" href="#StepsSentimentTrading">Figure 10-5</a>.</p>

<figure><div id="StepsSentimentTrading" class="figure">
<img src="Images/mlbf_1005.png" alt="mlbf 1005" width="1170" height="293"/>
<h6><span class="label">Figure 10-5. </span>Steps in a sentiment analysis–based trading strategy</h6>
</div></figure>

<p>Once we are done with preprocessing, we will look at the different sentiment analysis models. The results from the sentiment analysis step are used to develop the trading strategy.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="2. Getting started—loading the data and Python packages"><div class="sect3" id="idm45174898977032">
<h3>2. Getting started—loading the data and Python packages</h3>












<section data-type="sect4" data-pdf-bookmark="2.1. Loading the Python packages"><div class="sect4" id="idm45174898976024">
<h4>2.1. Loading the Python packages</h4>

<p><a data-type="indexterm" data-primary="sentiment analysis-based trading strategies" data-secondary="loading data and Python packages" id="idm45174898974856"/>The first set of libraries to be loaded are the NLP-specific libraries discussed above. Refer to the Jupyter notebook of this case study for details of the other libraries.</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">from</code> <code class="nn">textblob</code> <code class="k">import</code> <code class="n">TextBlob</code>
<code class="kn">import</code> <code class="nn">spacy</code>
<code class="kn">import</code> <code class="nn">nltk</code>
<code class="kn">import</code> <code class="nn">warnings</code>
<code class="kn">from</code> <code class="nn">nltk.sentiment.vader</code> <code class="k">import</code> <code class="n">SentimentIntensityAnalyzer</code>
<code class="n">nltk</code><code class="o">.</code><code class="n">download</code><code class="p">(</code><code class="s">'vader_lexicon'</code><code class="p">)</code>
<code class="n">nlp</code> <code class="o">=</code> <code class="n">spacy</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="s">"en_core_web_lg"</code><code class="p">)</code></pre>
</div></section>













<section data-type="sect4" data-pdf-bookmark="2.2. Loading the data"><div class="sect4" id="idm45174898967208">
<h4>2.2. Loading the data</h4>

<p>In this step, we load the stock price data from Yahoo Finance. We select 10 stocks for this case study. These stocks are some of the largest stocks in the S&amp;P 500 by market share:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">tickers</code> <code class="o">=</code> <code class="p">[</code><code class="s">'AAPL'</code><code class="p">,</code><code class="s">'MSFT'</code><code class="p">,</code><code class="s">'AMZN'</code><code class="p">,</code><code class="s">'GOOG'</code><code class="p">,</code><code class="s">'FB'</code><code class="p">,</code><code class="s">'WMT'</code><code class="p">,</code><code class="s">'JPM'</code><code class="p">,</code><code class="s">'TSLA'</code><code class="p">,</code><code class="s">'NFLX'</code><code class="p">,</code><code class="s">'ADBE'</code><code class="p">]</code>
<code class="n">start</code> <code class="o">=</code> <code class="s">'2010-01-01'</code>
<code class="n">end</code> <code class="o">=</code> <code class="s">'2018-12-31'</code>
<code class="n">df_ticker_return</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">()</code>
<code class="k">for</code> <code class="n">ticker</code> <code class="ow">in</code> <code class="n">tickers</code><code class="p">:</code>
    <code class="n">ticker_yf</code> <code class="o">=</code> <code class="n">yf</code><code class="o">.</code><code class="n">Ticker</code><code class="p">(</code><code class="n">ticker</code><code class="p">)</code>
    <code class="k">if</code> <code class="n">df_ticker_return</code><code class="o">.</code><code class="n">empty</code><code class="p">:</code>
        <code class="n">df_ticker_return</code> <code class="o">=</code> <code class="n">ticker_yf</code><code class="o">.</code><code class="n">history</code><code class="p">(</code><code class="n">start</code> <code class="o">=</code> <code class="n">start</code><code class="p">,</code> <code class="n">end</code> <code class="o">=</code> <code class="n">end</code><code class="p">)</code>
        <code class="n">df_ticker_return</code><code class="p">[</code><code class="s">'ticker'</code><code class="p">]</code><code class="o">=</code> <code class="n">ticker</code>
    <code class="k">else</code><code class="p">:</code>
        <code class="n">data_temp</code> <code class="o">=</code> <code class="n">ticker_yf</code><code class="o">.</code><code class="n">history</code><code class="p">(</code><code class="n">start</code> <code class="o">=</code> <code class="n">start</code><code class="p">,</code> <code class="n">end</code> <code class="o">=</code> <code class="n">end</code><code class="p">)</code>
        <code class="n">data_temp</code><code class="p">[</code><code class="s">'ticker'</code><code class="p">]</code><code class="o">=</code> <code class="n">ticker</code>
        <code class="n">df_ticker_return</code> <code class="o">=</code> <code class="n">df_ticker_return</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">data_temp</code><code class="p">)</code>
<code class="n">df_ticker_return</code><code class="o">.</code><code class="n">to_csv</code><code class="p">(</code><code class="s">r'Data\Step3.2_ReturnData.csv'</code><code class="p">)</code></pre>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">df_ticker_return</code><code class="o">.</code><code class="n">head</code><code class="p">(</code><code class="mi">2</code><code class="p">)</code></pre>

<figure><div class="figure">
<img src="Images/mlbf_10in01.png" alt="mlbf 10in01" width="517" height="88"/>
<h6/>
</div></figure>

<p>The data contains the price and volume data of the stocks along with their ticker name. In the next step, we look at the news data.</p>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="3. Data preparation"><div class="sect3" id="idm45174898651400">
<h3>3. Data preparation</h3>

<p><a data-type="indexterm" data-primary="sentiment analysis-based trading strategies" data-secondary="data preparation" id="ix_Chapter10-asciidoc12"/>In this step, we load and preprocess the news data, followed by combining the news data with the stock return data. This combined dataset will be used for the model development.</p>












<section data-type="sect4" data-pdf-bookmark="3.1. Preprocessing news data"><div class="sect4" id="idm45174898648536">
<h4>3.1. Preprocessing news data</h4>

<p>The news data is downloaded from the News RSS feed, and the file is available in JSON format. The JSON files for different dates are kept under a zipped folder. The data is downloaded using the standard web-scraping Python package Beautiful Soup, which is an open source framework. Let us look at the content of the downloaded JSON file:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">z</code> <code class="o">=</code> <code class="n">zipfile</code><code class="o">.</code><code class="n">ZipFile</code><code class="p">(</code><code class="s">"Data/Raw Headline Data.zip"</code><code class="p">,</code> <code class="s">"r"</code><code class="p">)</code>
<code class="n">testFile</code><code class="o">=</code><code class="n">z</code><code class="o">.</code><code class="n">namelist</code><code class="p">()[</code><code class="mi">10</code><code class="p">]</code>
<code class="n">fileData</code><code class="o">=</code> <code class="n">z</code><code class="o">.</code><code class="n">open</code><code class="p">(</code><code class="n">testFile</code><code class="p">)</code><code class="o">.</code><code class="n">read</code><code class="p">()</code>
<code class="n">fileDataSample</code> <code class="o">=</code> <code class="n">json</code><code class="o">.</code><code class="n">loads</code><code class="p">(</code><code class="n">fileData</code><code class="p">)[</code><code class="s">'content'</code><code class="p">][</code><code class="mi">1</code><code class="p">:</code><code class="mi">500</code><code class="p">]</code>
<code class="n">fileDataSample</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">'li class="n-box-item date-title" data-end="1305172799" data-start="1305086400"
data-txt="Tuesday, December 17, 2019"&gt;Wednesday, May 11,2011&lt;/li&gt;&lt;li
class="n-box-item sa-box-item" data-id="76179" data-ts="1305149244"&gt;&lt;div
class="media media-overflow-fix"&gt;&lt;div class-"media-left"&gt;&lt;a class="box-ticker"
href="/symbol/CSCO" target="blank"&gt;CSCO&lt;/a&gt;&lt;/div&gt;&lt;div class="media-body"&lt;h4
class="media-heading"&gt;&lt;a href="/news/76179" sasource="on_the_move_news_
fidelity" target="_blank"&gt;Cisco (NASDAQ:CSCO): Pr'</pre>

<p>We can see that the JSON format is not suitable for the algorithm. We need to get the news from the JSONs. <a data-type="indexterm" data-primary="regex" id="idm45174898585880"/>Regex becomes the vital part of this step. Regex can find a pattern in the raw, messy text and perform actions accordingly. The following function parses HTML by using information encoded in the JSON file:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="k">def</code> <code class="nf">jsonParser</code><code class="p">(</code><code class="n">json_data</code><code class="p">):</code>
    <code class="n">xml_data</code> <code class="o">=</code> <code class="n">json_data</code><code class="p">[</code><code class="s">'content'</code><code class="p">]</code>

    <code class="n">tree</code> <code class="o">=</code> <code class="n">etree</code><code class="o">.</code><code class="n">parse</code><code class="p">(</code><code class="n">StringIO</code><code class="p">(</code><code class="n">xml_data</code><code class="p">),</code> <code class="n">parser</code><code class="o">=</code><code class="n">etree</code><code class="o">.</code><code class="n">HTMLParser</code><code class="p">())</code>

    <code class="n">headlines</code> <code class="o">=</code> <code class="n">tree</code><code class="o">.</code><code class="n">xpath</code><code class="p">(</code><code class="s">"//h4[contains(@class, 'media-heading')]/a/text()"</code><code class="p">)</code>
    <code class="k">assert</code> <code class="nb">len</code><code class="p">(</code><code class="n">headlines</code><code class="p">)</code> <code class="o">==</code> <code class="n">json_data</code><code class="p">[</code><code class="s">'count'</code><code class="p">]</code>

    <code class="n">main_tickers</code> <code class="o">=</code> <code class="nb">list</code><code class="p">(</code><code class="nb">map</code><code class="p">(</code><code class="k">lambda</code> <code class="n">x</code><code class="p">:</code> <code class="n">x</code><code class="o">.</code><code class="n">replace</code><code class="p">(</code><code class="s">'/symbol/'</code><code class="p">,</code> <code class="s">''</code><code class="p">),</code>\
           <code class="n">tree</code><code class="o">.</code><code class="n">xpath</code><code class="p">(</code><code class="s">"//div[contains(@class, 'media-left')]//a/@href"</code><code class="p">)))</code>
    <code class="k">assert</code> <code class="nb">len</code><code class="p">(</code><code class="n">main_tickers</code><code class="p">)</code> <code class="o">==</code> <code class="n">json_data</code><code class="p">[</code><code class="s">'count'</code><code class="p">]</code>
    <code class="n">final_headlines</code> <code class="o">=</code> <code class="p">[</code><code class="s">''</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">f</code><code class="o">.</code><code class="n">xpath</code><code class="p">(</code><code class="s">'.//text()'</code><code class="p">))</code> <code class="k">for</code> <code class="n">f</code> <code class="ow">in</code>\
           <code class="n">tree</code><code class="o">.</code><code class="n">xpath</code><code class="p">(</code><code class="s">"//div[contains(@class, 'media-body')]/ul/li[1]"</code><code class="p">)]</code>
    <code class="k">if</code> <code class="nb">len</code><code class="p">(</code><code class="n">final_headlines</code><code class="p">)</code> <code class="o">==</code> <code class="mi">0</code><code class="p">:</code>
        <code class="n">final_headlines</code> <code class="o">=</code> <code class="p">[</code><code class="s">''</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">f</code><code class="o">.</code><code class="n">xpath</code><code class="p">(</code><code class="s">'.//text()'</code><code class="p">))</code> <code class="k">for</code> <code class="n">f</code> <code class="ow">in</code>\
           <code class="n">tree</code><code class="o">.</code><code class="n">xpath</code><code class="p">(</code><code class="s">"//div[contains(@class, 'media-body')]"</code><code class="p">)]</code>
        <code class="n">final_headlines</code> <code class="o">=</code> <code class="p">[</code><code class="n">f</code><code class="o">.</code><code class="n">replace</code><code class="p">(</code><code class="n">h</code><code class="p">,</code> <code class="s">''</code><code class="p">)</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="s">'</code><code class="se">\xa0</code><code class="s">'</code><code class="p">)[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">strip</code><code class="p">()</code>\
                           <code class="k">for</code> <code class="n">f</code><code class="p">,</code><code class="n">h</code> <code class="ow">in</code> <code class="nb">zip</code> <code class="p">(</code><code class="n">final_headlines</code><code class="p">,</code> <code class="n">headlines</code><code class="p">)]</code>
    <code class="k">return</code> <code class="n">main_tickers</code><code class="p">,</code> <code class="n">final_headlines</code></pre>

<p>Let us see how the output looks like after running the JSON parser:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">jsonParser</code><code class="p">(</code><code class="n">json</code><code class="o">.</code><code class="n">loads</code><code class="p">(</code><code class="n">fileData</code><code class="p">))[</code><code class="mi">1</code><code class="p">][</code><code class="mi">1</code><code class="p">]</code></pre>

<p class="pagebreak-before"><code>Output</code></p>

<pre data-type="programlisting">'Cisco Systems (NASDAQ:CSCO) falls further into the red on FQ4
 guidance of $0.37-0.39 vs. $0.42 Street consensus. Sales seen flat
 to +2% vs. 8% Street view. CSCO recently -2.1%.'</pre>

<p>As we can see, the output is converted into a more readable format after JSON 
<span class="keep-together">parsing</span>.</p>

<p>While evaluating the sentiment analysis models, we also analyze the relationship between the sentiments and subsequent stock performance. <a data-type="indexterm" data-primary="event return" id="idm45174898305096"/>In order to understand the relationship, we use <em>event return</em>, which is the return that corresponds to the event. We do this because at times the news is reported late (i.e., after market participants are aware of the announcement) or after market close. Having a slightly wider window ensures that we capture the essence of the event. <em>Event return</em> is defined as:</p>
<div data-type="equation">
<math>
  <mrow>
    <msub><mi>R</mi> <mrow><mi>t</mi><mo>–</mo><mn>1</mn></mrow> </msub>
    <mo>+</mo>
    <msub><mi>R</mi> <mi>t</mi> </msub>
    <mo>+</mo>
    <msub><mi>R</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow> </msub>
  </mrow>
</math>
</div>

<p>where <math display="inline">
  <mrow>
    <msub><mi>R</mi> <mrow><mi>t</mi><mo>–</mo><mn>1</mn></mrow> </msub>
    <mo>,</mo>
    <msub><mi>R</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow> </msub>
  </mrow>
</math> are the returns before and after the news data, and <math alttext="upper R Subscript t">
  <msub><mi>R</mi> <mi>t</mi> </msub>
</math> is the return on the day of the news (i.e., time <em>t</em>).</p>

<p>Let us extract the event return from the data:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c">#Computing the return</code>
<code class="n">df_ticker_return</code><code class="p">[</code><code class="s">'ret_curr'</code><code class="p">]</code> <code class="o">=</code> <code class="n">df_ticker_return</code><code class="p">[</code><code class="s">'Close'</code><code class="p">]</code><code class="o">.</code><code class="n">pct_change</code><code class="p">()</code>
<code class="c">#Computing the event return</code>
<code class="n">df_ticker_return</code><code class="p">[</code><code class="s">'eventRet'</code><code class="p">]</code> <code class="o">=</code> <code class="n">df_ticker_return</code><code class="p">[</code><code class="s">'ret_curr'</code><code class="p">]</code>\
 <code class="o">+</code> <code class="n">df_ticker_return</code><code class="p">[</code><code class="s">'ret_curr'</code><code class="p">]</code><code class="o">.</code><code class="n">shift</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">)</code> <code class="o">+</code> <code class="n">df_ticker_return</code><code class="p">[</code><code class="s">'ret_curr'</code><code class="p">]</code><code class="o">.</code><code class="n">shift</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code></pre>

<p>Now we have all the data in place. We will prepare a combined dataframe, which will have the news headlines mapped to the date, the returns (event return, current return, and next day’s return), and stock ticker. This dataframe will be used for building the sentiment analysis model and the trading strategy:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">combinedDataFrame</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">merge</code><code class="p">(</code><code class="n">data_df_news</code><code class="p">,</code> <code class="n">df_ticker_return</code><code class="p">,</code> <code class="n">how</code><code class="o">=</code><code class="s">'left'</code><code class="p">,</code> \
<code class="n">left_on</code><code class="o">=</code><code class="p">[</code><code class="s">'date'</code><code class="p">,</code><code class="s">'ticker'</code><code class="p">],</code> <code class="n">right_on</code><code class="o">=</code><code class="p">[</code><code class="s">'date'</code><code class="p">,</code><code class="s">'ticker'</code><code class="p">])</code>
<code class="n">combinedDataFrame</code> <code class="o">=</code> <code class="n">combinedDataFrame</code><code class="p">[</code><code class="n">combinedDataFrame</code><code class="p">[</code><code class="s">'ticker'</code><code class="p">]</code><code class="o">.</code><code class="n">isin</code><code class="p">(</code><code class="n">tickers</code><code class="p">)]</code>
<code class="n">data_df</code> <code class="o">=</code> <code class="n">combinedDataFrame</code><code class="p">[[</code><code class="s">'ticker'</code><code class="p">,</code><code class="s">'headline'</code><code class="p">,</code><code class="s">'date'</code><code class="p">,</code><code class="s">'eventRet'</code><code class="p">,</code><code class="s">'Close'</code><code class="p">]]</code>
<code class="n">data_df</code> <code class="o">=</code> <code class="n">data_df</code><code class="o">.</code><code class="n">dropna</code><code class="p">()</code>
<code class="n">data_df</code><code class="o">.</code><code class="n">head</code><code class="p">(</code><code class="mi">2</code><code class="p">)</code></pre>

<p><code>Output</code></p>
<table>

<thead>
<tr>
<th/>
<th>ticker</th>
<th>headline</th>
<th>date</th>
<th>eventRet</th>
<th>Close</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>5</p></td>
<td><p>AMZN</p></td>
<td><p>Whole Foods (WFMI) –5.2% following a downgrade…</p></td>
<td><p>2011-05-02</p></td>
<td><p>0.017650</p></td>
<td><p>201.19</p></td>
</tr>
<tr>
<td><p>11</p></td>
<td><p>NFLX</p></td>
<td><p>Netflix (NFLX +1.1%) shares post early gains a…</p></td>
<td><p>2011-05-02</p></td>
<td><p>–0.013003</p></td>
<td><p>33.88</p></td>
</tr>
</tbody>
</table>

<p class="pagebreak-before">Let us look at the overall shape of the data:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="nb">print</code><code class="p">(</code><code class="n">data_df</code><code class="o">.</code><code class="n">shape</code><code class="p">,</code> <code class="n">data_df</code><code class="o">.</code><code class="n">ticker</code><code class="o">.</code><code class="n">unique</code><code class="p">()</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">(2759, 5) (10,)</pre>

<p>In this step, we prepared a clean dataframe that has ticker, headline, event return, return for a given day, and future return for 10 stock tickers, totaling 2,759 rows of data. Let us evaluate the models for sentiment analysis in the next step.<a data-type="indexterm" data-startref="ix_Chapter10-asciidoc12" id="idm45174898061272"/></p>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="4. Evaluate models for sentiment analysis"><div class="sect3" id="idm45174898647496">
<h3>4. Evaluate models for sentiment analysis</h3>

<p><a data-type="indexterm" data-primary="sentiment analysis-based trading strategies" data-secondary="evaluating models for sentiment analysis" id="ix_Chapter10-asciidoc13"/>In this section, we will go through the following three approaches of computing sentiments for the news:</p>

<ul>
<li>
<p>Predefined model—TextBlob package</p>
</li>
<li>
<p>Tuned model—classification algorithms and LSTM</p>
</li>
<li>
<p>Model based on financial lexicon</p>
</li>
</ul>

<p>Let us go through the steps.</p>












<section data-type="sect4" data-pdf-bookmark="4.1. Predefined model—TextBlob package"><div class="sect4" id="idm45174898017112">
<h4>4.1. Predefined model—TextBlob package</h4>

<p><a data-type="indexterm" data-primary="TextBlob" id="ix_Chapter10-asciidoc14"/>The <code>TextBlob sentiment</code> function is a pretrained model based on the <a data-type="indexterm" data-primary="naive Bayes" data-secondary="TextBlob and" id="idm45174898013960"/>Naive Bayes classification algorithm. The function maps adjectives that are frequently found in movie reviews<sup><a data-type="noteref" id="idm45174898012728-marker" href="ch10.xhtml#idm45174898012728">4</a></sup> to sentiment polarity scores ranging from –1 to +1 (negative to positive), converting a sentence to a numerical value. We apply this on all headline articles. An example of getting the sentiment for a news text is shown below:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">text</code> <code class="o">=</code> <code class="s">"Bayer (OTCPK:BAYRY) started the week up 3.5% to €74/share in Frankfurt, </code><code class="se">\</code>
<code class="s">touching their</code>
<code class="n">highest</code> <code class="n">level</code> <code class="ow">in</code> <code class="mi">14</code> <code class="n">months</code><code class="p">,</code> <code class="n">after</code> <code class="n">the</code> <code class="n">U</code><code class="o">.</code><code class="n">S</code><code class="o">.</code> <code class="n">government</code> <code class="n">said</code> \
 <code class="n">a</code> <code class="err">$</code><code class="mi">25</code><code class="n">M</code> <code class="n">glyphosate</code> <code class="n">decision</code> <code class="n">against</code> <code class="n">the</code>
<code class="n">company</code> <code class="n">should</code> <code class="n">be</code> <code class="nb">reversed</code><code class="o">.</code><code class="s">"</code>

<code class="n">TextBlob</code><code class="p">(</code><code class="n">text</code><code class="p">)</code><code class="o">.</code><code class="n">sentiment</code><code class="o">.</code><code class="n">polarity</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">0.5</pre>

<p class="pagebreak-before">The sentiment for the statement is 0.5. We apply this on all headlines we have in the data:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">data_df</code><code class="p">[</code><code class="s">'sentiment_textblob'</code><code class="p">]</code> <code class="o">=</code> <code class="p">[</code><code class="n">TextBlob</code><code class="p">(</code><code class="n">s</code><code class="p">)</code><code class="o">.</code><code class="n">sentiment</code><code class="o">.</code><code class="n">polarity</code> <code class="k">for</code> <code class="n">s</code> <code class="ow">in</code> \
<code class="n">data_df</code><code class="p">[</code><code class="s">'headline'</code><code class="p">]]</code></pre>

<p>Let us inspect the scatterplot of the sentiments and returns to examine the correlation between the two for all 10 stocks.</p>

<figure><div class="figure">
<img src="Images/mlbf_1006.png" alt="mlbf 1006" width="389" height="263"/>
<h6/>
</div></figure>

<p>A plot for a single stock (APPL) is also shown in the following chart (see the code in the Jupyter notebook in the GitHub repository for this book for more details on the code):</p>

<figure><div class="figure">
<img src="Images/mlbf_10in03.png" alt="mlbf 10in03" width="389" height="263"/>
<h6/>
</div></figure>

<p>From the scatterplots, we can see that there is not a strong relationship between the news and the sentiments. The correlation between return and sentiments is positive (4.27%), which means that news with positive sentiments leads to positive return and is expected. However, the correlation is not very high. Even looking at the overall scatterplot, we see the majority of the sentiments concentrated around zero. This raises the question of whether a sentiment score trained on movie reviews is appropriate for stock prices. The <code>sentiment_assessments</code> attribute lists the underlying values for each token and can help us understand the reason for the overall sentiment of a sentence:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">text</code> <code class="o">=</code> <code class="s">"Bayer (OTCPK:BAYRY) started the week up 3.5% to €74/share</code><code class="se">\</code>
<code class="s">in Frankfurt, touching their highest level in 14 months, after the</code><code class="se">\</code>
<code class="s">U.S. government said a $25M glyphosate decision against the company</code><code class="se">\</code>
<code class="s">should be reversed."</code>
<code class="n">TextBlob</code><code class="p">(</code><code class="n">text</code><code class="p">)</code><code class="o">.</code><code class="n">sentiment_assessments</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">Sentiment(polarity=0.5, subjectivity=0.5, assessments=[(['touching'], 0.5, 0.5, \
None)])</pre>

<p>We see that the statement has a positive sentiment of 0.5, but it appears the word “touching” gave rise to the positive sentiment. More intuitive words, such as “high,” do not. This example shows that the context of the training data is important for the sentiment score to be meaningful. There are many predefined packages and functions available for sentiment analysis, but it is important to be careful and have a thorough understanding of the problem’s context before using a function or an algorithm for sentiment analysis.</p>

<p>For this case study, we may need sentiments trained on the financial news. Let us take a look at that in the next step.<a data-type="indexterm" data-startref="ix_Chapter10-asciidoc14" id="idm45174897851688"/></p>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="4.2. Supervised learning—classification algorithms and LSTM"><div class="sect3" id="idm45174898016616">
<h3>4.2. Supervised learning—classification algorithms and LSTM</h3>

<p><a data-type="indexterm" data-primary="long short-term memory (LSTM)" data-secondary="sentiment analysis" id="ix_Chapter10-asciidoc15"/><a data-type="indexterm" data-primary="sentiment analysis-based trading strategies" data-secondary="supervised learning: classification algorithms and LSTM" id="ix_Chapter10-asciidoc16"/>In this step, we develop a customized model for sentiment analysis based on available labeled data. The label data for this is obtained from the <a href="https://www.kaggle.com">Kaggle website</a>:</p>

<pre data-type="programlisting">sentiments_data = pd.read_csv(r'Data\LabelledNewsData.csv', \
encoding="ISO-8859-1")
sentiments_data.head(1)</pre>

<p><code>Output</code></p>
<table>

<thead>
<tr>
<th/>
<th>datetime</th>
<th>headline</th>
<th>ticker</th>
<th>sentiment</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>0</p></td>
<td><p>1/16/2020 5:25</p></td>
<td><p>$MMM fell on hard times but could be set to re…</p></td>
<td><p>MMM</p></td>
<td><p>0</p></td>
</tr>
<tr>
<td><p>1</p></td>
<td><p>1/11/2020 6:43</p></td>
<td><p>Wolfe Research Upgrades 3M $MMM to ¡§Peer Perf…</p></td>
<td><p>MMM</p></td>
<td><p>1</p></td>
</tr>
</tbody>
</table>

<p>The data has headlines for the news across 30 different stocks, totaling 9,470 rows, and has sentiments labeled zero and one. We perform the classification steps using the classification model development template presented in <a data-type="xref" href="ch06.xhtml#Chapter6">Chapter 6</a>.</p>

<p>In order to run a supervised learning model, we first need to convert the news headlines into a feature representation. For this exercise, the underlying vector 
<span class="keep-together">representations</span> come from a <em>spaCy word embedding model</em>, which generally produces a dense, multidimensional semantic representation of words (as shown in the example below). The word embedding model includes 20,000 unique vectors with 300 dimensions. We apply this on all headlines in the data processed in the previous step:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">all_vectors</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="n">pd</code><code class="o">.</code><code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="n">token</code><code class="o">.</code><code class="n">vector</code> <code class="k">for</code> <code class="n">token</code> <code class="ow">in</code> <code class="n">nlp</code><code class="p">(</code><code class="n">s</code><code class="p">)</code> <code class="p">])</code><code class="o">.</code>\
<code class="n">mean</code><code class="p">(</code><code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code><code class="o">*</code><code class="n">pd</code><code class="o">.</code><code class="n">np</code><code class="o">.</code><code class="n">ones</code><code class="p">((</code><code class="mi">300</code><code class="p">))</code>\
 <code class="k">for</code> <code class="n">s</code> <code class="ow">in</code> <code class="n">sentiments_data</code><code class="p">[</code><code class="s">'headline'</code><code class="p">]])</code></pre>

<p>Now that we have prepared the independent variable, we train the classification model in a similar manner as discussed in <a data-type="xref" href="ch06.xhtml#Chapter6">Chapter 6</a>. We have the sentiments label zero or one as the dependent variable. We first divide the data into training and test sets and run the key classification models (i.e., logistic regression, CART, SVM, random forest, and artificial neural network).</p>

<p>We will also include LSTM, which is an RNN-based model,<sup><a data-type="noteref" id="idm45174897768808-marker" href="ch10.xhtml#idm45174897768808">5</a></sup> in the list of models considered. An RNN-based model performs well for NLP, because it stores the information for current features as well neighboring ones for prediction. It maintains a memory based on past information, which enables the model to predict the current output conditioned on long distance features and looks at the words in the context of the entire sentence, rather than simply looking at the individual words.</p>

<p>For us to be able to feed the data into our LSTM model, all input documents must have the same length. We use the Keras <code>tokenizer</code> function to tokenize the strings and then use <code>texts_to_sequences</code> to make sequences of words. More details can be found on the <a href="https://oreil.ly/2YS-P">Keras website</a>. We will limit the maximum review length to <em>max_words</em> by truncating longer reviews and pad shorter reviews with a null value (0). We can accomplish this using the <code>pad_sequences</code> function, also in Keras. The third parameter is the <em>input_length</em> (set to 50), which is the length of each comment sequence:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c">### Create sequence</code>
<code class="n">vocabulary_size</code> <code class="o">=</code> <code class="mi">20000</code>
<code class="n">tokenizer</code> <code class="o">=</code> <code class="n">Tokenizer</code><code class="p">(</code><code class="n">num_words</code><code class="o">=</code> <code class="n">vocabulary_size</code><code class="p">)</code>
<code class="n">tokenizer</code><code class="o">.</code><code class="n">fit_on_texts</code><code class="p">(</code><code class="n">sentiments_data</code><code class="p">[</code><code class="s">'headline'</code><code class="p">])</code>
<code class="n">sequences</code> <code class="o">=</code> <code class="n">tokenizer</code><code class="o">.</code><code class="n">texts_to_sequences</code><code class="p">(</code><code class="n">sentiments_data</code><code class="p">[</code><code class="s">'headline'</code><code class="p">])</code>
<code class="n">X_LSTM</code> <code class="o">=</code> <code class="n">pad_sequences</code><code class="p">(</code><code class="n">sequences</code><code class="p">,</code> <code class="n">maxlen</code><code class="o">=</code><code class="mi">50</code><code class="p">)</code></pre>

<p>In the following code snippet, we use the Keras library to build an artificial neural network classifier based on an underlying LSTM model. The network starts with an <em>embedding</em> layer. This layer lets the system expand each token to a larger vector, allowing the network to represent a word in a meaningful way. The layer takes 20,000 as the first argument (i.e., the size of our vocabulary) and 300 as the second input parameter (i.e., the dimension of the embedding). Finally, given that this is a classification problem and the output needs to be labeled as zero or one, the 
<span class="keep-together"><code>KerasClassifier</code></span> function is used as a wrapper over the LSTM model to produce a binary (zero or one) output:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">from</code> <code class="nn">keras.wrappers.scikit_learn</code> <code class="k">import</code> <code class="n">KerasClassifier</code>
<code class="k">def</code> <code class="nf">create_model</code><code class="p">(</code><code class="n">input_length</code><code class="o">=</code><code class="mi">50</code><code class="p">):</code>
    <code class="n">model</code> <code class="o">=</code> <code class="n">Sequential</code><code class="p">()</code>
    <code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Embedding</code><code class="p">(</code><code class="mi">20000</code><code class="p">,</code> <code class="mi">300</code><code class="p">,</code> <code class="n">input_length</code><code class="o">=</code><code class="mi">50</code><code class="p">))</code>
    <code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">LSTM</code><code class="p">(</code><code class="mi">100</code><code class="p">,</code> <code class="n">dropout</code><code class="o">=</code><code class="mf">0.2</code><code class="p">,</code> <code class="n">recurrent_dropout</code><code class="o">=</code><code class="mf">0.2</code><code class="p">))</code>
    <code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dense</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s">'sigmoid'</code><code class="p">))</code>
    <code class="n">model</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">loss</code><code class="o">=</code><code class="s">'binary_crossentropy'</code><code class="p">,</code> <code class="n">optimizer</code><code class="o">=</code><code class="s">'adam'</code><code class="p">,</code> \
    <code class="n">metrics</code><code class="o">=</code><code class="p">[</code><code class="s">'accuracy'</code><code class="p">])</code>
    <code class="k">return</code> <code class="n">model</code>
<code class="n">model_LSTM</code> <code class="o">=</code> <code class="n">KerasClassifier</code><code class="p">(</code><code class="n">build_fn</code><code class="o">=</code><code class="n">create_model</code><code class="p">,</code> <code class="n">epochs</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">verbose</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> \
  <code class="n">validation_split</code><code class="o">=</code><code class="mf">0.4</code><code class="p">)</code>
<code class="n">model_LSTM</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train_LSTM</code><code class="p">,</code> <code class="n">Y_train_LSTM</code><code class="p">)</code></pre>

<p>The comparison of all the machine learning models is as follows:</p>

<figure><div class="figure">
<img src="Images/mlbf_10in04.png" alt="mlbf 10in04" width="860" height="472"/>
<h6/>
</div></figure>

<p>As expected, the LSTM model has the best performance in the test set (accuracy of 96.7%) as compared to all other models. The performance of the ANN, with a training set accuracy of 99% and a test set accuracy of 93.8%, is comparable to the LSTM-based model. The performances of random forest (RF), SVM, and logistic regression (LR) are reasonable as well. CART and KNN do not perform as well as other models. CART shows high overfitting. Let us use the LSTM model for the computation of the sentiments in the data in the following steps.<a data-type="indexterm" data-startref="ix_Chapter10-asciidoc16" id="idm45174897544920"/><a data-type="indexterm" data-startref="ix_Chapter10-asciidoc15" id="idm45174897544216"/></p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="4.3. Unsupervised—model based on a financial lexicon"><div class="sect3" id="idm45174897850392">
<h3>4.3. Unsupervised—model based on a financial lexicon</h3>

<p>In this case study, we update the VADER lexicon with words and sentiments from a lexicon adapted to stock market conversations in microblogging services:</p>
<dl>
<dt><a data-type="indexterm" data-primary="sentiment analysis-based trading strategies" data-secondary="unsupervised learning: model based on financial lexicon" id="ix_Chapter10-asciidoc17"/>Lexicons</dt>
<dd>
<p><a data-type="indexterm" data-primary="lexicons, defined" id="idm45174897539272"/>Special dictionaries or vocabularies that have been created for analyzing sentiments. Most lexicons have a list of positive and negative <em>polar</em> words with some score associated with them. Using various techniques, such as the position of words, the surrounding words, context, parts of speech, and phrases, scores are assigned to the text documents for which we want to compute the sentiment. After aggregating these scores, we get the final sentiment:</p>
</dd>
<dt>VADER (Valence Aware Dictionary for Sentiment Reasoning)</dt>
<dd>
<p><a data-type="indexterm" data-primary="VADER (Valence Aware Dictionary for Sentiment Reasoning)" id="idm45174897536328"/>A prebuilt sentiment analysis model included in the NLTK package. It can give both positive and negative polarity scores as well as the strength of the emotion of a text sample. It is rule-based and relies heavily on human-rated texts. These are words or any textual form of communication labeled according to their semantic orientation as either positive or negative.</p>
</dd>
</dl>

<p>This lexical resource was automatically created using diverse statistical measures and a large set of labeled messages from StockTwits, which is a social media platform designed for sharing ideas among investors, traders, and entrepreneurs.<sup><a data-type="noteref" id="idm45174897534248-marker" href="ch10.xhtml#idm45174897534248">6</a></sup> The sentiments are between –1 and 1, similar to the sentiments from TextBlob. In the following code snippet, we train the model based on the financial sentiments:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c"># stock market lexicon</code>
<code class="n">sia</code> <code class="o">=</code> <code class="n">SentimentIntensityAnalyzer</code><code class="p">()</code>
<code class="n">stock_lex</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">read_csv</code><code class="p">(</code><code class="s">'Data/lexicon_data/stock_lex.csv'</code><code class="p">)</code>
<code class="n">stock_lex</code><code class="p">[</code><code class="s">'sentiment'</code><code class="p">]</code> <code class="o">=</code> <code class="p">(</code><code class="n">stock_lex</code><code class="p">[</code><code class="s">'Aff_Score'</code><code class="p">]</code> <code class="o">+</code> <code class="n">stock_lex</code><code class="p">[</code><code class="s">'Neg_Score'</code><code class="p">])</code><code class="o">/</code><code class="mi">2</code>
<code class="n">stock_lex</code> <code class="o">=</code> <code class="nb">dict</code><code class="p">(</code><code class="nb">zip</code><code class="p">(</code><code class="n">stock_lex</code><code class="o">.</code><code class="n">Item</code><code class="p">,</code> <code class="n">stock_lex</code><code class="o">.</code><code class="n">sentiment</code><code class="p">))</code>
<code class="n">stock_lex</code> <code class="o">=</code> <code class="p">{</code><code class="n">k</code><code class="p">:</code><code class="n">v</code> <code class="k">for</code> <code class="n">k</code><code class="p">,</code><code class="n">v</code> <code class="ow">in</code> <code class="n">stock_lex</code><code class="o">.</code><code class="n">items</code><code class="p">()</code> <code class="k">if</code> <code class="nb">len</code><code class="p">(</code><code class="n">k</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="s">' '</code><code class="p">))</code><code class="o">==</code><code class="mi">1</code><code class="p">}</code>
<code class="n">stock_lex_scaled</code> <code class="o">=</code> <code class="p">{}</code>
<code class="k">for</code> <code class="n">k</code><code class="p">,</code> <code class="n">v</code> <code class="ow">in</code> <code class="n">stock_lex</code><code class="o">.</code><code class="n">items</code><code class="p">():</code>
    <code class="k">if</code> <code class="n">v</code> <code class="o">&gt;</code> <code class="mi">0</code><code class="p">:</code>
        <code class="n">stock_lex_scaled</code><code class="p">[</code><code class="n">k</code><code class="p">]</code> <code class="o">=</code> <code class="n">v</code> <code class="o">/</code> <code class="nb">max</code><code class="p">(</code><code class="n">stock_lex</code><code class="o">.</code><code class="n">values</code><code class="p">())</code> <code class="o">*</code> <code class="mi">4</code>
    <code class="k">else</code><code class="p">:</code>
        <code class="n">stock_lex_scaled</code><code class="p">[</code><code class="n">k</code><code class="p">]</code> <code class="o">=</code> <code class="n">v</code> <code class="o">/</code> <code class="nb">min</code><code class="p">(</code><code class="n">stock_lex</code><code class="o">.</code><code class="n">values</code><code class="p">())</code> <code class="o">*</code> <code class="o">-</code><code class="mi">4</code>

<code class="n">final_lex</code> <code class="o">=</code> <code class="p">{}</code>
<code class="n">final_lex</code><code class="o">.</code><code class="n">update</code><code class="p">(</code><code class="n">stock_lex_scaled</code><code class="p">)</code>
<code class="n">final_lex</code><code class="o">.</code><code class="n">update</code><code class="p">(</code><code class="n">sia</code><code class="o">.</code><code class="n">lexicon</code><code class="p">)</code>
<code class="n">sia</code><code class="o">.</code><code class="n">lexicon</code> <code class="o">=</code> <code class="n">final_lex</code></pre>

<p class="pagebreak-before">Let us check the sentiment of a news item:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">text</code> <code class="o">=</code> <code class="s">"AAPL is trading higher after reporting its October sales</code><code class="se">\</code>
<code class="s">rose 12.6% M/M. It has seen a 20%+ jump in orders"</code></pre>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">sia</code><code class="o">.</code><code class="n">polarity_scores</code><code class="p">(</code><code class="n">text</code><code class="p">)[</code><code class="s">'compound'</code><code class="p">]</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">0.4535</pre>

<p>We get the sentiments for all the news headlines based in our dataset:</p>

<pre data-type="programlisting">vader_sentiments = pd.np.array([sia.polarity_scores(s)['compound']\
 for s in data_df['headline']])</pre>

<p>Let us look at the relationship between the returns and sentiments, which is computed using the lexicon-based methodology for the entire dataset.</p>

<figure><div class="figure">
<img src="Images/mlbf_10in05.png" alt="mlbf 10in05" width="389" height="263"/>
<h6/>
</div></figure>

<p>There are not many instances of high returns for lower sentiment scores, but the data may not be very clear. We will look deeper into the comparison of different types of sentiment analysis in the next section.<a data-type="indexterm" data-startref="ix_Chapter10-asciidoc17" id="idm45174897321176"/></p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="4.4. Exploratory data analysis and comparison"><div class="sect3" id="idm45174897542952">
<h3>4.4. Exploratory data analysis and comparison</h3>

<p><a data-type="indexterm" data-primary="sentiment analysis-based trading strategies" data-secondary="exploratory data analysis and comparison" id="ix_Chapter10-asciidoc18"/>In this section, we compare the sentiments computed using the different techniques presented above. Let us look at the sample headlines and the sentiments from three different methodologies, followed by a visual analysis:</p>
<table>

<thead>
<tr>
<th/>
<th>ticker</th>
<th>headline</th>
<th>sentiment_textblob</th>
<th>sentiment_LSTM</th>
<th>sentiment_lex</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>4620</p></td>
<td><p>TSM</p></td>
<td><p>TSMC (TSM +1.8%) is trading higher after reporting its
October sales rose 12.6% M/M. DigiTimes adds TSMC has seen a 20%+ jump
in orders from QCOM, NVDA, SPRD, and Mediatek. The numbers suggest TSMC
could beat its Q4 guidance (though December tends to be weak), and that
chip demand could be stabilizing after getting hit hard by inventory
corrections. (earlier) (UMC sales)</p></td>
<td><p>0.036667</p></td>
<td><p>1</p></td>
<td><p>0.5478</p></td>
</tr>
</tbody>
</table>

<p>Looking at one of the headlines, the sentiment from this sentence is positive. However, the TextBlob sentiment result is smaller in magnitude, suggesting that the sentiment is more neutral. This points back to the previous assumption that the model trained on movie sentiments likely will not be accurate for stock sentiments. The classification-based model correctly suggests the sentiment is positive, but it is binary. <code>Sentiment_lex</code> has a more intuitive output with a significantly positive 
<span class="keep-together">sentiment</span>.</p>

<p>Let us review the correlation of all the sentiments from different methodologies versus returns:</p>

<figure><div class="figure">
<img src="Images/mlbf_10in06.png" alt="mlbf 10in06" width="376" height="303"/>
<h6/>
</div></figure>

<p>All sentiments have positive relationships with the returns, which is intuitive and expected. The sentiments from the lexicon methodology are highest, which means the stock’s event return can be predicted the best using the lexicon methodology. Recall that this methodology leverages financial terms in the model. The LSTM-based method also performs better than the TextBlob approach, but the performance is slightly worse compared to the lexicon-based methodology.</p>

<p class="pagebreak-before">Let us look at the 
<span class="keep-together">performance</span> of the methodology at the ticker level. We chose a few tickers with the highest market cap for the analysis:</p>

<figure><div class="figure">
<img src="Images/mlbf_10in07.png" alt="mlbf 10in07" width="596" height="486"/>
<h6/>
</div></figure>

<p>Looking at the chart, the correlation from the lexicon methodology is highest across all stock tickers, which corroborates the conclusion from the previous analysis. It means the returns can be predicted the best using the lexicon methodology. The TextBlob-based sentiments show unintuitive results in some cases, such as with JPM.</p>

<p class="pagebreak-before">Let us look at the scatterplot for lexicon versus TextBlob methodologies for AMZN and GOOG. We will set the LSTM-based method aside since the binary sentiments will not be meaningful in the scatterplot:</p>

<figure><div class="figure">
<img src="Images/mlbf_10in08.png" alt="mlbf 10in08" width="842" height="263"/>
<h6/>
</div></figure>

<figure><div class="figure">
<img src="Images/mlbf_10in09.png" alt="mlbf 10in09" width="843" height="263"/>
<h6/>
</div></figure>

<p>The lexicon-based sentiments on the left show a positive relationship between the sentiments and returns. Some of the points with the highest returns are associated with the most positive news. Also, the scatterplot is more uniformly distributed in the case of lexicon as compared to TextBlob. The sentiments for TextBlob are concentrated around zero, probably because the model is not able to categorize financial sentiments well. For the trading strategy, we will be using the lexicon-based sentiments, as these are the most appropriate based on the analysis in this section. The LSTM-based sentiments are good as well, but they are labeled either zero or one. The more granular lexicon-based sentiments are preferred<a data-type="indexterm" data-startref="ix_Chapter10-asciidoc18" id="idm45174897252472"/>.<a data-type="indexterm" data-startref="ix_Chapter10-asciidoc13" id="idm45174897251640"/></p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="5. Models evaluation—building a trading strategy"><div class="sect3" id="idm45174897319880">
<h3>5. Models evaluation—building a trading strategy</h3>

<p><a data-type="indexterm" data-primary="sentiment analysis-based trading strategies" data-secondary="models evaluation" id="ix_Chapter10-asciidoc19"/>The sentiment data can be used in several ways for building a trading strategy. The sentiments can be used as a stand-alone signal to decide buy, sell, or hold actions. The sentiment score or the word vectors can also be used to predict the return or price of a stock. That prediction can be used to build a trading strategy.</p>

<p>In this section, we demonstrate a trading strategy in which we buy or sell a stock based on the following approach:</p>

<ul>
<li>
<p>Buy a stock when the change in sentiment score (current sentiment score/previous sentiment score) is greater than 0.5. Sell a stock when the change in sentiment score is less than –0.5. The sentiment score used here is based on the lexicon-based sentiments computed in the previous step.</p>
</li>
<li>
<p>In addition to the sentiments, we use moving average (based on the last 15 days) while making a buy or sell decision.</p>
</li>
<li>
<p>Trades (i.e., buy or sell) are in 100 shares. The initial amount available for trading is set to $100,000.</p>
</li>
</ul>

<p>The strategy threshold, the lot size, and the initial capital can be tweaked depending on the performance of the strategy.</p>












<section data-type="sect4" data-pdf-bookmark="5.1. Setting up a strategy"><div class="sect4" id="idm45174897243016">
<h4>5.1. Setting up a strategy</h4>

<p><a data-type="indexterm" data-primary="backtrader" id="idm45174897241848"/><a data-type="indexterm" data-primary="sentiment analysis-based trading strategies" data-secondary="setting up a strategy" id="idm45174897241144"/>To set up the trading strategy, we use <em>backtrader</em>, which is a convenient Python-based framework for implementing and backtesting trading strategies. Backtrader allows us to write reusable trading strategies, indicators, and analyzers instead of having to spend time building infrastructure. We use the <a href="https://oreil.ly/lyYs4">Quickstart code in the backtrader documentation</a> as a base and adapt it to our sentiment-based trading strategy.</p>

<p>The following code snippet summarizes the buy and sell logic for the strategy. Refer to the Jupyter notebook of this case study for the detailed implementation:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c"># buy if current close more than simple moving average (sma)</code>
<code class="c"># AND sentiment increased by &gt;= 0.5</code>
<code class="k">if</code> <code class="bp">self</code><code class="o">.</code><code class="n">dataclose</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code> <code class="o">&gt;</code> <code class="bp">self</code><code class="o">.</code><code class="n">sma</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code> <code class="ow">and</code> <code class="bp">self</code><code class="o">.</code><code class="n">sentiment</code> <code class="o">-</code> <code class="n">prev_sentiment</code> <code class="o">&gt;=</code> <code class="mf">0.5</code><code class="p">:</code>
  <code class="bp">self</code><code class="o">.</code><code class="n">order</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">buy</code><code class="p">()</code>

<code class="c"># sell if current close less than simple moving average(sma)</code>
<code class="c"># AND sentiment decreased by &gt;= 0.5</code>
<code class="k">if</code> <code class="bp">self</code><code class="o">.</code><code class="n">dataclose</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code> <code class="o">&lt;</code> <code class="bp">self</code><code class="o">.</code><code class="n">sma</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code> <code class="ow">and</code> <code class="bp">self</code><code class="o">.</code><code class="n">sentiment</code> <code class="o">-</code> <code class="n">prev_sentiment</code> <code class="o">&lt;=</code> <code class="o">-</code><code class="mf">0.5</code><code class="p">:</code>
  <code class="bp">self</code><code class="o">.</code><code class="n">order</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">sell</code><code class="p">()</code></pre>
</div></section>













<section data-type="sect4" data-pdf-bookmark="5.2. Results for individual stocks"><div class="sect4" id="idm45174897236264">
<h4>5.2. Results for individual stocks</h4>

<p><a data-type="indexterm" data-primary="sentiment analysis-based trading strategies" data-secondary="results for individual stocks" id="ix_Chapter10-asciidoc20"/>First, we run our strategy on GOOG and look at the results:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">ticker</code> <code class="o">=</code> <code class="s">'GOOG'</code>
<code class="n">run_strategy</code><code class="p">(</code><code class="n">ticker</code><code class="p">,</code> <code class="n">start</code> <code class="o">=</code> <code class="s">'2012-01-01'</code><code class="p">,</code> <code class="n">end</code> <code class="o">=</code> <code class="s">'2018-12-12'</code><code class="p">)</code></pre>

<p>The output shows the trading log for some of the days and the final return:</p>

<p><code>Output</code></p>

<pre data-type="programlisting">Starting Portfolio Value: 100000.00
2013-01-10, Previous Sentiment 0.08, New Sentiment 0.80 BUY CREATE, 369.36
2014-07-17, Previous Sentiment 0.73, New Sentiment -0.22 SELL CREATE, 572.16
2014-07-18, OPERATION PROFIT, GROSS 22177.00, NET 22177.00
2014-07-18, Previous Sentiment -0.22, New Sentiment 0.77 BUY CREATE, 593.45
2014-09-12, Previous Sentiment 0.66, New Sentiment -0.05 SELL CREATE, 574.04
2014-09-15, OPERATION PROFIT, GROSS -1876.00, NET -1876.00
2015-07-17, Previous Sentiment 0.01, New Sentiment 0.90 BUY CREATE, 672.93
.
.
.
2018-12-11, Ending Value 149719.00</pre>

<p>We analyze the backtesting result in the following plot produced by the backtrader package. Refer to the Jupyter notebook of this case study for the detailed version of this chart.</p>

<figure><div class="figure">
<img src="Images/mlbf_10in10.png" alt="mlbf 10in10" width="700" height="428"/>
<h6/>
</div></figure>

<p>The results show an overall profit of $49,719. The chart is a typical chart<sup><a data-type="noteref" id="idm45174897105944-marker" href="ch10.xhtml#idm45174897105944">7</a></sup> produced by the backtrader package and is divided into four panels:</p>
<dl>
<dt>Top panel</dt>
<dd>
<p>The top panel is the <em>cash value observer</em>. It keeps track of the cash and the total portolio value during the life of the backtesting run. In this run, we started with $100,000 and ended with $149,719.</p>
</dd>
<dt>Second panel</dt>
<dd>
<p>This panel is the <em>trade observer</em>. It shows the realized profit/loss of each trade. A trade is defined as opening a position and taking the position back to zero (directly or crossing over from long to short or short to long). Looking at this panel, five out of eight trades are profitable for the strategy.</p>
</dd>
<dt>Third panel</dt>
<dd>
<p>This panel is <em>buy sell observer</em>. It indicates where buy and sell operations have taken place. In general, we see that the buy action takes place when the stock price is increasing, and the sell action takes place when the stock price has started declining.</p>
</dd>
<dt>Bottom panel</dt>
<dd>
<p>This panel shows the sentiment score, varying between –1 and 1.</p>
</dd>
</dl>

<p>Now we choose one of the days (2015-07-17) when a buy action was triggered and analyze the news for Google on that and the previous day:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">GOOG_ticker</code><code class="o">=</code> <code class="n">data_df</code><code class="p">[</code><code class="n">data_df</code><code class="p">[</code><code class="s">'ticker'</code><code class="p">]</code><code class="o">.</code><code class="n">isin</code><code class="p">([</code><code class="n">ticker</code><code class="p">])]</code>
<code class="n">New</code><code class="o">=</code> <code class="nb">list</code><code class="p">(</code><code class="n">GOOG_ticker</code><code class="p">[</code><code class="n">GOOG_ticker</code><code class="p">[</code><code class="s">'date'</code><code class="p">]</code> <code class="o">==</code>  <code class="s">'2015-07-17'</code><code class="p">][</code><code class="s">'headline'</code><code class="p">])</code>
<code class="n">Old</code><code class="o">=</code> <code class="nb">list</code><code class="p">(</code><code class="n">GOOG_ticker</code><code class="p">[</code><code class="n">GOOG_ticker</code><code class="p">[</code><code class="s">'date'</code><code class="p">]</code> <code class="o">==</code>  <code class="s">'2015-07-16'</code><code class="p">][</code><code class="s">'headline'</code><code class="p">])</code>
<code class="nb">print</code><code class="p">(</code><code class="s">"Current News:"</code><code class="p">,</code><code class="n">New</code><code class="p">,</code><code class="s">"</code><code class="se">\n\n</code><code class="s">"</code><code class="p">,</code><code class="s">"Previous News:"</code><code class="p">,</code> <code class="n">Old</code><code class="p">)</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">Current News: ["Axiom Securities has upgraded Google (GOOG +13.4%, GOOGL +14.8%)
to Buy following the company's Q2 beat and investor-pleasing comments about
spending discipline, potential capital returns, and YouTube/mobile growth. MKM
has launched coverage at Buy, and plenty of other firms have hiked their targets.
Google's market cap is now above $450B."]

Previous News: ["While Google's (GOOG, GOOGL) Q2 revenue slightly missed
estimates when factoring traffic acquisitions costs (TAC), its ex-TAC revenue of
$14.35B was slightly above a $14.3B consensus. The reason: TAC fell to 21% of ad
revenue from Q1's 22% and Q2 2014's 23%. That also, of course, helped EPS beat
estimates.", 'Google (NASDAQ:GOOG): QC2 EPS of $6.99 beats by $0.28.']</pre>

<p>Clearly, the news on the selected day mentions the upgrade of Google, a piece of positive news. The previous day mentions the revenue missing estimates, which is negative news. Hence, there was a significant change of the news sentiment on the selected day, resulting in a buy action triggered by the trading algorithm.</p>

<p>Next, we run the strategy for FB:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">ticker</code> <code class="o">=</code> <code class="s">'FB'</code>
<code class="n">run_strategy</code><code class="p">(</code><code class="n">ticker</code><code class="p">,</code> <code class="n">start</code> <code class="o">=</code> <code class="s">'2012-01-01'</code><code class="p">,</code> <code class="n">end</code> <code class="o">=</code> <code class="s">'2018-12-12'</code><code class="p">)</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">Start Portfolio value: 100000.00
Final Portfolio Value: 108041.00
Profit: 8041.00</pre>

<figure><div class="figure">
<img src="Images/mlbf_10in12.png" alt="mlbf 10in12" width="700" height="426"/>
<h6/>
</div></figure>

<p>The details of the backtesting results of the strategy are as follows:</p>
<dl>
<dt>Top panel</dt>
<dd>
<p>The cash value panel shows an overall profit of $8,041.</p>
</dd>
<dt>Second panel</dt>
<dd>
<p>The trade observer panel shows that six out of seven actions were profitable.</p>
</dd>
<dt>Third panel</dt>
<dd>
<p>The buy/sell observer shows that in general the buy (sell) action took place when the stock price was increasing (decreasing).</p>
</dd>
<dt>Bottom panel</dt>
<dd>
<p>It shows a high number of positive sentiments for FB around the 2013–2014 period.<a data-type="indexterm" data-startref="ix_Chapter10-asciidoc20" id="idm45174896937000"/></p>
</dd>
</dl>
</div></section>













<section data-type="sect4" data-pdf-bookmark="5.3. Results for multiple stocks"><div class="sect4" id="idm45174897119608">
<h4>5.3. Results for multiple stocks</h4>

<p><a data-type="indexterm" data-primary="sentiment analysis-based trading strategies" data-secondary="results for multiple stocks" id="idm45174896935144"/>In the previous step, we executed the trading strategy on individual stocks. Here, we run it on all 10 stocks for which we computed the 
<span class="keep-together">sentiments</span>:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">results_tickers</code> <code class="o">=</code> <code class="p">{}</code>
<code class="k">for</code> <code class="n">ticker</code> <code class="ow">in</code> <code class="n">tickers</code><code class="p">:</code>
    <code class="n">results_tickers</code><code class="p">[</code><code class="n">ticker</code><code class="p">]</code> <code class="o">=</code> <code class="n">run_strategy</code><code class="p">(</code><code class="n">ticker</code><code class="p">,</code> <code class="n">start</code> <code class="o">=</code> <code class="s">'2012-01-01'</code><code class="p">,</code> \
    <code class="n">end</code> <code class="o">=</code> <code class="s">'2018-12-12'</code><code class="p">)</code>
<code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="o">.</code><code class="n">from_dict</code><code class="p">(</code><code class="n">results_tickers</code><code class="p">)</code><code class="o">.</code><code class="n">set_index</code><code class="p">(</code>\
  <code class="p">[</code><code class="n">pd</code><code class="o">.</code><code class="n">Index</code><code class="p">([</code><code class="s">"PerUnitStartPrice"</code><code class="p">,</code> <code class="n">StrategyProfit</code><code class="s">'])])</code></pre>

<p class="pagebreak-before"><code>Output</code></p>

<figure><div class="figure">
<img src="Images/mlbf_10in13.png" alt="mlbf 10in13" width="706" height="64"/>
<h6/>
</div></figure>

<p>The strategy performs quite well and yields an overall profit for all the stocks. As mentioned before, the buy and sell actions are performed in a lot size of 100. Hence, the dollar amount used is proportional to the stock price. We see the highest nominal profit from AMZN and GOOG, which is primarily attributed to the high dollar amounts invested for these stocks given their high stock price. Other than overall profit, several other metrics, such as Sharpe ratio and maximum drawdown, can be used to analyze the performance.</p>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="5.4. Varying the strategy time period"><div class="sect3" id="idm45174896835320">
<h3>5.4. Varying the strategy time period</h3>

<p><a data-type="indexterm" data-primary="sentiment analysis-based trading strategies" data-secondary="varying the strategy time period" id="idm45174896833752"/>In the previous analysis, we used the time period from 2011 to 2018 for our backtesting. In this step, to further analyze the effectiveness of our strategy, we vary the time period of the backtesting and analyze the results. First, we run the strategy for all the stocks for the time period between 2012 and 2014:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">results_tickers</code> <code class="o">=</code> <code class="p">{}</code>
<code class="k">for</code> <code class="n">ticker</code> <code class="ow">in</code> <code class="n">tickers</code><code class="p">:</code>
    <code class="n">results_tickers</code><code class="p">[</code><code class="n">ticker</code><code class="p">]</code> <code class="o">=</code> <code class="n">run_strategy</code><code class="p">(</code><code class="n">ticker</code><code class="p">,</code> <code class="n">start</code> <code class="o">=</code> <code class="s">'2012-01-01'</code><code class="p">,</code> \
    <code class="n">end</code> <code class="o">=</code> <code class="s">'2014-12-31'</code><code class="p">)</code></pre>

<p><code>Output</code></p>

<figure><div class="figure">
<img src="Images/mlbf_10in14.png" alt="mlbf 10in14" width="700" height="64"/>
<h6/>
</div></figure>

<p>The strategy yields an overall profit for all the stocks except AMZN and WMT. Now we run the strategy between 2016 and 2018:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">results_tickers</code> <code class="o">=</code> <code class="p">{}</code>
<code class="k">for</code> <code class="n">ticker</code> <code class="ow">in</code> <code class="n">tickers</code><code class="p">:</code>
    <code class="n">results_tickers</code><code class="p">[</code><code class="n">ticker</code><code class="p">]</code> <code class="o">=</code> <code class="n">run_strategy</code><code class="p">(</code><code class="n">ticker</code><code class="p">,</code> <code class="n">start</code> <code class="o">=</code> <code class="s">'2016-01-01'</code><code class="p">,</code> \
    <code class="n">end</code> <code class="o">=</code> <code class="s">'2018-12-31'</code><code class="p">)</code></pre>

<p><code>Output</code></p>

<figure><div class="figure">
<img src="Images/mlbf_10in15.png" alt="mlbf 10in15" width="677" height="64"/>
<h6/>
</div></figure>

<p>We see a good performance of the sentiment-based strategy across all the stocks except AAPL, and we can conclude that it performs quite well on different time periods. The strategy can be adjusted by modifying the trading rules or lot sizes. Additional metrics can also be used to understand the performance of the strategy. The sentiments can also be used along with the other features, such as correlated variables and technical indicators for prediction.<a data-type="indexterm" data-startref="ix_Chapter10-asciidoc19" id="idm45174896715080"/></p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Conclusion"><div class="sect3" id="idm45174896834696">
<h3>Conclusion</h3>

<p>In this case study, we looked at various ways in which unstructured data can be converted to structured data and then used for analysis and prediction using tools for NLP. We have demonstrated three different approaches, including deep learning models to develop a model for computing the sentiments. We performed a comparison of the models and concluded that one of the most important steps in training the model for sentiment analysis is using a domain-specific vocabulary.</p>

<p>We also used a pretrained English model by spaCy to convert a sentence into sentiments and used the sentiments as signals to develop a trading strategy. The initial results suggested that the model trained on a financial lexicon–based sentiment could prove to be a viable model for a trading strategy. Additional improvements to this can be made by using more complex pretrained sentiment analysis models, such as BERT by Google, or different pretrained NLP models available in open source platforms. Existing NLP libraries fill in some of the preprocessing and encoding steps to allow us to focus on the inference step.</p>

<p>We could build on the trading strategy by including more correlated variables, technical indicators, or even improved sentiment analysis by using more sophisticated preprocessing steps and models based on more relevant financial text data.<a data-type="indexterm" data-startref="ix_Chapter10-asciidoc11" id="idm45174896710616"/></p>
<img class="blueprint bottom" src="Images/bracket_bottom.png" width="1625" height="32"/>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Case Study 2: Chatbot Digital Assistant"><div class="sect1" id="CaseStudy2NLP">
<h1>Case Study 2: Chatbot Digital Assistant</h1>

<p><a data-type="indexterm" data-primary="chatbots" id="ix_Chapter10-asciidoc21"/><a data-type="indexterm" data-primary="chatbots" data-secondary="NLP case study" id="ix_Chapter10-asciidoc22"/><em>Chatbots</em> are computer programs that maintain a conversation with a user in natural language. They can understand the user’s intent and send responses based on an organization’s business rules and data. These chatbots use deep learning and NLP to process language, enabling them to understand human speech.</p>

<p>Chatbots are increasingly being implemented across many domains for financial services. Banking bots enable consumers to check their balance, transfer money, pay bills, and more. Brokering bots enable consumers to find investment options, make investments, and track balances. Customer support bots provide instant responses, dramatically increasing customer satisfaction. News bots deliver personalized current events information, while enterprise bots enable employees to check leave balance, file expenses, check their inventory balance, and approve transactions. In addition to automating the process of assisting customers and employees, chatbots can help financial institutions gain information about their customers. The bot phenomenon has the potential to cause broad disruption in many areas within the finance sector.</p>

<p>Depending on the way bots are programmed, we can categorize chatbots into two variants:</p>
<dl>
<dt>Rule-based</dt>
<dd>
<p><a data-type="indexterm" data-primary="chatbots" data-secondary="rule-based versus self-learning" id="idm45174896701496"/><a data-type="indexterm" data-primary="rule-based chatbots" id="idm45174896700552"/>This variety of chatbots is trained according to rules. These chatbots do not learn through interactions and may sometimes fail to answer complex queries outside of the defined rules.<a data-type="indexterm" data-primary="self-learning chatbots" id="idm45174896699560"/><a data-type="indexterm" data-primary="retrieval-based chatbots" id="idm45174896698888"/><a data-type="indexterm" data-primary="generative chatbots" id="idm45174896698248"/></p>
</dd>
</dl>
<dl>
<dt>Self-learning</dt>
<dd><p>This variety of bots relies on ML and AI technologies to converse with users. Self-learning chatbots are further divided into <em>retrieval-based</em> and <em>generative</em>:</p>
<dl>
<dt>Retrieval-based</dt>
<dd><p>These chatbot are trained to rank the best response from a finite set of predefined responses.</p></dd>
<dt>Generative</dt>
<dd><p>These chatbots are not built with predefined responses. Instead, they are trained using a large number of previous conversations. They require a very large amount of conversational data to train.</p></dd>
</dl>
</dd>
</dl>

<p>In this case study, we will prototype a self-learning chatbot that can answer financial questions.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45174896691304">
<h5/>
<p>This case study focuses on:</p>

<ul>
<li>
<p>Building a customized logic and rules parser using NLP for a chatbot.</p>
</li>
<li>
<p>Understanding the data preparation required for building a chatbot.</p>
</li>
<li>
<p>Understanding the basic building blocks of a chatbot.</p>
</li>
<li>
<p>Leveraging available Python packages and corpuses to train a chatbot in a few lines of code.</p>
</li>
</ul>
</div></aside>
<img class="blueprint top" src="Images/bracket_top.png" width="1625" height="32"/>








<section data-type="sect2" class="blueprint" data-pdf-bookmark="Blueprint for Creating a Custom Chatbot Using NLP"><div class="sect2" id="idm45174896661208">
<h2>Blueprint for Creating a Custom Chatbot Using NLP</h2>










<section data-type="sect3" data-pdf-bookmark="1. Problem definition"><div class="sect3" id="idm45174896659528">
<h3>1. Problem definition</h3>

<p>The goal of this case study is to build a basic prototype of a conversational chatbot powered by NLP. The primary purpose of this chatbot is to help a user retrieve a financial ratio about a particular company. Such chatbots are designed to quickly retrieve the details about a stock or an instrument that may help the user make a trading decision.</p>

<p>In addition to retrieving a financial ratio, the chatbot could also engage in casual conversations with a user, perform basic mathematical calculations, and provide answers to questions from a list used to train it. We intend to use Python packages and functions for chatbot creation and to customize several components of the chatbot architecture to adapt to our requirements.</p>

<p>The chatbot prototype created in this case study is designed to understand user inputs and intention and retrieve the information they are seeking. It is a small prototype that could be enhanced for use as an information retrieval bot in banking, brokering, or customer support.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="2. Getting started—loading the libraries"><div class="sect3" id="idm45174896656056">
<h3>2. Getting started—loading the libraries</h3>

<p><a data-type="indexterm" data-primary="chatbots" data-secondary="loading libraries" id="idm45174896654824"/>For this case study, we will use two text-based libraries: spaCy and <a href="https://oreil.ly/_1DPE">ChatterBot</a>. spaCy has been previously introduced; ChatterBot is a Python library used to create simple chatbots with minimal programming required.</p>

<p>An untrained instance of ChatterBot starts off with no knowledge of how to communicate. Each time a user enters a statement, the library saves the input and response text. As ChatterBot receives more inputs, the number of responses it can offer and the accuracy of those responses increase. The program selects the response by searching for the closest matching known statement to the input. It then returns the most likely response to that statement based on how frequently each response is issued by the people the bot communicates with.</p>












<section data-type="sect4" data-pdf-bookmark="2.1. Load libraries"><div class="sect4" id="idm45174896651800">
<h4>2.1. Load libraries</h4>

<p><a data-type="indexterm" data-primary="spaCy" data-secondary="importing" id="idm45174896650424"/>We import spaCy using the following Python code:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">import</code> <code class="nn">spacy</code> <code class="c">#Custom NER model.</code>
<code class="kn">from</code> <code class="nn">spacy.util</code> <code class="k">import</code> <code class="n">minibatch</code><code class="p">,</code> <code class="n">compounding</code></pre>

<p><a data-type="indexterm" data-primary="ChatterBot" id="idm45174896641336"/>The ChatterBot library has the modules <code>LogicAdapter</code>, <code>ChatterBotCorpusTrainer</code>, and <code>ListTrainer</code>. These modules are used by our bot in order to construct responses to user queries. We begin by importing the following:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">from</code> <code class="nn">chatterbot</code> <code class="k">import</code> <code class="n">ChatBot</code>
<code class="kn">from</code> <code class="nn">chatterbot.logic</code> <code class="k">import</code> <code class="n">LogicAdapter</code>
<code class="kn">from</code> <code class="nn">chatterbot.trainers</code> <code class="k">import</code> <code class="n">ChatterBotCorpusTrainer</code>
<code class="kn">from</code> <code class="nn">chatterbot.trainers</code> <code class="k">import</code> <code class="n">ListTrainer</code></pre>

<p>Other libraries used in this exercise are as follows:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">import</code> <code class="nn">random</code>
<code class="kn">from</code> <code class="nn">itertools</code> <code class="k">import</code> <code class="n">product</code></pre>

<p>Before we move to the customized chatbot, let us develop a chatbot using the default features of the ChatterBot package.</p>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="3. Training a default chatbot"><div class="sect3" id="idm45174896572440">
<h3>3. Training a default chatbot</h3>

<p><a data-type="indexterm" data-primary="chatbots" data-secondary="training a default chatbot" id="ix_Chapter10-asciidoc23"/>ChatterBot and many other chatbot packages come with a data utility module that can be used to train chatbots. Here are the ChatterBot components we will be using:</p>
<dl>
<dt>Logic adapters</dt>
<dd>
<p>Logic adapters determine the logic for how ChatterBot selects a response to a given input statement. It is possible to enter any number of logic adapters for your bot to use. In the example below, we are using two inbuilt adapters: <em>BestMatch</em>, which returns the best known responses, and <em>MathematicalEvaluation</em>, which performs mathematical operations.</p>
</dd>
<dt>Preprocessors</dt>
<dd>
<p>ChatterBot’s preprocessors are simple functions that modify the input statement a chatbot receives before the statement gets processed by the logic adapter. The preprocessors can be customized to perform different preprocessing steps, such as tokenization and lemmatization, in order to have clean and processed data. In the example below, the default preprocessor for cleaning white spaces, <code>clean_whitespace</code>, is used.</p>
</dd>
<dt>Corpus training</dt>
<dd>
<p>ChatterBot comes with a corpus data and utility module that makes it easy to quickly train the bot to communicate. We use the already existing corpuses <em>english, english.greetings</em>, and <em>english.conversations</em> for training the chatbot.</p>
</dd>
<dt>List training</dt>
<dd>
<p>Just like the corpus training, we train the chatbot with the conversations that can be used for training using <em>ListTrainer</em>. In the example below, we have trained the chatbot using some sample commands. The chatbot can be trained using a significant amount of conversation data.</p>
</dd>
</dl>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">chatB</code> <code class="o">=</code> <code class="n">ChatBot</code><code class="p">(</code><code class="s">"Trader"</code><code class="p">,</code>
                <code class="n">preprocessors</code><code class="o">=</code><code class="p">[</code><code class="s">'chatterbot.preprocessors.clean_whitespace'</code><code class="p">],</code>
                <code class="n">logic_adapters</code><code class="o">=</code><code class="p">[</code><code class="s">'chatterbot.logic.BestMatch'</code><code class="p">,</code>
                                <code class="s">'chatterbot.logic.MathematicalEvaluation'</code><code class="p">])</code>

<code class="c"># Corpus Training</code>
<code class="n">trainerCorpus</code> <code class="o">=</code> <code class="n">ChatterBotCorpusTrainer</code><code class="p">(</code><code class="n">chatB</code><code class="p">)</code>

<code class="c"># Train based on English Corpus</code>
<code class="n">trainerCorpus</code><code class="o">.</code><code class="n">train</code><code class="p">(</code>
    <code class="s">"chatterbot.corpus.english"</code>
<code class="p">)</code>
<code class="c"># Train based on english greetings corpus</code>
<code class="n">trainerCorpus</code><code class="o">.</code><code class="n">train</code><code class="p">(</code><code class="s">"chatterbot.corpus.english.greetings"</code><code class="p">)</code>

<code class="c"># Train based on the english conversations corpus</code>
<code class="n">trainerCorpus</code><code class="o">.</code><code class="n">train</code><code class="p">(</code><code class="s">"chatterbot.corpus.english.conversations"</code><code class="p">)</code>

<code class="n">trainerConversation</code> <code class="o">=</code> <code class="n">ListTrainer</code><code class="p">(</code><code class="n">chatB</code><code class="p">)</code>
<code class="c"># Train based on conversations</code>

<code class="c"># List training</code>
<code class="n">trainerConversation</code><code class="o">.</code><code class="n">train</code><code class="p">([</code>
    <code class="s">'Help!'</code><code class="p">,</code>
    <code class="s">'Please go to google.com'</code><code class="p">,</code>
    <code class="s">'What is Bitcoin?'</code><code class="p">,</code>
    <code class="s">'It is a decentralized digital currency'</code>
<code class="p">])</code>

<code class="c"># You can train with a second list of data to add response variations</code>
<code class="n">trainerConversation</code><code class="o">.</code><code class="n">train</code><code class="p">([</code>
    <code class="s">'What is Bitcoin?'</code><code class="p">,</code>
    <code class="s">'Bitcoin is a cryptocurrency.'</code>
<code class="p">])</code></pre>

<p>Once the chatbot is trained, we can test the trained chatbot by having the following conversation:</p>

<pre data-type="programlisting">&gt;Hi
How are you doing?

&gt;I am doing well.
That is good to hear

&gt;What is 78964 plus 5970
78964 plus 5970 = 84934

&gt;what is a dollar
dollar: unit of currency in the united states.

&gt;What is Bitcoin?
It is a decentralized digital currency

&gt;Help!
Please go to google.com

&gt;Tell me a joke
Did you hear the one about the mountain goats in the andes? It was "ba a a a d".

&gt;What is Bitcoin?
Bitcoin is a cryptocurrency.</pre>

<p>In this example, we see a chatbot that gives an intuitive reply in response to the input. The first two responses are due to the training on the English greetings and English conversation corpuses. Additionally, the responses to <em>Tell me a joke</em> and <em>what is a dollar</em> are due to the training on the English corpus. The computation in the fourth line is the result of the chatbot being trained on the <code>MathematicalEvaluation</code> logical adapter. The responses to <em>Help!</em> and <em>What is Bitcoin?</em> are the result of the customized list trainers. Additionally, we see two different replies to <em>What is Bitcoin?</em>, given that we trained it using the list trainers.</p>

<p>Next, we move on to creating a chatbot designed to use a customized logical adapter to give financial ratios.<a data-type="indexterm" data-startref="ix_Chapter10-asciidoc23" id="idm45174896454760"/></p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="4. Data preparation: Customized chatbot"><div class="sect3" id="idm45174896453928">
<h3>4. Data preparation: Customized chatbot</h3>

<p><a data-type="indexterm" data-primary="chatbots" data-secondary="data preparation for customized chatbot" id="idm45174896452392"/>We want our chatbot to be able to recognize and group subtly different inquiries. For example, one might want to ask about the company <em>Apple Inc.</em> by simply referring to it as <em>Apple</em>, and we would want to map it to a ticker—<em>AAPL</em>, in this case. Constructing commonly used phrases in order to refer to firms can be built by using a dictionary as follows:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">companies</code> <code class="o">=</code> <code class="p">{</code>
    <code class="s">'AAPL'</code><code class="p">:</code>  <code class="p">[</code><code class="s">'Apple'</code><code class="p">,</code> <code class="s">'Apple Inc'</code><code class="p">],</code>
    <code class="s">'BAC'</code><code class="p">:</code> <code class="p">[</code><code class="s">'BAML'</code><code class="p">,</code> <code class="s">'BofA'</code><code class="p">,</code> <code class="s">'Bank of America'</code><code class="p">],</code>
    <code class="s">'C'</code><code class="p">:</code> <code class="p">[</code><code class="s">'Citi'</code><code class="p">,</code> <code class="s">'Citibank'</code><code class="p">],</code>
    <code class="s">'DAL'</code><code class="p">:</code> <code class="p">[</code><code class="s">'Delta'</code><code class="p">,</code> <code class="s">'Delta Airlines'</code><code class="p">]</code>
<code class="p">}</code></pre>

<p>Similarly, we want to build a map for financial ratios:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">ratios</code> <code class="o">=</code> <code class="p">{</code>
    <code class="s">'return-on-equity-ttm'</code><code class="p">:</code> <code class="p">[</code><code class="s">'ROE'</code><code class="p">,</code> <code class="s">'Return on Equity'</code><code class="p">],</code>
    <code class="s">'cash-from-operations-quarterly'</code><code class="p">:</code> <code class="p">[</code><code class="s">'CFO'</code><code class="p">,</code> <code class="s">'Cash Flow from Operations'</code><code class="p">],</code>
    <code class="s">'pe-ratio-ttm'</code><code class="p">:</code> <code class="p">[</code><code class="s">'PE'</code><code class="p">,</code> <code class="s">'Price to equity'</code><code class="p">,</code> <code class="s">'pe ratio'</code><code class="p">],</code>
    <code class="s">'revenue-ttm'</code><code class="p">:</code> <code class="p">[</code><code class="s">'Sales'</code><code class="p">,</code> <code class="s">'Revenue'</code><code class="p">],</code>
<code class="p">}</code></pre>

<p>The keys of this dictionary can be used to map to an internal system or API. Finally, we want the user to be able to request the phrase in multiple formats. Saying <em>Get me the [RATIO] for [COMPANY]</em> should be treated similarly to <em>What is the [RATIO] for [COMPANY]?</em> We build these sentence templates for our model to train on by building a list as follows:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">string_templates</code> <code class="o">=</code> <code class="p">[</code><code class="s">'Get me the {ratio} for {company}'</code><code class="p">,</code>
                   <code class="s">'What is the {ratio} for {company}?'</code><code class="p">,</code>
                   <code class="s">'Tell me the {ratio} for {company}'</code><code class="p">,</code>
                  <code class="p">]</code></pre>












<section data-type="sect4" data-pdf-bookmark="4.1. Data construction"><div class="sect4" id="idm45174896280184">
<h4>4.1. Data construction</h4>

<p>We begin constructing our model by creating <em>reverse</em> 
<span class="keep-together"><em>dictionaries</em></span>:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">companies_rev</code> <code class="o">=</code> <code class="p">{}</code>
<code class="k">for</code> <code class="n">k</code><code class="p">,</code> <code class="n">v</code> <code class="ow">in</code> <code class="n">companies</code><code class="o">.</code><code class="n">items</code><code class="p">():</code>
  <code class="k">for</code> <code class="n">ve</code> <code class="ow">in</code> <code class="n">v</code><code class="p">:</code>
      <code class="n">companies_rev</code><code class="p">[</code><code class="n">ve</code><code class="p">]</code> <code class="o">=</code> <code class="n">k</code>
  <code class="n">ratios_rev</code> <code class="o">=</code> <code class="p">{}</code>
  <code class="k">for</code> <code class="n">k</code><code class="p">,</code> <code class="n">v</code> <code class="ow">in</code> <code class="n">ratios</code><code class="o">.</code><code class="n">items</code><code class="p">():</code>
      		<code class="k">for</code> <code class="n">ve</code> <code class="ow">in</code> <code class="n">v</code><code class="p">:</code>
          			<code class="n">ratios_rev</code><code class="p">[</code><code class="n">ve</code><code class="p">]</code> <code class="o">=</code> <code class="n">k</code>
  <code class="n">companies_list</code> <code class="o">=</code> <code class="nb">list</code><code class="p">(</code><code class="n">companies_rev</code><code class="o">.</code><code class="n">keys</code><code class="p">())</code>
  <code class="n">ratios_list</code> <code class="o">=</code> <code class="nb">list</code><code class="p">(</code><code class="n">ratios_rev</code><code class="o">.</code><code class="n">keys</code><code class="p">())</code></pre>

<p>Next, we create sample statements for our model. We build a function that gives us a random sentence structure, inquiring about a random financial ratio for a random company. We will be creating a custom named entity recognition_ model in the spaCy framework. This requires training the model to pick up the word or phrase in a sample sentence. To train the spaCy model, we need to provide it with an example, such as <em>(<em>Get me the ROE for Citi</em>, {"entities”: [(11, 14, <em>RATIO</em>), (19, 23, <em>COMPANY</em>) ]})</em>.</p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="4.2. Training data"><div class="sect4" id="idm45174896158328">
<h4>4.2. Training data</h4>

<p>The first part of the training example is the sentence. The second is a dictionary that consists of entities and the starting and ending index of the label:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">N_training_samples</code> <code class="o">=</code> <code class="mi">100</code>
<code class="k">def</code> <code class="nf">get_training_sample</code><code class="p">(</code><code class="n">string_templates</code><code class="p">,</code> <code class="n">ratios_list</code><code class="p">,</code> <code class="n">companies_list</code><code class="p">):</code>
  <code class="n">string_template</code><code class="o">=</code><code class="n">string_templates</code><code class="p">[</code><code class="n">random</code><code class="o">.</code><code class="n">randint</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="nb">len</code><code class="p">(</code><code class="n">string_templates</code><code class="p">)</code><code class="o">-</code><code class="mi">1</code><code class="p">)]</code>
      <code class="n">ratio</code> <code class="o">=</code> <code class="n">ratios_list</code><code class="p">[</code><code class="n">random</code><code class="o">.</code><code class="n">randint</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="nb">len</code><code class="p">(</code><code class="n">ratios_list</code><code class="p">)</code><code class="o">-</code><code class="mi">1</code><code class="p">)]</code>
      <code class="n">company</code> <code class="o">=</code> <code class="n">companies_list</code><code class="p">[</code><code class="n">random</code><code class="o">.</code><code class="n">randint</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="nb">len</code><code class="p">(</code><code class="n">companies_list</code><code class="p">)</code><code class="o">-</code><code class="mi">1</code><code class="p">)]</code>
      <code class="n">sent</code> <code class="o">=</code> <code class="n">string_template</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">ratio</code><code class="o">=</code><code class="n">ratio</code><code class="p">,</code><code class="n">company</code><code class="o">=</code><code class="n">company</code><code class="p">)</code>
      <code class="n">ents</code> <code class="o">=</code> <code class="p">{</code><code class="s">"entities"</code><code class="p">:</code> <code class="p">[(</code><code class="n">sent</code><code class="o">.</code><code class="n">index</code><code class="p">(</code><code class="n">ratio</code><code class="p">),</code> <code class="n">sent</code><code class="o">.</code><code class="n">index</code><code class="p">(</code><code class="n">ratio</code><code class="p">)</code><code class="o">+</code>\
  <code class="nb">len</code><code class="p">(</code><code class="n">ratio</code><code class="p">),</code> <code class="s">'RATIO'</code><code class="p">),</code>
                   	<code class="p">(</code><code class="n">sent</code><code class="o">.</code><code class="n">index</code><code class="p">(</code><code class="n">company</code><code class="p">),</code> <code class="n">sent</code><code class="o">.</code><code class="n">index</code><code class="p">(</code><code class="n">company</code><code class="p">)</code><code class="o">+</code><code class="nb">len</code><code class="p">(</code><code class="n">company</code><code class="p">),</code> \
                    <code class="s">'COMPANY'</code><code class="p">)]}</code>
       <code class="k">return</code> <code class="p">(</code><code class="n">sent</code><code class="p">,</code> <code class="n">ents</code><code class="p">)</code></pre>

<p>Let us define the training data:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">TRAIN_DATA</code> <code class="o">=</code> <code class="p">[</code>
<code class="n">get_training_sample</code><code class="p">(</code><code class="n">string_templates</code><code class="p">,</code> <code class="n">ratios_list</code><code class="p">,</code> <code class="n">companies_list</code><code class="p">)</code> \
<code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">N_training_samples</code><code class="p">)</code>
<code class="p">]</code></pre>
</div></section>













<section data-type="sect4" class="pagebreak-before less_space" data-pdf-bookmark="5. Model creation and training"><div class="sect4" id="idm45174895932632">
<h4>5. Model creation and training</h4>

<p><a data-type="indexterm" data-primary="chatbots" data-secondary="model creation and training" id="ix_Chapter10-asciidoc24"/>Once we have the training data, we construct a <em>blank</em> model in spaCy. spaCy’s models are statistical, and every decision they make—for example, which part-of-speech tag to assign, or whether a word is a named entity—is a prediction. This prediction is based on the examples the model has seen during training. To train a model, you first need training data—examples of text and the labels you want the model to predict. This could be a part-of-speech tag, a named entity, or any other information. The model is then shown the unlabeled text and makes a prediction. <a data-type="indexterm" data-primary="error gradient" id="idm45174895955528"/>Because we know the correct answer, we can give the model feedback on its prediction in the form of an <em>error gradient</em> of the loss function. This calculates the difference between the training example and the expected output, as shown in  <a data-type="xref" href="#MLTraining">Figure 10-6</a>. The greater the difference, the more significant the gradient, and the more updates we need to make to our model.</p>

<figure><div id="MLTraining" class="figure">
<img src="Images/mlbf_1007.png" alt="mlbf 1007" width="1339" height="256"/>
<h6><span class="label">Figure 10-6. </span>Machine learning–based training in spaCy</h6>
</div></figure>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">nlp</code> <code class="o">=</code> <code class="n">spacy</code><code class="o">.</code><code class="n">blank</code><code class="p">(</code><code class="s">"en"</code><code class="p">)</code></pre>

<p>Next, we create an NER pipeline to our model:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">ner</code> <code class="o">=</code> <code class="n">nlp</code><code class="o">.</code><code class="n">create_pipe</code><code class="p">(</code><code class="s">"ner"</code><code class="p">)</code>
<code class="n">nlp</code><code class="o">.</code><code class="n">add_pipe</code><code class="p">(</code><code class="n">ner</code><code class="p">)</code></pre>

<p>Then we add the training labels that we use:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">ner</code><code class="o">.</code><code class="n">add_label</code><code class="p">(</code><code class="s">'RATIO'</code><code class="p">)</code>
<code class="n">ner</code><code class="o">.</code><code class="n">add_label</code><code class="p">(</code><code class="s">'COMPANY'</code><code class="p">)</code></pre>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="5.1. Model optimization function"><div class="sect3" id="idm45174895868232">
<h3>5.1. Model optimization function</h3>

<p>Now we start optimization of our models:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">optimizer</code> <code class="o">=</code> <code class="n">nlp</code><code class="o">.</code><code class="n">begin_training</code><code class="p">()</code>
<code class="n">move_names</code> <code class="o">=</code> <code class="nb">list</code><code class="p">(</code><code class="n">ner</code><code class="o">.</code><code class="n">move_names</code><code class="p">)</code>
<code class="n">pipe_exceptions</code> <code class="o">=</code> <code class="p">[</code><code class="s">"ner"</code><code class="p">,</code> <code class="s">"trf_wordpiecer"</code><code class="p">,</code> <code class="s">"trf_tok2vec"</code><code class="p">]</code>
<code class="n">other_pipes</code> <code class="o">=</code> <code class="p">[</code><code class="n">pipe</code> <code class="k">for</code> <code class="n">pipe</code> <code class="ow">in</code> <code class="n">nlp</code><code class="o">.</code><code class="n">pipe_names</code> <code class="k">if</code> <code class="n">pipe</code> <code class="ow">not</code> <code class="ow">in</code> <code class="n">pipe_exceptions</code><code class="p">]</code>
<code class="k">with</code> <code class="n">nlp</code><code class="o">.</code><code class="n">disable_pipes</code><code class="p">(</code><code class="o">*</code><code class="n">other_pipes</code><code class="p">):</code>  <code class="c"># only train NER</code>
     <code class="n">sizes</code> <code class="o">=</code> <code class="n">compounding</code><code class="p">(</code><code class="mf">1.0</code><code class="p">,</code> <code class="mf">4.0</code><code class="p">,</code> <code class="mf">1.001</code><code class="p">)</code>
     <code class="c"># batch up the examples using spaCy's minibatch</code>
     <code class="k">for</code> <code class="n">itn</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">30</code><code class="p">):</code>
        <code class="n">random</code><code class="o">.</code><code class="n">shuffle</code><code class="p">(</code><code class="n">TRAIN_DATA</code><code class="p">)</code>
        <code class="n">batches</code> <code class="o">=</code> <code class="n">minibatch</code><code class="p">(</code><code class="n">TRAIN_DATA</code><code class="p">,</code> <code class="n">size</code><code class="o">=</code><code class="n">sizes</code><code class="p">)</code>
        <code class="n">losses</code> <code class="o">=</code> <code class="p">{}</code>
        <code class="k">for</code> <code class="n">batch</code> <code class="ow">in</code> <code class="n">batches</code><code class="p">:</code>
           <code class="n">texts</code><code class="p">,</code> <code class="n">annotations</code> <code class="o">=</code> <code class="nb">zip</code><code class="p">(</code><code class="o">*</code><code class="n">batch</code><code class="p">)</code>
           <code class="n">nlp</code><code class="o">.</code><code class="n">update</code><code class="p">(</code><code class="n">texts</code><code class="p">,</code> <code class="n">annotations</code><code class="p">,</code> <code class="n">sgd</code><code class="o">=</code><code class="n">optimizer</code><code class="p">,</code>
           <code class="n">drop</code><code class="o">=</code><code class="mf">0.35</code><code class="p">,</code> <code class="n">losses</code><code class="o">=</code><code class="n">losses</code><code class="p">)</code>
        <code class="nb">print</code><code class="p">(</code><code class="s">"Losses"</code><code class="p">,</code> <code class="n">losses</code><code class="p">)</code></pre>

<p>Training the NER model is akin to updating the weights for each token. The most important step is to use a good optimizer. The more examples of our training data that we provide spaCy, the better it will be at recognizing generalized results.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="5.2. Custom logic adapter"><div class="sect3" id="idm45174895814824">
<h3>5.2. Custom logic adapter</h3>

<p>Next, we build our custom logic adapter:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">from</code> <code class="nn">chatterbot.conversation</code> <code class="k">import</code> <code class="n">Statement</code>
<code class="k">class</code> <code class="nc">FinancialRatioAdapter</code><code class="p">(</code><code class="n">LogicAdapter</code><code class="p">):</code>
    	<code class="k">def</code> <code class="nf">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">chatbot</code><code class="p">,</code> <code class="o">**</code><code class="n">kwargs</code><code class="p">):</code>
        		<code class="nb">super</code><code class="p">(</code><code class="n">FinancialRatioAdapter</code><code class="p">,</code> <code class="bp">self</code><code class="p">)</code><code class="o">.</code><code class="n">__init__</code><code class="p">(</code><code class="n">chatbot</code><code class="p">,</code> <code class="o">**</code><code class="n">kwargs</code><code class="p">)</code>
    	<code class="k">def</code> <code class="nf">process</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">statement</code><code class="p">,</code> <code class="n">additional_response_selection_parameters</code><code class="p">):</code>
      		<code class="n">user_input</code> <code class="o">=</code> <code class="n">statement</code><code class="o">.</code><code class="n">text</code>
      		<code class="n">doc</code> <code class="o">=</code> <code class="n">nlp</code><code class="p">(</code><code class="n">user_input</code><code class="p">)</code>
      		<code class="n">company</code> <code class="o">=</code> <code class="k">None</code>
      		<code class="n">ratio</code> <code class="o">=</code> <code class="k">None</code>
      		<code class="n">confidence</code> <code class="o">=</code> <code class="mi">0</code>
      		<code class="c"># We need exactly 1 company and one ratio</code>
      		<code class="k">if</code> <code class="nb">len</code><code class="p">(</code><code class="n">doc</code><code class="o">.</code><code class="n">ents</code><code class="p">)</code> <code class="o">==</code> <code class="mi">2</code><code class="p">:</code>
      			<code class="k">for</code> <code class="n">ent</code> <code class="ow">in</code> <code class="n">doc</code><code class="o">.</code><code class="n">ents</code><code class="p">:</code>
          			<code class="k">if</code> <code class="n">ent</code><code class="o">.</code><code class="n">label_</code> <code class="o">==</code> <code class="s">"RATIO"</code><code class="p">:</code>
              				<code class="n">ratio</code> <code class="o">=</code> <code class="n">ent</code><code class="o">.</code><code class="n">text</code>
              			<code class="k">if</code> <code class="n">ratio</code> <code class="ow">in</code> <code class="n">ratios_rev</code><code class="p">:</code>
                  				<code class="n">confidence</code> <code class="o">+=</code> <code class="mf">0.5</code>
          			<code class="k">if</code> <code class="n">ent</code><code class="o">.</code><code class="n">label_</code> <code class="o">==</code> <code class="s">"COMPANY"</code><code class="p">:</code>
              				<code class="n">company</code> <code class="o">=</code> <code class="n">ent</code><code class="o">.</code><code class="n">text</code>
              				<code class="k">if</code> <code class="n">company</code> <code class="ow">in</code> <code class="n">companies_rev</code><code class="p">:</code>
                  					<code class="n">confidence</code> <code class="o">+=</code> <code class="mf">0.5</code>
      		<code class="k">if</code> <code class="n">confidence</code> <code class="o">&gt;</code> <code class="mf">0.99</code><code class="p">:</code> <code class="p">(</code><code class="n">its</code> <code class="n">found</code> <code class="n">a</code> <code class="n">ratio</code> <code class="ow">and</code> <code class="n">company</code><code class="p">)</code>
      			<code class="n">outtext</code> <code class="o">=</code> <code class="s">'''https://www.zacks.com/stock/chart</code><code class="se">\</code>
<code class="s">                /{comanpy}/fundamental/{ratio} '''</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">ratio</code><code class="o">=</code><code class="n">ratios_rev</code><code class="p">[</code><code class="n">ratio</code><code class="p">]</code>\
                  <code class="p">,</code> <code class="n">company</code><code class="o">=</code><code class="n">companies_rev</code><code class="p">[</code><code class="n">company</code><code class="p">])</code>
      			<code class="n">confidence</code> <code class="o">=</code> <code class="mi">1</code>
      		<code class="k">else</code><code class="p">:</code>
      			<code class="n">outtext</code> <code class="o">=</code> <code class="s">'Sorry! Could not figure out what the user wants'</code>
      			<code class="n">confidence</code> <code class="o">=</code> <code class="mi">0</code>
      		<code class="n">output_statement</code> <code class="o">=</code> <code class="n">Statement</code><code class="p">(</code><code class="n">text</code><code class="o">=</code><code class="n">outtext</code><code class="p">)</code>
      		<code class="n">output_statement</code><code class="o">.</code><code class="n">confidence</code> <code class="o">=</code> <code class="n">confidence</code>
      		<code class="k">return</code> <code class="n">output_statement</code></pre>

<p>With this custom logic adapter, our chatbot will take each input statement and try to recognize a <em>RATIO</em> and/or <em>COMPANY</em> using our NER model. If the model finds exactly one <em>COMPANY</em> and exactly one <em>RATIO</em>, it constructs a URL to guide the user.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="5.3. Model usage—training and testing"><div class="sect3" id="idm45174895646536">
<h3>5.3. Model usage—training and testing</h3>

<p>Now we begin using our chatbot by using the following import:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">from</code> <code class="nn">chatterbot</code> <code class="k">import</code> <code class="n">ChatBot</code></pre>

<p>We construct our chatbot by adding the <code>FinancialRatioAdapter</code> logical adapter that we created above to the chatbot. Although the following code snippet only shows us adding the <code>FinancialRatioAdapter</code>, note that other logical adapters, lists, and corpuses used in the prior training of the chatbot are also included. Please refer to the Jupyter notebook of the case study for more details.</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">chatbot</code> <code class="o">=</code> <code class="n">ChatBot</code><code class="p">(</code>
    			<code class="s">"My ChatterBot"</code><code class="p">,</code>
    			<code class="n">logic_adapters</code><code class="o">=</code><code class="p">[</code>
        <code class="s">'financial_ratio_adapter.FinancialRatioAdapter'</code>
    <code class="p">]</code>
<code class="p">)</code></pre>

<p>Now we test our chatbot using the following statements:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">converse</code><code class="p">()</code>

<code class="o">&gt;</code><code class="n">What</code> <code class="ow">is</code> <code class="n">ROE</code> <code class="k">for</code> Citibank<code class="o">?</code>
<code class="n">https</code><code class="p">:</code><code class="o">//</code><code class="n">www</code><code class="o">.</code><code class="n">zacks</code><code class="o">.</code><code class="n">com</code><code class="o">/</code><code class="n">stock</code><code class="o">/</code><code class="n">chart</code><code class="o">/</code><code class="n">C</code><code class="o">/</code><code class="n">fundamental</code><code class="o">/</code><code class="k">return</code><code class="o">-</code><code class="n">on</code><code class="o">-</code><code class="n">equity</code><code class="o">-</code><code class="n">ttm</code>

<code class="o">&gt;</code><code class="n">Tell</code> <code class="n">me</code> <code class="n">PE</code> <code class="k">for</code> Delta<code class="o">?</code>
<code class="n">https</code><code class="p">:</code><code class="o">//</code><code class="n">www</code><code class="o">.</code><code class="n">zacks</code><code class="o">.</code><code class="n">com</code><code class="o">/</code><code class="n">stock</code><code class="o">/</code><code class="n">chart</code><code class="o">/</code><code class="n">DAL</code><code class="o">/</code><code class="n">fundamental</code><code class="o">/</code><code class="n">pe</code><code class="o">-</code><code class="n">ratio</code><code class="o">-</code><code class="n">ttm</code>

<code class="o">&gt;</code><code class="n">What</code> <code class="ow">is</code> Bitcoin<code class="o">?</code>
<code class="n">It</code> <code class="ow">is</code> <code class="n">a</code> <code class="n">decentralized</code> <code class="n">digital</code> <code class="n">currency</code>

<code class="o">&gt;</code><code class="n">Help</code><code class="err">!</code>
<code class="n">Please</code> <code class="n">go</code> <code class="n">to</code> <code class="n">google</code><code class="o">.</code><code class="n">com</code>

<code class="o">&gt;</code><code class="n">What</code> <code class="ow">is</code> <code class="mi">786940</code> <code class="n">plus</code> <code class="mi">75869</code>
<code class="mi">786940</code> <code class="n">plus</code> <code class="mi">75869</code> <code class="o">=</code> <code class="mi">862809</code>

<code class="o">&gt;</code><code class="n">Do</code> <code class="n">you</code> <code class="n">like</code> dogs<code class="o">?</code>
<code class="n">Sorry</code><code class="o">!</code> Could not figure out what the user wants</pre>

<p>As shown above, the custom logic adapter for our chatbot finds a RATIO and/or COMPANY in the sentence using our NLP model. If an exact pair is detected, the model constructs a URL to guide the user to the answer. Additionally, other logical adapters, such as mathematical evaluation, work as expected.<a data-type="indexterm" data-startref="ix_Chapter10-asciidoc24" id="idm45174895392568"/></p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Conclusion"><div class="sect3" id="idm45174895158856">
<h3>Conclusion</h3>

<p>Overall, this case study provides an introduction to a number of aspects of chatbot development.</p>

<p>Using the ChatterBot library in Python allows us to build a simple interface to resolve user inputs. To train a blank model, one must have a substantial training dataset. In this case study, we looked at patterns available to us and used them to generate 
<span class="keep-together">training</span> samples. Getting the right amount of training data is usually the hardest part of constructing a custom chatbot.</p>

<p>This case study is a demo project, and significant enhancements can be made to each component to extend it to a wide variety of tasks. Additional preprocessing steps can be added to have cleaner data to work with. To generate a response from our bot for input questions, the logic can be refined further to incorporate better similarity measures and embeddings. The chatbot can be trained on a bigger dataset using more advanced ML techniques. A series of custom logic adapters can be used to construct a more sophisticated ChatterBot. This can be generalized to more interesting tasks, such as retrieving information from a database or asking for more input from the user.<a data-type="indexterm" data-startref="ix_Chapter10-asciidoc22" id="idm45174895154792"/><a data-type="indexterm" data-startref="ix_Chapter10-asciidoc21" id="idm45174895154088"/></p>
<img class="blueprint bottom" src="Images/bracket_bottom.png" width="1625" height="32"/>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Case Study 3: Document Summarization"><div class="sect1" id="CaseStudy3NLP">
<h1>Case Study 3: Document Summarization</h1>

<p><a data-type="indexterm" data-primary="document summarization" id="ix_Chapter10-asciidoc25"/>Document summarization refers to the selection of the most important points and topics in a document and arranging them in a comprehensive manner. As discussed earlier, analysts at banks and other financial service organizations pore over, analyze, and attempt to quantify qualitative data from news, reports, and documents. Document summarization using NLP can provide in-depth support in this analyzing and interpretation. When tailored to financial documents, such as earning reports and financial news, it can help analysts quickly derive key topics and market signals from content. Document summarization can also be used to improve reporting efforts and can provide timely updates on key matters.</p>

<p>In NLP, <em>topic models</em> (such as LDA, introduced earlier in the chapter) are the most frequently used tools for the extraction of sophisticated, interpretable text features. These models can surface key topics, themes, or signals from large collections of documents and can be effectively used for document summarization.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45174895148072">
<h5/>
<p>In this case study, we will focus on:</p>

<ul>
<li>
<p>Implementing the LDA model for topic modeling.</p>
</li>
<li>
<p>Understanding the necessary data preparation (i.e., converting a PDF for an NLP-related problem).</p>
</li>
<li>
<p>Topic visualization.</p>
</li>
</ul>
</div></aside>
<img class="blueprint top" src="Images/bracket_top.png" width="1625" height="32"/>








<section data-type="sect2" class="blueprint" data-pdf-bookmark="Blueprint for Using NLP for Document Summarization"><div class="sect2" id="idm45174895142632">
<h2>Blueprint for Using NLP for Document Summarization</h2>










<section data-type="sect3" data-pdf-bookmark="1. Problem definition"><div class="sect3" id="idm45174895141112">
<h3>1. Problem definition</h3>

<p>The goal of this case study is to effectively discover common topics from earnings call transcripts of publicly traded companies using LDA. A core advantage of this technique compared to other approaches, is that no prior knowledge of the topics is needed.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="2. Getting started—loading the data and Python packages"><div class="sect3" id="idm45174895139256">
<h3>2. Getting started—loading the data and Python packages</h3>












<section data-type="sect4" data-pdf-bookmark="2.1. Loading the Python packages"><div class="sect4" id="idm45174895138248">
<h4>2.1. Loading the Python packages</h4>

<p><a data-type="indexterm" data-primary="document summarization" data-secondary="loading data and Python packages" id="idm45174895136840"/>For this case study, we will extract the text from a PDF. Hence, the Python library <em>pdf-miner</em> is used for processing the PDF files into a text format. Libraries for feature extraction and topic modeling are also loaded. The libraries for the visualization will be loaded later in the case study:</p>

<p><code>Libraries for pdf conversion</code></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">from</code> <code class="nn">pdfminer.pdfinterp</code> <code class="k">import</code> <code class="n">PDFResourceManager</code><code class="p">,</code> <code class="n">PDFPageInterpreter</code>
<code class="kn">from</code> <code class="nn">pdfminer.converter</code> <code class="k">import</code> <code class="n">TextConverter</code>
<code class="kn">from</code> <code class="nn">pdfminer.layout</code> <code class="k">import</code> <code class="n">LAParams</code>
<code class="kn">from</code> <code class="nn">pdfminer.pdfpage</code> <code class="k">import</code> <code class="n">PDFPage</code>
<code class="kn">import</code> <code class="nn">re</code>
<code class="kn">from</code> <code class="nn">io</code> <code class="k">import</code> <code class="n">StringIO</code></pre>

<p><code>Libraries for feature extraction and topic modeling</code></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">from</code> <code class="nn">sklearn.feature_extraction.text</code> <code class="k">import</code> <code class="n">CountVectorizer</code><code class="p">,</code><code class="n">TfidfVectorizer</code>
<code class="kn">from</code> <code class="nn">sklearn.decomposition</code> <code class="k">import</code> <code class="n">LatentDirichletAllocation</code>
<code class="kn">from</code> <code class="nn">sklearn.feature_extraction.stop_words</code> <code class="k">import</code> <code class="n">ENGLISH_STOP_WORDS</code></pre>

<p><code>Other libraries</code></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">import</code> <code class="nn">pandas</code> <code class="k">as</code> <code class="nn">pd</code></pre>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="3. Data preparation"><div class="sect3" id="idm45174895030808">
<h3>3. Data preparation</h3>

<p><a data-type="indexterm" data-primary="document summarization" data-secondary="data preparation" id="ix_Chapter10-asciidoc26"/>The <code>convert_pdf_to_txt</code> function defined below pulls out all characters from a PDF document except the images. The function simply takes in the PDF document, extracts all characters from the document, and outputs the extracted text as a Python list of strings:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="k">def</code> <code class="nf">convert_pdf_to_txt</code><code class="p">(</code><code class="n">path</code><code class="p">):</code>
    <code class="n">rsrcmgr</code> <code class="o">=</code> <code class="n">PDFResourceManager</code><code class="p">()</code>
    <code class="n">retstr</code> <code class="o">=</code> <code class="n">StringIO</code><code class="p">()</code>
    <code class="n">laparams</code> <code class="o">=</code> <code class="n">LAParams</code><code class="p">()</code>
    <code class="n">device</code> <code class="o">=</code> <code class="n">TextConverter</code><code class="p">(</code><code class="n">rsrcmgr</code><code class="p">,</code> <code class="n">retstr</code><code class="p">,</code> <code class="n">laparams</code><code class="o">=</code><code class="n">laparams</code><code class="p">)</code>
    <code class="n">fp</code> <code class="o">=</code> <code class="nb">open</code><code class="p">(</code><code class="n">path</code><code class="p">,</code> <code class="s">'rb'</code><code class="p">)</code>
    <code class="n">interpreter</code> <code class="o">=</code> <code class="n">PDFPageInterpreter</code><code class="p">(</code><code class="n">rsrcmgr</code><code class="p">,</code> <code class="n">device</code><code class="p">)</code>
    <code class="n">password</code> <code class="o">=</code> <code class="s">""</code>
    <code class="n">maxpages</code> <code class="o">=</code> <code class="mi">0</code>
    <code class="n">caching</code> <code class="o">=</code> <code class="k">True</code>
    <code class="n">pagenos</code><code class="o">=</code><code class="nb">set</code><code class="p">()</code>

    <code class="k">for</code> <code class="n">page</code> <code class="ow">in</code> <code class="n">PDFPage</code><code class="o">.</code><code class="n">get_pages</code><code class="p">(</code><code class="n">fp</code><code class="p">,</code> <code class="n">pagenos</code><code class="p">,</code>\
            <code class="n">maxpages</code><code class="o">=</code><code class="n">maxpages</code><code class="p">,</code> <code class="n">password</code><code class="o">=</code><code class="n">password</code><code class="p">,</code><code class="n">caching</code><code class="o">=</code><code class="n">caching</code><code class="p">,</code>\
            <code class="n">check_extractable</code><code class="o">=</code><code class="k">True</code><code class="p">):</code>
        <code class="n">interpreter</code><code class="o">.</code><code class="n">process_page</code><code class="p">(</code><code class="n">page</code><code class="p">)</code>

    <code class="n">text</code> <code class="o">=</code> <code class="n">retstr</code><code class="o">.</code><code class="n">getvalue</code><code class="p">()</code>

    <code class="n">fp</code><code class="o">.</code><code class="n">close</code><code class="p">()</code>
    <code class="n">device</code><code class="o">.</code><code class="n">close</code><code class="p">()</code>
    <code class="n">retstr</code><code class="o">.</code><code class="n">close</code><code class="p">()</code>
    <code class="k">return</code> <code class="n">text</code></pre>

<p>In the next step, the PDF is converted to text using the above function and saved in a text file:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">Document</code><code class="o">=</code><code class="n">convert_pdf_to_txt</code><code class="p">(</code><code class="s">'10K.pdf'</code><code class="p">)</code>
<code class="n">f</code><code class="o">=</code><code class="nb">open</code><code class="p">(</code><code class="s">'Finance10k.txt'</code><code class="p">,</code><code class="s">'w'</code><code class="p">)</code>
<code class="n">f</code><code class="o">.</code><code class="n">write</code><code class="p">(</code><code class="n">Document</code><code class="p">)</code>
<code class="n">f</code><code class="o">.</code><code class="n">close</code><code class="p">()</code>
<code class="k">with</code> <code class="nb">open</code><code class="p">(</code><code class="s">'Finance10k.txt'</code><code class="p">)</code> <code class="k">as</code> <code class="n">f</code><code class="p">:</code>
    <code class="n">clean_cont</code> <code class="o">=</code> <code class="n">f</code><code class="o">.</code><code class="n">read</code><code class="p">()</code><code class="o">.</code><code class="n">splitlines</code><code class="p">()</code></pre>

<p>Let us look at the raw document:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">clean_cont</code><code class="p">[</code><code class="mi">1</code><code class="p">:</code><code class="mi">15</code><code class="p">]</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">[' ',
 '',
 'SECURITIES AND EXCHANGE COMMISSION',
 ' ',
 '',
 'Washington, D.C. 20549',
 ' ',
 '',
 '\xa0',
 'FORM ',
 '\xa0',
 '',
 'QUARTERLY REPORT PURSUANT TO SECTION 13 OR 15(d) OF',
 ' ']</pre>

<p>The text extracted from the PDF document contains uninformative characters that need to be removed. These characters reduce the effectiveness of our models as they provide unnecessary count ratios. The following function uses a series of regular expression (<em>regex</em>) searches as well as list comprehension to replace uninformative characters with a blank space:<a data-type="indexterm" data-startref="ix_Chapter10-asciidoc26" id="idm45174894841192"/></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">doc</code><code class="o">=</code><code class="p">[</code><code class="n">i</code><code class="o">.</code><code class="n">replace</code><code class="p">(</code><code class="s">'</code><code class="se">\xe2\x80\x9c</code><code class="s">'</code><code class="p">,</code> <code class="s">''</code><code class="p">)</code> <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="n">clean_cont</code> <code class="p">]</code>
<code class="n">doc</code><code class="o">=</code><code class="p">[</code><code class="n">i</code><code class="o">.</code><code class="n">replace</code><code class="p">(</code><code class="s">'</code><code class="se">\xe2\x80\x9d</code><code class="s">'</code><code class="p">,</code> <code class="s">''</code><code class="p">)</code> <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="n">doc</code> <code class="p">]</code>
<code class="n">doc</code><code class="o">=</code><code class="p">[</code><code class="n">i</code><code class="o">.</code><code class="n">replace</code><code class="p">(</code><code class="s">'</code><code class="se">\xe2\x80\x99</code><code class="s">s'</code><code class="p">,</code> <code class="s">''</code><code class="p">)</code> <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="n">doc</code> <code class="p">]</code>

<code class="n">docs</code> <code class="o">=</code> <code class="p">[</code><code class="n">x</code> <code class="k">for</code> <code class="n">x</code> <code class="ow">in</code> <code class="n">doc</code> <code class="k">if</code> <code class="n">x</code> <code class="o">!=</code> <code class="s">' '</code><code class="p">]</code>
<code class="n">docss</code> <code class="o">=</code> <code class="p">[</code><code class="n">x</code> <code class="k">for</code> <code class="n">x</code> <code class="ow">in</code> <code class="n">docs</code> <code class="k">if</code> <code class="n">x</code> <code class="o">!=</code> <code class="s">''</code><code class="p">]</code>
<code class="n">financedoc</code><code class="o">=</code><code class="p">[</code><code class="n">re</code><code class="o">.</code><code class="n">sub</code><code class="p">(</code><code class="s">"[^a-zA-Z]+"</code><code class="p">,</code> <code class="s">" "</code><code class="p">,</code> <code class="n">s</code><code class="p">)</code> <code class="k">for</code> <code class="n">s</code> <code class="ow">in</code> <code class="n">docss</code><code class="p">]</code></pre>
</div></section>













<section data-type="sect3" data-pdf-bookmark="4. Model construction and training"><div class="sect3" id="idm45174895030216">
<h3>4. Model construction and training</h3>

<p><a data-type="indexterm" data-primary="document summarization" data-secondary="model construction and training" id="idm45174894605416"/><a data-type="indexterm" data-primary="document term matrix" id="idm45174894604616"/>The <code>CountVectorizer</code> function from the sklearn module is used with minimal parameter tuning to represent the clean document as a <em>document term matrix</em>. This is performed because our modeling requires that strings be represented as integers. The <code>CountVectorizer</code> shows the number of times a word occurs in the list after the removal of stop words. The document term matrix was formatted into a Pandas dataframe in order to inspect the dataset. This dataframe shows the word-occurrence count of each term in the document:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">vect</code><code class="o">=</code><code class="n">CountVectorizer</code><code class="p">(</code><code class="n">ngram_range</code><code class="o">=</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">),</code><code class="n">stop_words</code><code class="o">=</code><code class="s">'english'</code><code class="p">)</code>
<code class="n">fin</code><code class="o">=</code><code class="n">vect</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">financedoc</code><code class="p">)</code></pre>

<p>In the next step, the document term matrix will be used as the input data to the LDA algorithm for topic modeling. The algorithm was fitted to isolate five distinct topic contexts, as shown by the following code. This value can be adjusted depending on the level of granularity one intends to obtain from the modeling:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">lda</code><code class="o">=</code><code class="n">LatentDirichletAllocation</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mi">5</code><code class="p">)</code>
<code class="n">lda</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">fin</code><code class="p">)</code>
<code class="n">lda_dtf</code><code class="o">=</code><code class="n">lda</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">fin</code><code class="p">)</code></pre>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">sorting</code><code class="o">=</code><code class="n">np</code><code class="o">.</code><code class="n">argsort</code><code class="p">(</code><code class="n">lda</code><code class="o">.</code><code class="n">components_</code><code class="p">)[:,</code> <code class="p">::</code><code class="o">-</code><code class="mi">1</code><code class="p">]</code>
<code class="n">features</code><code class="o">=</code><code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">vect</code><code class="o">.</code><code class="n">get_feature_names</code><code class="p">())</code></pre>

<p>The following code uses the <em>mglearn</em> library to display the top 10 words within each specific topic model:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">import</code> <code class="nn">mglearn</code>
<code class="n">mglearn</code><code class="o">.</code><code class="n">tools</code><code class="o">.</code><code class="n">print_topics</code><code class="p">(</code><code class="n">topics</code><code class="o">=</code><code class="nb">range</code><code class="p">(</code><code class="mi">5</code><code class="p">),</code> <code class="n">feature_names</code><code class="o">=</code><code class="n">features</code><code class="p">,</code>
<code class="n">sorting</code><code class="o">=</code><code class="n">sorting</code><code class="p">,</code> <code class="n">topics_per_chunk</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code> <code class="n">n_words</code><code class="o">=</code><code class="mi">10</code><code class="p">)</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">topic 1       topic 2       topic 3       topic 4       topic 5
--------      --------      --------      --------      --------
assets        quarter       loans         securities    value
balance       million       mortgage      rate          total
losses        risk          loan          investment    income
credit        capital       commercial    contracts     net
period        months        total         credit        fair
derivatives   financial     real          market        billion
liabilities   management    estate        federal       equity
derivative    billion       securities    stock         september
allowance     ended         consumer      debt          december
average       september     backed        sales         table</pre>

<p>Each topic in the table is expected to represent a broader theme. However, given that we trained the model on only a single document, the themes across the topics may not be very distinct from each other.</p>

<p>Looking at the broader theme, topic 2 discusses quarters, months, and currency units related to asset valuation. Topic 3 reveals information on income from real estate, mortgages, and related instrument. Topic 5 also has terms related to asset valuation. The first topic references balance sheet items and derivatives. Topic 4 is slightly similar to topic 1 and has words related to an investment process.</p>

<p>In terms of overall theme, topics 2 and 5 are quite distinct from the others. There may also be some similarity between topics 1 and 4, based on the top words. In the next section, we will try to understand the separation between these topics using the Python library <em>pyLDAvis</em>.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="5. Visualization of topics"><div class="sect3" id="idm45174894837848">
<h3>5. Visualization of topics</h3>

<p><a data-type="indexterm" data-primary="document summarization" data-secondary="visualization of topics" id="ix_Chapter10-asciidoc27"/>In this section, we visualize the topics using different techniques.</p>












<section data-type="sect4" data-pdf-bookmark="5.1. Topic visualization"><div class="sect4" id="idm45174894411912">
<h4>5.1. Topic visualization</h4>

<p><a data-type="indexterm" data-primary="topic visualization" id="idm45174894410536"/><em>Topic visualization</em> facilitates the evaluation of topic quality using human judgment. <em>pyLDAvis</em> is a library that displays the global relationships between topics while also facilitating their semantic evaluation by inspecting the terms most closely associated with each topic and, inversely, the topics associated with each term. It also addresses the challenge in which frequently used terms in a document tend to dominate the distribution over words that define a topic.</p>

<p>Below, the <em>pyLDAvis_</em> library is used to visualize the topic models:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">from</code> <code class="nn">__future__</code> <code class="k">import</code>  <code class="n">print_function</code>
<code class="kn">import</code> <code class="nn">pyLDAvis</code>
<code class="kn">import</code> <code class="nn">pyLDAvis.sklearn</code>

<code class="n">zit</code><code class="o">=</code><code class="n">pyLDAvis</code><code class="o">.</code><code class="n">sklearn</code><code class="o">.</code><code class="n">prepare</code><code class="p">(</code><code class="n">lda</code><code class="p">,</code><code class="n">fin</code><code class="p">,</code><code class="n">vect</code><code class="p">)</code>
<code class="n">pyLDAvis</code><code class="o">.</code><code class="n">show</code><code class="p">(</code><code class="n">zit</code><code class="p">)</code></pre>

<p class="pagebreak-before"><code>Output</code></p>

<figure><div class="figure">
<img src="Images/mlbf_10in16.png" alt="mlbf 10in16" width="1189" height="736"/>
<h6/>
</div></figure>

<p>We notice that topics 2 and 5 are quite distant from each other. This is what we observed in the section above from the overall theme and list of words under these topics. Topics 1 and 4 are quite close, which validates our observation above. Such close topics should be analyzed more intricately and might be combined if needed. The relevance of the terms under each topic, as shown in the right panel of the chart, can also be used to understand the differences. Topics 3 and 4 are relatively close as well, although topic 3 is quite distant from the others.</p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="5.2. Word cloud"><div class="sect4" id="idm45174894347352">
<h4>5.2. Word cloud</h4>

<p><a data-type="indexterm" data-primary="word cloud" id="idm45174894345912"/>In this step, a <em>word cloud</em> is generated for the entire document to note the most recurrent terms in the document:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c">#Loading the additional packages for word cloud</code>
<code class="kn">from</code> <code class="nn">os</code> <code class="k">import</code> <code class="n">path</code>
<code class="kn">from</code> <code class="nn">PIL</code> <code class="k">import</code> <code class="n">Image</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="k">as</code> <code class="nn">plt</code>
<code class="kn">from</code> <code class="nn">wordcloud</code> <code class="k">import</code> <code class="n">WordCloud</code><code class="p">,</code><code class="n">STOPWORDS</code>

<code class="c">#Loading the document and generating the word cloud</code>
<code class="n">d</code> <code class="o">=</code> <code class="n">path</code><code class="o">.</code><code class="n">dirname</code><code class="p">(</code><code class="n">__name__</code><code class="p">)</code>
<code class="n">text</code> <code class="o">=</code> <code class="nb">open</code><code class="p">(</code><code class="n">path</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">d</code><code class="p">,</code> <code class="s">'Finance10k.txt'</code><code class="p">))</code><code class="o">.</code><code class="n">read</code><code class="p">()</code>

<code class="n">stopwords</code> <code class="o">=</code> <code class="nb">set</code><code class="p">(</code><code class="n">STOPWORDS</code><code class="p">)</code>
<code class="n">wc</code> <code class="o">=</code> <code class="n">WordCloud</code><code class="p">(</code><code class="n">background_color</code><code class="o">=</code><code class="s">"black"</code><code class="p">,</code> <code class="n">max_words</code><code class="o">=</code><code class="mi">2000</code><code class="p">,</code> <code class="n">stopwords</code><code class="o">=</code><code class="n">stopwords</code><code class="p">)</code>
<code class="n">wc</code><code class="o">.</code><code class="n">generate</code><code class="p">(</code><code class="n">text</code><code class="p">)</code>

<code class="n">plt</code><code class="o">.</code><code class="n">figure</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">16</code><code class="p">,</code><code class="mi">13</code><code class="p">))</code>
<code class="n">plt</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">wc</code><code class="p">,</code> <code class="n">interpolation</code><code class="o">=</code><code class="s">'bilinear'</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s">"off"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>

<p><code>Output</code></p>

<figure><div class="figure">
<img src="Images/mlbf_10in17.png" alt="mlbf 10in17" width="893" height="447"/>
<h6/>
</div></figure>

<p>The word cloud generally agrees with the results from the topic modeling, as recurrent words, such as <em>loan</em>, <em>real estate</em>, <em>third quarter</em>, and <em>fair value</em>, are larger and bolder.</p>

<p>By integrating the information from the steps above, we may come up with the list of topics represented by the document. For the document in our case study, we see that words like <em>third quarter</em>, <em>first nine</em>, and <em>nine months</em> occur quite frequently. In the word list, there are several topics related to balance sheet items. So the document might be a third-quarter financial balance sheet with all credit and assets values in that quarter.<a data-type="indexterm" data-startref="ix_Chapter10-asciidoc27" id="idm45174894182712"/></p>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="Conclusion"><div class="sect3" id="idm45174894181752">
<h3>Conclusion</h3>

<p>In this case study, we explored the use of topic modeling to gain insights into the content of a document. We demonstrated the use of the LDA model, which extracts plausible topics and allows us to gain a high-level understanding of large amounts of text in an automated way.</p>

<p>We performed extraction of the text from a document in PDF format and performed further data preprocessing. The results, alongside the visualizations, demonstrated that the topics are intuitive and meaningful.</p>

<p>Overall, the case study shows how machine learning and NLP can be applied across many domains—such as investment analysis, asset modeling, risk management, and regulatory compliance—to summarize documents, news, and reports in order to significantly reduce manual processing. Given this ability to quickly access and verify relevant, filtered information, analysts may be able to provide more comprehensive and informative reports on which management can base their decisions.<a data-type="indexterm" data-startref="ix_Chapter10-asciidoc25" id="idm45174894178376"/></p>
<img class="blueprint bottom" src="Images/bracket_bottom.png" width="1625" height="32"/>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Chapter Summary"><div class="sect1" id="idm45174894176776">
<h1>Chapter Summary</h1>

<p>The field of NLP has made significant progress, resulting in technologies that have and will continue to revolutionize how financial institutions operate. In the near term, we are likely to see an increase in NLP-based technologies across different domains of finance, including asset management, risk management, and process automation. The adoption and understanding of NLP methodologies and related infrastructure are very important for financial institutions.</p>

<p>Overall, the concepts in Python, machine learning, and finance presented in this chapter through the case studies can be used as a blueprint for any other NLP-based problem in finance.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Exercises"><div class="sect1" id="idm45174894174200">
<h1>Exercises</h1>

<ul>
<li>
<p>Using the concepts from case study 1, use NLP-based techniques to develop a trading strategy using Twitter data.</p>
</li>
<li>
<p>In case study 1, use the word2vec word embedding method to generate the word vectors and incorporate it into the trading strategy.</p>
</li>
<li>
<p>Using the concepts from case study 2, test a few more logical adapters to the chatbot.</p>
</li>
<li>
<p>Using the concepts from case study 3, perform topic modeling on a set of financial news articles for a given day and retrieve the key themes of the day.<a data-type="indexterm" data-startref="ix_Chapter10-asciidoc0" id="idm45174894169192"/></p>
</li>
</ul>
</div></section>







<div data-type="footnotes"><p data-type="footnote" id="idm45174899480680"><sup><a href="ch10.xhtml#idm45174899480680-marker">1</a></sup> A customized deep learning–based feature representation model is built in case study 1 of this chapter.</p><p data-type="footnote" id="idm45174898988232"><sup><a href="ch10.xhtml#idm45174898988232-marker">2</a></sup> The news can be downloaded by a simple web-scraping program in Python using packages such as Beautiful Soup. Readers should talk to the website or follow its terms of service in order to use the news for commercial purpose.</p><p data-type="footnote" id="idm45174898982312"><sup><a href="ch10.xhtml#idm45174898982312-marker">3</a></sup> The source of this lexicon is Nuno Oliveira, Paulo Cortez, and Nelson Areal, “Stock Market Sentiment Lexicon Acquisition Using Microblogging Data and Statistical Measures,” <em>Decision Support Systems</em> 85 (March 2016): 62–73.</p><p data-type="footnote" id="idm45174898012728"><sup><a href="ch10.xhtml#idm45174898012728-marker">4</a></sup> We also train a sentiment analysis model on the financial data in the subsequent section and compare the results against the TextBlob model.</p><p data-type="footnote" id="idm45174897768808"><sup><a href="ch10.xhtml#idm45174897768808-marker">5</a></sup> Refer to <a data-type="xref" href="ch05.xhtml#Chapter5">Chapter 5</a> for more details on RNN models.</p><p data-type="footnote" id="idm45174897534248"><sup><a href="ch10.xhtml#idm45174897534248-marker">6</a></sup> The source of this lexicon is Nuno Oliveira, Paulo Cortez, and Nelson Areal, “Stock Market Sentiment Lexicon Acquisition Using Microblogging Data and Statistical Measures,” <em>Decision Support Systems</em> 85 (March 2016): 62–73.</p><p data-type="footnote" id="idm45174897105944"><sup><a href="ch10.xhtml#idm45174897105944-marker">7</a></sup> Refer to the plotting section of the <a href="https://oreil.ly/j2pT0">backtrader website</a> for more details on the backtrader’s charts and the  <span class="keep-together">panels</span>.</p></div></div></section></div>



  </body></html>