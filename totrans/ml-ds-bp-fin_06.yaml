- en: 'Chapter 4\. Supervised Learning: Models and Concepts'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章。监督学习：模型与概念
- en: Supervised learning is an area of machine learning where the chosen algorithm
    tries to fit a target using the given input. A set of training data that contains
    labels is supplied to the algorithm. Based on a massive set of data, the algorithm
    will learn a rule that it uses to predict the labels for new observations. In
    other words, supervised learning algorithms are provided with historical data
    and asked to find the relationship that has the best predictive power.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习是机器学习的一个领域，选择的算法试图使用给定的输入来拟合目标。算法提供了包含标签的训练数据集。基于大量数据，算法将学习一条规则，用于预测新观察值的标签。换句话说，监督学习算法基于历史数据，并试图找到具有最佳预测能力的关系。
- en: 'There are two varieties of supervised learning algorithms: regression and classification
    algorithms. Regression-based supervised learning methods try to predict outputs
    based on input variables. Classification-based supervised learning methods identify
    which category a set of data items belongs to. Classification algorithms are probability-based,
    meaning the outcome is the category for which the algorithm finds the highest
    probability that the dataset belongs to it. Regression algorithms, in contrast,
    estimate the outcome of problems that have an infinite number of solutions (continuous
    set of possible outcomes).'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种类型的监督学习算法：回归算法和分类算法。基于回归的监督学习方法试图根据输入变量预测输出。基于分类的监督学习方法确定数据项属于哪个类别。分类算法是基于概率的，意味着算法找到数据集属于哪个类别的概率最高，就会输出该类别。相反，回归算法估计具有无限解（可能结果的连续集）的问题的结果。
- en: In the context of finance, supervised learning models represent one of the most-used
    class of machine learning models. Many algorithms that are widely applied in algorithmic
    trading rely on supervised learning models because they can be efficiently trained,
    they are relatively robust to noisy financial data, and they have strong links
    to the theory of finance.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在金融领域，监督学习模型代表了最常用的机器学习模型之一。许多在算法交易中广泛应用的算法依赖于监督学习模型，因为它们可以高效训练，相对稳健地处理嘈杂的金融数据，并且与金融理论有着密切联系。
- en: Regression-based algorithms have been leveraged by academic and industry researchers
    to develop numerous asset pricing models. These models are used to predict returns
    over various time periods and to identify significant factors that drive asset
    returns. There are many other use cases of regression-based supervised learning
    in portfolio management and derivatives pricing.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 学术界和行业研究人员已经利用基于回归的算法开发了许多资产定价模型。这些模型用于预测各种时间段的回报，并识别影响资产回报的重要因素。在投资组合管理和衍生品定价中还有许多其他基于回归的监督学习的用例。
- en: Classification-based algorithms, on the other hand, have been leveraged across
    many areas within finance that require predicting a categorical response. These
    include fraud detection, default prediction, credit scoring, directional forecast
    of asset price movement, and Buy/Sell recommendations. There are many other use
    cases of classification-based supervised learning in portfolio management and
    algorithmic trading.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，基于分类的算法已经在金融领域的许多领域中得到应用，这些领域需要预测分类响应。这些包括欺诈检测、违约预测、信用评分、资产价格运动方向的预测以及买入/卖出建议。在投资组合管理和算法交易中还有许多其他基于分类的监督学习的用例。
- en: Many use cases of regression-based and classification-based supervised machine
    learning are presented in Chapters [5](ch05.xhtml#Chapter5) and [6](ch06.xhtml#Chapter6).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在第[5](ch05.xhtml#Chapter5)章和第[6](ch06.xhtml#Chapter6)章中介绍了基于回归和分类的监督学习的许多用例。
- en: Python and its libraries provide methods and ways to implement these supervised
    learning models in few lines of code. Some of these libraries were covered in
    [Chapter 2](ch02.xhtml#Chapter2). With easy-to-use machine learning libraries
    like Scikit-learn and Keras, it is straightforward to fit different machine learning
    models on a given predictive modeling dataset.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Python及其库提供了在几行代码中实现这些监督学习模型的方法和方式。这些库中的一些在[第2章](ch02.xhtml#Chapter2)中有详细介绍。借助易于使用的机器学习库，如Scikit-learn和Keras，可以简单地对给定的预测建模数据集拟合不同的机器学习模型。
- en: In this chapter, we present a high-level overview of supervised learning models.
    For a thorough coverage of the topics, the reader is referred to *Hands-On Machine
    Learning with Scikit-Learn, Keras, and TensorFlow*, 2nd Edition, by Aurélien Géron
    (O’Reilly).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们提供了监督学习模型的高级概述。有关这些主题的全面覆盖，请参阅Aurélien Géron的《使用Scikit-Learn、Keras和TensorFlow进行实战机器学习》，第2版（O'Reilly）。
- en: 'Supervised Learning Models: An Overview'
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习模型：概述
- en: Classification predictive modeling problems are different from regression predictive
    modeling problems, as classification is the task of predicting a discrete class
    label and regression is the task of predicting a continuous quantity. However,
    both share the same concept of utilizing known variables to make predictions,
    and there is a significant overlap between the two models. Hence, the models for
    classification and regression are presented together in this chapter. [Figure 4-1](#ModelsSupervised)
    summarizes the list of the models commonly used for classification and regression.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 分类预测建模问题与回归预测建模问题有所不同，因为分类是预测离散类别标签的任务，而回归是预测连续量的任务。然而，两者共享利用已知变量进行预测的概念，并且在两种模型之间存在显著的重叠。因此，分类和回归模型在本章中一起介绍。[图
    4-1](#ModelsSupervised) 总结了用于分类和回归的常用模型列表。
- en: Some models can be used for both classification and regression with small modifications.
    These are *K*-nearest neighbors, decision trees, support vector, ensemble bagging/boosting
    methods, and ANNs (including deep neural networks), as shown in [Figure 4-1](#ModelsSupervised).
    However, some models, such as linear regression and logistic regression, cannot
    (or cannot easily) be used for both problem types.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 一些模型可以通过小的修改同时用于分类和回归。这些模型包括 *K* 近邻、决策树、支持向量机、集成装袋/提升方法以及人工神经网络（包括深度神经网络），如图
    4-1 所示。然而，一些模型，如线性回归和逻辑回归，不能（或者不容易）同时用于两种问题类型。
- en: '![mlbf 0401](Images/mlbf_0401.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![mlbf 0401](Images/mlbf_0401.png)'
- en: Figure 4-1\. Models for regression and classification
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-1\. 回归和分类模型
- en: 'This section contains the following details about the models:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本节包含以下有关模型的详细信息：
- en: Theory of the models.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型理论。
- en: Implementation in Scikit-learn or Keras.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Scikit-learn 或 Keras 中的实现。
- en: Grid search for different models.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同模型的网格搜索。
- en: Pros and cons of the models.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的优缺点。
- en: Note
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In finance, a key focus is on models that extract signals from previously observed
    data in order to predict future values for the same time series. This family of
    time series models predicts continuous output and is more aligned with the supervised
    regression models. Time series models are covered separately in the supervised
    regression chapter ([Chapter 5](ch05.xhtml#Chapter5)).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在金融领域，重点放在从先前观察到的数据中提取信号以预测同一时间序列的未来值的模型上。这类时间序列模型预测连续输出，并且更符合监督回归模型的特性。时间序列模型在监督回归章节中单独进行讨论（[第
    5 章](ch05.xhtml#Chapter5)）。
- en: Linear Regression (Ordinary Least Squares)
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性回归（普通最小二乘法）
- en: '*Linear regression* (Ordinary Least Squares Regression or OLS Regression) is
    perhaps one of the most well-known and best-understood algorithms in statistics
    and machine learning. Linear regression is a linear model, e.g., a model that
    assumes a linear relationship between the input variables (*x*) and the single
    output variable (*y*). The goal of linear regression is to train a linear model
    to predict a new *y* given a previously unseen *x* with as little error as possible.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*线性回归*（普通最小二乘回归或OLS回归）或许是统计学和机器学习中最为知名且理解最透彻的算法之一。线性回归是一个线性模型，例如，它假设输入变量 (*x*)
    和单一输出变量 (*y*) 之间存在线性关系。线性回归的目标是训练一个线性模型，以尽可能小的误差来预测新的 *y* 给定先前未见的 *x*。'
- en: 'Our model will be a function that predicts *y* given <math alttext="x 1 comma
    x 2 period period period x Subscript i Baseline"><mrow><msub><mi>x</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>.</mo> <mo>.</mo> <mo>.</mo>
    <msub><mi>x</mi> <mi>i</mi></msub></mrow></math> :'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型将是一个函数，其预测 *y* 给定 <math alttext="x 1 comma x 2 period period period x Subscript
    i Baseline"><mrow><msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi>
    <mn>2</mn></msub> <mo>.</mo> <mo>.</mo> <mo>.</mo> <msub><mi>x</mi> <mi>i</mi></msub></mrow></math>
    ：
- en: <math alttext="y equals beta 0 plus beta 1 x 1 plus period period period plus
    beta Subscript i Baseline x Subscript i Baseline" display="block"><mrow><mi>y</mi>
    <mo>=</mo> <msub><mi>β</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>β</mi> <mn>1</mn></msub>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <mo>.</mo> <mo>.</mo> <mo>.</mo>
    <mo>+</mo> <msub><mi>β</mi> <mi>i</mi></msub> <msub><mi>x</mi> <mi>i</mi></msub></mrow></math>
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="y equals beta 0 plus beta 1 x 1 plus period period period plus
    beta Subscript i Baseline x Subscript i Baseline" display="block"><mrow><mi>y</mi>
    <mo>=</mo> <msub><mi>β</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>β</mi> <mn>1</mn></msub>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <mo>.</mo> <mo>.</mo> <mo>.</mo>
    <mo>+</mo> <msub><mi>β</mi> <mi>i</mi></msub> <msub><mi>x</mi> <mi>i</mi></msub></mrow></math>
- en: where, <math alttext="beta 0"><msub><mi>β</mi> <mn>0</mn></msub></math> is called
    intercept and <math alttext="beta 1 period period period beta Subscript i Baseline"><mrow><msub><mi>β</mi>
    <mn>1</mn></msub> <mo>.</mo> <mo>.</mo> <mo>.</mo> <msub><mi>β</mi> <mi>i</mi></msub></mrow></math>
    are the coefficient of the regression.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，<math alttext="beta 0"><msub><mi>β</mi> <mn>0</mn></msub></math> 被称为截距，<math
    alttext="beta 1 period period period beta Subscript i Baseline"><mrow><msub><mi>β</mi>
    <mn>1</mn></msub> <mo>.</mo> <mo>.</mo> <mo>.</mo> <msub><mi>β</mi> <mi>i</mi></msub></mrow></math>
    是回归系数。
- en: Implementation in Python
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Python中的实现
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the following section, we cover the training of a linear regression model
    and grid search of the model. However, the overall concepts and related approaches
    are applicable to all other supervised learning models.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们涵盖了线性回归模型的训练和模型的网格搜索。然而，总体概念和相关方法适用于所有其他监督学习模型。
- en: Training a model
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'As we mentioned in [Chapter 3](ch03.xhtml#Chapter3), training a model basically
    means retrieving the model parameters by minimizing the cost (loss) function.
    The two steps for training a linear regression model are:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第三章](ch03.xhtml#Chapter3)中提到的，训练模型基本上意味着通过最小化成本（损失）函数来检索模型参数。训练线性回归模型的两个步骤是：
- en: Define a cost function (or loss function)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 定义成本函数（或损失函数）
- en: Measures how inaccurate the model’s predictions are. The *sum of squared residuals
    (RSS)* as defined in [Equation 4-1](#EquationRSS) measures the squared sum of
    the difference between the actual and predicted value and is the cost function
    for linear regression.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 衡量模型预测的不准确性。如[方程 4-1](#EquationRSS)中定义的*残差平方和（RSS）*，衡量实际值与预测值之间差异的平方和，是线性回归的成本函数。
- en: Equation 4-1\. Sum of squared residuals
  id: totrans-33
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 4-1\. 残差平方和
- en: <math display="block"><mrow><mi>R</mi> <mi>S</mi> <mi>S</mi> <mo>=</mo> <munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></munderover> <msup><mfenced
    separators="" open="(" close=")"><msub><mi>y</mi> <mi>i</mi></msub> <mo>–</mo>
    <msub><mi>β</mi> <mn>0</mn></msub> <mo>–</mo> <munderover><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></munderover> <msub><mi>β</mi> <mi>j</mi></msub> <msub><mi>x</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub></mfenced>
    <mn>2</mn></msup></mrow></math>
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>R</mi> <mi>S</mi> <mi>S</mi> <mo>=</mo> <munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></munderover> <msup><mfenced
    separators="" open="(" close=")"><msub><mi>y</mi> <mi>i</mi></msub> <mo>–</mo>
    <msub><mi>β</mi> <mn>0</mn></msub> <mo>–</mo> <munderover><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></munderover> <msub><mi>β</mi> <mi>j</mi></msub> <msub><mi>x</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub></mfenced>
    <mn>2</mn></msup></mrow></math>
- en: In this equation, <math alttext="beta 0"><msub><mi>β</mi> <mn>0</mn></msub></math>
    is the intercept; <math alttext="beta Subscript j"><msub><mi>β</mi> <mi>j</mi></msub></math>
    represents the coefficient; <math alttext="beta 1 comma period period comma beta
    Subscript j Baseline"><mrow><msub><mi>β</mi> <mn>1</mn></msub> <mo>,</mo> <mo>.</mo>
    <mo>.</mo> <mo>,</mo> <msub><mi>β</mi> <mi>j</mi></msub></mrow></math> are the
    coefficients of the regression; and <math alttext="x Subscript i j"><msub><mi>x</mi>
    <mrow><mi>i</mi><mi>j</mi></mrow></msub></math> represents the <math alttext="i
    Superscript t h"><msup><mi>i</mi> <mrow><mi>t</mi><mi>h</mi></mrow></msup></math>
    observation and <math alttext="j Superscript t h"><msup><mi>j</mi> <mrow><mi>t</mi><mi>h</mi></mrow></msup></math>
    variable.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中，<math alttext="beta 0"><msub><mi>β</mi> <mn>0</mn></msub></math> 是截距；<math
    alttext="beta Subscript j"><msub><mi>β</mi> <mi>j</mi></msub></math> 代表系数；<math
    alttext="beta 1 comma period period comma beta Subscript j Baseline"><mrow><msub><mi>β</mi>
    <mn>1</mn></msub> <mo>,</mo> <mo>.</mo> <mo>.</mo> <mo>,</mo> <msub><mi>β</mi>
    <mi>j</mi></msub></mrow></math> 是回归的系数；<math alttext="x Subscript i j"><msub><mi>x</mi>
    <mrow><mi>i</mi><mi>j</mi></mrow></msub></math> 表示第<math alttext="i Superscript
    t h"><msup><mi>i</mi> <mrow><mi>t</mi><mi>h</mi></mrow></msup></math>观测和第<math
    alttext="j Superscript t h"><msup><mi>j</mi> <mrow><mi>t</mi><mi>h</mi></mrow></msup></math>变量。
- en: Find the parameters that minimize loss
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 找到最小化损失的参数
- en: For example, make our model as accurate as possible. Graphically, in two dimensions,
    this results in a line of best fit as shown in [Figure 4-2](#LinReg). In higher
    dimensions, we would have higher-dimensional hyperplanes. Mathematically, we look
    at the difference between each real data point (*y*) and our model’s prediction
    (*ŷ*). Square these differences to avoid negative numbers and penalize larger
    differences, and then add them up and take the average. This is a measure of how
    well our data fits the line.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，使我们的模型尽可能准确。在二维图中，这会导致最佳拟合线，如[图 4-2](#LinReg)所示。在更高维度中，我们将会有更高维的超平面。从数学角度来看，我们关注每个真实数据点(*y*)与我们模型预测(*ŷ*)之间的差异。平方这些差异以避免负数并惩罚较大的差异，然后将它们相加并取平均值。这衡量了我们的数据与拟合线的拟合程度。
- en: '![mlbf 0402](Images/mlbf_0402.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![mlbf 0402](Images/mlbf_0402.png)'
- en: Figure 4-2\. Linear regression
  id: totrans-39
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-2\. 线性回归
- en: Grid search
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 网格搜索
- en: The overall idea of the grid search is to create a grid of all possible hyperparameter
    combinations and train the model using each one of them. Hyperparameters are the
    external characteristic of the model, can be considered the model’s settings,
    and are not estimated based on data-like model parameters. These hyperparameters
    are tuned during grid search to achieve better model performance.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索的总体思想是创建所有可能的超参数组合的网格，并使用每个组合来训练模型。超参数是模型的外部特征，可以被视为模型的设置，不像模型参数那样基于数据估计。这些超参数在网格搜索过程中被调整以实现更好的模型性能。
- en: Due to its exhaustive search, a grid search is guaranteed to find the optimal
    parameter within the grid. The drawback is that the size of the grid grows exponentially
    with the addition of more parameters or more considered values.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其穷举搜索，网格搜索保证在网格内找到最优参数。缺点是随着参数或考虑值的增加，网格的大小呈指数增长。
- en: The `GridSearchCV` class in the `model_selection` module of the sklearn package
    facilitates the systematic evaluation of all combinations of the hyperparameter
    values that we would like to test.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: sklearn软件包的`model_selection`模块中的`GridSearchCV`类有助于系统地评估我们希望测试的所有超参数值的组合。
- en: 'The first step is to create a model object. We then define a dictionary where
    the keywords name the hyperparameters and the values list the parameter settings
    to be tested. For linear regression, the hyperparameter is `fit_intercept`, which
    is a boolean variable that determines whether or not to calculate the *intercept*
    for this model. If set to `False`, no intercept will be used in calculations:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是创建一个模型对象。然后我们定义一个字典，其中关键字命名超参数，值列表显示要测试的参数设置。对于线性回归，超参数是`fit_intercept`，它是一个布尔变量，确定是否计算此模型的*截距*。如果设置为`False`，计算中将不使用截距：
- en: '[PRE1]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The second step is to instantiate the `GridSearchCV` object and provide the
    estimator object and parameter grid, as well as a scoring method and cross validation
    choice, to the initialization method. Cross validation is a resampling procedure
    used to evaluate machine learning models, and scoring parameter is the evaluation
    metrics of the model:^([1](ch04.xhtml#idm45174932998104))
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步是实例化`GridSearchCV`对象，并提供评估器对象和参数网格，以及评分方法和交叉验证选择给初始化方法。交叉验证是一种用于评估机器学习模型的重采样过程，评分参数是模型的评估指标：^([1](ch04.xhtml#idm45174932998104))
- en: 'With all settings in place, we can fit `GridSearchCV`:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有设置就位后，我们可以拟合`GridSearchCV`：
- en: '[PRE2]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Advantages and disadvantages
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优点和缺点
- en: In terms of advantages, linear regression is easy to understand and interpret.
    However, it may not work well when there is a nonlinear relationship between predicted
    and predictor variables. Linear regression is prone to *overfitting* (which we
    will discuss in the next section) and when a large number of features are present,
    it may not handle irrelevant features well. Linear regression also requires the
    data to follow certain [assumptions](https://oreil.ly/tNDnc), such as the absence
    of multicollinearity. If the assumptions fail, then we cannot trust the results
    obtained.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在优点方面，线性回归易于理解和解释。然而，当预测变量与预测变量之间存在非线性关系时，它可能效果不佳。线性回归容易出现*过拟合*（我们将在下一节讨论）问题，而且当存在大量特征时，可能无法很好地处理不相关的特征。线性回归还要求数据遵循某些[假设](https://oreil.ly/tNDnc)，如不存在多重共线性。如果假设不成立，则无法信任所得到的结果。
- en: Regularized Regression
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正则化回归
- en: When a linear regression model contains many independent variables, their coefficients
    will be poorly determined, and the model will have a tendency to fit extremely
    well to the training data (data used to build the model) but fit poorly to testing
    data (data used to test how good the model is). This is known as overfitting or
    high variance.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 当线性回归模型包含许多自变量时，它们的系数将难以确定，模型将倾向于非常适合训练数据（用于构建模型的数据），但对测试数据（用于测试模型好坏的数据）适配不佳。这被称为过拟合或高方差。
- en: 'One popular technique to control overfitting is *regularization*, which involves
    the addition of a *penalty* term to the error or loss function to discourage the
    coefficients from reaching large values. Regularization, in simple terms, is a
    penalty mechanism that applies shrinkage to model parameters (driving them closer
    to zero) in order to build a model with higher prediction accuracy and interpretation.
    Regularized regression has two advantages over linear regression:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 控制过拟合的一种流行技术是*正则化*，它涉及向误差或损失函数添加一个*惩罚*项，以防止系数达到较大值。简单来说，正则化是一种惩罚机制，通过收缩模型参数（使其接近于零）来建立预测精度更高且易于解释的模型。正则化回归比线性回归有两个优点：
- en: Prediction accuracy
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 预测精度
- en: The performance of the model working better on the testing data suggests that
    the model is trying to generalize from training data. A model with too many parameters
    might try to fit noise specific to the training data. By shrinking or setting
    some coefficients to zero, we trade off the ability to fit complex models (higher
    bias) for a more generalizable model (lower variance).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 模型在测试数据上的表现更好表明，模型试图从训练数据中概括。具有过多参数的模型可能尝试拟合特定于训练数据的噪声。通过收缩或将一些系数设为零，我们权衡了适合复杂模型（更高偏差）的能力，以换取更具泛化能力的模型（更低方差）。
- en: Interpretation
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 解释
- en: A large number of predictors may complicate the interpretation or communication
    of the big picture of the results. It may be preferable to sacrifice some detail
    to limit the model to a smaller subset of parameters with the strongest effects.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 大量预测子可能会复杂化结果的解释或传达大局。为了限制模型仅包括对结果影响最大的一小部分参数，可能需要牺牲一些细节。
- en: 'The common ways to regularize a linear regression model are as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化线性回归模型的常见方法如下：
- en: L1 regularization or Lasso regression
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: L1 正则化或 Lasso 回归
- en: '*Lasso regression* performs *L1 regularization* by adding a factor of the sum
    of the absolute value of coefficients in the cost function (RSS) for linear regression,
    as mentioned in [Equation 4-1](#EquationRSS). The equation for lasso regularization
    can be represented as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*Lasso 回归* 通过在线性回归的成本函数（RSS）中添加系数绝对值之和的因素（如 [Equation 4-1](#EquationRSS) 所述）执行
    *L1 正则化*。Lasso 正则化的方程可以表示如下：'
- en: <math alttext="upper C o s t upper F u n c t i o n equals upper R upper S upper
    S plus lamda asterisk sigma-summation Underscript j equals 1 Overscript p Endscripts
    StartAbsoluteValue beta Subscript j Baseline EndAbsoluteValue"><mrow><mi>C</mi>
    <mi>o</mi> <mi>s</mi> <mi>t</mi> <mi>F</mi> <mi>u</mi> <mi>n</mi> <mi>c</mi> <mi>t</mi>
    <mi>i</mi> <mi>o</mi> <mi>n</mi> <mo>=</mo> <mi>R</mi> <mi>S</mi> <mi>S</mi> <mo>+</mo>
    <mi>λ</mi> <mo>*</mo> <msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>p</mi></msubsup> <mfenced separators="" open="|" close="|"><msub><mi>β</mi>
    <mi>j</mi></msub></mfenced></mrow></math>
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper C o s t upper F u n c t i o n equals upper R upper S upper
    S plus lamda asterisk sigma-summation Underscript j equals 1 Overscript p Endscripts
    StartAbsoluteValue beta Subscript j Baseline EndAbsoluteValue"><mrow><mi>C</mi>
    <mi>o</mi> <mi>s</mi> <mi>t</mi> <mi>F</mi> <mi>u</mi> <mi>n</mi> <mi>c</mi> <mi>t</mi>
    <mi>i</mi> <mi>o</mi> <mi>n</mi> <mo>=</mo> <mi>R</mi> <mi>S</mi> <mi>S</mi> <mo>+</mo>
    <mi>λ</mi> <mo>*</mo> <msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>p</mi></msubsup> <mfenced separators="" open="|" close="|"><msub><mi>β</mi>
    <mi>j</mi></msub></mfenced></mrow></math>
- en: L1 regularization can lead to zero coefficients (i.e., some of the features
    are completely neglected for the evaluation of output). The larger the value of
    <math alttext="lamda"><mi>λ</mi></math> , the more features are shrunk to zero.
    This can eliminate some features entirely and give us a subset of predictors,
    reducing model complexity. So Lasso regression not only helps in reducing overfitting,
    but also can help in feature selection. Predictors not shrunk toward zero signify
    that they are important, and thus L1 regularization allows for feature selection
    (sparse selection). The regularization parameter ( <math alttext="lamda"><mi>λ</mi></math>
    ) can be controlled, and a `lambda` value of zero produces the basic linear regression
    equation.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: L1 正则化可以导致零系数（即某些特征在输出评估中被完全忽略）。<math alttext="lamda"><mi>λ</mi></math> 值越大，被收缩至零的特征越多。这可以完全消除一些特征，并给出预测子集，从而降低模型复杂度。因此，Lasso
    回归不仅有助于减少过拟合，还可以帮助进行特征选择。未收缩至零的预测子集表明它们很重要，因此 L1 正则化允许进行特征选择（稀疏选择）。正则化参数（ <math
    alttext="lamda"><mi>λ</mi></math> ）可控制，`lambda` 值为零时产生基本线性回归方程。
- en: 'A lasso regression model can be constructed using the `Lasso` class of the
    sklearn package of Python, as shown in the code snippet that follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用 Python 的 sklearn 包中的 `Lasso` 类构建 Lasso 回归模型，如下所示的代码片段：
- en: '[PRE3]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: L2 regularization or Ridge regression
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: L2 正则化或 Ridge 回归
- en: '*Ridge regression* performs *L2 regularization* by adding a factor of the sum
    of the square of coefficients in the cost function (RSS) for linear regression,
    as mentioned in [Equation 4-1](#EquationRSS). The equation for ridge regularization
    can be represented as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '*Ridge 回归* 通过在线性回归的成本函数（RSS）中添加系数平方和的因素执行 *L2 正则化*（如 [Equation 4-1](#EquationRSS)
    所述）。Ridge 正则化的方程可以表示如下：'
- en: <math><mrow><mi>C</mi> <mi>o</mi> <mi>s</mi> <mi>t</mi> <mi>F</mi> <mi>u</mi>
    <mi>n</mi> <mi>c</mi> <mi>t</mi> <mi>i</mi> <mi>o</mi> <mi>n</mi> <mo>=</mo> <mi>R</mi>
    <mi>S</mi> <mi>S</mi> <mo>+</mo> <mi>λ</mi> <mo>*</mo> <msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>p</mi></msubsup> <msubsup><mi>β</mi> <mrow><mi>j</mi></mrow> <mn>2</mn></msubsup></mrow></math>
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><mi>C</mi> <mi>o</mi> <mi>s</mi> <mi>t</mi> <mi>F</mi> <mi>u</mi>
    <mi>n</mi> <mi>c</mi> <mi>t</mi> <mi>i</mi> <mi>o</mi> <mi>n</mi> <mo>=</mo> <mi>R</mi>
    <mi>S</mi> <mi>S</mi> <mo>+</mo> <mi>λ</mi> <mo>*</mo> <msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>p</mi></msubsup> <msubsup><mi>β</mi> <mrow><mi>j</mi></mrow> <mn>2</mn></msubsup></mrow></math>
- en: Ridge regression puts constraint on the coefficients. The penalty term ( <math
    alttext="lamda"><mi>λ</mi></math> ) regularizes the coefficients such that if
    the coefficients take large values, the optimization function is penalized. So
    ridge regression shrinks the coefficients and helps to reduce the model complexity.
    Shrinking the coefficients leads to a lower variance and a lower error value.
    Therefore, ridge regression decreases the complexity of a model but does not reduce
    the number of variables; it just shrinks their effect. When <math alttext="lamda"><mi>λ</mi></math>
    is closer to zero, the cost function becomes similar to the linear regression
    cost function. So the lower the constraint (low <math alttext="lamda"><mi>λ</mi></math>
    ) on the features, the more the model will resemble the linear regression model.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 岭回归对系数施加了约束。惩罚项（ <math alttext="lamda"><mi>λ</mi></math> ）正则化系数，如果系数取大值，则优化函数受到惩罚。因此，岭回归会收缩系数并有助于降低模型复杂度。收缩系数会导致较低的方差和较低的误差值。因此，岭回归减少了模型的复杂度，但并不减少变量的数量；它只是缩小它们的影响。当
    <math alttext="lamda"><mi>λ</mi></math> 接近零时，成本函数变得类似于线性回归成本函数。因此，对于特征的约束越低（低
    <math alttext="lamda"><mi>λ</mi></math> ），模型越类似于线性回归模型。
- en: 'A ridge regression model can be constructed using the `Ridge` class of the
    sklearn package of Python, as shown in the code snippet that follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用 Python 的 sklearn 包中的 `Ridge` 类构建岭回归模型，如下面的代码片段所示：
- en: '[PRE4]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Elastic net
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 弹性网络
- en: '*Elastic nets* add regularization terms to the model, which are a combination
    of both L1 and L2 regularization, as shown in the following equation:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '*弹性网络* 向模型添加了正则化项，这是 L1 和 L2 正则化的组合，如下方程所示：'
- en: <math display="inline"><mrow><mi>C</mi> <mi>o</mi> <mi>s</mi> <mi>t</mi> <mi>F</mi>
    <mi>u</mi> <mi>n</mi> <mi>c</mi> <mi>t</mi> <mi>i</mi> <mi>o</mi> <mi>n</mi> <mo>=</mo>
    <mi>R</mi> <mi>S</mi> <mi>S</mi> <mo>+</mo> <mi>λ</mi> <mo>*</mo> <mfenced separators=""
    open="(" close=")"><mrow><mo>(</mo><mn>1</mn><mo>–</mo><mi>α</mi><mo>)</mo></mrow>
    <mo>/</mo> <mn>2</mn> <mo>*</mo> <msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>p</mi></msubsup> <msubsup><mi>β</mi> <mrow><mi>j</mi></mrow> <mn>2</mn></msubsup>
    <mo>+</mo> <mi>α</mi> <mo>*</mo> <msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>p</mi></msubsup> <mfenced separators="" open="|" close="|"><msub><mi>β</mi>
    <mi>j</mi></msub></mfenced></mfenced></mrow></math>
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="inline"><mrow><mi>C</mi> <mi>o</mi> <mi>s</mi> <mi>t</mi> <mi>F</mi>
    <mi>u</mi> <mi>n</mi> <mi>c</mi> <mi>t</mi> <mi>i</mi> <mi>o</mi> <mi>n</mi> <mo>=</mo>
    <mi>R</mi> <mi>S</mi> <mi>S</mi> <mo>+</mo> <mi>λ</mi> <mo>*</mo> <mfenced separators=""
    open="(" close=")"><mrow><mo>(</mo><mn>1</mn><mo>–</mo><mi>α</mi><mo>)</mo></mrow>
    <mo>/</mo> <mn>2</mn> <mo>*</mo> <msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>p</mi></msubsup> <msubsup><mi>β</mi> <mrow><mi>j</mi></mrow> <mn>2</mn></msubsup>
    <mo>+</mo> <mi>α</mi> <mo>*</mo> <msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>p</mi></msubsup> <mfenced separators="" open="|" close="|"><msub><mi>β</mi>
    <mi>j</mi></msub></mfenced></mfenced></mrow></math>
- en: In addition to setting and choosing a <math alttext="lamda"><mi>λ</mi></math>
    value, an elastic net also allows us to tune the alpha parameter, where <math
    alttext="alpha"><mi>α</mi></math> = *0* corresponds to ridge and <math alttext="alpha"><mi>α</mi></math>
    = *1* to lasso. Therefore, we can choose an <math alttext="alpha"><mi>α</mi></math>
    value between *0* and *1* to optimize the elastic net. Effectively, this will
    shrink some coefficients and set some to *0* for sparse selection.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 除了设置和选择 <math alttext="lamda"><mi>λ</mi></math> 值外，弹性网还允许我们调整 alpha 参数，其中 <math
    alttext="alpha"><mi>α</mi></math> = *0* 对应于岭回归，<math alttext="alpha"><mi>α</mi></math>
    = *1* 对应于拉索。因此，我们可以选择一个介于 *0* 和 *1* 之间的 <math alttext="alpha"><mi>α</mi></math>
    值来优化弹性网。这将有效地收缩一些系数并将一些系数设置为 *0* 以进行稀疏选择。
- en: 'An elastic net regression model can be constructed using the `ElasticNet` class
    of the sklearn package of Python, as shown in the following code snippet:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用 Python 的 sklearn 包中的 `ElasticNet` 类构建弹性网络回归模型，如下面的代码片段所示：
- en: '[PRE5]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: For all the regularized regression, <math alttext="lamda"><mi>λ</mi></math>
    is the key parameter to tune during grid search in Python. In an elastic net,
    <math alttext="alpha"><mi>α</mi></math> can be an additional parameter to tune.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有正则化回归，在 Python 的网格搜索期间调整的关键参数是 <math alttext="lamda"><mi>λ</mi></math> 。在弹性网中，<math
    alttext="alpha"><mi>α</mi></math> 可以是一个额外的可调参数。
- en: Logistic Regression
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: '*Logistic regression* is one of the most widely used algorithms for classification.
    The logistic regression model arises from the desire to model the probabilities
    of the output classes given a function that is linear in *x*, at the same time
    ensuring that output probabilities sum up to one and remain between zero and one
    as we would expect from probabilities.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '*逻辑回归* 是最广泛使用的分类算法之一。逻辑回归模型出现的原因是希望在 *x* 的线性函数中建模输出类的概率，同时确保输出概率总和为一，并且保持在零到一之间，这是我们从概率中期望的结果。'
- en: If we train a linear regression model on several examples where *Y = 0* or *1*,
    we might end up predicting some probabilities that are less than zero or greater
    than one, which doesn’t make sense. Instead, we use a logistic regression model
    (or *logit* model), which is a modification of linear regression that makes sure
    to output a probability between zero and one by applying the `sigmoid` function.^([2](ch04.xhtml#idm45174932709384))
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在几个示例上训练线性回归模型，其中 *Y = 0* 或 *1*，我们可能会预测出一些小于零或大于一的概率，这是不合理的。相反，我们使用逻辑回归模型（或
    *logit* 模型），这是线性回归的修改版，通过应用 `sigmoid` 函数确保输出的概率在零到一之间。^([2](ch04.xhtml#idm45174932709384))
- en: '[Equation 4-2](#LogisticEq) shows the equation for a logistic regression model.
    Similar to linear regression, input values (*x*) are combined linearly using weights
    or coefficient values to predict an output value (*y*). The output coming from
    [Equation 4-2](#LogisticEq) is a probability that is transformed into a binary
    value (*0* or *1*) to get the model prediction.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[方程 4-2](#LogisticEq) 展示了逻辑回归模型的方程式。类似于线性回归，输入值(*x*)通过权重或系数值线性组合以预测输出值(*y*)。从[方程
    4-2](#LogisticEq)得到的输出是一个概率，被转换成二进制值(*0*或*1*)以获取模型预测。'
- en: Equation 4-2\. Logistic regression equation
  id: totrans-82
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 4-2\. 逻辑回归方程
- en: <math alttext="y equals StartFraction exp left-parenthesis beta 0 plus beta
    1 x 1 plus period period period period plus beta Subscript i Baseline x 1 right-parenthesis
    Over 1 plus exp left-parenthesis beta 0 plus beta 1 x 1 plus period period period
    period plus beta Subscript i Baseline x 1 right-parenthesis EndFraction" display="block"><mrow><mi>y</mi>
    <mo>=</mo> <mfrac><mrow><mo form="prefix">exp</mo><mo>(</mo><msub><mi>β</mi> <mn>0</mn></msub>
    <mo>+</mo><msub><mi>β</mi> <mn>1</mn></msub> <msub><mi>x</mi> <mn>1</mn></msub>
    <mo>+</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>+</mo><msub><mi>β</mi> <mi>i</mi></msub>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>)</mo></mrow> <mrow><mn>1</mn><mo>+</mo><mo
    form="prefix">exp</mo><mo>(</mo><msub><mi>β</mi> <mn>0</mn></msub> <mo>+</mo><msub><mi>β</mi>
    <mn>1</mn></msub> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>+</mo><msub><mi>β</mi>
    <mi>i</mi></msub> <msub><mi>x</mi> <mn>1</mn></msub> <mo>)</mo></mrow></mfrac></mrow></math>
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="y equals StartFraction exp left-parenthesis beta 0 plus beta
    1 x 1 plus period period period period plus beta Subscript i Baseline x 1 right-parenthesis
    Over 1 plus exp left-parenthesis beta 0 plus beta 1 x 1 plus period period period
    period plus beta Subscript i Baseline x 1 right-parenthesis EndFraction" display="block"><mrow><mi>y</mi>
    <mo>=</mo> <mfrac><mrow><mo form="prefix">exp</mo><mo>(</mo><msub><mi>β</mi> <mn>0</mn></msub>
    <mo>+</mo><msub><mi>β</mi> <mn>1</mn></msub> <msub><mi>x</mi> <mn>1</mn></msub>
    <mo>+</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>+</mo><msub><mi>β</mi> <mi>i</mi></msub>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>)</mo></mrow> <mrow><mn>1</mn><mo>+</mo><mo
    form="prefix">exp</mo><mo>(</mo><msub><mi>β</mi> <mn>0</mn></msub> <mo>+</mo><msub><mi>β</mi>
    <mn>1</mn></msub> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>+</mo><msub><mi>β</mi>
    <mi>i</mi></msub> <msub><mi>x</mi> <mn>1</mn></msub> <mo>)</mo></mrow></mfrac></mrow></math>
- en: Where *y* is the predicted output, <math alttext="beta 0"><msub><mi>β</mi> <mn>0</mn></msub></math>
    is the bias or intercept term and B[1] is the coefficient for the single input
    value (*x*). Each column in the input data has an associated <math alttext="beta"><mi>β</mi></math>
    coefficient (a constant real value) that must be learned from the training data.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*y*是预测输出，<math alttext="beta 0"><msub><mi>β</mi> <mn>0</mn></msub></math>是偏置或截距项，而B[1]是单个输入值(*x*)的系数。输入数据中的每一列都有一个关联的<math
    alttext="beta"><mi>β</mi></math>系数（一个常数实数值），必须从训练数据中学习。
- en: In logistic regression, the cost function is basically a measure of how often
    we predicted one when the true answer was zero, or vice versa. Training the logistic
    regression coefficients is done using techniques such as maximum likelihood estimation
    (MLE) to predict values close to *1* for the default class and close to *0* for
    the other class.^([3](ch04.xhtml#idm45174932651752))
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在逻辑回归中，成本函数基本上是衡量我们在真实答案为零时多少次预测为一，反之亦然。训练逻辑回归系数使用诸如最大似然估计（MLE）的技术，以预测接近*1*的默认类别值和接近*0*的其他类别值。^([3](ch04.xhtml#idm45174932651752))
- en: 'A logistic regression model can be constructed using the `LogisticRegression`
    class of the sklearn package of Python, as shown in the following code snippet:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用Python的sklearn包的`LogisticRegression`类构建逻辑回归模型，如下代码片段所示：
- en: '[PRE6]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Hyperparameters
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 超参数
- en: Regularization (`penalty` in sklearn)
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化（`penalty`在sklearn中）
- en: Similar to linear regression, logistic regression can have regularization, which
    can be **L1**, **L2**, or **elasticnet**. The values in the *sklearn* library
    are *[*l1*, *l2*, *elasticnet*]*.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于线性回归，逻辑回归可以进行正则化，可以是**L1**、**L2**或**elasticnet**。在*sklearn*库中的取值为[*l1*, *l2*,
    *elasticnet*]。
- en: Regularization strength (`C` in sklearn)
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化强度（`C`在sklearn中）
- en: This parameter controls the regularization strength. Good values of the penalty
    parameters can be *[100, 10, 1.0, 0.1, 0.01]*.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 此参数控制正则化强度。想要的惩罚参数的好值可以是*[100, 10, 1.0, 0.1, 0.01]*。
- en: Advantages and disadvantages
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优缺点
- en: In terms of the advantages, the logistic regression model is easy to implement,
    has good interpretability, and performs very well on linearly separable classes.
    The output of the model is a probability, which provides more insight and can
    be used for ranking. The model has small number of hyperparameters. Although there
    may be risk of overfitting, this may be addressed using *L1/L2* regularization,
    similar to the way we addressed overfitting for the linear regression models.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 就优点而言，逻辑回归模型易于实现，具有良好的解释性，并且在线性可分的类别上表现非常好。模型的输出是概率，提供更多的见解并可用于排名。该模型具有少量的超参数。虽然可能存在过拟合的风险，但可以通过类似于线性回归模型的*L1/L2*正则化来解决这个问题。
- en: In terms of disadvantages, the model may overfit when provided with large numbers
    of features. Logistic regression can only learn linear functions and is less suitable
    to complex relationships between features and the target variable. Also, it may
    not handle irrelevant features well, especially if the features are strongly correlated.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在缺点方面，当提供大量特征时，模型可能会过拟合。逻辑回归只能学习线性函数，并且不太适合处理特征与目标变量之间的复杂关系。此外，如果特征强相关，可能无法很好地处理无关特征。
- en: Support Vector Machine
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 支持向量机
- en: The objective of the *support vector machine* (SVM) algorithm is to maximize
    the margin (shown as shaded area in [Figure 4-3](#SVM)), which is defined as the
    distance between the separating hyperplane (or decision boundary) and the training
    samples that are closest to this hyperplane, the so-called support vectors. The
    margin is calculated as the perpendicular distance from the line to only the closest
    points, as shown in [Figure 4-3](#SVM). Hence, SVM calculates a maximum-margin
    boundary that leads to a homogeneous partition of all data points.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '*支持向量机*（SVM）算法的目标是最大化边界（如[图 4-3](#SVM)中的阴影区域所示），即分隔超平面（或决策边界）与最接近该超平面的训练样本之间的距离，即所谓的支持向量。边界计算为从线到仅最接近点的垂直距离，如[图 4-3](#SVM)所示。因此，SVM
    计算出导致所有数据点均匀分区的最大间隔边界。'
- en: '![mlbf 0403](Images/mlbf_0403.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![mlbf 0403](Images/mlbf_0403.png)'
- en: Figure 4-3\. Support vector machine
  id: totrans-99
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-3\. 支持向量机
- en: In practice, the data is messy and cannot be separated perfectly with a hyperplane.
    The constraint of maximizing the margin of the line that separates the classes
    must be relaxed. This change allows some points in the training data to violate
    the separating line. An additional set of coefficients is introduced that give
    the margin wiggle room in each dimension. A tuning parameter is introduced, simply
    called *C*, that defines the magnitude of the wiggle allowed across all dimensions.
    The larger the value of *C*, the more violations of the hyperplane are permitted.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，数据杂乱无章，并且不能使用超平面完美分离。必须放宽最大化分隔类别的线条的约束。此变更允许训练数据中的一些点违反分隔线。引入了一组额外的系数，这些系数在每个维度中提供间隙余地。引入了一个调整参数，简称为*C*，它定义了允许跨所有维度的摆动幅度。*C*
    的值越大，允许的超平面违规就越多。
- en: In some cases, it is not possible to find a hyperplane or a linear decision
    boundary, and kernels are used. A kernel is just a transformation of the input
    data that allows the SVM algorithm to treat/process the data more easily. Using
    kernels, the original data is projected into a higher dimension to classify the
    data better.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，无法找到超平面或线性决策边界，因此使用内核。内核只是输入数据的转换，允许 SVM 算法更轻松地处理数据。使用内核，原始数据被投影到更高的维度以更好地分类数据。
- en: SVM is used for both classification and regression. We achieve this by converting
    the original optimization problem into a dual problem. For regression, the trick
    is to reverse the objective. Instead of trying to fit the largest possible street
    between two classes while limiting margin violations, SVM regression tries to
    fit as many instances as possible on the street (shaded area in [Figure 4-3](#SVM))
    while limiting margin violations. The width of the street is controlled by a hyperparameter.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: SVM 用于分类和回归。我们通过将原始优化问题转换为对偶问题来实现这一点。对于回归，技巧在于颠倒目标。在试图在两个类之间拟合尽可能大的街道同时限制边界违规时，SVM
    回归试图在街道上（[图 4-3](#SVM)中阴影区域）尽可能多地拟合实例，同时限制边界违规。街道的宽度由超参数控制。
- en: 'The SVM regression and classification models can be constructed using the sklearn
    package of Python, as shown in the following code snippets:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: SVM 回归和分类模型可以使用 Python 的 `sklearn` 包构建，如下面的代码片段所示：
- en: '`Regression`'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`回归`'
- en: '[PRE7]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '`Classification`'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '`分类`'
- en: '[PRE8]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Hyperparameters
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 超参数
- en: 'The following key parameters are present in the sklearn implementation of SVM
    and can be tweaked while performing the grid search:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `sklearn` 实现的 SVM 中存在以下关键参数，并可在执行网格搜索时进行调整：
- en: Kernels (`kernel` in sklearn)
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 内核（`sklearn` 中的 `kernel`）
- en: The choice of kernel controls the manner in which the input variables will be
    projected. There are many kernels to choose from, but *linear* and [*RBF*](https://oreil.ly/XpBOi)
    are the most common.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 内核的选择控制输入变量将被投影的方式。有许多内核可供选择，但*线性*和[*RBF*](https://oreil.ly/XpBOi)是最常见的。
- en: Penalty (`C` in sklearn)
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 惩罚（`sklearn` 中的 `C`）
- en: The penalty parameter tells the SVM optimization how much you want to avoid
    misclassifying each training example. For large values of the penalty parameter,
    the optimization will choose a smaller-margin hyperplane. Good values might be
    a log scale from 10 to 1,000.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 惩罚参数告诉 SVM 优化希望避免每个训练样本的错误分类程度。对于惩罚参数的大值，优化会选择一个较小间隔的超平面。良好的值可能在对数尺度从 10 到 1,000
    之间。
- en: Advantages and disadvantages
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优缺点
- en: In terms of advantages, SVM is fairly robust against overfitting, especially
    in higher dimensional space. It handles the nonlinear relationships quite well,
    with many kernels to choose from. Also, there is no distributional requirement
    for the data.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 就优点而言，SVM对过拟合相当鲁棒，特别是在高维空间中。它能够很好地处理非线性关系，提供多种核函数选择。此外，对数据没有分布要求。
- en: In terms of disadvantages, SVM can be inefficient to train and memory-intensive
    to run and tune. It doesn’t perform well with large datasets. It requires the
    feature scaling of the data. There are also many hyperparameters, and their meanings
    are often not intuitive.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 就缺点而言，SVM在训练时可能效率低且内存占用高，并且调整困难。它在处理大型数据集时表现不佳。它要求对数据进行特征缩放。还有许多超参数，它们的含义通常不直观。
- en: K-Nearest Neighbors
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: K-最近邻算法
- en: '*K-nearest neighbors* (KNN) is considered a “lazy learner,” as there is no
    learning required in the model. For a new data point, predictions are made by
    searching through the entire training set for the *K* most similar instances (the
    neighbors) and summarizing the output variable for those *K* instances.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '*K-最近邻*（KNN）被认为是一种“惰性学习器”，因为模型不需要学习。对于新数据点，预测是通过在整个训练集中搜索*K*个最相似的实例（邻居），并总结这些*K*个实例的输出变量来实现的。'
- en: To determine which of the *K* instances in the training dataset are most similar
    to a new input, a distance measure is used. The most popular distance measure
    is *Euclidean distance*, which is calculated as the square root of the sum of
    the squared differences between a point *a* and a point *b* across all input attributes
    **i**, and which is represented as <math display="inline"><mrow><mi>d</mi> <mrow><mo>(</mo>
    <mi>a</mi> <mo>,</mo> <mi>b</mi> <mo>)</mo></mrow> <mo>=</mo> <msqrt><mrow><msubsup><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></msubsup> <msup><mrow><mo>(</mo><msub><mi>a</mi>
    <mi>i</mi></msub> <mo>–</mo><msub><mi>b</mi> <mi>i</mi></msub> <mo>)</mo></mrow>
    <mn>2</mn></msup></mrow></msqrt></mrow></math> . Euclidean distance is a good
    distance measure to use if the input variables are similar in type.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定训练数据集中与新输入最相似的*K*个实例，使用了一个距离度量。最流行的距离度量是*欧几里得距离*，其计算方法是在所有输入属性**i**上，点*a*和点*b*之间的平方差的平方根，表示为
    <math display="inline"><mrow><mi>d</mi> <mrow><mo>(</mo> <mi>a</mi> <mo>,</mo>
    <mi>b</mi> <mo>)</mo></mrow> <mo>=</mo> <msqrt><mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></msubsup> <msup><mrow><mo>(</mo><msub><mi>a</mi> <mi>i</mi></msub>
    <mo>–</mo><msub><mi>b</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mn>2</mn></msup></mrow></msqrt></mrow></math>
    。欧几里得距离在输入变量类型相似时是一种很好的距离度量。
- en: Another distance metric is *Manhattan distance*, in which the distance between
    point *a* and point *b* is represented as <math display="inline"><mrow><mi>d</mi>
    <mrow><mo>(</mo> <mi>a</mi> <mo>,</mo> <mi>b</mi> <mo>)</mo></mrow> <mo>=</mo>
    <msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></msubsup>
    <mrow><mo>|</mo> <msub><mi>a</mi> <mi>i</mi></msub> <mo>–</mo> <msub><mi>b</mi>
    <mi>i</mi></msub> <mo>|</mo></mrow></mrow></math> . Manhattan distance is a good
    measure to use if the input variables are not similar in type.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种距离度量是*曼哈顿距离*，其中点*a*和点*b*之间的距离表示为 <math display="inline"><mrow><mi>d</mi>
    <mrow><mo>(</mo> <mi>a</mi> <mo>,</mo> <mi>b</mi> <mo>)</mo></mrow> <mo>=</mo>
    <msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></msubsup>
    <mrow><mo>|</mo> <msub><mi>a</mi> <mi>i</mi></msub> <mo>–</mo> <msub><mi>b</mi>
    <mi>i</mi></msub> <mo>|</mo></mrow></mrow></math> 。曼哈顿距离在输入变量类型不相似时是一种很好的度量。
- en: 'The steps of KNN can be summarized as follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: KNN的步骤可以总结如下：
- en: Choose the number of *K* and a distance metric.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择*K*的数量和距离度量。
- en: Find the *K*-nearest neighbors of the sample that we want to classify.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到我们要分类的样本的*K*个最近邻居。
- en: Assign the class label by majority vote.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过多数投票分配类别标签。
- en: 'KNN regression and classification models can be constructed using the sklearn
    package of Python, as shown in the following code:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用Python的sklearn包构建KNN回归和分类模型，如下所示：
- en: '`Classification`'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '`分类`'
- en: '[PRE9]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '`Regression`'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '`回归`'
- en: '[PRE10]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Hyperparameters
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 超参数
- en: 'The following key parameters are present in the sklearn implementation of KNN
    and can be tweaked while performing the grid search:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在sklearn实现的KNN中存在以下关键参数，可以在执行网格搜索时调整：
- en: Number of neighbors (`n_neighbors` in sklearn)
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 邻居数（sklearn中的`n_neighbors`）
- en: The most important hyperparameter for KNN is the number of neighbors (n_neighbors).
    Good values are between 1 and 20.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: KNN最重要的超参数是邻居数（n_neighbors）。良好的值在1到20之间。
- en: Distance metric (`metric` in sklearn)
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 距离度量（在sklearn中称为`metric`）
- en: It may also be interesting to test different distance metrics for choosing the
    composition of the neighborhood. Good values are *euclidean* and *manhattan*.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 测试不同距离度量来选择邻域组成可能也会很有趣。良好的值为*euclidean*和*manhattan*。
- en: Advantages and disadvantages
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优缺点
- en: In terms of advantages, no training is involved and hence there is no learning
    phase. Since the algorithm requires no training before making predictions, new
    data can be added seamlessly without impacting the accuracy of the algorithm.
    It is intuitive and easy to understand. The model naturally handles multiclass
    classification and can learn complex decision boundaries. KNN is effective if
    the training data is large. It is also robust to noisy data, and there is no need
    to filter the outliers.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在优点方面，无需训练，因此没有学习阶段。由于算法在进行预测之前不需要训练，因此可以轻松添加新数据而不影响算法的准确性。它直观且易于理解。该模型自然处理多类别分类，并能学习复杂的决策边界。如果训练数据量大，则KNN非常有效。它还对噪声数据具有鲁棒性，无需过滤异常值。
- en: In terms of the disadvantages, the distance metric to choose is not obvious
    and difficult to justify in many cases. KNN performs poorly on high dimensional
    datasets. It is expensive and slow to predict new instances because the distance
    to all neighbors must be recalculated. KNN is sensitive to noise in the dataset.
    We need to manually input missing values and remove outliers. Also, feature scaling
    (standardization and normalization) is required before applying the KNN algorithm
    to any dataset; otherwise, KNN may generate wrong predictions.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在缺点方面，选择距离度量并不明显，很难在许多情况下进行证明。KNN在高维数据集上表现不佳。预测新实例的成本高且速度慢，因为必须重新计算到所有邻居的距离。KNN对数据集中的噪声敏感。我们需要手动输入缺失值并移除异常值。此外，在应用KNN算法之前需要进行特征缩放（标准化和归一化），否则KNN可能会生成错误的预测。
- en: Linear Discriminant Analysis
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性判别分析
- en: The objective of the *linear discriminant analysis* (LDA) algorithm is to project
    the data onto a lower-dimensional space in a way that the class separability is
    maximized and the variance within a class is minimized.^([4](ch04.xhtml#idm45174932370936))
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '*线性判别分析*（LDA）算法的目标是以一种方式将数据投影到低维空间，使得类别的可分离性最大化，类内方差最小化。^([4](ch04.xhtml#idm45174932370936))'
- en: 'During the training of the LDA model, the statistical properties (i.e., mean
    and covariance matrix) of each class are computed. The statistical properties
    are estimated on the basis of the following assumptions about the data:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练LDA模型期间，会计算每个类别的统计特性（即均值和协方差矩阵）。这些统计特性是基于以下关于数据的假设进行估计的：
- en: Data is [normally distributed](https://oreil.ly/cuc7p), so that each variable
    is shaped like a bell curve when plotted.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据是[normally distributed](https://oreil.ly/cuc7p)，因此每个变量在绘制时都呈钟形曲线。
- en: Each attribute has the same variance, and the values of each variable vary around
    the mean by the same amount on average.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个属性具有相同的方差，并且每个变量的值平均围绕均值变化相同量。
- en: To make a prediction, LDA estimates the probability that a new set of inputs
    belongs to every class. The output class is the one that has the highest probability.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行预测，LDA估计新输入数据属于每个类别的概率。输出类别是具有最高概率的类别。
- en: Implementation in Python and hyperparameters
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Python中的实现及超参数
- en: 'The LDA classification model can be constructed using the sklearn package of
    Python, as shown in the following code snippet:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用Python的sklearn包构建LDA分类模型，如下面的代码片段所示：
- en: '[PRE11]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The key hyperparameter for the LDA model is `number of components` for dimensionality
    reduction, which is represented by `n_components` in sklearn.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: LDA模型的关键超参数是`number of components`，用于降维，而在sklearn中表示为`n_components`。
- en: Advantages and disadvantages
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优缺点
- en: In terms of advantages, LDA is a relatively simple model with fast implementation
    and is easy to implement. In terms of disadvantages, it requires feature scaling
    and involves complex matrix operations.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在优点方面，LDA是一个相对简单的模型，实现快速且易于实现。在缺点方面，它需要特征缩放并涉及复杂的矩阵操作。
- en: Classification and Regression Trees
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类和回归树
- en: In the most general terms, the purpose of an analysis via tree-building algorithms
    is to determine a set of *if–then* logical (split) conditions that permit accurate
    prediction or classification of cases. *Classification and regression trees* (or
    *CART* or *decision tree classifiers*) are attractive models if we care about
    interpretability. We can think of this model as breaking down our data and making
    a decision based on asking a series of questions. This algorithm is the foundation
    of ensemble methods such as random forest and gradient boosting method.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 从最一般的角度来看，通过树构建算法进行分析的目的是确定一组*if–then*逻辑（分裂）条件，以便准确预测或分类案例。*分类与回归树*（或*CART*或*决策树分类器*）是具有吸引力的模型，如果我们关心解释性的话。我们可以将这个模型视为分解数据并基于一系列问题做出决策的过程。这种算法是随机森林和梯度提升方法等集成方法的基础。
- en: Representation
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 表示
- en: The model can be represented by a *binary tree* (or *decision tree*), where
    each node is an input variable *x* with a split point and each leaf contains an
    output variable *y* for prediction.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型可以通过*二叉树*（或*决策树*）来表示，其中每个节点是一个输入变量 *x*，带有一个分割点，每个叶子包含用于预测的输出变量 *y*。
- en: '[Figure 4-4](#CART) shows an example of a simple classification tree to predict
    whether a person is a male or a female based on two inputs of height (in centimeters)
    and weight (in kilograms).'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-4](#CART) 展示了一个简单分类树的示例，根据身高（以厘米为单位）和体重（以公斤为单位）两个输入预测一个人是男性还是女性。'
- en: '![mlbf 0404](Images/mlbf_0404.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![mlbf 0404](Images/mlbf_0404.png)'
- en: Figure 4-4\. Classification and regression tree example
  id: totrans-157
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-4\. 分类与回归树示例
- en: Learning a CART model
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习 CART 模型
- en: Creating a binary tree is actually a process of dividing up the input space.
    A *greedy approach* called *recursive binary splitting* is used to divide the
    space. This is a numerical procedure in which all the values are lined up and
    different split points are tried and tested using a cost (loss) function. The
    split with the best cost (lowest cost, because we minimize cost) is selected.
    All input variables and all possible split points are evaluated and chosen in
    a greedy manner (e.g., the very best split point is chosen each time).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个二叉树实际上是一个将输入空间分割的过程。采用一种称为*递归二元分割*的贪婪方法来分割空间。这是一个数值过程，在此过程中，所有值都被排列，并尝试使用成本（损失）函数测试不同的分割点。选择具有最佳成本（因为我们最小化成本）的分割点。所有输入变量和所有可能的分割点都以贪婪的方式进行评估和选择（例如，每次都选择最佳分割点）。
- en: 'For regression predictive modeling problems, the cost function that is minimized
    to choose split points is the *sum of squared errors* across all training samples
    that fall within the rectangle:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 对于回归预测建模问题，用于选择分裂点的成本函数是在所有落入矩形内的训练样本上最小化的*平方误差和*：
- en: <math><mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></msubsup> <msup><mrow><mo>(</mo><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>–</mo><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>o</mi><msub><mi>n</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow> <mn>2</mn></msup></mrow></math>
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></msubsup> <msup><mrow><mo>(</mo><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>–</mo><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>o</mi><msub><mi>n</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow> <mn>2</mn></msup></mrow></math>
- en: 'where <math alttext="y Subscript i"><msub><mi>y</mi> <mi>i</mi></msub></math>
    is the output for the training sample and prediction is the predicted output for
    the rectangle. For classification, the *Gini cost function* is used; it provides
    an indication of how pure the leaf nodes are (i.e., how mixed the training data
    assigned to each node is) and is defined as:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 其中<math alttext="y Subscript i"><msub><mi>y</mi> <mi>i</mi></msub></math>是训练样本的输出，prediction
    是矩形的预测输出。对于分类问题，使用*基尼成本函数*；它提供了叶子节点纯度的指示（即分配给每个节点的训练数据的混合程度），并定义为：
- en: <math><mrow><mi>G</mi> <mo>=</mo> <msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></msubsup> <msub><mi>p</mi> <mi>k</mi></msub> <mo>*</mo> <mrow><mo>(</mo>
    <mn>1</mn> <mo>–</mo> <msub><mi>p</mi> <mi>k</mi></msub> <mo>)</mo></mrow></mrow></math>
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><mi>G</mi> <mo>=</mo> <msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></msubsup> <msub><mi>p</mi> <mi>k</mi></msub> <mo>*</mo> <mrow><mo>(</mo>
    <mn>1</mn> <mo>–</mo> <msub><mi>p</mi> <mi>k</mi></msub> <mo>)</mo></mrow></mrow></math>
- en: where *G* is the Gini cost over all classes and <math alttext="p Subscript k"><msub><mi>p</mi>
    <mi>k</mi></msub></math> is the number of training instances with class *k* in
    the rectangle of interest. A node that has all classes of the same type (perfect
    class purity) will have *G = 0*, while a node that has a *50–50* split of classes
    for a binary classification problem (worst purity) will have *G = 0.5*.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*G*是兴趣区域矩形中所有类的基尼成本，<math alttext="p Subscript k"><msub><mi>p</mi> <mi>k</mi></msub></math>是具有类*k*的训练实例数量。具有完全类纯度（完美类纯度）的节点将具有*G
    = 0*，而在二元分类问题中具有*50-50*类分布（最差纯度）的节点将具有*G = 0.5*。
- en: Stopping criterion
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 停止准则
- en: The recursive binary splitting procedure described in the preceding section
    needs to know when to stop splitting as it works its way down the tree with the
    training data. The most common stopping procedure is to use a minimum count on
    the number of training instances assigned to each leaf node. If the count is less
    than some minimum, then the split is not accepted and the node is taken as a final
    leaf node.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分描述的递归二元分裂过程中，需要知道何时停止分裂，因为它在训练数据中沿着树的路径工作。最常见的停止程序是在每个叶节点分配的训练实例数达到最小值时停止。如果计数少于某个最小值，则不接受分裂，并将节点视为最终叶节点。
- en: Pruning the tree
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 修剪树
- en: The stopping criterion is important as it strongly influences the performance
    of the tree. Pruning can be used after learning the tree to further lift performance.
    The complexity of a decision tree is defined as the number of splits in the tree.
    Simpler trees are preferred as they are faster to run and easy to understand,
    consume less memory during processing and storage, and are less likely to overfit
    the data. The fastest and simplest pruning method is to work through each leaf
    node in the tree and evaluate the effect of removing it using a test set. A leaf
    node is removed only if doing so results in a drop in the overall cost function
    on the entire test set. The removal of nodes can be stopped when no further improvements
    can be made.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 停止准则非常重要，因为它强烈影响树的性能。学习树后可以使用修剪来进一步提升性能。决策树的复杂性定义为树中的分裂数。更简单的树更受欢迎，因为它们运行更快，易于理解，在处理和存储过程中消耗更少的内存，并且不太可能过度拟合数据。最快和最简单的修剪方法是通过测试集逐个处理树中的每个叶节点，并评估移除它的效果。仅当这样做会导致整个测试集上成本函数的下降时才移除叶节点。当不能进一步改善时，可以停止删除节点。
- en: Implementation in Python
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Python实现
- en: 'CART regression and classification models can be constructed using the sklearn
    package of Python, as shown in the following code snippet:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Python的sklearn包可以构建CART回归和分类模型，如下面的代码片段所示：
- en: '`Classification`'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '`分类`'
- en: '[PRE12]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '`Regression`'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '`回归`'
- en: '[PRE13]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Hyperparameters
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 超参数
- en: CART has many hyperparameters. However, the key hyperparameter is the maximum
    depth of the tree model, which is the number of components for dimensionality
    reduction, and which is represented by `max_depth` in the sklearn package. Good
    values can range from *2* to *30* depending on the number of features in the data.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: CART有许多超参数。然而，关键的超参数是树模型的最大深度，这是降维的组件数量，用`max_depth`在sklearn包中表示。好的取值范围可以从*2*到*30*，具体取决于数据中的特征数量。
- en: Advantages and disadvantages
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优缺点
- en: In terms of advantages, CART is easy to interpret and can adapt to learn complex
    relationships. It requires little data preparation, and data typically does not
    need to be scaled. Feature importance is built in due to the way decision nodes
    are built. It performs well on large datasets. It works for both regression and
    classification problems.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在优点方面，CART易于解释并能够适应学习复杂关系。它需要很少的数据准备工作，通常不需要缩放数据。由于决策节点的构建方式，特征重要性是内置的。它在大型数据集上表现良好。它适用于回归和分类问题。
- en: In terms of disadvantages, CART is prone to overfitting unless pruning is used.
    It can be very nonrobust, meaning that small changes in the training dataset can
    lead to quite major differences in the hypothesis function that gets learned.
    CART generally has worse performance than ensemble models, which are covered next.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在缺点方面，除非使用修剪，否则CART易于过拟合。它可能非常不稳健，意味着训练数据集的小变化可能导致所学习的假设函数存在较大差异。通常情况下，CART的性能不如下面将要讨论的集成模型。
- en: Ensemble Models
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集成模型
- en: The goal of *ensemble models* is to combine different classifiers into a meta-classifier
    that has better generalization performance than each individual classifier alone.
    For example, assuming that we collected predictions from 10 experts, ensemble
    methods would allow us to strategically combine their predictions to come up with
    a prediction that is more accurate and robust than the experts’ individual predictions.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '*集成模型*的目标是将不同的分类器组合成一个元分类器，其泛化性能优于单个分类器。例如，假设我们从10位专家收集了预测结果，集成方法允许我们策略性地结合他们的预测，得到比专家个体预测更准确和更稳健的预测。'
- en: The two most popular ensemble methods are bagging and boosting. *Bagging* (or
    *bootstrap aggregation*) is an ensemble technique of training several individual
    models in a parallel way. Each model is trained by a random subset of the data.
    *Boosting*, on the other hand, is an ensemble technique of training several individual
    models in a sequential way. This is done by building a model from the training
    data and then creating a second model that attempts to correct the errors of the
    first model. Models are added until the training set is predicted perfectly or
    a maximum number of models is added. Each individual model learns from mistakes
    made by the previous model. Just like the decision trees themselves, bagging and
    boosting can be used for classification and regression problems.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 最流行的两种集成方法是装袋（bagging）和提升（boosting）。*装袋*（或*自举聚合*）是一种并行训练多个独立模型的集成技术。每个模型由数据的随机子集训练。*提升*则是一种串行训练多个独立模型的集成技术。通过从训练数据构建一个模型，然后创建第二个模型来纠正第一个模型的错误。模型逐步添加，直到训练集被完美预测或者达到最大模型数量。每个独立模型从前一个模型的错误中学习。就像决策树本身一样，装袋和提升可用于分类和回归问题。
- en: By combining individual models, the ensemble model tends to be more flexible
    (less bias) and less data-sensitive (less variance).^([5](ch04.xhtml#idm45174932160872))
    Ensemble methods combine multiple, simpler algorithms to obtain better performance.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合独立模型，集成模型更加灵活（偏差较小）且对数据不敏感（方差较小）。^([5](ch04.xhtml#idm45174932160872)) 集成方法结合多个简单算法以获得更好的性能。
- en: In this section we will cover random forest, AdaBoost, the gradient boosting
    method, and extra trees, along with their implementation using sklearn package.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将涵盖随机森林、AdaBoost、梯度提升方法和额外树，以及它们在sklearn包中的实现。
- en: Random forest
  id: totrans-185
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 随机森林
- en: '*Random forest* is a tweaked version of bagged decision trees. In order to
    understand a random forest algorithm, let us first understand the *bagging algorithm*.
    Assuming we have a dataset of one thousand instances, the steps of bagging are:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '*随机森林*是装袋决策树的调整版本。为了理解随机森林算法，首先了解*装袋算法*。假设我们有一个包含一千个实例的数据集，装袋的步骤如下：'
- en: Create many (e.g., one hundred) random subsamples of our dataset.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建许多（例如一百个）数据集的随机子样本。
- en: Train a CART model on each sample.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对每个样本训练一个CART模型。
- en: Given a new dataset, calculate the average prediction from each model and aggregate
    the prediction by each tree to assign the final label by majority vote.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定一个新数据集，计算每个模型的平均预测，并通过每棵树的预测结果进行多数投票来确定最终标签。
- en: A problem with decision trees like CART is that they are greedy. They choose
    the variable to split by using a greedy algorithm that minimizes error. Even after
    bagging, the decision trees can have a lot of structural similarities and result
    in high correlation in their predictions. Combining predictions from multiple
    models in ensembles works better if the predictions from the submodels are uncorrelated,
    or at best are weakly correlated. Random forest changes the learning algorithm
    in such a way that the resulting predictions from all of the subtrees have less
    correlation.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 像CART这样的决策树存在一个问题，即它们是贪婪的。它们通过最小化误差的贪婪算法选择分裂变量。即使在装袋之后，决策树可能具有很多结构相似性，并导致其预测高度相关。如果子模型的预测相互不相关，或者最好是弱相关，那么从多个模型的预测中组合预测将效果更好。随机森林通过改变学习算法的方式，使得所有子树的预测结果相关性更低。
- en: In CART, when selecting a split point, the learning algorithm is allowed to
    look through all variables and all variable values in order to select the most
    optimal split point. The random forest algorithm changes this procedure such that
    each subtree can access only a random sample of features when selecting the split
    points. The number of features that can be searched at each split point (*m*)
    must be specified as a parameter to the algorithm.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在CART中，选择分裂点时，学习算法允许查看所有变量和所有变量值，以选择最优的分裂点。随机森林算法改变了这个过程，使得每个子树在选择分裂点时只能访问一部分随机抽取的特征。在算法中必须指定一个参数来表示每个分裂点可以搜索的特征数量（*m*）。
- en: As the bagged decision trees are constructed, we can calculate how much the
    error function drops for a variable at each split point. In regression problems,
    this may be the drop in sum squared error, and in classification, this might be
    the Gini cost. The bagged method can provide feature importance by calculating
    and averaging the error function drop for individual variables.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 当构建装袋决策树时，我们可以计算每个分裂点的变量的误差函数降低量。在回归问题中，这可能是总平方误差的减少量，在分类问题中，可能是基尼成本。装袋方法可以通过计算并平均单个变量的误差函数降低量来提供特征重要性。
- en: Implementation in Python
  id: totrans-193
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Python 实现
- en: 'Random forest regression and classification models can be constructed using
    the sklearn package of Python, as shown in the following code:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 可使用Python的sklearn包构建随机森林回归和分类模型，如下所示的代码：
- en: '`Classification`'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '`分类`'
- en: '[PRE14]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '`Regression`'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '`回归`'
- en: '[PRE15]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Hyperparameters
  id: totrans-199
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 超参数
- en: 'Some of the main hyperparameters that are present in the sklearn implementation
    of random forest and that can be tweaked while performing the grid search are:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: sklearn实现的随机森林中存在一些主要的超参数，可以在执行网格搜索时进行调整。
- en: Maximum number of features (`max_features` in sklearn)
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 最大特征数（`max_features`在sklearn中）
- en: This is the most important parameter. It is the number of random features to
    sample at each split point. You could try a range of integer values, such as 1
    to 20, or 1 to half the number of input features.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最重要的参数。它是在每个分裂点随机抽取的特征数量。您可以尝试一系列整数值，例如从1到20，或从1到输入特征数量的一半。
- en: Number of estimators (`n_estimators` in sklearn)
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 树的数量（`n_estimators`在sklearn中）
- en: This parameter represents the number of trees. Ideally, this should be increased
    until no further improvement is seen in the model. Good values might be a log
    scale from 10 to 1,000.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 此参数代表树的数量。理想情况下，应该增加此数量，直到模型不再显示进一步改善为止。良好的值可能在对数尺度从10到1,000之间。
- en: Advantages and disadvantages
  id: totrans-205
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优缺点
- en: The random forest algorithm (or model) has gained huge popularity in ML applications
    during the last decade due to its good performance, scalability, and ease of use.
    It is flexible and naturally assigns feature importance scores, so it can handle
    redundant feature columns. It scales to large datasets and is generally robust
    to overfitting. The algorithm doesn’t need the data to be scaled and can model
    a nonlinear relationship.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林算法（或模型）由于其良好的性能、可扩展性和易用性，在过去十年中在机器学习应用中获得了巨大的流行。它灵活，并自然地分配特征重要性分数，因此可以处理冗余的特征列。它适用于大型数据集，并且通常对过拟合具有较强的鲁棒性。该算法不需要对数据进行缩放，并且可以建模非线性关系。
- en: In terms of disadvantages, random forest can feel like a black box approach,
    as we have very little control over what the model does, and the results may be
    difficult to interpret. Although random forest does a good job at classification,
    it may not be good for regression problems, as it does not give a precise continuous
    nature prediction. In the case of regression, it doesn’t predict beyond the range
    in the training data and may overfit datasets that are particularly noisy.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在缺点方面，随机森林可能感觉像一个黑盒方法，因为我们对模型的操作非常有限，结果可能难以解释。虽然随机森林在分类方面表现良好，但对于回归问题可能不太适用，因为它无法给出精确的连续性预测。在回归情况下，它不会预测超出训练数据范围，并可能在特别嘈杂的数据集上过拟合。
- en: Extra trees
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 极端随机树（Extra trees）
- en: '*Extra trees*, otherwise known as *extremely randomized trees*, is a variant
    of a random forest; it builds multiple trees and splits nodes using random subsets
    of features similar to random forest. However, unlike random forest, where observations
    are drawn with replacement, the observations are drawn without replacement in
    extra trees. So there is no repetition of observations.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '*Extra trees*，又称*极端随机树*，是随机森林的一个变种；它构建多棵树，并使用特征的随机子集来分裂节点，类似于随机森林。然而，与随机森林不同的是，在
    extra trees 中，观测样本是不重复抽取的。因此，观测样本不会重复出现。'
- en: Additionally, random forest selects the best split to convert the parent into
    the two most homogeneous child nodes.^([6](ch04.xhtml#idm45174932048648)) However,
    extra trees selects a random split to divide the parent node into two random child
    nodes. In extra trees, randomness doesn’t come from bootstrapping the data; it
    comes from the random splits of all observations.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，随机森林选择最佳分裂点将父节点转换为两个最同质的子节点。^([6](ch04.xhtml#idm45174932048648)) 然而，extra
    trees 选择一个随机分裂来将父节点分割为两个随机子节点。在 extra trees 中，随机性不是来自于数据的自助抽样，而是来自于所有观测样本的随机分割。
- en: In real-world cases, performance is comparable to an ordinary random forest,
    sometimes a bit better. The advantages and disadvantages of extra trees are similar
    to those of random forest.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际案例中，性能与普通随机森林可比，有时稍好。额外树的优缺点与随机森林类似。
- en: Implementation in Python
  id: totrans-212
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Python 实现
- en: 'Extra trees regression and classification models can be constructed using the
    sklearn package of Python, as shown in the following code snippet. The hyperparameters
    of extra trees are similar to random forest, as shown in the previous section:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用 Python 的 sklearn 包构建 Extra trees 的回归和分类模型，如下面的代码片段所示。Extra trees 的超参数与随机森林相似，如前一节所示：
- en: '`Classification`'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '`分类`'
- en: '[PRE16]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '`Regression`'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '`回归`'
- en: '[PRE17]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Adaptive Boosting (AdaBoost)
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自适应增强（AdaBoost）
- en: '*Adaptive Boosting* or *AdaBoost* is a boosting technique in which the basic
    idea is to try predictors sequentially, and each subsequent model attempts to
    fix the errors of its predecessor. At each iteration, the AdaBoost algorithm changes
    the sample distribution by modifying the weights attached to each of the instances.
    It increases the weights of the wrongly predicted instances and decreases the
    ones of the correctly predicted instances.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '*自适应增强*或*AdaBoost*是一种提升技术，其基本思想是依次尝试预测器，每个后续模型试图修正其前任的错误。每次迭代中，AdaBoost 算法通过修改附加到每个实例的权重来改变样本分布。它增加错误预测实例的权重，减少正确预测实例的权重。'
- en: 'The steps of the AdaBoost algorithm are:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost 算法的步骤如下：
- en: Initially, all observations are given equal weights.
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最初，所有观测样本被赋予相等的权重。
- en: A model is built on a subset of data, and using this model, predictions are
    made on the whole dataset. Errors are calculated by comparing the predictions
    and actual values.
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型建立在数据子集上，使用该模型对整个数据集进行预测。通过比较预测值和实际值来计算错误。
- en: While creating the next model, higher weights are given to the data points that
    were predicted incorrectly. Weights can be determined using the error value. For
    instance, the higher the error, the more weight is assigned to the observation.
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在创建下一个模型时，对预测错误的数据点赋予更高的权重。可以使用错误值确定权重。例如，错误越大，分配给观察值的权重越大。
- en: This process is repeated until the error function does not change, or until
    the maximum limit of the number of estimators is reached.
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该过程重复进行，直到错误函数不再改变，或者达到最大估计器数量的限制。
- en: Implementation in Python
  id: totrans-225
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Python 实现
- en: 'AdaBoost regression and classification models can be constructed using the
    sklearn package of Python, as shown in the following code snippet:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用 Python 的 sklearn 包构建 AdaBoost 的回归和分类模型，如下面的代码片段所示：
- en: '`Classification`'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '`分类`'
- en: '[PRE18]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '`Regression`'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '`回归`'
- en: '[PRE19]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Hyperparameters
  id: totrans-231
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 超参数
- en: 'Some of the main hyperparameters that are present in the sklearn implementation
    of AdaBoost and that can be tweaked while performing the grid search are as follows:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: sklearn 实现的 AdaBoost 中的一些主要超参数，在执行网格搜索时可以进行调整，包括以下内容：
- en: Learning rate (`learning_rate` in sklearn)
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率 (`learning_rate` 在 sklearn 中)
- en: Learning rate shrinks the contribution of each classifier/regressor. It can
    be considered on a log scale. The sample values for grid search can be 0.001,
    0.01, and 0.1.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率缩小每个分类器/回归器的贡献。可以考虑在对数尺度上。网格搜索的样本值可以是 0.001、0.01 和 0.1。
- en: Number of estimators (`n_estimators` in sklearn)
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 估计器数量 (`n_estimators` 在 sklearn 中)
- en: This parameter represents the number of trees. Ideally, this should be increased
    until no further improvement is seen in the model. Good values might be a log
    scale from 10 to 1,000.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 此参数表示树的数量。理想情况下，应该增加到在模型中不再看到进一步改进的情况下。良好的值可能是从10到1,000的对数尺度。
- en: Advantages and disadvantages
  id: totrans-237
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优势和劣势
- en: In terms of advantages, AdaBoost has a high degree of precision. AdaBoost can
    achieve similar results to other models with much less tweaking of parameters
    or settings. The algorithm doesn’t need the data to be scaled and can model a
    nonlinear relationship.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在优势方面，AdaBoost具有较高的精度。AdaBoost可以在几乎不调整参数或设置的情况下达到与其他模型类似的结果。该算法不需要数据进行缩放，并且可以建模非线性关系。
- en: In terms of disadvantages, the training of AdaBoost is time consuming. AdaBoost
    can be sensitive to noisy data and outliers, and data imbalance leads to a decrease
    in classification accuracy
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在劣势方面，AdaBoost的训练时间较长。AdaBoost对噪声数据和异常值敏感，并且数据不平衡导致分类精度降低。
- en: Gradient boosting method
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 梯度提升方法
- en: '*Gradient boosting method* (GBM) is another boosting technique similar to AdaBoost,
    where the general idea is to try predictors sequentially. Gradient boosting works
    by sequentially adding the previous underfitted predictions to the ensemble, ensuring
    the errors made previously are corrected.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '*梯度提升方法*（GBM）是另一种类似于AdaBoost的提升技术，其一般思想是顺序地尝试预测器。梯度提升通过将前一步骤中未拟合的预测逐步添加到集成中来工作，确保先前的错误得到纠正。'
- en: 'The following are the steps of the gradient boosting algorithm:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升算法的步骤如下：
- en: A model (which can be referred to as the first weak learner) is built on a subset
    of data. Using this model, predictions are made on the whole dataset.
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在一个数据子集上构建模型（可以称为第一个弱学习器）。使用该模型，在整个数据集上进行预测。
- en: Errors are calculated by comparing the predictions and actual values, and the
    loss is calculated using the loss function.
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过比较预测值和实际值计算错误，并使用损失函数计算损失。
- en: A new model is created using the errors of the previous step as the target variable.
    The objective is to find the best split in the data to minimize the error. The
    predictions made by this new model are combined with the predictions of the previous.
    New errors are calculated using this predicted value and actual value.
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用前一步骤的错误作为目标变量创建一个新模型。其目标是找到数据中的最佳分割以最小化误差。该新模型的预测值与前一模型的预测值相结合。使用此预测值和实际值计算新的错误。
- en: This process is repeated until the error function does not change or until the
    maximum limit of the number of estimators is reached.
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 直到错误函数不再变化或达到最大估计器数的限制为止，重复此过程。
- en: Contrary to AdaBoost, which tweaks the instance weights at every interaction,
    this method tries to fit the new predictor to the residual errors made by the
    previous predictor.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 与AdaBoost相反，后者在每次交互中调整实例权重，该方法试图将新的预测器拟合到前一个预测器产生的残差错误中。
- en: Implementation in Python and hyperparameters
  id: totrans-248
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Python中的实现和超参数
- en: 'Gradient boosting method regression and classification models can be constructed
    using the sklearn package of Python, as shown in the following code snippet. The
    hyperparameters of gradient boosting method are similar to AdaBoost, as shown
    in the previous section:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Python的sklearn包可以构建梯度提升方法的回归和分类模型，如下面的代码片段所示。梯度提升方法的超参数与AdaBoost相似，如前一节所示：
- en: '`Classification`'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '`分类`'
- en: '[PRE20]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '`Regression`'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '`回归`'
- en: '[PRE21]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Advantages and disadvantages
  id: totrans-254
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优势和劣势
- en: In terms of advantages, gradient boosting method is robust to missing data,
    highly correlated features, and irrelevant features in the same way as random
    forest. It naturally assigns feature importance scores, with slightly better performance
    than random forest. The algorithm doesn’t need the data to be scaled and can model
    a nonlinear relationship.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在优势方面，梯度提升方法对于缺失数据、高度相关的特征和无关特征具有鲁棒性，与随机森林相似。它自然地分配特征重要性分数，稍微优于随机森林的性能。该算法不需要数据进行缩放，并且可以建模非线性关系。
- en: In terms of disadvantages, it may be more prone to overfitting than random forest,
    as the main purpose of the boosting approach is to reduce bias and not variance.
    It has many hyperparameters to tune, so model development may not be as fast.
    Also, feature importance may not be robust to variation in the training dataset.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在劣势方面，可能比随机森林更容易过拟合，因为提升方法的主要目的是减少偏差而不是方差。它有许多超参数需要调整，因此模型开发可能不那么迅速。此外，特征重要性可能对训练数据集的变化不太稳健。
- en: ANN-Based Models
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于ANN的模型
- en: In [Chapter 3](ch03.xhtml#Chapter3) we covered the basics of ANNs, along with
    the architecture of ANNs and their training and implementation in Python. The
    details provided in that chapter are applicable across all areas of machine learning,
    including supervised learning. However, there are a few additional details from
    the supervised learning perspective, which we will cover in this section.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3章](ch03.xhtml#Chapter3)中，我们讨论了ANN的基础知识，以及ANN的架构及其在Python中的训练和实现。该章提供的细节适用于机器学习的所有领域，包括监督学习。然而，从监督学习的角度来看，还有一些额外的细节，我们将在本节中介绍。
- en: Neural networks are reducible to a classification or regression model with the
    activation function of the node in the output layer. In the case of a regression
    problem, the output node has linear activation function (or no activation function).
    A linear function produces a continuous output ranging from `-inf` to `+inf`.
    Hence, the output layer will be the linear function of the nodes in the layer
    before the output layer, and it will be a regression-based model.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络可以通过输出层节点的激活函数减少到分类或回归模型。在回归问题中，输出节点具有线性激活函数（或无激活函数）。线性函数产生从`-inf`到`+inf`的连续输出。因此，输出层将是前一层节点的线性函数，并且它将是基于回归的模型。
- en: In the case of a classification problem, the output node has a sigmoid or softmax
    activation function. A sigmoid or softmax function produces an output ranging
    from zero to one to represent the probability of target value. Softmax function
    can also be used for multiple groups for classification.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类问题中，输出节点具有sigmoid或softmax激活函数。sigmoid或softmax函数产生一个从零到一的输出，表示目标值的概率。softmax函数还可以用于多组分类。
- en: ANN using sklearn
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用sklearn的ANN
- en: 'ANN regression and classification models can be constructed using the sklearn
    package of Python, as shown in the following code snippet:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用Python的sklearn包构建ANN回归和分类模型，如下面的代码片段所示：
- en: '`Classification`'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '`分类`'
- en: '[PRE22]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '`Regression`'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '`回归`'
- en: '[PRE23]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Hyperparameters
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 超参数
- en: 'As we saw in [Chapter 3](ch03.xhtml#Chapter3), ANN has many hyperparameters.
    Some of the hyperparameters that are present in the sklearn implementation of
    ANN and can be tweaked while performing the grid search are:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第3章](ch03.xhtml#Chapter3)中看到的，ANN具有许多超参数。在sklearn的ANN实现中存在的一些超参数，在执行网格搜索时可以调整：
- en: Hidden Layers (`hidden_layer_sizes` in sklearn)
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层（sklearn中的`hidden_layer_sizes`）
- en: It represents the number of layers and nodes in the ANN architecture. In sklearn
    implementation of ANN, the ith element represents the number of neurons in the
    ith hidden layer. A sample value for grid search in the sklearn implementation
    can be [(*20*,), (*50*,), (*20*, *20*), (*20*, *30*, *20*)].
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 它代表ANN架构中的层数和节点数。在sklearn的ANN实现中，第i个元素表示第i个隐藏层中的神经元数。在sklearn实现的网格搜索中，用于示例值的样本值可以是[(*20*,),
    (*50*,), (*20*, *20*), (*20*, *30*, *20*)]。
- en: Activation Function (`activation` in sklearn)
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数（sklearn中的`activation`）
- en: It represents the activation function of a hidden layer. Some of the activation
    functions defined in [Chapter 3](ch03.xhtml#Chapter3), such as `sigmoid`, `relu`,
    or `tanh`, can be used.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 它代表隐藏层的激活函数。一些在[第3章](ch03.xhtml#Chapter3)中定义的激活函数，如`sigmoid`、`relu`或`tanh`，可以使用。
- en: Deep neural network
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度神经网络
- en: ANNs with more than a single hidden layer are often called deep networks. We
    prefer using the library Keras to implement such networks, given the flexibility
    of the library. The detailed implementation of a deep neural network in Keras
    was shown in [Chapter 3](ch03.xhtml#Chapter3). Similar to `MLPClassifier` and
    `MLPRegressor` in sklearn for classification and regression, Keras has modules
    called `KerasClassifier` and `KerasRegressor` that can be used for creating classification
    and regression models with deep network.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 具有多个隐藏层的人工神经网络通常被称为深度网络。我们喜欢使用库Keras来实现这样的网络，因为这个库非常灵活。详细介绍了在[Keras中实现深度神经网络](ch03.xhtml#Chapter3)的具体实现。类似于`MLPClassifier`和`MLPRegressor`在sklearn中用于分类和回归，Keras还有名为`KerasClassifier`和`KerasRegressor`的模块，可用于创建具有深度网络的分类和回归模型。
- en: A popular problem in finance is time series prediction, which is predicting
    the next value of a time series based on a historical overview. Some of the deep
    neural networks, such as recurrent neural network (RNN), can be directly used
    for time series prediction. The details of this approach are provided in [Chapter 5](ch05.xhtml#Chapter5).
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 金融领域中一个流行的问题是时间序列预测，即基于历史概述预测时间序列的下一个值。一些深度神经网络，如循环神经网络（RNN），可以直接用于时间序列预测。该方法的详细信息在[第五章](ch05.xhtml#Chapter5)中提供。
- en: Advantages and disadvantages
  id: totrans-276
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优缺点
- en: The main advantage of an ANN is that it captures the nonlinear relationship
    between the variables quite well. ANN can more easily learn rich representations
    and is good with a large number of input features with a large dataset. ANN is
    flexible in how it can be used. This is evident from its use across a wide variety
    of areas in machine learning and AI, including reinforcement learning and NLP,
    as discussed in [Chapter 3](ch03.xhtml#Chapter3).
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络（ANN）的主要优势在于它相当好地捕捉了变量之间的非线性关系。ANN可以更轻松地学习丰富的表示，并且在大数据集和大量输入特征的情况下表现良好。ANN在使用方式上非常灵活。这一点可以从其在机器学习和人工智能中广泛应用的各种领域中看出，包括强化学习和自然语言处理，正如[第三章](ch03.xhtml#Chapter3)所讨论的那样。
- en: The main disadvantage of ANN is the interpretability of the model, which is
    a drawback that often cannot be ignored and is sometimes the determining factor
    when choosing a model. ANN is not good with small datasets and requires a lot
    of tweaking and guesswork. Choosing the right topology/algorithms to solve a problem
    is difficult. Also, ANN is computationally expensive and can take a lot of time
    to train.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: ANN的主要缺点是模型的可解释性，这是一个常常不能忽视的缺点，有时在选择模型时是决定性因素。ANN在处理小数据集方面表现不佳，需要大量的调整和猜测。选择正确的拓扑结构/算法来解决问题是困难的。此外，ANN在计算上很昂贵，训练所需时间较长。
- en: Using ANNs for supervised learning in finance
  id: totrans-279
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在金融监督学习中使用人工神经网络
- en: If a simple model such as linear or logistic regression perfectly fits your
    problem, don’t bother with ANN. However, if you are modeling a complex dataset
    and feel a need for better prediction power, give ANN a try. ANN is one of the
    most flexible models in adapting itself to the shape of the data, and using it
    for supervised learning problems can be an interesting and valuable exercise.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个简单的模型（如线性或逻辑回归）完全适合您的问题，那么不要考虑使用ANN。然而，如果您正在建模复杂的数据集并感觉需要更好的预测能力，那么试试ANN吧。ANN是最灵活的模型之一，能够自适应数据的形状，在监督学习问题中使用它可能是一个有趣且有价值的练习。
- en: Model Performance
  id: totrans-281
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型性能
- en: In the previous section, we discussed grid search as a way to find the right
    hyperparameter to achieve better performance. In this section, we will expand
    on that process by discussing the key components of evaluating the model performance,
    which are overfitting, cross validation, and evaluation metrics.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们讨论了网格搜索作为寻找正确超参数以获得更好性能的方法。在本节中，我们将扩展该过程，讨论评估模型性能的关键组成部分，即过拟合、交叉验证和评估指标。
- en: Overfitting and Underfitting
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 过拟合和欠拟合
- en: A common problem in machine learning is *overfitting*, which is defined by learning
    a function that perfectly explains the training data that the model learned from
    but doesn’t generalize well to unseen test data. Overfitting happens when a model
    overlearns from the training data to the point that it starts picking up idiosyncrasies
    that aren’t representative of patterns in the real world. This becomes especially
    problematic as we make our models increasingly more complex. *Underfitting* is
    a related issue in which the model is not complex enough to capture the underlying
    trend in the data. [Figure 4-5](#OverfittingUnderfitting) illustrates overfitting
    and underfitting. The left-hand panel of [Figure 4-5](#OverfittingUnderfitting)
    shows a linear regression model; a straight line clearly underfits the true function.
    The middle panel shows that a high degree polynomial approximates the true relationship
    reasonably well. On the other hand, a polynomial of a very high degree fits the
    small sample almost perfectly, and performs best on the training data, but this
    doesn’t generalize, and it would do a horrible job at explaining a new data point.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中常见的问题是*过拟合*，它被定义为学习一个完美解释模型从中学到的训练数据的函数，但对未见过的测试数据泛化效果不佳。过拟合发生在模型从训练数据中过度学习，以至于开始捕捉到不代表真实世界模式的特质。随着我们的模型变得越来越复杂，这种问题变得尤为严重。*欠拟合*是一个相关问题，即模型不够复杂，无法捕捉数据中的潜在趋势。[Figure 4-5](#OverfittingUnderfitting)说明了过拟合和欠拟合。[Figure 4-5](#OverfittingUnderfitting)的左侧面板显示了一个线性回归模型；一条直线明显地对真实函数拟合不足。中间面板显示了高次多项式相对合理地近似了真实关系。另一方面，高次多项式几乎完美地适应了小样本，并且在训练数据上表现最好，但这种情况并不具有泛化性，并且在解释新数据点时效果非常糟糕。
- en: The concepts of overfitting and underfitting are closely linked to *bias-variance
    trade-off*. *Bias* refers to the error due to overly simplistic assumptions or
    faulty assumptions in the learning algorithm. Bias results in underfitting of
    the data, as shown in the left-hand panel of [Figure 4-5](#OverfittingUnderfitting).
    A high bias means our learning algorithm is missing important trends among the
    features. *Variance* refers to the error due to an overly complex model that tries
    to fit the training data as closely as possible. In high variance cases, the model’s
    predicted values are extremely close to the actual values from the training set.
    High variance gives rise to overfitting, as shown in the right-hand panel of [Figure 4-5](#OverfittingUnderfitting).
    Ultimately, in order to have a good model, we need low bias and low variance.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合和欠拟合的概念与*偏差-方差权衡*密切相关。*偏差*是由于学习算法中过于简化或错误的假设而导致的错误。偏差导致数据的拟合不足，如[Figure 4-5](#OverfittingUnderfitting)的左侧面板所示。高偏差意味着我们的学习算法忽略了特征之间的重要趋势。*方差*是由于一个过于复杂的模型试图尽可能紧密地拟合训练数据而导致的错误。在高方差的情况下，模型的预测值与训练集中的实际值非常接近。高方差导致过拟合，如[Figure 4-5](#OverfittingUnderfitting)的右侧面板所示。最终，为了得到一个好的模型，我们需要低偏差和低方差。
- en: '![mlbf 0405](Images/mlbf_0405.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![mlbf 0405](Images/mlbf_0405.png)'
- en: Figure 4-5\. Overfitting and underfitting
  id: totrans-287
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-5\. 过拟合和欠拟合
- en: 'There can be two ways to combat overfitting:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 可以有两种方法来对抗过拟合：
- en: Using more training data
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 使用更多的训练数据
- en: The more training data we have, the harder it is to overfit the data by learning
    too much from any single training example.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们拥有的训练数据越多，就越难通过从任一训练示例中学习过多来过拟合数据。
- en: Using regularization
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 使用正则化
- en: Adding a penalty in the loss function for building a model that assigns too
    much explanatory power to any one feature, or allows too many features to be taken
    into account.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在损失函数中为建立模型增加一项惩罚，以使模型不会赋予任何一个特征过多的解释力量，或者允许考虑太多的特征。
- en: The concept of overfitting and the ways to combat it are applicable across all
    the supervised learning models. For example, regularized regressions address overfitting
    in linear regression, as discussed earlier in this chapter.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合的概念及其对策适用于所有监督学习模型。例如，正则化回归可以解决线性回归中的过拟合问题，正如本章前面讨论的那样。
- en: Cross Validation
  id: totrans-294
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交叉验证
- en: 'One of the challenges of machine learning is training models that are able
    to generalize well to unseen data (overfitting versus underfitting or a bias-variance
    trade-off). The main idea behind *cross validation* is to split the data one time
    or several times so that each split is used once as a validation set and the remainder
    is used as a training set: part of the data (the training sample) is used to train
    the algorithm, and the remaining part (the validation sample) is used for estimating
    the risk of the algorithm. Cross validation allows us to obtain reliable estimates
    of the model’s generalization error. It is easiest to understand it with an example.
    When doing *k*-fold cross validation, we randomly split the training data into
    *k* folds. Then we train the model using *k-1* folds and evaluate the performance
    on the *k*th fold. We repeat this process *k* times and average the resulting
    scores.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习面临的挑战之一是训练能够很好地泛化到未见数据的模型（过拟合与欠拟合或偏差-方差权衡）。*交叉验证*的主要思想是将数据一次或多次分割，以便每次分割都将一个部分作为验证集，其余部分作为训练集：数据的一部分（训练样本）用于训练算法，剩余部分（验证样本）用于估计算法的风险。交叉验证允许我们获得模型泛化误差的可靠估计。通过一个例子最容易理解它。在进行*k*折交叉验证时，我们将训练数据随机分成*k*折。然后我们使用*k-1*折训练模型，并在第*k*折上评估性能。我们重复这个过程*k*次，并平均得分。
- en: '[Figure 4-6](#CrossValidation) shows an example of cross validation, where
    the data is split into five sets and in each round one of the sets is used for
    validation.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '[Figure 4-6](#CrossValidation) 显示了一个交叉验证的示例，其中数据被分成五组，在每一轮中，其中一个组被用作验证集。'
- en: '![mlbf 0406](Images/mlbf_0406.png)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![mlbf 0406](Images/mlbf_0406.png)'
- en: Figure 4-6\. Cross validation
  id: totrans-298
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-6\. 交叉验证
- en: A potential drawback of cross validation is the computational cost, especially
    when paired with a grid search for hyperparameter tuning. Cross validation can
    be performed in a couple of lines using the sklearn package; we will perform cross
    validation in the supervised learning case studies.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证的一个潜在缺点是计算成本，特别是与超参数调整的网格搜索结合时。使用sklearn包可以在几行代码中执行交叉验证；我们将在监督学习案例研究中执行交叉验证。
- en: In the next section, we cover the evaluation metrics for the supervised learning
    models that are used to measure and compare the models’ performance.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍用于测量和比较模型性能的监督学习模型的评估指标。
- en: Evaluation Metrics
  id: totrans-301
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估指标
- en: The metrics used to evaluate the machine learning algorithms are very important.
    The choice of metrics to use influences how the performance of machine learning
    algorithms is measured and compared. The metrics influence both how you weight
    the importance of different characteristics in the results and your ultimate choice
    of algorithm.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 评估机器学习算法使用的指标非常重要。选择使用的指标影响着如何衡量和比较机器学习算法的性能。这些指标影响您如何权衡结果中不同特征的重要性，以及最终选择的算法。
- en: The main evaluation metrics for regression and classification are illustrated
    in [Figure 4-7](#EvaluationMetrics).
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 回归和分类的主要评估指标在 [Figure 4-7](#EvaluationMetrics) 中有所说明。
- en: '![mlbf 0407](Images/mlbf_0407.png)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![mlbf 0407](Images/mlbf_0407.png)'
- en: Figure 4-7\. Evaluation metrics for regression and classification
  id: totrans-305
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-7\. 回归和分类的评估指标
- en: Let us first look at the evaluation metrics for supervised regression.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先看一下监督回归的评估指标。
- en: Mean absolute error
  id: totrans-307
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 平均绝对误差
- en: The *mean absolute error* (MAE) is the sum of the absolute differences between
    predictions and actual values. The MAE is a linear score, which means that all
    the individual differences are weighted equally in the average. It gives an idea
    of how wrong the predictions were. The measure gives an idea of the magnitude
    of the error, but no idea of the direction (e.g., over- or underpredicting).
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '*平均绝对误差*（MAE）是预测值与实际值之间绝对差值的总和。MAE是线性评分，这意味着平均值中所有个体差异的权重相等。它提供了预测有多大错误的想法。该指标给出了误差的大小，但不指示方向（例如，过高或过低预测）。'
- en: Mean squared error
  id: totrans-309
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 均方误差
- en: The *mean squared error* (MSE) represents the sample standard deviation of the
    differences between predicted values and observed values (called residuals). This
    is much like the mean absolute error in that it provides a gross idea of the magnitude
    of the error. Taking the square root of the mean squared error converts the units
    back to the original units of the output variable and can be meaningful for description
    and presentation. This is called the *root mean squared error* (RMSE).
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '*均方误差*（MSE）表示预测值与观测值（称为残差）之间差异的样本标准偏差。这与平均绝对误差类似，提供了误差大小的大致概念。将均方误差的平方根取出，可以将单位转换回输出变量的原始单位，并且对描述和展示有意义。这被称为*均方根误差*（RMSE）。'
- en: R² metric
  id: totrans-311
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: R²指标
- en: The *R² metric* provides an indication of the “goodness of fit” of the predictions
    to actual value. In statistical literature this measure is called the coefficient
    of determination. This is a value between zero and one, for no-fit and perfect
    fit, respectively.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '*R²指标*提供了预测与实际值“拟合度”的指示。在统计文献中，这个度量被称为决定系数。它的值介于零和一之间，分别表示无拟合和完美拟合。'
- en: Adjusted R² metric
  id: totrans-313
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 调整后的R²指标
- en: 'Just like *R²*, *adjusted R²* also shows how well terms fit a curve or line
    but adjusts for the number of terms in a model. It is given in the following formula:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 像*R²*一样，*调整后的R²*也显示了项在曲线或直线上拟合的程度，但会根据模型中的项数进行调整。其表达式如下：
- en: <math display="block"><mrow><msubsup><mi>R</mi> <mrow><mi>a</mi><mi>d</mi><mi>j</mi></mrow>
    <mn>2</mn></msubsup> <mo>=</mo> <mn>1</mn> <mo>–</mo> <mfenced separators="" open="["
    close="]"><mfrac><mrow><mrow><mo>(</mo><mn>1</mn><mo>–</mo><msup><mi>R</mi> <mn>2</mn></msup>
    <mo>)</mo></mrow><mrow><mo>(</mo><mi>n</mi><mo>–</mo><mn>1</mn><mo>)</mo></mrow><mrow><mo>)</mo></mrow></mrow>
    <mrow><mi>n</mi><mo>–</mo><mi>k</mi><mo>–</mo><mn>1</mn></mrow></mfrac></mfenced></mrow></math>
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msubsup><mi>R</mi> <mrow><mi>a</mi><mi>d</mi><mi>j</mi></mrow>
    <mn>2</mn></msubsup> <mo>=</mo> <mn>1</mn> <mo>–</mo> <mfenced separators="" open="["
    close="]"><mfrac><mrow><mrow><mo>(</mo><mn>1</mn><mo>–</mo><msup><mi>R</mi> <mn>2</mn></msup>
    <mo>)</mo></mrow><mrow><mo>(</mo><mi>n</mi><mo>–</mo><mn>1</mn><mo>)</mo></mrow><mrow><mo>)</mo></mrow></mrow>
    <mrow><mi>n</mi><mo>–</mo><mi>k</mi><mo>–</mo><mn>1</mn></mrow></mfrac></mfenced></mrow></math>
- en: where *n* is the total number of observations and *k* is the number of predictors.
    Adjusted *R²* will always be less than or equal to *R²*.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*n*为总观测数，*k*为预测变量数。调整后的*R²*永远小于或等于*R²*。
- en: Selecting an evaluation metric for supervised regression
  id: totrans-317
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择监督回归的评估指标
- en: In terms of a preference among these evaluation metrics, if the main goal is
    predictive accuracy, then RMSE is best. It is computationally simple and is easily
    differentiable. The loss is symmetric, but larger errors weigh more in the calculation.
    The MAEs are symmetric but do not weigh larger errors more. *R²* and adjusted
    *R²* are often used for explanatory purposes by indicating how well the selected
    independent variable(s) explains the variability in the dependent variable(s).
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些评估指标中，如果主要目标是预测准确性，则RMSE是最好的选择。它计算简单，易于区分。损失对称，但较大误差在计算中权重更大。MAE对称，但不会给较大误差加权。*R²*和调整后的*R²*通常用于解释目的，指示选择的独立变量如何解释因变量的变异性。
- en: Let us first look at the evaluation metrics for supervised classification.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先看看监督分类的评估指标。
- en: Classification
  id: totrans-320
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分类
- en: 'For simplicity, we will mostly discuss things in terms of a binary classification
    problem (i.e., only two outcomes, such as true or false); some common terms are:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 为简便起见，我们将大多数讨论限于二元分类问题（例如，仅有两个结果，如真或假）；一些常见术语包括：
- en: True positives (TP)
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 真阳性（TP）
- en: Predicted positive and are actually positive.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 预测为正实际为正。
- en: False positives (FP)
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 假阳性（FP）
- en: Predicted positive and are actually negative.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 预测为正实际为负。
- en: True negatives (TN)
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 真阴性（TN）
- en: Predicted negative and are actually negative.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 预测为负实际为负。
- en: False negatives (FN)
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 假阴性（FN）
- en: Predicted negative and are actually positive.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 预测为负实际为正。
- en: The difference between three commonly used evaluation metrics for classification,
    accuracy, precision, and recall, is illustrated in [Figure 4-8](#AccuracyPrecisionRecall).
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 展示了分类常用的三个评估指标——准确率、精确率和召回率——之间的区别，如[图 4-8](#AccuracyPrecisionRecall)所示。
- en: '![mlbf 0408](Images/mlbf_0408.png)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
  zh: '![mlbf 0408](Images/mlbf_0408.png)'
- en: Figure 4-8\. Accuracy, precision, and recall
  id: totrans-332
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-8\. 准确率、精确率和召回率
- en: Accuracy
  id: totrans-333
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准确率
- en: As shown in [Figure 4-8](#AccuracyPrecisionRecall), *accuracy* is the number
    of correct predictions made as a ratio of all predictions made. This is the most
    common evaluation metric for classification problems and is also the most misused.
    It is most suitable when there are an equal number of observations in each class
    (which is rarely the case) and when all predictions and the related prediction
    errors are equally important, which is often not the case.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图 4-8](#AccuracyPrecisionRecall)所示，*准确率*是所有预测中正确预测的比例。这是分类问题最常见的评估指标，但也是最常被误用的。它在每个类中观察数相等时最合适（这种情况很少出现），以及所有预测和相关预测误差同等重要时最合适（这种情况通常不成立）。
- en: Precision
  id: totrans-335
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 精确率
- en: '*Precision* is the percentage of positive instances out of the total predicted
    positive instances. Here, the denominator is the model prediction done as positive
    from the whole given dataset. Precision is a good measure to determine when the
    cost of false positives is high (e.g., email spam detection).'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '*精确率* 是总预测正实例中的正实例百分比。在这里，分母是整个给定数据集中模型预测为正的部分。当假阳性的成本较高时（例如电子邮件垃圾邮件检测），精确率是一个很好的度量标准。'
- en: Recall
  id: totrans-337
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 召回率
- en: '*Recall* (or *sensitivity* or *true positive rate*) is the percentage of positive
    instances out of the total actual positive instances. Therefore, the denominator
    (true positive + false negative) is the actual number of positive instances present
    in the dataset. Recall is a good measure when there is a high cost associated
    with false negatives (e.g., fraud detection).'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '*召回率*（或*敏感度*或*真正率*）是总实际正实例中的正实例百分比。因此，分母（真正阳性+假阴性）是数据集中实际存在的正实例数量。当虚假阴性成本高昂时（例如欺诈检测），召回率是一个很好的度量标准。'
- en: In addition to accuracy, precision, and recall, some of the other commonly used
    evaluation metrics for classification are discussed in the following sections.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 除了准确率、精确率和召回率之外，还讨论了分类的其他常用评估指标。
- en: Area under ROC curve
  id: totrans-340
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ROC 曲线下的面积
- en: '*Area under ROC curve* (AUC) is an evaluation metric for binary classification
    problems. ROC is a probability curve, and AUC represents degree or measure of
    separability. It tells how much the model is capable of distinguishing between
    classes. The higher the AUC, the better the model is at predicting zeros as zeros
    and ones as ones. An AUC of *0.5* means that the model has no class separation
    capacity whatsoever. The probabilistic interpretation of the AUC score is that
    if you randomly choose a positive case and a negative case, the probability that
    the positive case outranks the negative case according to the classifier is given
    by the AUC.'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '*ROC 曲线下的面积*（AUC）是用于二元分类问题的评估指标。ROC 是一个概率曲线，AUC 表示可分离性的程度或度量。它告诉我们模型区分类别的能力有多强。AUC
    越高，模型在将零预测为零和一预测为一方面的能力越好。AUC 为*0.5*意味着模型根本没有类别分离能力。AUC 分数的概率解释是，如果您随机选择一个正案例和一个负案例，那么正案例根据分类器的排序超过负案例的概率由AUC
    给出。'
- en: Confusion matrix
  id: totrans-342
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 混淆矩阵
- en: A confusion matrix lays out the performance of a learning algorithm. The confusion
    matrix is simply a square matrix that reports the counts of the true positive
    (TP), true negative (TN), false positive (FP), and false negative (FN) predictions
    of a classifier, as shown in [Figure 4-9](#ConfusionMetrix).
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵展示了学习算法的性能。混淆矩阵简单地是报告分类器预测的真阳性（TP）、真阴性（TN）、假阳性（FP）和假阴性（FN）的计数的方阵，如[图4-9](#ConfusionMetrix)所示。
- en: '![mlbf 0409](Images/mlbf_0409.png)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
  zh: '![mlbf 0409](Images/mlbf_0409.png)'
- en: Figure 4-9\. Confusion matrix
  id: totrans-345
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-9\. 混淆矩阵
- en: The confusion matrix is a handy presentation of the accuracy of a model with
    two or more classes. The table presents predictions on the *x-axis* and accuracy
    outcomes on the *y-axis*. The cells of the table are the number of predictions
    made by the model. For example, a model can predict zero or one, and each prediction
    may actually have been a zero or a one. Predictions for zero that were actually
    zero appear in the cell for prediction = 0 and actual = 0, whereas predictions
    for zero that were actually one appear in the cell for prediction = 0 and actual
    = 1.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵是对具有两个或多个类别的模型准确性的便利展示。表格展示了*x轴*上的预测和*y轴*上的准确结果。表格的单元格是模型做出的预测数量。例如，模型可以预测零或一，每个预测实际上可能是零或一。实际为零的预测出现在预测=0且实际=0的单元格中，而实际为一的预测出现在预测=0且实际=1的单元格中。
- en: Selecting an evaluation metric for supervised classification
  id: totrans-347
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为监督分类选择评估指标
- en: The evaluation metric for classification depends heavily on the task at hand.
    For example, recall is a good measure when there is a high cost associated with
    false negatives such as fraud detection. We will further examine these evaluation
    metrics in the case studies.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 分类的评估指标严重依赖于手头的任务。例如，当虚假阴性（如欺诈检测）带来高昂成本时，召回率是一个很好的度量标准。我们将在案例研究中进一步研究这些评估指标。
- en: Model Selection
  id: totrans-349
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型选择
- en: Selecting the perfect machine learning model is both an art and a science. Looking
    at machine learning models, there is no one solution or approach that fits all.
    There are several factors that can affect your choice of a machine learning model.
    The main criteria in most of the cases is the model performance that we discussed
    in the previous section. However, there are many other factors to consider while
    performing model selection. In the following section, we will go over all such
    factors, followed by a discussion of model trade-offs.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 选择完美的机器学习模型既是艺术也是科学。观察机器学习模型时，并没有适合所有情况的单一解决方案或方法。有几个因素可能会影响您选择机器学习模型的选择。大多数情况下的主要标准是我们在前一节中讨论的模型性能。然而，在进行模型选择时，还有许多其他因素需要考虑。在接下来的部分中，我们将详细介绍所有这些因素，并讨论模型的权衡。
- en: Factors for Model Selection
  id: totrans-351
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型选择的因素
- en: 'The factors considered for the model selection process are as follows:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 模型选择过程中考虑的因素如下：
- en: Simplicity
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 简单性
- en: The degree of simplicity of the model. Simplicity usually results in quicker,
    more scalable, and easier to understand models and results.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的简单程度。简单性通常导致模型和结果更快、更可扩展和更易理解。
- en: Training time
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 训练时间
- en: Speed, performance, memory usage and overall time taken for model training.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练中的速度、性能、内存使用情况和总体时间。
- en: Handle nonlinearity in the data
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 处理数据中的非线性关系
- en: The ability of the model to handle the nonlinear relationship between the variables.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 模型处理变量之间的非线性关系的能力。
- en: Robustness to overfitting
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 对过拟合的鲁棒性
- en: The ability of the model to handle overfitting.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 模型处理过拟合的能力。
- en: Size of the dataset
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的大小
- en: The ability of the model to handle large number of training examples in the
    dataset.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 模型处理数据集中大量训练样本的能力。
- en: Number of features
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 特征数量
- en: The ability of the model to handle high dimensionality of the feature space.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 模型处理特征空间高维度的能力。
- en: Model interpretation
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 模型解释性
- en: How explainable is the model? Model interpretability is important because it
    allows us to take concrete actions to solve the underlying problem.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的解释能力如何？模型的可解释性很重要，因为它允许我们采取具体措施解决潜在问题。
- en: Feature scaling
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 特征缩放
- en: Does the model require variables to be scaled or normally distributed?
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 模型是否要求变量进行缩放或者服从正态分布？
- en: '[Figure 4-10](#ModelSelection) compares the supervised learning models on the
    factors mentioned previously and outlines a general rule-of-thumb to narrow down
    the search for the best machine learning algorithm^([7](ch04.xhtml#idm45174931487736))
    for a given problem. The table is based on the advantages and disadvantages of
    different models discussed in the individual model section in this chapter.'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-10](#ModelSelection) 对之前提到的因素比较了监督学习模型，并概述了在给定问题下缩小最佳机器学习算法搜索范围的经验法则^([7](ch04.xhtml#idm45174931487736))。该表基于本章节中讨论的不同模型的优缺点。'
- en: '![mlbf 0410](Images/mlbf_0410.png)'
  id: totrans-370
  prefs: []
  type: TYPE_IMG
  zh: '![mlbf 0410](Images/mlbf_0410.png)'
- en: Figure 4-10\. Model selection
  id: totrans-371
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-10\. 模型选择
- en: We can see from the table that relatively simple models include linear and logistic
    regression and as we move towards the ensemble and ANN, the complexity increases.
    In terms of the training time, the linear models and CART are relatively faster
    to train as compared to ensemble methods and ANN.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 从表中可以看出，相对简单的模型包括线性回归和逻辑回归，而随着向集成和人工神经网络（ANN）方向发展，复杂性增加。在训练时间方面，与集成方法和ANN相比，线性模型和CART的训练速度相对较快。
- en: Linear and logistic regression can’t handle nonlinear relationships, while all
    other models can. SVM can handle the nonlinear relationship between dependent
    and independent variables with nonlinear kernels.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 线性和逻辑回归不能处理非线性关系，而其他所有模型都可以。支持向量机（SVM）可以通过非线性核处理因变量和自变量之间的非线性关系。
- en: SVM and random forest tend to overfit less as compared to the linear regression,
    logistic regression, gradient boosting, and ANN. The degree of overfitting also
    depends on other parameters, such as size of the data and model tuning, and can
    be checked by looking at the results of the test set for each model. Also, the
    boosting methods such as gradient boosting have higher overfitting risk compared
    to the bagging methods, such as random forest. Recall the focus of gradient boosting
    is to minimize the bias and not variance.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机（SVM）和随机森林倾向于比线性回归、逻辑回归、梯度提升和ANN过拟合较少。过拟合程度还取决于其他参数，如数据集大小和模型调整，并且可以通过查看每个模型的测试集结果来检查。此外，与随机森林等装袋方法相比，梯度提升等增强方法的过拟合风险更高。需要注意的是，梯度提升的重点是最小化偏差而不是方差。
- en: Linear and logistic regressions are not able to handle large datasets and large
    number of features well. However, CART, ensemble methods, and ANN are capable
    of handling large datasets and many features quite well. The linear and logistic
    regression generally perform better than other models in case the size of the
    dataset is small. Application of variable reduction techniques (shown in [Chapter 7](ch07.xhtml#Chapter7))
    enables the linear models to handle large datasets. The performance of ANN increases
    with an increase in the size of the dataset.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归和逻辑回归在处理大数据集和大量特征时效果不佳。然而，CART、集成方法和人工神经网络（ANN）能够很好地处理大数据集和许多特征。在数据集较小的情况下，线性回归和逻辑回归通常表现优于其他模型。应用变量减少技术（如[第7章](ch07.xhtml#Chapter7)所示）可以使线性模型处理大数据集。随着数据集大小的增加，ANN的性能也会提高。
- en: Given linear regression, logistic regression, and CART are relatively simpler
    models, they have better model interpretation as compared to the ensemble models
    and ANN.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 相比于集成模型和人工神经网络（ANN），线性回归、逻辑回归和CART等相对简单的模型具有更好的模型解释性能。
- en: Model Trade-off
  id: totrans-377
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型权衡
- en: Often, it’s a trade-off between different factors when selecting a model. ANN,
    SVM, and some ensemble methods can be used to create very accurate predictive
    models, but they may lack simplicity and interpretability and may take a significant
    amount of resources to train.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择模型时，通常需要在不同因素之间进行权衡。人工神经网络（ANN）、支持向量机（SVM）和某些集成方法可以用来创建非常精确的预测模型，但它们可能缺乏简单性和可解释性，并且可能需要大量资源进行训练。
- en: In terms of selecting the final model, models with lower interpretability may
    be preferred when predictive performance is the most important goal, and it’s
    not necessary to explain how the model works and makes predictions. In some cases,
    however, model interpretability is mandatory.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 选择最终模型时，当预测性能是最重要的目标时，通常会更倾向于可解释性较低的模型，而不需要解释模型的工作原理和预测过程。然而，在某些情况下，模型的解释性是必须的。
- en: Interpretability-driven examples are often seen in the financial industry. In
    many cases, choosing a machine learning algorithm has less to do with the optimization
    or the technical aspects of the algorithm and more to do with business decisions.
    Suppose a machine learning algorithm is used to accept or reject an individual’s
    credit card application. If the applicant is rejected and decides to file a complaint
    or take legal action, the financial institution will need to explain how that
    decision was made. While that can be nearly impossible for ANN, it’s relatively
    straightforward for decision tree–based models.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 在金融行业经常看到以可解释性为驱动的示例。在许多情况下，选择机器学习算法与算法的优化或技术方面关系较少，更多地与业务决策有关。假设一个机器学习算法用于接受或拒绝个人的信用卡申请。如果申请人被拒绝并决定提出投诉或采取法律行动，金融机构将需要解释如何做出这个决定。对于人工神经网络（ANN）而言，这几乎是不可能的，但对于基于决策树的模型而言相对简单。
- en: Different classes of models are good at modeling different types of underlying
    patterns in data. So a good first step is to quickly test out a few different
    classes of models to know which ones capture the underlying structure of the dataset
    most efficiently. We will follow this approach while performing model selection
    in all our supervised learning–based case studies.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 不同类别的模型擅长建模不同类型的数据潜在模式。因此，一个很好的第一步是快速测试几种不同类别的模型，以了解哪些模型最有效地捕捉数据集的潜在结构。在所有基于监督学习的案例研究中，我们将遵循这种方法进行模型选择。
- en: Chapter Summary
  id: totrans-382
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 章节总结
- en: In this chapter, we discussed the importance of supervised learning models in
    finance, followed by a brief introduction to several supervised learning models,
    including linear and logistic regression, SVM, decision trees, ensemble, KNN,
    LDA, and ANN. We demonstrated training and tuning of these models in a few lines
    of code using sklearn and Keras libraries.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了监督学习模型在金融中的重要性，随后简要介绍了几种监督学习模型，包括线性回归、逻辑回归、SVM、决策树、集成学习、KNN、LDA和ANN。我们展示了如何使用sklearn和Keras库的几行代码进行这些模型的训练和调优。
- en: We discussed the most common error metrics for regression and classification
    models, explained the bias-variance trade-off, and illustrated the various tools
    for managing the model selection process using cross validation.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了回归和分类模型中最常见的错误度量标准，解释了偏差-方差的权衡，并且用交叉验证说明了管理模型选择过程的各种工具。
- en: We introduced the strengths and weaknesses of each model and discussed the factors
    to consider when selecting the best model. We also discussed the trade-off between
    model performance and interpretability.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了每个模型的优缺点，并讨论了在选择最佳模型时需要考虑的因素。我们还讨论了模型性能和可解释性之间的权衡。
- en: In the following chapter, we will dive into the case studies for regression
    and classification. All case studies in the next two chapters leverage the concepts
    presented in this chapter and in the previous two chapters.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将深入研究回归和分类的案例研究。在接下来的两章中的所有案例研究都利用了本章和前两章提出的概念。
- en: ^([1](ch04.xhtml#idm45174932998104-marker)) Cross validation will be covered
    in detail later in this chapter.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch04.xhtml#idm45174932998104-marker)) 交叉验证将在本章后面详细介绍。
- en: ^([2](ch04.xhtml#idm45174932709384-marker)) See the activation function section
    of [Chapter 3](ch03.xhtml#Chapter3) for details on the `sigmoid` function.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch04.xhtml#idm45174932709384-marker)) 有关`sigmoid`函数的详细信息，请参阅第3章的激活函数部分。
- en: ^([3](ch04.xhtml#idm45174932651752-marker)) [MLE](https://oreil.ly/y9atF) is
    a method of estimating the parameters of a probability distribution so that under
    the assumed statistical model the observed data is most probable.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch04.xhtml#idm45174932651752-marker)) [MLE](https://oreil.ly/y9atF) 是一种估计概率分布参数的方法，使得在假定的统计模型下观察到的数据最有可能。
- en: ^([4](ch04.xhtml#idm45174932370936-marker)) The approach of projecting data
    is similar to the PCA algorithm discussed in [Chapter 7](ch07.xhtml#Chapter7).
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch04.xhtml#idm45174932370936-marker)) 数据投影的方法类似于第7章中讨论的PCA算法。
- en: ^([5](ch04.xhtml#idm45174932160872-marker)) Bias and variance are described
    in detail later in this chapter.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch04.xhtml#idm45174932160872-marker)) 偏差和方差将在本章后面详细描述。
- en: ^([6](ch04.xhtml#idm45174932048648-marker)) Split is the process of converting
    a nonhomogeneous parent node into two homogeneous child nodes (best possible).
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch04.xhtml#idm45174932048648-marker)) 分裂是将非同质父节点转换为两个同质子节点的过程（最佳可能的）。
- en: ^([7](ch04.xhtml#idm45174931487736-marker)) In this table we do not include
    *AdaBoost* and *extra trees* as their overall behavior across all the parameters
    are similar to *Gradient Boosting* and *Random Forest*, respectively.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch04.xhtml#idm45174931487736-marker)) 在这张表中，我们不包括*AdaBoost*和*extra trees*，因为它们在所有参数上的整体行为与*Gradient
    Boosting*和*Random Forest*类似。
