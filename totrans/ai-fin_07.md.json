["```py\nIn [1]: import numpy as np\n        import pandas as pd\n        from pylab import plt, mpl\n        np.random.seed(100)\n        plt.style.use('seaborn')\n        mpl.rcParams['savefig.dpi'] = 300\n        mpl.rcParams['font.family'] = 'serif'\n\nIn [2]: url = 'http://hilpisch.com/aiif_eikon_eod_data.csv'  ![1](Images/1.png)\n\nIn [3]: raw = pd.read_csv(url, index_col=0, parse_dates=True)['EUR=']  ![1](Images/1.png)\n\nIn [4]: raw.head()\nOut[4]: Date\n        2010-01-01    1.4323\n        2010-01-04    1.4411\n        2010-01-05    1.4368\n        2010-01-06    1.4412\n        2010-01-07    1.4318\n        Name: EUR=, dtype: float64\n\nIn [5]: raw.tail()\nOut[5]: Date\n        2019-12-26    1.1096\n        2019-12-27    1.1175\n        2019-12-30    1.1197\n        2019-12-31    1.1210\n        2020-01-01    1.1210\n        Name: EUR=, dtype: float64\n\nIn [6]: l = raw.resample('1M').last()  ![2](Images/2.png)\n\nIn [7]: l.plot(figsize=(10, 6), title='EUR/USD monthly');\n```", "```py\nIn [8]: l = l.values  ![1](Images/1.png)\n        l -= l.mean()  ![2](Images/2.png)\n\nIn [9]: f = np.linspace(-2, 2, len(l))  ![3](Images/3.png)\n\nIn [10]: plt.figure(figsize=(10, 6))\n         plt.plot(f, l, 'ro')\n         plt.title('Sample Data Set')\n         plt.xlabel('features')\n         plt.ylabel('labels');\n```", "```py\nIn [11]: def MSE(l, p):\n             return np.mean((l - p) ** 2)  ![1](Images/1.png)\n\nIn [12]: reg = np.polyfit(f, l, deg=5)  ![2](Images/2.png)\n         reg  ![2](Images/2.png)\nOut[12]: array([-0.01910626, -0.0147182 ,  0.10990388,  0.06007211, -0.20833598,\n                -0.03275423])\n\nIn [13]: p = np.polyval(reg, f)  ![3](Images/3.png)\n\nIn [14]: MSE(l, p)  ![4](Images/4.png)\nOut[14]: 0.0034166422957371025\n\nIn [15]: plt.figure(figsize=(10, 6))\n         plt.plot(f, l, 'ro', label='sample data')\n         plt.plot(f, p, '--', label='regression')\n         plt.legend();\n```", "```py\nIn [16]: for i in range(10, len(f) + 1, 20):\n             reg = np.polyfit(f[:i], l[:i], deg=3)  ![1](Images/1.png)\n             p = np.polyval(reg, f)  ![2](Images/2.png)\n             mse = MSE(l, p)  ![3](Images/3.png)\n             print(f'{i:3d} | MSE={mse}')\n          10 | MSE=248628.10681642237\n          30 | MSE=731.9382249304651\n          50 | MSE=12.236088505004465\n          70 | MSE=0.7410590619743301\n          90 | MSE=0.0057430617304093275\n         110 | MSE=0.006492800939555582\n```", "```py\nIn [17]: import tensorflow as tf\n         tf.random.set_seed(100)\n\nIn [18]: from keras.layers import Dense\n         from keras.models import Sequential\n         Using TensorFlow backend.\n\nIn [19]: model = Sequential()\n         model.add(Dense(256, activation='relu', input_dim=1))  ![1](Images/1.png)\n         model.add(Dense(1, activation='linear')) ![1](Images/1.png)\n         model.compile(loss='mse', optimizer='rmsprop')\n\nIn [20]: model.summary()\n         Model: \"sequential_1\"\n         _________________________________________________________________\n         Layer (type)                 Output Shape              Param #\n         =================================================================\n         dense_1 (Dense)              (None, 256)               512\n         _________________________________________________________________\n         dense_2 (Dense)              (None, 1)                 257\n         =================================================================\n         Total params: 769\n         Trainable params: 769\n         Non-trainable params: 0\n         _________________________________________________________________\n\nIn [21]: %time model.fit(f, l, epochs=1500, verbose=False)  ![2](Images/2.png)\n         CPU times: user 5.89 s, sys: 761 ms, total: 6.66 s\n         Wall time: 4.43 s\n\nOut[21]: <keras.callbacks.callbacks.History at 0x7fc05d599d90>\n\nIn [22]: p = model.predict(f).flatten()  ![3](Images/3.png)\n\nIn [23]: MSE(l, p)  ![4](Images/4.png)\nOut[23]: 0.0020217512014360102\n\nIn [24]: plt.figure(figsize=(10, 6))\n         plt.plot(f, l, 'ro', label='sample data')\n         plt.plot(f, p, '--', label='DNN approximation')\n         plt.legend();\n```", "```py\nIn [25]: import pandas as pd\n\nIn [26]: res = pd.DataFrame(model.history.history)\n\nIn [27]: res.tail()\nOut[27]:           loss\n         1495  0.001547\n         1496  0.001520\n         1497  0.001456\n         1498  0.001356\n         1499  0.001325\n\nIn [28]: res.iloc[100:].plot(figsize=(10, 6))\n         plt.ylabel('MSE')\n         plt.xlabel('epochs');\n```", "```py\nIn [29]: reg = {}\n         for d in range(1, 12, 2):\n             reg[d] = np.polyfit(f, l, deg=d)  ![1](Images/1.png)\n             p = np.polyval(reg[d], f)\n             mse = MSE(l, p)\n             print(f'{d:2d} | MSE={mse}')\n          1 | MSE=0.005322474034260403\n          3 | MSE=0.004353110724143185\n          5 | MSE=0.0034166422957371025\n          7 | MSE=0.0027389501772354025\n          9 | MSE=0.001411961626330845\n         11 | MSE=0.0012651237868752322\n\nIn [30]: plt.figure(figsize=(10, 6))\n         plt.plot(f, l, 'ro', label='sample data')\n         for d in reg:\n             p = np.polyval(reg[d], f)\n             plt.plot(f, p, '--', label=f'deg={d}')\n         plt.legend();\n```", "```py\nIn [31]: def create_dnn_model(hl=1, hu=256):\n             ''' Function to create Keras DNN model.\n\n             Parameters\n             ==========\n             hl: int\n                 number of hidden layers\n             hu: int\n                 number of hidden units (per layer)\n             '''\n             model = Sequential()\n             for _ in range(hl):\n                 model.add(Dense(hu, activation='relu', input_dim=1))  ![1](Images/1.png)\n             model.add(Dense(1, activation='linear'))\n             model.compile(loss='mse', optimizer='rmsprop')\n             return model\n\nIn [32]: model = create_dnn_model(3)  ![2](Images/2.png)\n\nIn [33]: model.summary()  ![3](Images/3.png)\n         Model: \"sequential_2\"\n         _________________________________________________________________\n         Layer (type)                 Output Shape              Param #\n         =================================================================\n         dense_3 (Dense)              (None, 256)               512\n         _________________________________________________________________\n         dense_4 (Dense)              (None, 256)               65792\n         _________________________________________________________________\n         dense_5 (Dense)              (None, 256)               65792\n         _________________________________________________________________\n         dense_6 (Dense)              (None, 1)                 257\n         =================================================================\n         Total params: 132,353\n         Trainable params: 132,353\n         Non-trainable params: 0\n         _________________________________________________________________\n\nIn [34]: %time model.fit(f, l, epochs=2500, verbose=False)\n         CPU times: user 34.9 s, sys: 5.91 s, total: 40.8 s\n         Wall time: 15.5 s\n\nOut[34]: <keras.callbacks.callbacks.History at 0x7fc03fc18890>\n\nIn [35]: p = model.predict(f).flatten()\n\nIn [36]: MSE(l, p)\nOut[36]: 0.00046612284916401614\n\nIn [37]: plt.figure(figsize=(10, 6))\n         plt.plot(f, l, 'ro', label='sample data')\n         plt.plot(f, p, '--', label='DNN approximation')\n         plt.legend();\n```", "```py\nIn [38]: te = int(0.25 * len(f))  ![1](Images/1.png)\n         va = int(0.25 * len(f))  ![2](Images/2.png)\n\nIn [39]: np.random.seed(100)\n         ind = np.arange(len(f))  ![3](Images/3.png)\n         np.random.shuffle(ind)  ![3](Images/3.png)\n\nIn [40]: ind_te = np.sort(ind[:te])  ![4](Images/4.png)\n         ind_va = np.sort(ind[te:te + va])  ![4](Images/4.png)\n         ind_tr = np.sort(ind[te + va:])  ![4](Images/4.png)\n\nIn [41]: f_te = f[ind_te]  ![5](Images/5.png)\n         f_va = f[ind_va]  ![5](Images/5.png)\n         f_tr = f[ind_tr]  ![5](Images/5.png)\n\nIn [42]: l_te = l[ind_te]  ![6](Images/6.png)\n         l_va = l[ind_va]  ![6](Images/6.png)\n         l_tr = l[ind_tr]  ![6](Images/6.png)\n```", "```py\nIn [43]: reg = {}\n         mse = {}\n         for d in range(1, 22, 4):\n             reg[d] = np.polyfit(f_tr, l_tr, deg=d)\n             p = np.polyval(reg[d], f_tr)\n             mse_tr = MSE(l_tr, p)  ![1](Images/1.png)\n             p = np.polyval(reg[d], f_va)\n             mse_va = MSE(l_va, p)  ![2](Images/2.png)\n             mse[d] = (mse_tr, mse_va)\n             print(f'{d:2d} | MSE_tr={mse_tr:7.5f} | MSE_va={mse_va:7.5f}')\n          1 | MSE_tr=0.00574 | MSE_va=0.00492\n          5 | MSE_tr=0.00375 | MSE_va=0.00273\n          9 | MSE_tr=0.00132 | MSE_va=0.00243\n         13 | MSE_tr=0.00094 | MSE_va=0.00183\n         17 | MSE_tr=0.00060 | MSE_va=0.00153\n         21 | MSE_tr=0.00046 | MSE_va=0.00837\n\nIn [44]: fig, ax = plt.subplots(2, 1, figsize=(10, 8), sharex=True)\n         ax[0].plot(f_tr, l_tr, 'ro', label='training data')\n         ax[1].plot(f_va, l_va, 'go', label='validation data')\n         for d in reg:\n             p = np.polyval(reg[d], f_tr)\n             ax[0].plot(f_tr, p, '--', label=f'deg={d} (tr)')\n             p = np.polyval(reg[d], f_va)\n             plt.plot(f_va, p, '--', label=f'deg={d} (va)')\n         ax[0].legend()\n         ax[1].legend();\n```", "```py\nIn [45]: from keras.callbacks import EarlyStopping\n\nIn [46]: model = create_dnn_model(2, 256)\n\nIn [47]: callbacks = [EarlyStopping(monitor='loss',  ![1](Images/1.png)\n                                    patience=100,  ![2](Images/2.png)\n                                   restore_best_weights=True)]  ![3](Images/3.png)\n\nIn [48]: %%time\n         model.fit(f_tr, l_tr, epochs=3000, verbose=False,\n                   validation_data=(f_va, l_va),  ![4](Images/4.png)\n                   callbacks=callbacks)  ![5](Images/5.png)\n         CPU times: user 8.07 s, sys: 1.33 s, total: 9.4 s\n         Wall time: 4.81 s\n\nOut[48]: <keras.callbacks.callbacks.History at 0x7fc0438b47d0>\n\nIn [49]: fig, ax = plt.subplots(2, 1, sharex=True, figsize=(10, 8))\n         ax[0].plot(f_tr, l_tr, 'ro', label='training data')\n         p = model.predict(f_tr)\n         ax[0].plot(f_tr, p, '--', label=f'DNN (tr)')\n         ax[0].legend()\n         ax[1].plot(f_va, l_va, 'go', label='validation data')\n         p = model.predict(f_va)\n         ax[1].plot(f_va, p, '--', label=f'DNN (va)')\n         ax[1].legend();\n```", "```py\nIn [50]: res = pd.DataFrame(model.history.history)\n\nIn [51]: res.tail()\nOut[51]:       val_loss      loss\n         1375  0.000854  0.000544\n         1376  0.000685  0.000473\n         1377  0.001326  0.000942\n         1378  0.001026  0.000867\n         1379  0.000710  0.000500\n\nIn [52]: res.iloc[35::25].plot(figsize=(10, 6))\n         plt.ylabel('MSE')\n         plt.xlabel('epochs');\n```", "```py\nIn [53]: p_ols = np.polyval(reg[5], f_te)\n         p_dnn = model.predict(f_te).flatten()\n\nIn [54]: MSE(l_te, p_ols)\nOut[54]: 0.0038960346771028356\n\nIn [55]: MSE(l_te, p_dnn)\nOut[55]: 0.000705705678438721\n\nIn [56]: plt.figure(figsize=(10, 6))\n         plt.plot(f_te, l_te, 'ro', label='test data')\n         plt.plot(f_te, p_ols, '--', label='OLS prediction')\n         plt.plot(f_te, p_dnn, '-.', label='DNN prediction');\n         plt.legend();\n```", "```py\nIn [57]: f_tr = f[:20:2]  ![1](Images/1.png)\n         l_tr = l[:20:2]  ![1](Images/1.png)\n\nIn [58]: f_va = f[1:20:2]  ![2](Images/2.png)\n         l_va = l[1:20:2]  ![2](Images/2.png)\n\nIn [59]: reg_b = np.polyfit(f_tr, l_tr, deg=1)  ![3](Images/3.png)\n\nIn [60]: reg_v = np.polyfit(f_tr, l_tr, deg=9, full=True)[0]  ![4](Images/4.png)\n\nIn [61]: f_ = np.linspace(f_tr.min(), f_va.max(), 75)  ![5](Images/5.png)\n\nIn [62]: plt.figure(figsize=(10, 6))\n         plt.plot(f_tr, l_tr, 'ro', label='training data')\n         plt.plot(f_va, l_va, 'go', label='validation data')\n         plt.plot(f_, np.polyval(reg_b, f_), '--', label='high bias')\n         plt.plot(f_, np.polyval(reg_v, f_), '--', label='high variance')\n         plt.ylim(-0.2)\n         plt.legend(loc=2);\n```", "```py\nIn [63]: from sklearn.metrics import r2_score\n\nIn [64]: def evaluate(reg, f, l):\n             p = np.polyval(reg, f)\n             bias = np.abs(l - p).mean()  ![1](Images/1.png)\n             var = p.var()  ![2](Images/2.png)\n             msg = f'MSE={MSE(l, p):.4f} | R2={r2_score(l, p):9.4f} | '\n             msg += f'bias={bias:.4f} | var={var:.4f}'\n             print(msg)\n\nIn [65]: evaluate(reg_b, f_tr, l_tr)  ![3](Images/3.png)\n         MSE=0.0026 | R2=   0.3484 | bias=0.0423 | var=0.0014\n\nIn [66]: evaluate(reg_b, f_va, l_va)  ![4](Images/4.png)\n         MSE=0.0032 | R2=   0.4498 | bias=0.0460 | var=0.0014\n\nIn [67]: evaluate(reg_v, f_tr, l_tr)  ![5](Images/5.png)\n         MSE=0.0000 | R2=   1.0000 | bias=0.0000 | var=0.0040\n\nIn [68]: evaluate(reg_v, f_va, l_va)  ![6](Images/6.png)\n         MSE=0.8752 | R2=-149.2658 | bias=0.3565 | var=0.7539\n```", "```py\nIn [69]: from sklearn.model_selection import cross_val_score\n         from sklearn.preprocessing import PolynomialFeatures\n         from sklearn.linear_model import LinearRegression\n         from sklearn.pipeline import make_pipeline\n\nIn [70]: def PolynomialRegression(degree=None, **kwargs):\n             return make_pipeline(PolynomialFeatures(degree),\n                                 LinearRegression(**kwargs))  ![1](Images/1.png)\n\nIn [71]: np.set_printoptions(suppress=True,\n                 formatter={'float': lambda x: f'{x:12.2f}'})  ![2](Images/2.png)\n\nIn [72]: print('\\nCross-validation scores')\n         print(74 * '=')\n         for deg in range(0, 10, 1):\n             model = PolynomialRegression(deg)\n             cvs = cross_val_score(model, f.reshape(-1, 1), l, cv=5)  ![3](Images/3.png)\n             print(f'deg={deg} | ' + str(cvs.round(2)))\n\n         Cross-validation scores\n         ==========================================================================\n         deg=0 | [       -6.07        -7.34        -0.09        -6.32        -8.69]\n         deg=1 | [       -0.28        -1.40         0.16        -1.66        -4.62]\n         deg=2 | [       -3.48        -2.45         0.19        -1.57       -12.94]\n         deg=3 | [       -0.00        -1.24         0.32        -0.48       -43.62]\n         deg=4 | [     -222.81        -2.88         0.37        -0.32      -496.61]\n         deg=5 | [     -143.67        -5.85         0.49         0.12     -1241.04]\n         deg=6 | [    -4038.96       -14.71         0.49        -0.33      -317.32]\n         deg=7 | [    -9937.83       -13.98         0.64         0.22    -18725.61]\n         deg=8 | [    -3514.36       -11.22        -0.15        -6.29   -298744.18]\n         deg=9 | [    -7454.15        -0.91         0.15        -0.41    -13580.75]\n```", "```py\nIn [73]: np.random.seed(100)\n         tf.random.set_seed(100)\n         from keras.wrappers.scikit_learn import KerasRegressor\n\nIn [74]: model = KerasRegressor(build_fn=create_dnn_model,\n                               verbose=False, epochs=1000,\n                               hl=1, hu=36)  ![1](Images/1.png)\n\nIn [75]: %time cross_val_score(model, f, l, cv=5)  ![2](Images/2.png)\n         CPU times: user 18.6 s, sys: 2.17 s, total: 20.8 s\n         Wall time: 14.6 s\n\nOut[75]: array([       -0.02,        -0.01,        -0.00,        -0.00,\n                       -0.01])\n\nIn [76]: model = KerasRegressor(build_fn=create_dnn_model,\n                               verbose=False, epochs=1000,\n                               hl=3, hu=256)  ![3](Images/3.png)\n\nIn [77]: %time cross_val_score(model, f, l, cv=5)  ![4](Images/4.png)\n         CPU times: user 1min 5s, sys: 11.6 s, total: 1min 16s\n         Wall time: 30.1 s\n\nOut[77]: array([       -0.08,        -0.00,        -0.00,        -0.00,\n                       -0.05])\n```"]