<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 2. Superintelligence"><div class="chapter" id="superintelligence">
<h1><span class="label">Chapter 2. </span>Superintelligence</h1>

<blockquote>
<p>The fact that there are many paths that lead to superintelligence should increase our confidence that we will eventually get there. If one path turns out to be blocked, we can still progress.</p>
<p data-type="attribution">Nick Bostrom (2014)</p>
</blockquote>

<p><a data-type="indexterm" data-primary="technological singularity" id="idm45625339036632"/>There are multiple definitions for the term <em>technological singularity</em>. Its use dates back at least to the article by Vinge (1993), which the author provocatively begins like this:</p>
<blockquote>
<p>Within thirty years, we will have the technological means to create superhuman intelligence. Shortly after, the human era will be ended.</p></blockquote>

<p>For the purposes of this chapter and book, <em>technological singularity</em> refers to a point in time at which certain machines achieve superhuman intelligence, or <em>superintelligence</em>—this is mostly in line with the original idea of Vinge (1993). The idea and concept was further popularized by the widely read and cited book by Kurzweil (2005). Barrat (2013) has a wealth of historical and anecdotal information around the topic. Shanahan (2015) provides an informal introduction and overview of its central aspects. The expression <em>technological singularity</em> itself has its origin in the concept of a <em>singularity</em> in physics. It refers to the center of a black hole, where mass is highly concentrated, gravitation becomes infinite, and traditional laws of physics break down. The beginning of the universe, the so-called Big Bang, is also referred to as a singularity.</p>

<p>Although the general ideas and concepts of the technological singularity and of superintelligence might not have an obvious and direct relationship to AI applied to finance, a better understanding of their background, related problems, and potential consequences is beneficial. The insights gained in the general framework are important in a narrower context as well, such as for AI in finance. Those insights also help guide the discussion about how AI might reshape the financial industry in the near and long term.</p>

<p><a data-type="xref" href="#si_success_stories">“Success Stories”</a> takes a look at a selection of recent success stories in the field of AI. Among others, it covers how the company DeepMind solved the problem of playing Atari 2600 games with neural networks. It also tells the story of how the same company solved the problem of playing the game of Go at above-human-expert level. The story of chess and computer programs is also recounted in that section. <a data-type="xref" href="#si_importance_of_hardware">“Importance of Hardware”</a> discusses the importance of hardware in the context of these recent success stories. <a data-type="xref" href="#si_forms_intelligence">“Forms of Intelligence”</a> introduces different forms of intelligence, such as artificial narrow intelligence (ANI), artificial general intelligence (AGI), and superintelligence (SI). <a data-type="xref" href="#si_paths_to_superintelligence">“Paths to Superintelligence”</a> is about potential paths to superintelligence, such as whole brain emulation (WBE), while <a data-type="xref" href="#si_intelligence_explosion">“Intelligence Explosion”</a> is about what researchers call intelligence explosion. <a data-type="xref" href="#si_control_problem">“Goals and Control”</a> provides a discussion of aspects related to the so-called control problem in the context of superintelligence. Finally, <a data-type="xref" href="#si_potential_outcomes">“Potential Outcomes”</a> briefly looks at potential future outcomes and 
<span class="keep-together">scenarios</span> once superintelligence has been achieved.</p>






<section data-type="sect1" data-pdf-bookmark="Success Stories"><div class="sect1" id="si_success_stories">
<h1>Success Stories</h1>

<p><a data-type="indexterm" data-primary="AI (artificial intelligence)" data-secondary="success stories" id="ix_AI_succ_story"/>Many ideas and algorithms in AI date back a few decades already. Over these decades there have been longer periods of hope on the one hand and despair on the other hand. Bostrom (2014, ch. 1) provides a review of these periods.</p>

<p>In 2020, one can say for sure that AI is in the middle of a period of hope, if not excitement. One reason for this is recent successes in applying AI to domains and problems that even a few years ago seemed immune to AI dominance for decades to come. The list of such success stories is long and growing rapidly. Therefore, this section focuses on three such stories only. Gerrish (2018) provides a broader selection and more detailed accounts of the single cases.</p>








<section data-type="sect2" data-pdf-bookmark="Atari"><div class="sect2" id="idm45625339011560">
<h2>Atari</h2>

<p><a data-type="indexterm" data-primary="RL (reinforcement learning)" data-secondary="Atari story" id="ix_RL_atari"/><a data-type="indexterm" data-primary="DeepMind" id="ix_deepmind_atari"/><a data-type="indexterm" data-primary="Atari" id="ix_atari_success"/>This sub-section first tells the success story of how DeepMind mastered playing Atari 2600 games with reinforcement learning and neural networks, and then illustrates the basic approach that led to its success based on a concrete code example.</p>










<section data-type="sect3" data-pdf-bookmark="The story"><div class="sect3" id="idm45625339006744">
<h3>The story</h3>

<p>The first success story is about playing Atari 2600 games on a superhuman level.<sup><a data-type="noteref" id="idm45625339005208-marker" href="ch02.xhtml#idm45625339005208">1</a></sup> The Atari 2600 Video Computer System (VCS) was released in 1977 and was one of the first widespread game-playing consoles in the 1980s. Selected popular games from that period, such as <em>Space Invaders</em>, <em>Asteroids</em>, or <em>Missile Command</em>, count as classics and are still played decades later by retro games enthusiasts.</p>

<p><a href="https://deepmind.com">DeepMind</a> published a paper (Mnih et al. 2013) in which its team detailed results from applying reinforcement learning to the problem of playing Atari 2600 games by an AI algorithm, or a so-called AI agent. <a data-type="indexterm" data-primary="agents" data-secondary="QL agent" id="idm45625339305384"/><a data-type="indexterm" data-primary="neural networks" data-secondary="in Atari story" data-secondary-sortas="Atari story" id="ix_neuralnet_atari"/><a data-type="indexterm" data-primary="CNNs (convolutional neural networks)" id="idm45625339302920"/><a data-type="indexterm" data-primary="DL (deep learning)" data-secondary="convolutional neural networks" id="idm45625339302232"/><a data-type="indexterm" data-primary="neural networks" data-secondary="CNNs" id="idm45625339301272"/><a data-type="indexterm" data-primary="QL (Q-learning)" data-seealso="risk management" id="idm45625339300328"/>The algorithm is a variant of Q-learning applied to a convolutional neural network.<sup><a data-type="noteref" id="idm45625339299256-marker" href="ch02.xhtml#idm45625339299256">2</a></sup> The algorithm is trained on high-dimensional visual input (raw pixels) only, without any guidance by or input from a human.
The original project focused on seven Atari 2600 games, and for three of them—<em>Pong</em>, <em>Enduro</em>, and <em>Breakout</em>—the DeepMind team reported above-human expert performance of the AI agent.</p>

<p>From an AI point of view, it is remarkable not only that the DeepMind team achieved such a result, but also how it achieved it. First, the team only used a single neural network to learn and play all seven games. <a data-type="indexterm" data-primary="ALE (Arcade Learning Environment)" id="idm45625339296584"/>Second, no human guidance or humanly labeled data was provided, just the interactive learning experience based on visual input properly transformed into features data.<sup><a data-type="noteref" id="idm45625339295608-marker" href="ch02.xhtml#idm45625339295608">3</a></sup> Third, the approach used is reinforcement learning, which relies on observation of the relationships between actions and outcomes (rewards) only—basically the same way a human player learns to play such a game.</p>

<p>One of the Atari 2600 games, for which the DeepMind AI agent achieved above-human expert performance, is <a href="http://bit.ly/aiif_breakout"><em>Breakout</em></a>. In this game, the goal is to destroy lines of bricks at the top of the screen by using a paddle at the bottom of the screen from which a ball bounces back and moves straight across the screen. Whenever the ball hits a brick, the brick is destroyed and the ball bounces back. The ball also bounces back from the left, right, and top walls. The player loses a life in this game whenever the ball reaches the bottom of the screen without being hit by the paddle.</p>

<p>The action space has three elements, all related to the paddle: staying at current position, moving left, and moving right. The state space is represented by frames of the game screen of size 210 x 160 pixels with a 128-color palette. The reward is represented by the game score, which the DeepMind algorithm is programmed to maximize. With regard to the action policy, the algorithm learns which action is best to take, given a certain game state, to maximize the game score (total reward).</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="An example"><div class="sect3" id="idm45625339291032">
<h3>An example</h3>

<p><a data-type="indexterm" data-primary="neural networks" data-secondary="CartPole game" id="ix_neuralnet_cartpole"/><a data-type="indexterm" data-primary="CartPole game" data-secondary="neural network approach" id="ix_cartpole_nn_app"/>There is not enough room in this chapter to explore in detail the approach taken by DeepMind for <em>Breakout</em> and the other Atari 2600 games. <a data-type="indexterm" data-primary="OpenAI Gym environment" id="ix_openai_gym_env_c2"/>However, the OpenAI Gym environment (see <a href="https://gym.openai.com"><em class="hyperlink">https://gym.openai.com</em></a>) allows for the illustration of a similar, but simpler, neural network approach for a similar, but again simpler, game.</p>

<p>The Python code in this section works with the <code>CartPole</code> environment of the OpenAI Gym (see <a href="http://bit.ly/aiif_cartpole"><em class="hyperlink">http://bit.ly/aiif_cartpole</em></a>).<sup><a data-type="noteref" id="idm45625339281976-marker" href="ch02.xhtml#idm45625339281976">4</a></sup> In this environment, a cart needs to be moved to the right or left to balance an upright pole on top of the paddle. Therefore, the action space is similar to the <em>Breakout</em> action space. The state space consists of four physical data points: cart position, cart velocity, pole angle, and pole angular velocity (see <a data-type="xref" href="#figure_cart_pole">Figure 2-1</a>). If, after having taken an action, the pole is still in balance, the agent gets a reward of 1. If the pole is out of balance, the game ends. The agent is considered successful if it reaches a total reward of 200.<sup><a data-type="noteref" id="idm45625339278824-marker" href="ch02.xhtml#idm45625339278824">5</a></sup></p>

<figure><div id="figure_cart_pole" class="figure">
<img src="Images/aiif_0201.png" alt="aiif 0201" width="1182" height="282"/>
<h6><span class="label">Figure 2-1. </span>Graphical representation of the <code>CartPole</code> environment</h6>
</div></figure>

<p>The following code first instantiates a <code>CartPole</code> environment object, and then inspects the action and state spaces, takes a random action, and captures the results. The AI agent moves on toward the next round when the <code>done</code> variable is <code>False</code>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">1</code><code class="p">]</code><code class="p">:</code><code> </code><code class="kn">import</code><code> </code><code class="nn">gym</code><code>
</code><code>        </code><code class="kn">import</code><code> </code><code class="nn">numpy</code><code> </code><code class="kn">as</code><code> </code><code class="nn">np</code><code>
</code><code>        </code><code class="kn">import</code><code> </code><code class="nn">pandas</code><code> </code><code class="kn">as</code><code> </code><code class="nn">pd</code><code>
</code><code>        </code><code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="mi">100</code><code class="p">)</code><code>
</code><code>
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">2</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">env</code><code> </code><code class="o">=</code><code> </code><code class="n">gym</code><code class="o">.</code><code class="n">make</code><code class="p">(</code><code class="s1">'</code><code class="s1">CartPole-v0</code><code class="s1">'</code><code class="p">)</code><code>  </code><a class="co" id="co_superintelligence_CO1-1" href="#callout_superintelligence_CO1-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>
</code><code>
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">3</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">env</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="mi">100</code><code class="p">)</code><code>  </code><a class="co" id="co_superintelligence_CO1-2" href="#callout_superintelligence_CO1-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a><code>
</code><code class="n">Out</code><code class="p">[</code><code class="mi">3</code><code class="p">]</code><code class="p">:</code><code> </code><code class="p">[</code><code class="mi">100</code><code class="p">]</code><code>
</code><code>
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">4</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">action_size</code><code> </code><code class="o">=</code><code> </code><code class="n">env</code><code class="o">.</code><code class="n">action_space</code><code class="o">.</code><code class="n">n</code><code>  </code><a class="co" id="co_superintelligence_CO1-3" href="#callout_superintelligence_CO1-3"><img src="Images/3.png" alt="3" width="12" height="12"/></a><code>
</code><code>        </code><code class="n">action_size</code><code>  </code><a class="co" id="co_superintelligence_CO1-4" href="#callout_superintelligence_CO1-3"><img src="Images/3.png" alt="3" width="12" height="12"/></a><code>
</code><code class="n">Out</code><code class="p">[</code><code class="mi">4</code><code class="p">]</code><code class="p">:</code><code> </code><code class="mi">2</code><code>
</code><code>
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">5</code><code class="p">]</code><code class="p">:</code><code> </code><code class="p">[</code><code class="n">env</code><code class="o">.</code><code class="n">action_space</code><code class="o">.</code><code class="n">sample</code><code class="p">(</code><code class="p">)</code><code> </code><code class="k">for</code><code> </code><code class="n">_</code><code> </code><code class="ow">in</code><code> </code><code class="nb">range</code><code class="p">(</code><code class="mi">10</code><code class="p">)</code><code class="p">]</code><code>  </code><a class="co" id="co_superintelligence_CO1-5" href="#callout_superintelligence_CO1-4"><img src="Images/4.png" alt="4" width="12" height="12"/></a><code>
</code><code class="n">Out</code><code class="p">[</code><code class="mi">5</code><code class="p">]</code><code class="p">:</code><code> </code><code class="p">[</code><code class="mi">1</code><code class="p">,</code><code> </code><code class="mi">0</code><code class="p">,</code><code> </code><code class="mi">0</code><code class="p">,</code><code> </code><code class="mi">0</code><code class="p">,</code><code> </code><code class="mi">1</code><code class="p">,</code><code> </code><code class="mi">1</code><code class="p">,</code><code> </code><code class="mi">0</code><code class="p">,</code><code> </code><code class="mi">0</code><code class="p">,</code><code> </code><code class="mi">0</code><code class="p">,</code><code> </code><code class="mi">0</code><code class="p">]</code><code>
</code><code>
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">6</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">state_size</code><code> </code><code class="o">=</code><code> </code><code class="n">env</code><code class="o">.</code><code class="n">observation_space</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code>  </code><a class="co" id="co_superintelligence_CO1-6" href="#callout_superintelligence_CO1-5"><img src="Images/5.png" alt="5" width="12" height="12"/></a><code>
</code><code>        </code><code class="n">state_size</code><code>  </code><a class="co" id="co_superintelligence_CO1-7" href="#callout_superintelligence_CO1-5"><img src="Images/5.png" alt="5" width="12" height="12"/></a><code>
</code><code class="n">Out</code><code class="p">[</code><code class="mi">6</code><code class="p">]</code><code class="p">:</code><code> </code><code class="mi">4</code><code>
</code><code>
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">7</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">state</code><code> </code><code class="o">=</code><code> </code><code class="n">env</code><code class="o">.</code><code class="n">reset</code><code class="p">(</code><code class="p">)</code><code>  </code><a class="co" id="co_superintelligence_CO1-8" href="#callout_superintelligence_CO1-6"><img src="Images/6.png" alt="6" width="12" height="12"/></a><code>
</code><code>        </code><code class="n">state</code><code>  </code><code class="c1"># [cart position, cart velocity, pole angle, pole angular velocity]</code><code>
</code><code class="n">Out</code><code class="p">[</code><code class="mi">7</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">array</code><code class="p">(</code><code class="p">[</code><code class="o">-</code><code class="mf">0.01628537</code><code class="p">,</code><code>  </code><code class="mf">0.02379786</code><code class="p">,</code><code> </code><code class="o">-</code><code class="mf">0.0391981</code><code> </code><code class="p">,</code><code> </code><code class="o">-</code><code class="mf">0.01476447</code><code class="p">]</code><code class="p">)</code><code>
</code><code>
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">8</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">state</code><code class="p">,</code><code> </code><code class="n">reward</code><code class="p">,</code><code> </code><code class="n">done</code><code class="p">,</code><code> </code><code class="n">_</code><code> </code><code class="o">=</code><code> </code><code class="n">env</code><code class="o">.</code><code class="n">step</code><code class="p">(</code><code class="n">env</code><code class="o">.</code><code class="n">action_space</code><code class="o">.</code><code class="n">sample</code><code class="p">(</code><code class="p">)</code><code class="p">)</code><code>  </code><a class="co" id="co_superintelligence_CO1-9" href="#callout_superintelligence_CO1-7"><img src="Images/7.png" alt="7" width="12" height="12"/></a><code>
</code><code>        </code><code class="n">state</code><code class="p">,</code><code> </code><code class="n">reward</code><code class="p">,</code><code> </code><code class="n">done</code><code class="p">,</code><code> </code><code class="n">_</code><code>  </code><a class="co" id="co_superintelligence_CO1-10" href="#callout_superintelligence_CO1-7"><img src="Images/7.png" alt="7" width="12" height="12"/></a><code>
</code><code class="n">Out</code><code class="p">[</code><code class="mi">8</code><code class="p">]</code><code class="p">:</code><code> </code><code class="p">(</code><code class="n">array</code><code class="p">(</code><code class="p">[</code><code class="o">-</code><code class="mf">0.01580941</code><code class="p">,</code><code> </code><code class="o">-</code><code class="mf">0.17074066</code><code class="p">,</code><code> </code><code class="o">-</code><code class="mf">0.03949338</code><code class="p">,</code><code>  </code><code class="mf">0.26529786</code><code class="p">]</code><code class="p">)</code><code class="p">,</code><code> </code><code class="mf">1.0</code><code class="p">,</code><code> </code><code class="bp">False</code><code class="p">,</code><code> </code><code class="p">{</code><code class="p">}</code><code class="p">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_superintelligence_CO1-1" href="#co_superintelligence_CO1-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>Instantiates the environment object</p></dd>
<dt><a class="co" id="callout_superintelligence_CO1-2" href="#co_superintelligence_CO1-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a></dt>
<dd><p>Fixes the random number seed for the environment</p></dd>
<dt><a class="co" id="callout_superintelligence_CO1-3" href="#co_superintelligence_CO1-3"><img src="Images/3.png" alt="3" width="12" height="12"/></a></dt>
<dd><p>Shows the size of the action space</p></dd>
<dt><a class="co" id="callout_superintelligence_CO1-4" href="#co_superintelligence_CO1-5"><img src="Images/4.png" alt="4" width="12" height="12"/></a></dt>
<dd><p>Takes some random actions and collects them</p></dd>
<dt><a class="co" id="callout_superintelligence_CO1-5" href="#co_superintelligence_CO1-6"><img src="Images/5.png" alt="5" width="12" height="12"/></a></dt>
<dd><p>Shows the size of the state space</p></dd>
<dt><a class="co" id="callout_superintelligence_CO1-6" href="#co_superintelligence_CO1-8"><img src="Images/6.png" alt="6" width="12" height="12"/></a></dt>
<dd><p>Resets (initializes) the environment and captures the state</p></dd>
<dt><a class="co" id="callout_superintelligence_CO1-7" href="#co_superintelligence_CO1-9"><img src="Images/7.png" alt="7" width="12" height="12"/></a></dt>
<dd><p>Takes a random action and steps the environment forward to the next state</p></dd>
</dl>

<p>The next step is to play the game based on random actions to generate a large enough data set. However, to increase the quality of the data set, only data that results from games with a total reward of 110 or more is collected. To this end, a few thousand games are played to collect enough data for the training of a neural network:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">9</code><code class="p">]</code><code class="p">:</code><code> </code><code class="o">%</code><code class="o">%</code><code class="n">time</code><code>
</code><code>        </code><code class="n">data</code><code> </code><code class="o">=</code><code> </code><code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="p">)</code><code>
</code><code>        </code><code class="n">state</code><code> </code><code class="o">=</code><code> </code><code class="n">env</code><code class="o">.</code><code class="n">reset</code><code class="p">(</code><code class="p">)</code><code>
</code><code>        </code><code class="n">length</code><code> </code><code class="o">=</code><code> </code><code class="p">[</code><code class="p">]</code><code>
</code><code>        </code><code class="k">for</code><code> </code><code class="n">run</code><code> </code><code class="ow">in</code><code> </code><code class="nb">range</code><code class="p">(</code><code class="mi">25000</code><code class="p">)</code><code class="p">:</code><code>
</code><code>            </code><code class="n">done</code><code> </code><code class="o">=</code><code> </code><code class="bp">False</code><code>
</code><code>            </code><code class="n">prev_state</code><code> </code><code class="o">=</code><code> </code><code class="n">env</code><code class="o">.</code><code class="n">reset</code><code class="p">(</code><code class="p">)</code><code>
</code><code>            </code><code class="n">treward</code><code> </code><code class="o">=</code><code> </code><code class="mi">1</code><code>
</code><code>            </code><code class="n">results</code><code> </code><code class="o">=</code><code> </code><code class="p">[</code><code class="p">]</code><code>
</code><code>            </code><code class="k">while</code><code> </code><code class="ow">not</code><code> </code><code class="n">done</code><code class="p">:</code><code>
</code><code>                </code><code class="n">action</code><code> </code><code class="o">=</code><code> </code><code class="n">env</code><code class="o">.</code><code class="n">action_space</code><code class="o">.</code><code class="n">sample</code><code class="p">(</code><code class="p">)</code><code>
</code><code>                </code><code class="n">state</code><code class="p">,</code><code> </code><code class="n">reward</code><code class="p">,</code><code> </code><code class="n">done</code><code class="p">,</code><code> </code><code class="n">_</code><code> </code><code class="o">=</code><code> </code><code class="n">env</code><code class="o">.</code><code class="n">step</code><code class="p">(</code><code class="n">action</code><code class="p">)</code><code>
</code><code>                </code><code class="n">results</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="p">{</code><code class="s1">'</code><code class="s1">s1</code><code class="s1">'</code><code class="p">:</code><code> </code><code class="n">prev_state</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="p">,</code><code> </code><code class="s1">'</code><code class="s1">s2</code><code class="s1">'</code><code class="p">:</code><code> </code><code class="n">prev_state</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code><code class="p">,</code><code>
</code><code>                                </code><code class="s1">'</code><code class="s1">s3</code><code class="s1">'</code><code class="p">:</code><code> </code><code class="n">prev_state</code><code class="p">[</code><code class="mi">2</code><code class="p">]</code><code class="p">,</code><code> </code><code class="s1">'</code><code class="s1">s4</code><code class="s1">'</code><code class="p">:</code><code> </code><code class="n">prev_state</code><code class="p">[</code><code class="mi">3</code><code class="p">]</code><code class="p">,</code><code>
</code><code>                                </code><code class="s1">'</code><code class="s1">a</code><code class="s1">'</code><code class="p">:</code><code> </code><code class="n">action</code><code class="p">,</code><code> </code><code class="s1">'</code><code class="s1">r</code><code class="s1">'</code><code class="p">:</code><code> </code><code class="n">reward</code><code class="p">}</code><code class="p">)</code><code>
</code><code>                </code><code class="n">treward</code><code> </code><code class="o">+</code><code class="o">=</code><code> </code><code class="n">reward</code><code> </code><code class="k">if</code><code> </code><code class="ow">not</code><code> </code><code class="n">done</code><code> </code><code class="k">else</code><code> </code><code class="mi">0</code><code>
</code><code>                </code><code class="n">prev_state</code><code> </code><code class="o">=</code><code> </code><code class="n">state</code><code>
</code><code>            </code><code class="k">if</code><code> </code><code class="n">treward</code><code> </code><code class="o">&gt;</code><code class="o">=</code><code> </code><code class="mi">110</code><code class="p">:</code><code>  </code><a class="co" id="co_superintelligence_CO2-1" href="#callout_superintelligence_CO2-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>
</code><code>                </code><code class="n">data</code><code> </code><code class="o">=</code><code> </code><code class="n">data</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">results</code><code class="p">)</code><code class="p">)</code><code>  </code><a class="co" id="co_superintelligence_CO2-2" href="#callout_superintelligence_CO2-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a><code>
</code><code>                </code><code class="n">length</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">treward</code><code class="p">)</code><code>  </code><a class="co" id="co_superintelligence_CO2-3" href="#callout_superintelligence_CO2-3"><img src="Images/3.png" alt="3" width="12" height="12"/></a><code>
</code><code>        </code><code class="n">CPU</code><code> </code><code class="n">times</code><code class="p">:</code><code> </code><code class="n">user</code><code> </code><code class="mf">9.84</code><code> </code><code class="n">s</code><code class="p">,</code><code> </code><code class="n">sys</code><code class="p">:</code><code> </code><code class="mf">48.7</code><code> </code><code class="n">ms</code><code class="p">,</code><code> </code><code class="n">total</code><code class="p">:</code><code> </code><code class="mf">9.89</code><code> </code><code class="n">s</code><code>
</code><code>        </code><code class="n">Wall</code><code> </code><code class="n">time</code><code class="p">:</code><code> </code><code class="mf">9.89</code><code> </code><code class="n">s</code><code>
</code><code>
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">10</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">length</code><code class="p">)</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="p">)</code><code>  </code><a class="co" id="co_superintelligence_CO2-4" href="#callout_superintelligence_CO2-4"><img src="Images/4.png" alt="4" width="12" height="12"/></a><code>
</code><code class="n">Out</code><code class="p">[</code><code class="mi">10</code><code class="p">]</code><code class="p">:</code><code> </code><code class="mf">119.75</code><code>
</code><code>
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">11</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">data</code><code class="o">.</code><code class="n">info</code><code class="p">(</code><code class="p">)</code><code>  </code><a class="co" id="co_superintelligence_CO2-5" href="#callout_superintelligence_CO2-5"><img src="Images/5.png" alt="5" width="12" height="12"/></a><code>
</code><code>         </code><code class="o">&lt;</code><code class="k">class</code><code> </code><code class="err">'</code><code class="nc">pandas</code><code class="o">.</code><code class="n">core</code><code class="o">.</code><code class="n">frame</code><code class="o">.</code><code class="n">DataFrame</code><code class="s1">'</code><code class="s1">&gt;</code><code>
</code><code>         </code><code class="n">Int64Index</code><code class="p">:</code><code> </code><code class="mi">479</code><code> </code><code class="n">entries</code><code class="p">,</code><code> </code><code class="mi">0</code><code> </code><code class="n">to</code><code> </code><code class="mi">143</code><code>
</code><code>         </code><code class="n">Data</code><code> </code><code class="n">columns</code><code> </code><code class="p">(</code><code class="n">total</code><code> </code><code class="mi">6</code><code> </code><code class="n">columns</code><code class="p">)</code><code class="p">:</code><code>
</code><code>          </code><code class="c1">#   Column  Non-Null Count  Dtype</code><code>
</code><code>         </code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code>  </code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code>  </code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code>  </code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code>
</code><code>          </code><code class="mi">0</code><code>   </code><code class="n">s1</code><code>      </code><code class="mi">479</code><code> </code><code class="n">non</code><code class="o">-</code><code class="n">null</code><code>    </code><code class="n">float64</code><code>
</code><code>          </code><code class="mi">1</code><code>   </code><code class="n">s2</code><code>      </code><code class="mi">479</code><code> </code><code class="n">non</code><code class="o">-</code><code class="n">null</code><code>    </code><code class="n">float64</code><code>
</code><code>          </code><code class="mi">2</code><code>   </code><code class="n">s3</code><code>      </code><code class="mi">479</code><code> </code><code class="n">non</code><code class="o">-</code><code class="n">null</code><code>    </code><code class="n">float64</code><code>
</code><code>          </code><code class="mi">3</code><code>   </code><code class="n">s4</code><code>      </code><code class="mi">479</code><code> </code><code class="n">non</code><code class="o">-</code><code class="n">null</code><code>    </code><code class="n">float64</code><code>
</code><code>          </code><code class="mi">4</code><code>   </code><code class="n">a</code><code>       </code><code class="mi">479</code><code> </code><code class="n">non</code><code class="o">-</code><code class="n">null</code><code>    </code><code class="n">int64</code><code>
</code><code>          </code><code class="mi">5</code><code>   </code><code class="n">r</code><code>       </code><code class="mi">479</code><code> </code><code class="n">non</code><code class="o">-</code><code class="n">null</code><code>    </code><code class="n">float64</code><code>
</code><code>         </code><code class="n">dtypes</code><code class="p">:</code><code> </code><code class="n">float64</code><code class="p">(</code><code class="mi">5</code><code class="p">)</code><code class="p">,</code><code> </code><code class="n">int64</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code><code>
</code><code>         </code><code class="n">memory</code><code> </code><code class="n">usage</code><code class="p">:</code><code> </code><code class="mf">26.2</code><code> </code><code class="n">KB</code><code>
</code><code>
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">12</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">data</code><code class="o">.</code><code class="n">tail</code><code class="p">(</code><code class="p">)</code><code>  </code><a class="co" id="co_superintelligence_CO2-6" href="#callout_superintelligence_CO2-5"><img src="Images/5.png" alt="5" width="12" height="12"/></a><code>
</code><code class="n">Out</code><code class="p">[</code><code class="mi">12</code><code class="p">]</code><code class="p">:</code><code>            </code><code class="n">s1</code><code>        </code><code class="n">s2</code><code>        </code><code class="n">s3</code><code>        </code><code class="n">s4</code><code>  </code><code class="n">a</code><code>    </code><code class="n">r</code><code>
</code><code>         </code><code class="mi">139</code><code>  </code><code class="mf">0.639509</code><code>  </code><code class="mf">0.992699</code><code> </code><code class="o">-</code><code class="mf">0.112029</code><code> </code><code class="o">-</code><code class="mf">1.548863</code><code>  </code><code class="mi">0</code><code>  </code><code class="mf">1.0</code><code>
</code><code>         </code><code class="mi">140</code><code>  </code><code class="mf">0.659363</code><code>  </code><code class="mf">0.799086</code><code> </code><code class="o">-</code><code class="mf">0.143006</code><code> </code><code class="o">-</code><code class="mf">1.293131</code><code>  </code><code class="mi">0</code><code>  </code><code class="mf">1.0</code><code>
</code><code>         </code><code class="mi">141</code><code>  </code><code class="mf">0.675345</code><code>  </code><code class="mf">0.606042</code><code> </code><code class="o">-</code><code class="mf">0.168869</code><code> </code><code class="o">-</code><code class="mf">1.048421</code><code>  </code><code class="mi">0</code><code>  </code><code class="mf">1.0</code><code>
</code><code>         </code><code class="mi">142</code><code>  </code><code class="mf">0.687466</code><code>  </code><code class="mf">0.413513</code><code> </code><code class="o">-</code><code class="mf">0.189837</code><code> </code><code class="o">-</code><code class="mf">0.813148</code><code>  </code><code class="mi">1</code><code>  </code><code class="mf">1.0</code><code>
</code><code>         </code><code class="mi">143</code><code>  </code><code class="mf">0.695736</code><code>  </code><code class="mf">0.610658</code><code> </code><code class="o">-</code><code class="mf">0.206100</code><code> </code><code class="o">-</code><code class="mf">1.159030</code><code>  </code><code class="mi">0</code><code>  </code><code class="mf">1.0</code></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_superintelligence_CO2-1" href="#co_superintelligence_CO2-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>Only if the total reward of the random agent is at least 100…</p></dd>
<dt><a class="co" id="callout_superintelligence_CO2-2" href="#co_superintelligence_CO2-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a></dt>
<dd><p>…is the data is collected…</p></dd>
<dt><a class="co" id="callout_superintelligence_CO2-3" href="#co_superintelligence_CO2-3"><img src="Images/3.png" alt="3" width="12" height="12"/></a></dt>
<dd><p>…and the total reward recorded.</p></dd>
<dt><a class="co" id="callout_superintelligence_CO2-4" href="#co_superintelligence_CO2-4"><img src="Images/4.png" alt="4" width="12" height="12"/></a></dt>
<dd><p>The average total reward of all random games included in the data set.</p></dd>
<dt><a class="co" id="callout_superintelligence_CO2-5" href="#co_superintelligence_CO2-5"><img src="Images/5.png" alt="5" width="12" height="12"/></a></dt>
<dd><p>A look at the collected data in the <code>DataFrame</code> object.</p></dd>
</dl>

<p>Equipped with the data, a neural network can be trained as follows. Set up a neural network for classification. Train it with the columns representing the state data as features and the column with the taken actions as labels data. Given that the data set only includes actions that have been successful for the given state, the neural network learns about what action to take (label) given the state (features):</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">13</code><code class="p">]</code><code class="p">:</code><code> </code><code class="kn">from</code><code> </code><code class="nn">pylab</code><code> </code><code class="kn">import</code><code> </code><code class="n">plt</code><code>
</code><code>         </code><code class="n">plt</code><code class="o">.</code><code class="n">style</code><code class="o">.</code><code class="n">use</code><code class="p">(</code><code class="s1">'</code><code class="s1">seaborn</code><code class="s1">'</code><code class="p">)</code><code>
</code><code>         </code><code class="o">%</code><code class="n">matplotlib</code><code> </code><code class="n">inline</code><code>
</code><code>
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">14</code><code class="p">]</code><code class="p">:</code><code> </code><code class="kn">import</code><code> </code><code class="nn">tensorflow</code><code> </code><code class="kn">as</code><code> </code><code class="nn">tf</code><code>
</code><code>         </code><code class="n">tf</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">set_seed</code><code class="p">(</code><code class="mi">100</code><code class="p">)</code><code>
</code><code>
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">15</code><code class="p">]</code><code class="p">:</code><code> </code><code class="kn">from</code><code> </code><code class="nn">keras.layers</code><code> </code><code class="kn">import</code><code> </code><code class="n">Dense</code><code>
</code><code>         </code><code class="kn">from</code><code> </code><code class="nn">keras.models</code><code> </code><code class="kn">import</code><code> </code><code class="n">Sequential</code><code>
</code><code>         </code><code class="n">Using</code><code> </code><code class="n">TensorFlow</code><code> </code><code class="n">backend</code><code class="o">.</code><code>
</code><code>
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">16</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">model</code><code> </code><code class="o">=</code><code> </code><code class="n">Sequential</code><code class="p">(</code><code class="p">)</code><code>  </code><a class="co" id="co_superintelligence_CO3-1" href="#callout_superintelligence_CO3-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>
</code><code>         </code><code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dense</code><code class="p">(</code><code class="mi">64</code><code class="p">,</code><code> </code><code class="n">activation</code><code class="o">=</code><code class="s1">'</code><code class="s1">relu</code><code class="s1">'</code><code class="p">,</code><code>
</code><code>                         </code><code class="n">input_dim</code><code class="o">=</code><code class="n">env</code><code class="o">.</code><code class="n">observation_space</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="p">)</code><code class="p">)</code><code>  </code><a class="co" id="co_superintelligence_CO3-2" href="#callout_superintelligence_CO3-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>
</code><code>         </code><code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dense</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code><code> </code><code class="n">activation</code><code class="o">=</code><code class="s1">'</code><code class="s1">sigmoid</code><code class="s1">'</code><code class="p">)</code><code class="p">)</code><code>  </code><a class="co" id="co_superintelligence_CO3-3" href="#callout_superintelligence_CO3-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>
</code><code>         </code><code class="n">model</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">loss</code><code class="o">=</code><code class="s1">'</code><code class="s1">binary_crossentropy</code><code class="s1">'</code><code class="p">,</code><code>
</code><code>                       </code><code class="n">optimizer</code><code class="o">=</code><code class="s1">'</code><code class="s1">adam</code><code class="s1">'</code><code class="p">,</code><code>
</code><code>                       </code><code class="n">metrics</code><code class="o">=</code><code class="p">[</code><code class="s1">'</code><code class="s1">acc</code><code class="s1">'</code><code class="p">]</code><code class="p">)</code><code>  </code><a class="co" id="co_superintelligence_CO3-4" href="#callout_superintelligence_CO3-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>
</code><code>
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">17</code><code class="p">]</code><code class="p">:</code><code> </code><code class="o">%</code><code class="o">%</code><code class="n">time</code><code>
</code><code>         </code><code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">data</code><code class="p">[</code><code class="p">[</code><code class="s1">'</code><code class="s1">s1</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="s1">'</code><code class="s1">s2</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="s1">'</code><code class="s1">s3</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="s1">'</code><code class="s1">s4</code><code class="s1">'</code><code class="p">]</code><code class="p">]</code><code class="p">,</code><code> </code><code class="n">data</code><code class="p">[</code><code class="s1">'</code><code class="s1">a</code><code class="s1">'</code><code class="p">]</code><code class="p">,</code><code>
</code><code>                   </code><code class="n">epochs</code><code class="o">=</code><code class="mi">25</code><code class="p">,</code><code> </code><code class="n">verbose</code><code class="o">=</code><code class="bp">False</code><code class="p">,</code><code> </code><code class="n">validation_split</code><code class="o">=</code><code class="mf">0.2</code><code class="p">)</code><code>  </code><a class="co" id="co_superintelligence_CO3-5" href="#callout_superintelligence_CO3-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a><code>
</code><code>         </code><code class="n">CPU</code><code> </code><code class="n">times</code><code class="p">:</code><code> </code><code class="n">user</code><code> </code><code class="mf">1.02</code><code> </code><code class="n">s</code><code class="p">,</code><code> </code><code class="n">sys</code><code class="p">:</code><code> </code><code class="mi">166</code><code> </code><code class="n">ms</code><code class="p">,</code><code> </code><code class="n">total</code><code class="p">:</code><code> </code><code class="mf">1.18</code><code> </code><code class="n">s</code><code>
</code><code>         </code><code class="n">Wall</code><code> </code><code class="n">time</code><code class="p">:</code><code> </code><code class="mi">797</code><code> </code><code class="n">ms</code><code>
</code><code>
</code><code class="n">Out</code><code class="p">[</code><code class="mi">17</code><code class="p">]</code><code class="p">:</code><code> </code><code class="o">&lt;</code><code class="n">keras</code><code class="o">.</code><code class="n">callbacks</code><code class="o">.</code><code class="n">callbacks</code><code class="o">.</code><code class="n">History</code><code> </code><code class="n">at</code><code> </code><code class="mh">0x7ffa53685190</code><code class="o">&gt;</code><code>
</code><code>
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">18</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">res</code><code> </code><code class="o">=</code><code> </code><code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">history</code><code class="o">.</code><code class="n">history</code><code class="p">)</code><code>  </code><a class="co" id="co_superintelligence_CO3-6" href="#callout_superintelligence_CO3-3"><img src="Images/3.png" alt="3" width="12" height="12"/></a><code>
</code><code>         </code><code class="n">res</code><code class="o">.</code><code class="n">tail</code><code class="p">(</code><code class="mi">3</code><code class="p">)</code><code>  </code><a class="co" id="co_superintelligence_CO3-7" href="#callout_superintelligence_CO3-3"><img src="Images/3.png" alt="3" width="12" height="12"/></a><code>
</code><code class="n">Out</code><code class="p">[</code><code class="mi">18</code><code class="p">]</code><code class="p">:</code><code>     </code><code class="n">val_loss</code><code>  </code><code class="n">val_acc</code><code>      </code><code class="n">loss</code><code>       </code><code class="n">acc</code><code>
</code><code>         </code><code class="mi">22</code><code>  </code><code class="mf">0.660300</code><code>  </code><code class="mf">0.59375</code><code>  </code><code class="mf">0.646965</code><code>  </code><code class="mf">0.626632</code><code>
</code><code>         </code><code class="mi">23</code><code>  </code><code class="mf">0.660828</code><code>  </code><code class="mf">0.59375</code><code>  </code><code class="mf">0.646794</code><code>  </code><code class="mf">0.621410</code><code>
</code><code>         </code><code class="mi">24</code><code>  </code><code class="mf">0.659114</code><code>  </code><code class="mf">0.59375</code><code>  </code><code class="mf">0.645908</code><code>  </code><code class="mf">0.626632</code></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_superintelligence_CO3-1" href="#co_superintelligence_CO3-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>A neural network with one hidden layer only is used.</p></dd>
<dt><a class="co" id="callout_superintelligence_CO3-2" href="#co_superintelligence_CO3-5"><img src="Images/2.png" alt="2" width="12" height="12"/></a></dt>
<dd><p>The model is trained on the previously collected data.</p></dd>
<dt><a class="co" id="callout_superintelligence_CO3-3" href="#co_superintelligence_CO3-6"><img src="Images/3.png" alt="3" width="12" height="12"/></a></dt>
<dd><p>The metrics per training step are shown for the final few steps.</p></dd>
</dl>

<p>The trained neural network, or AI agent, can then play the <code>CartPole</code> game given 
<span class="keep-together">its learned</span> best actions for any state it is presented with. The AI agent achieves the maximum total reward of 200 for each of the 100 games played. This is based on a 
<span class="keep-together">relatively</span> small data set in combination with a relatively simple neural network:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">20</code><code class="p">]</code><code class="p">:</code><code> </code><code class="k">def</code><code> </code><code class="nf">epoch</code><code class="p">(</code><code class="p">)</code><code class="p">:</code><code>
</code><code>             </code><code class="n">done</code><code> </code><code class="o">=</code><code> </code><code class="bp">False</code><code>
</code><code>             </code><code class="n">state</code><code> </code><code class="o">=</code><code> </code><code class="n">env</code><code class="o">.</code><code class="n">reset</code><code class="p">(</code><code class="p">)</code><code>
</code><code>             </code><code class="n">treward</code><code> </code><code class="o">=</code><code> </code><code class="mi">1</code><code>
</code><code>             </code><code class="k">while</code><code> </code><code class="ow">not</code><code> </code><code class="n">done</code><code class="p">:</code><code>
</code><code>                 </code><code class="n">action</code><code> </code><code class="o">=</code><code> </code><code class="n">np</code><code class="o">.</code><code class="n">where</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">atleast_2d</code><code class="p">(</code><code class="n">state</code><code class="p">)</code><code class="p">)</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code> </code><code class="o">&gt;</code><code> </code><code>\
</code><code>                          </code><code class="mf">0.5</code><code class="p">,</code><code> </code><code class="mi">1</code><code class="p">,</code><code> </code><code class="mi">0</code><code class="p">)</code><code>  </code><a class="co" id="co_superintelligence_CO4-1" href="#callout_superintelligence_CO4-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>
</code><code>                 </code><code class="n">state</code><code class="p">,</code><code> </code><code class="n">reward</code><code class="p">,</code><code> </code><code class="n">done</code><code class="p">,</code><code> </code><code class="n">_</code><code> </code><code class="o">=</code><code> </code><code class="n">env</code><code class="o">.</code><code class="n">step</code><code class="p">(</code><code class="n">action</code><code class="p">)</code><code>  </code><a class="co" id="co_superintelligence_CO4-2" href="#callout_superintelligence_CO4-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a><code>
</code><code>                 </code><code class="n">treward</code><code> </code><code class="o">+</code><code class="o">=</code><code> </code><code class="n">reward</code><code> </code><code class="k">if</code><code> </code><code class="ow">not</code><code> </code><code class="n">done</code><code> </code><code class="k">else</code><code> </code><code class="mi">0</code><code>
</code><code>             </code><code class="k">return</code><code> </code><code class="n">treward</code><code>
</code><code>
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">21</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">res</code><code> </code><code class="o">=</code><code> </code><code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="p">[</code><code class="n">epoch</code><code class="p">(</code><code class="p">)</code><code> </code><code class="k">for</code><code> </code><code class="n">_</code><code> </code><code class="ow">in</code><code> </code><code class="nb">range</code><code class="p">(</code><code class="mi">100</code><code class="p">)</code><code class="p">]</code><code class="p">)</code><code>
</code><code>         </code><code class="n">res</code><code> </code><a class="co" id="co_superintelligence_CO4-3" href="#callout_superintelligence_CO4-3"><img src="Images/3.png" alt="3" width="12" height="12"/></a><code>
</code><code class="n">Out</code><code class="p">[</code><code class="mi">21</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">array</code><code class="p">(</code><code class="p">[</code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code>
</code><code>                </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code>
</code><code>                </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code>
</code><code>                </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code>
</code><code>                </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code>
</code><code>                </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code>
</code><code>                </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code>
</code><code>                </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code>
</code><code>                </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code> </code><code class="mf">200.</code><code class="p">,</code><code>
</code><code>                </code><code class="mf">200.</code><code class="p">]</code><code class="p">)</code><code>
</code><code>
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">22</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">res</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="p">)</code><code>  </code><a class="co" id="co_superintelligence_CO4-4" href="#callout_superintelligence_CO4-4"><img src="Images/4.png" alt="4" width="12" height="12"/></a><code>
</code><code class="n">Out</code><code class="p">[</code><code class="mi">22</code><code class="p">]</code><code class="p">:</code><code> </code><code class="mf">200.0</code></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_superintelligence_CO4-1" href="#co_superintelligence_CO4-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>Chooses an action given the state and the trained model</p></dd>
<dt><a class="co" id="callout_superintelligence_CO4-2" href="#co_superintelligence_CO4-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a></dt>
<dd><p>Moves the environment one step forward based on the learned action</p></dd>
<dt><a class="co" id="callout_superintelligence_CO4-3" href="#co_superintelligence_CO4-3"><img src="Images/3.png" alt="3" width="12" height="12"/></a></dt>
<dd><p>Plays a number of games and records the total reward for each game</p></dd>
<dt><a class="co" id="callout_superintelligence_CO4-4" href="#co_superintelligence_CO4-4"><img src="Images/4.png" alt="4" width="12" height="12"/></a></dt>
<dd><p>Calculates the average total reward for all games</p></dd>
</dl>

<p><a data-type="indexterm" data-primary="ALE (Arcade Learning Environment)" id="idm45625332307496"/>The Arcade Learning Environment (ALE) works similarly to OpenAI Gym. It allows one to programmatically interact with emulated Atari 2600 games, take actions, collect the results from a taken action, and so on. The task of learning to play <em>Breakout</em>, for example, is of course more involved, if only because the state space is much larger. The basic approach, however, is similar to the one taken here, with several algorithmic refinements.<a data-type="indexterm" data-primary="" data-startref="ix_atari_success" id="idm45625332305960"/><a data-type="indexterm" data-primary="" data-startref="ix_neuralnet_atari" id="idm45625332304984"/><a data-type="indexterm" data-primary="" data-startref="ix_RL_atari" id="idm45625332304040"/><a data-type="indexterm" data-primary="" data-startref="ix_cartpole_nn_app" id="idm45625341475896"/><a data-type="indexterm" data-primary="" data-startref="ix_neuralnet_cartpole" id="idm45625341474952"/><a data-type="indexterm" data-primary="" data-startref="ix_openai_gym_env_c2" id="idm45625341474008"/></p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Go"><div class="sect2" id="idm45625341726072">
<h2>Go</h2>

<p><a data-type="indexterm" data-primary="Go, game of" id="ix_go_game"/>The board game <a href="http://bit.ly/aiif_go">Go</a> is more than 2,000 years old. It has long been considered a creation of beauty and art—because it is simple in principle but nevertheless highly complex—and was expected to withstand the advance of game-playing AI agents for decades to come. The strength of a Go player is measured 
<span class="keep-together">in <em>dans</em>,</span> in line with graduation systems for many martial arts systems. For example, Lee Sedol, who was the Go world champion for years, holds the 9th dan. In 2014, 
<span class="keep-together">Bostrom</span> postulated:</p>
<blockquote>
<p>Go-playing programs have been improving at a rate of about 1 dan/year in recent years. If this rate of improvement continues, they might beat the human world champion in about a decade.</p></blockquote>

<p><a data-type="indexterm" data-primary="AlphaGo algorithm" id="ix_alpha_go_alg"/>Again, it was a team at DeepMind that was able to achieve breakthroughs for AI agents playing Go with its AlphaGo algorithm (see the AlphaGo page on <a href="https://oreil.ly/y6n5N">DeepMind’s website</a>). In Silver et al. (2016), the researchers describe the situation as follows:</p>
<blockquote>
<p>The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves.</p></blockquote>

<p><a data-type="indexterm" data-primary="Monte Carlo simulations" id="idm45625341715288"/>The members of the team used a combination of a neural network with a Monte Carlo tree search algorithm, which they briefly sketch in their paper. Recounting their early successes from 2015, the team points out in the introduction:</p>
<blockquote>
<p>[O]ur program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the human European Go champion [Fan Hui] by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.</p></blockquote>

<p><a data-type="indexterm" data-primary="Bostrom, Nick" id="idm45625341879288"/>It is remarkable that this milestone was achieved just one year after a leading AI researcher, Nick Bostrom, predicted that it might take another decade to reach that level. Many observers remarked, however, that the beating European Go champion of that time, Fan Hui, cannot really be considered a benchmark since the world Go elite play on a much higher level. <a data-type="indexterm" data-primary="Sedol, Lee" id="idm45625341878072"/>The DeepMind team took on the challenge and organized in March 2016 a best-of-five-games competition against the then 18-time world Go champion Lee Sedol—for sure a proper benchmark for elite-level human Go playing. (A wealth of background information is provided on the <a href="https://oreil.ly/EL51T">AlphaGo Korea web page</a>, and there is even a <a href="https://oreil.ly/1vYQ5">movie</a> available about the event.) To this end, the DeepMind team further improved the AlphaGo Fan version to the AlphaGo Lee iteration.</p>

<p>The story of the competition and AlphaGo Lee is well documented and has drawn attention all over the world. DeepMind writes on its <a href="https://oreil.ly/h0WEs">web page</a>:</p>
<blockquote>
<p>AlphaGo’s 4-1 victory in Seoul, South Korea, on March 2016 was watched by over 200 million people worldwide. This landmark achievement was a decade ahead of its time. The game earned AlphaGo a 9 dan professional ranking, the highest certification. This was the first time a computer Go player had ever received the accolade.</p></blockquote>

<p><a data-type="indexterm" data-primary="AlphaGo Zero (AlphaZero)" id="ix_alpha_zero_alg"/>Up until that point, AlphaGo used, among other resources, training data sets based on millions of human expert games for its supervised learning. The team’s next iteration, AlphaGo Zero, skipped that approach completely and relied instead on reinforcement learning and self-play only, putting together different generations of trained, neural network–based AI agents to compete against each other. Silver et al.’s article (2017b) provides details of AlphaGo Zero. In the abstract, the researchers summarize:</p>
<blockquote>
<p>AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo’s own move selections and also the winner of AlphaGo’s games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100–0 against the previously published, champion-defeating AlphaGo.</p></blockquote>

<p>It is remarkable that a neural network trained not too dissimilarly to the <code>CartPole</code> example from the previous section (that is, based on self-play) can crack a game as complex as Go, whose possible board positions outnumber the atoms of the universe. It is also remarkable that the Go wisdom collected over centuries by human players is simply not necessary to achieve this milestone.<a data-type="indexterm" data-primary="" data-startref="ix_alpha_go_alg" id="idm45625341809288"/></p>

<p>The DeepMind team did not stop there. AlphaZero was intended to be a general game-playing AI agent that was supposed to be able to learn different complex board games, such as Go, chess, and shogi. With regard to AlphaZero, the team summarizes in Silver (2017a):</p>
<blockquote>
<p>In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.<a data-type="indexterm" data-primary="" data-startref="ix_alpha_zero_alg" id="idm45625341806472"/></p></blockquote>

<p>Again, a remarkable milestone was reached by DeepMind in 2017: a game-playing AI agent that, after less than 24 hours of self-playing and training, achieved above-human-expert levels in three intensely studied board games with centuries-long 
<span class="keep-together">histories</span> in each case.<a data-type="indexterm" data-primary="" data-startref="ix_go_game" id="idm45625341720184"/><a data-type="indexterm" data-primary="" data-startref="ix_deepmind_atari" id="idm45625341719176"/></p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Chess"><div class="sect2" id="idm45625341725544">
<h2>Chess</h2>

<p><a data-type="indexterm" data-primary="chess" id="ix_chess_story"/>Chess is, of course, one of the most popular board games in the world. Chess-playing computer programs have been around since the very early days of computing, and in particular, home computing. For example, an almost complete chess engine called <em>ZX Chess</em>, which only consisted of about 672 bytes of machine code, was introduced in 1983 for the ZX-81 Spectrum home computer.<sup><a data-type="noteref" id="idm45625341766120-marker" href="ch02.xhtml#idm45625341766120">6</a></sup> Although an incomplete implementation that lacked certain rules like castling, it was a great achievement at the time and is still fascinating for computer chess fans today. The record of <em>ZX Chess</em> as the smallest chess program stood for 32 years and was broken only by <em>BootChess</em> in 2015, at 487 bytes.<sup><a data-type="noteref" id="idm45625341829208-marker" href="ch02.xhtml#idm45625341829208">7</a></sup></p>

<p>It can almost be considered software engineering genius to write a computer program with such a small code base that can play a board game that has more possible permutations of a game than the universe has atoms. While not being as complex with regard to the pure numbers as Go, chess can be considered one of the most challenging board games, as players take decades to reach grandmaster level.</p>

<p>In the mid-1980s, expert-level computer chess programs were still far away, even on better hardware with many fewer constraints than the basic home computer ZX-81 Spectrum. No wonder then that leading chess players at that time felt confident when playing against computers. <a data-type="indexterm" data-primary="Kasparov, Garry" id="idm45625341479944"/>For example, Garry Kasparov (2017) recalls an event in 1985 during which he played 32 simultaneous games as follows:</p>
<blockquote>
<p>It was a pleasant day in Hamburg in June 6, 1985….Each of my opponents, all thirty-two of them, was a computer…it didn’t come as much of a surprise…when I achieved a perfect 32-0 score.</p></blockquote>

<p><a data-type="indexterm" data-primary="Deep Blue" id="idm45625341684120"/>It took computer chess developers and the hardware experts from International Business Machines Corporation (IBM) 12 years until a computer called Deep Blue was able to beat Kasparov, then the human world chess champion. In his book, published 20 years after his historic loss against Deep Blue, he writes:</p>
<blockquote>
<p>Twelve years later I was in New York City fighting for my chess life. Against just one machine, a $10 million IBM supercomputer nicknamed “Deep Blue.”</p></blockquote>

<p>Kasparov played a total of six games against Deep Blue. The computer won with 3.5 points to Kasparov’s 2.5; whereby a full point is awarded for a win and half a point to each player for a draw. While Deep Blue lost the first game, it would win two of the remaining five, with three games ending in a draw by mutual agreement. It has been pointed out that Deep Blue should not be considered a form of AI since it mainly relied on a huge hardware cluster. This hardware cluster with 30 nodes and 480 special-purpose chess chips—designed by IBM specifically for this event—could analyze some 200 million positions per second. In that sense, Deep Blue mainly relied on brute force techniques rather than modern AI algorithms such as neural networks.</p>

<p>Since 1997, both hardware and software have seen tremendous advancements. Kasparov sums it up as follows when he refers in his book to chess applications on modern smartphones:</p>
<blockquote>
<p>Jump forward another 20 years to today, to 2017, and you can download any number of free chess apps for your phone that rival any human grandmaster.</p></blockquote>

<p>The hardware requirements to beat a human grandmaster have fallen from $10 million to about $100 (that is, by a factor of 100,000). However, chess applications for regular computers and smartphones still rely on the collected wisdom of decades of computer chess. They embody a large number of human-designed rules and strategies for the game, rely on a large database for openings, and then benefit from the increased compute power and memory of modern devices for their mostly brute force–based evaluation of millions of chess positions.</p>

<p><a data-type="indexterm" data-primary="AlphaGo Zero (AlphaZero)" id="ix_alpha_zero_chess"/>This is where AlphaZero comes in. <a data-type="indexterm" data-primary="RL (reinforcement learning)" data-secondary="chess story" id="ix_RL_chess"/>The approach of AlphaZero to mastering 
<span class="keep-together">the game</span> of chess is exclusively based on reinforcement learning with self-play of 
<span class="keep-together">different</span> versions of the AI agent competing against each other. The DeepMind team contrasts the traditional approach to computer chess with AlphaZero as follows (see <a href="https://oreil.ly/Ur-fI">AlphaZero research paper</a>):</p>
<blockquote>
<p><a data-type="indexterm" data-primary="Stockfish chess engine" id="idm45625341489128"/>Traditional chess engines, including the world computer chess champion Stockfish and IBM’s ground-breaking Deep Blue, rely on thousands of rules and heuristics handcrafted by strong human players that try to account for every eventuality in a game….AlphaZero takes a totally different approach, replacing these hand-crafted rules with a deep neural network and general purpose algorithms that know nothing about the game beyond the basic rules.</p></blockquote>

<p>Given this tabula rasa approach of AlphaZero, its performance after a few hours of self-play-based training is exceptional when compared to the leading traditional chess-playing computer programs. AlphaZero only needs nine hours or less of training to master chess on a level that surpasses every human player and every other computer chess program, including the Stockfish engine, which at one time dominated computer chess. In a 2016 test series comprising 1,000 games, AlphaZero beat Stockfish by winning 155 games (mostly while playing white), losing just six games, and drawing the rest.</p>

<p>While IBM’s Deep Blue was able to analyze 200 million positions per second, modern chess engines, such as Stockfish, on many-core commodity hardware, can analyze some 60 million positions per second. At the same time, AlphaZero only analyzes about 60,000 positions per second. Despite analyzing 1,000 times fewer positions per second, it nevertheless is able to beat Stockfish. One might be inclined to think that AlphaZero indeed shows some form of intelligence that sheer brute force cannot compensate for. Given that human grandmasters can maybe analyze a few hundred positions per second based on experience, patterns, and intuition, AlphaZero might inhabit a sweet spot between expert human chess player and traditional chess engine based on a brute-force approach, aided by handcrafted rules and stored chess knowledge. One could speculate that AlphaZero acquires something similar to human pattern recognition, foresight, and intuition combined with higher computational speeds due to its comparatively better hardware for that purpose.<a data-type="indexterm" data-primary="" data-startref="ix_AI_succ_story" id="idm45625341706248"/><a data-type="indexterm" data-primary="" data-startref="ix_chess_story" id="idm45625341867080"/><a data-type="indexterm" data-primary="" data-startref="ix_alpha_zero_chess" id="idm45625341866136"/><a data-type="indexterm" data-primary="" data-startref="ix_RL_chess" id="idm45625341865192"/></p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Importance of Hardware"><div class="sect1" id="si_importance_of_hardware">
<h1>Importance of Hardware</h1>

<p><a data-type="indexterm" data-primary="hardware" data-secondary="advances in AI" id="ix_hardware_AI_imp"/><a data-type="indexterm" data-primary="AI (artificial intelligence)" data-secondary="importance of hardware for progress in" id="ix_AI_hardware_imp"/>AI researchers and practitioners have made tremendous progress over the past decade with regard to AI algorithms. <a data-type="indexterm" data-primary="RL (reinforcement learning)" data-secondary="hardware advances and" id="ix_RL_hardware_adv"/>Reinforcement learning, generally combined with neural networks for action policy representation, has proven useful and superior in many different areas, as the previous section illustrates.</p>

<p><a data-type="indexterm" data-primary="AlphaGo algorithm" id="ix_alpha_go_alg_hardware"/>However, without advances on the hardware side, the recent AI achievements would not have been possible. <a data-type="indexterm" data-primary="DeepMind" id="ix_deepmind_go_hardware"/>Again, the story of DeepMind and its effort to master the game of Go with reinforcement learning (RL) provides some valuable insights. <a data-type="xref" href="#si_hardware_table">Table 2-1</a> provides an overview of the hardware usage and power consumption for the major AlphaGo versions from 2015 onwards.<sup><a data-type="noteref" id="idm45625341895352-marker" href="ch02.xhtml#idm45625341895352">8</a></sup> Not only has the strength of AlphaGo increased steadily, but both the hardware requirements and the associated power consumption have also come down dramatically.<sup><a data-type="noteref" id="idm45625341893368-marker" href="ch02.xhtml#idm45625341893368">9</a></sup></p>
<table id="si_hardware_table">
<caption><span class="label">Table 2-1. </span>DeepMind hardware for AlphaGo</caption>
<thead>
<tr>
<th>Version</th>
<th>Year</th>
<th>Elo rating<sup><a data-type="noteref" id="idm45625341729752-marker" href="ch02.xhtml#idm45625341729752">a</a></sup></th>
<th>Hardware</th>
<th>Power consumption [TDP]</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>AlphaGo Fan</p></td>
<td><p>2015</p></td>
<td><p>&gt;3,000</p></td>
<td><p>176 GPUs</p></td>
<td><p>&gt;40,000</p></td>
</tr>
<tr>
<td><p>AlphaGo Lee</p></td>
<td><p>2016</p></td>
<td><p>&gt;3,500</p></td>
<td><p>48 TPUs</p></td>
<td><p>10,000+</p></td>
</tr>
<tr>
<td><p>AlphaGo Master</p></td>
<td><p>2016</p></td>
<td><p>&gt;4,500</p></td>
<td><p>4 TPUs</p></td>
<td><p>&lt;2,000</p></td>
</tr>
<tr>
<td><p>AlphaGo Zero</p></td>
<td><p>2017</p></td>
<td><p>&gt;5,000</p></td>
<td><p>4 TPUs</p></td>
<td><p>&lt;2,000</p></td>
</tr>
</tbody>
<tbody><tr class="footnotes"><td colspan="5"><p data-type="footnote" id="idm45625341729752"><sup><a href="ch02.xhtml#idm45625341729752-marker">a</a></sup> For the Elo ratings of the world’s best human Go players, see <a href="https://www.goratings.org/en"><em class="hyperlink">https://www.goratings.org/en</em></a>.</p></td></tr></tbody></table>

<p>The first major hardware push in AI came from GPUs. Although developed originally to generate fast high-resolution graphics for computer games, modern GPUs can be used for many other purposes as well. <a data-type="indexterm" data-primary="linear algebra" id="idm45625341803688"/>One of these other purposes involves linear algebra (for example, in the form of matrix multiplication), a mathematical discipline of paramount importance for AI in general and neural networks in particular.</p>

<p>As of mid-2020, one of the fastest consumer CPUs on the market is the Intel i9 processor in its latest iteration (with 8 cores and a maximum of 16 parallel threads).<sup><a data-type="noteref" id="idm45625341802072-marker" href="ch02.xhtml#idm45625341802072">10</a></sup> It reaches, depending on the benchmark task at hand, speeds of about 1 TFLOPS or slightly above (that is, one trillion floating point operations per second).</p>

<p>At the same time, one of the fastest consumer GPUs on the market has been the 
<span class="keep-together">Nvidia</span> GTX 2080 Ti. It has 4,352 CUDA cores, Nvidia’s version of GPU cores. This allows for a high degree of parallelism (for example, in the context of linear algebra operations). This GPU reaches a speed of up to 15 TFLOPS, which is about 15 times faster than the fastest consumer CPU from Intel. GPUs have been faster than CPUs for quite a while. However, one major limiting factor usually has been the relatively small and specialized memory of GPUs. This has been notably mitigated with newer GPU models, such as the GTX 2080 Ti, which has up to 11 GB of fast GDDR6 memory and high bus speeds to transfer data to and from the GPU.<sup><a data-type="noteref" id="idm45625341512360-marker" href="ch02.xhtml#idm45625341512360">11</a></sup></p>

<p>In mid-2020, the retail price for such a GPU was about $1,400, which is orders of magnitude cheaper than comparably powerful hardware a decade ago. This development has made AI research, for example, more affordable for individual academic researchers with relatively small budgets compared to those of companies such as DeepMind.</p>

<p>Another hardware trend is spurring further developments and adoption of AI approaches and algorithms: GPUs and TPUs in the cloud. <a data-type="indexterm" data-primary="cloud options for AI hardware" id="idm45625341862328"/>Cloud providers such as Scaleway offer cloud instances that can be rented by the hour and that have powerful GPUs available (see <a href="https://oreil.ly/bkaH3">Scaleway GPU instances</a>). Others such as Google have developed TPUs, chips dedicated explicitly to AI, that, similar to GPUs, make linear algebra operations more efficient (see <a href="https://oreil.ly/xnmdw">Google TPUs</a>).</p>

<p>All in all, from the point of view of AI, hardware has improved tremendously over the last few years. In summary, three aspects are worth highlighting:</p>
<dl>
<dt>Performance</dt>
<dd>
<p>GPUs and TPUs provide hardware with heavily parallel architectures that are well suited to AI algorithms and neural networks.</p>
</dd>
<dt>Costs</dt>
<dd>
<p>The costs per TFLOPS compute power have come down significantly, allowing for smaller AI-related budgets or rather more compute power for the same budget.</p>
</dd>
<dt>Power</dt>
<dd>
<p>Power consumption has come down as well. The same AI-related tasks require less power while usually also executing much faster.<a data-type="indexterm" data-primary="" data-startref="ix_AI_hardware_imp" id="idm45625332023224"/><a data-type="indexterm" data-primary="" data-startref="ix_alpha_go_alg_hardware" id="idm45625332022216"/><a data-type="indexterm" data-primary="" data-startref="ix_deepmind_go_hardware" id="idm45625332021304"/><a data-type="indexterm" data-primary="" data-startref="ix_hardware_AI_imp" id="idm45625332020360"/><a data-type="indexterm" data-primary="" data-startref="ix_RL_hardware_adv" id="idm45625332019416"/></p>
</dd>
</dl>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Forms of Intelligence"><div class="sect1" id="si_forms_intelligence">
<h1>Forms of Intelligence</h1>

<p><a data-type="indexterm" data-primary="intelligence, defining for AI" id="idm45625341825144"/><a data-type="indexterm" data-primary="AI (artificial intelligence)" data-secondary="forms of intelligence" id="idm45625341824472"/>Is AlphaGo Zero intelligent? It’s hard to tell without a specific definition of <em>intelligence</em>. AI researcher Max Tegmark (2017) defines intelligence concisely as the “ability to accomplish complex goals.”</p>

<p><a data-type="indexterm" data-primary="AlphaGo Zero (AlphaZero)" id="idm45625341844552"/>This definition is general enough to encompass more specific definitions. AlphaZero is intelligent given that definition since it is able to accomplish a complex goal, namely to win games of Go or chess against human players or other AI agents. Of course, human beings, and animals in general, are consequently considered intelligent as well.</p>

<p>For the purposes of this book, the following more specific definitions seem appropriate and precise enough.</p>
<dl>
<dt>Artificial narrow intelligence (ANI)</dt>
<dd>
<p><a data-type="indexterm" data-primary="ANI (artificial narrow intelligence)" id="idm45625341841320"/>This specifies an AI agent that exceeds human-expert-level capabilities and skills in a narrow field. AlphaZero can be considered an ANI in the fields of Go, chess, and shogi. An algorithmic stock-trading AI agent that realizes a net return of consistently 100% per year (per anno) on the invested capital could be considered an ANI.</p>
</dd>
<dt>Artificial general intelligence (AGI)</dt>
<dd>
<p><a data-type="indexterm" data-primary="AGI (artificial general intelligence)" id="idm45625341856568"/>This specifies an AI agent that reaches human-level intelligence in any field, such as chess, mathematics, text composition, or finance, and might exceed human-level intelligence in some other domains.</p>
</dd>
<dt>Superintelligence (SI)</dt>
<dd>
<p><a data-type="indexterm" data-primary="SI (superintelligence)" id="ix_SI_gen"/>This specifies an intellect or AI agent that exceeds human-level intelligence in any respect.</p>
</dd>
</dl>

<p>An ANI has the ability to reach a complex goal in a narrow field on a level higher than any human. An AGI is equally as good as any human being in achieving complex goals in a wide variety of fields. Finally, a superintelligence is significantly better than any human being, or even a collective of human beings, at achieving complex goals in almost any conceivable field.</p>

<p><a data-type="indexterm" data-primary="Bostrom, Nick" id="idm45625341115800"/>The preceding definition of superintelligence is in line with the one provided by Bostrom in his book titled <em>Superintelligence</em> (2014):</p>
<blockquote>
<p>We can tentatively define a superintelligence as <em>any intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest.</em></p></blockquote>

<p>As defined earlier, the technological singularity is the point in time from which a superintelligence exists. However, which paths might lead to superintelligence? This is the topic of the next section.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Paths to Superintelligence"><div class="sect1" id="si_paths_to_superintelligence">
<h1>Paths to Superintelligence</h1>

<p>Researchers and practitioners alike have debated for years whether it is possible to create a superintelligence. Estimates for the materialization of the technological singularity range from a few years to decades, to centuries, to never. No matter whether one believes in the feasibility of a superintelligence or not, the discussion of potential paths to achieve it is a fruitful one.</p>

<p>First, the following is a somewhat longer quote from Bostrom (2014, ch. 2), which sets out some general considerations that probably are valid for any potential path to superintelligence:</p>
<blockquote>
<p>We can, however, discern some general features of the kind of system that would be required. It now seems clear that a capacity to learn would be an integral feature of the core design of a system intended to attain general intelligence, not something to be tacked on later as an extension or an afterthought. The same holds for the ability to deal effectively with uncertainty and probabilistic information. Some faculty for extracting useful concepts from sensory data and internal states, and for leveraging acquired concepts into flexible combinatorial representations for use in logical and intuitive reasoning, also likely belong among the core design features in a modern AI intended to attain general intelligence.</p></blockquote>

<p><a data-type="indexterm" data-primary="AlphaGo Zero (AlphaZero)" id="idm45625341564584"/>These general features are reminiscent of the approach and capabilities of AlphaZero, although terms like <em>intuitive</em> might need to be defined to apply to an AI agent. But how to practically implement these general features? Bostrom (2014, ch. 2) discusses five possible paths, explored in the following sub-sections.</p>








<section data-type="sect2" data-pdf-bookmark="Networks and Organizations"><div class="sect2" id="idm45625341563032">
<h2>Networks and Organizations</h2>

<p><a data-type="indexterm" data-primary="SI (superintelligence)" data-secondary="networks and organizations" id="idm45625341561864"/>The first path to a superintelligent intellect is via networks and organizations involving a possibly large number of human beings, coordinated in such a way that their individual intelligences are amplified and working synchronously. Teams, comprising people with different skills, are a simple example of such a network or organization. One example mentioned often in this context is the team of leading experts that the United States government assembled for the Manhattan Project to build nuclear weapons as a means to decisively end World War II.</p>

<p>This path seems to have natural limits since the individual capabilities and capacities of a single human being are relatively fixed. Evolution also has shown that human beings have difficulty coordinating within networks and organizations of more than 150 individuals. Large corporations often form much smaller teams, departments, or groups than that.</p>

<p>On the other hand, networks of computers and machines, such as the internet, tend to work mostly seamlessly, even with millions of compute nodes. Such networks are today at least capable of organizing humankind’s knowledge and other data (sounds, pictures, videos, and so on). And, of course, AI algorithms already help humans navigate all this knowledge and data. However, it is doubtful whether a superintelligence might arise “spontaneously,” say, from the internet. A dedicated effort seems required from today’s perspective.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Biological Enhancements"><div class="sect2" id="idm45625341875320">
<h2>Biological Enhancements</h2>

<p><a data-type="indexterm" data-primary="biological enhancements, superintelligence" id="idm45625341873880"/><a data-type="indexterm" data-primary="humans, improving cognitive and physical performance of" id="idm45625341873208"/><a data-type="indexterm" data-primary="SI (superintelligence)" data-secondary="biological enhancements" id="idm45625341872568"/>A lot of effort is spent these days on improving the cognitive and physical performance of individual human beings. From more natural approaches, such as better training and learning methods, to those involving substances, such as supplements or smart and even psychedelic drugs, to those involving special tools, humankind today tries more than ever to systematically and scientifically improve the cognitive and physical performance of individuals. Harari (2015) describes this effort as the quest of <em>homo sapiens</em> to create a new and better version of itself, <em>homo deus</em>.</p>

<p>However, this approach again faces the obstacle that human hardware is basically fixed. It has evolved over hundreds of thousands of years and will probably continue to do so for the foreseeable future. But this will happen at a rather slow pace and over many generations only. It will also happen only to a very small extent, since natural selection for human beings plays a reduced role nowadays, and natural selection is what gives evolution its power for improvement. Domingos (2015, ch. 5) discusses central aspects of progress through evolution.</p>

<p><a data-type="indexterm" data-primary="versions of life, Tegmark’s" id="idm45625341853272"/>In this context, it is helpful to think in terms of the <em>versions of life</em> as outlined in 
<span class="keep-together">Tegmark</span> (2017, ch. 1):</p>

<ul>
<li>
<p><strong>Life 1.0</strong> (biological): Life-forms with basically fixed hardware (biological bodies) and software (genes). Both are slowly evolved simultaneously through evolution. Examples are bacteria or insects.</p>
</li>
<li>
<p><strong>Life 2.0</strong> (cultural): Life-forms with basically fixed and slowly evolving hardware but mostly designed and learned software (genes plus language, knowledge, skills, etc.). An example is human beings.</p>
</li>
<li>
<p><strong>Life 3.0</strong> (technological): Life-forms with designed and adjustable hardware and fully learned and evolved software. An example would be a superintelligence created with computer hardware, software, and AI algorithms.</p>
</li>
</ul>

<p>With technological life embodied in a machine superintelligence, the limitations of the available hardware would more or less completely vanish. Therefore, paths to superintelligence other than networks or biological enhancements might prove more promising for the time being.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Brain-Machine Hybrids"><div class="sect2" id="idm45625341875048">
<h2>Brain-Machine Hybrids</h2>

<p><a data-type="indexterm" data-primary="SI (superintelligence)" data-secondary="brain-machine hybrids" id="idm45625341798408"/><a data-type="indexterm" data-primary="brain-machine hybrids" id="idm45625341797432"/>The hybrid approach to improving human performance in any field is omnipresent in our lives and symbolized by the use of diverse hardware and software tools by humans. Humankind has used tools since its beginning. Today, billions of people carry a smartphone with Google Maps on it, allowing for easy navigation even through areas and cities they have never been to before. This is a luxury our ancestors did not have, so they needed to acquire navigation skills based on objects seen in the sky or use much less sophisticated tools, such as a compass.</p>

<p><a data-type="indexterm" data-primary="chess" id="idm45625341795800"/>In the context of chess, for example, it is not the case that humans stopped playing once computers, such as Deep Blue, were proven to be superior. To the contrary, improvements in the performance of computer chess programs have made them indispensable tools for every grandmaster to systematically improve their game. The human grandmaster and the fast-calculating chess engine form a human-machine team that, everything else equal, performs better than a human alone. There are even chess tournaments during which humans play against each other while making use of a computer to come up with the next move.</p>

<p>Similarly, one can imagine directly connecting the human brain to a machine via appropriate interfaces such that the brain could communicate properly with the machine, exchanging data and initiating certain computational, analysis, or learning tasks. What sounds like science fiction is an active field of research. <a data-type="indexterm" data-primary="Musk, Elon" id="idm45625341793624"/><a data-type="indexterm" data-primary="Neuralink" id="idm45625341792920"/>Musk, ElonFor example, Elon Musk is the founder behind a startup called Neuralink, which focuses on <em>neurotech</em>, as the field often is called.</p>

<p>All in all, the brain-machine hybrid seems practically feasible and likely to surpass human intelligence significantly. However, whether it will lead to superintelligence is not obvious.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Whole Brain Emulation"><div class="sect2" id="idm45625341737416">
<h2>Whole Brain Emulation</h2>

<p><a data-type="indexterm" data-primary="SI (superintelligence)" data-secondary="whole brain emulation" id="idm45625341736328"/><a data-type="indexterm" data-primary="WBE (whole brain emulation)" id="idm45625341735352"/>Another suggested path to superintelligence is to first emulate the human brain completely and then improve it. The idea here is to map a whole human brain by modern brain scanning along with biological and medical analysis methods to exactly replicate its structure in the form of neurons, synapses, and so on through software. The software is to be run on appropriate hardware. Domingos (2015, ch. 4) gives background information about the human brain and what characterizes it with regard to learning. Kurzweil (2012) offers a book-length treatment of this topic, providing detailed background information and sketching out ways to achieve whole brain emulation (WBE, sometimes also called <em>uploading</em>).<sup><a data-type="noteref" id="idm45625341733496-marker" href="ch02.xhtml#idm45625341733496">12</a></sup></p>

<p><a data-type="indexterm" data-primary="neural networks" data-secondary="whole brain emulation and" id="idm45625341731688"/>On a less ambitious level, neural networks do exactly what WBE tries to achieve. Neural networks, as the name suggests, are inspired by the brain, and because they have already proven so useful and successful in many different areas, one might be inclined to conclude that WBE could indeed be considered a viable path to superintelligence. However, the necessary technology to map out the complete human brain is so far only partially available. Even if the mapping out is successful, it is not clear whether the software version would be able to do the same things that a human brain is capable of.</p>

<p>However, if WBE is successful, then the human brain software could, for example, be run on more powerful and faster hardware than the human body, potentially leading to superintelligence. The software could also be easily replicated then, and a large number of emulated brains could be put together in a coordinated way, also potentially leading to superintelligence. The human brain software could also be enhanced in ways that humans are incapable of due to biological limitations.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Artificial Intelligence"><div class="sect2" id="idm45625341754328">
<h2>Artificial Intelligence</h2>

<p><a data-type="indexterm" data-primary="SI (superintelligence)" data-secondary="artificial intelligence as path to" id="ix_SI_AI_path"/><a data-type="indexterm" data-primary="AI (artificial intelligence)" data-secondary="SI and" id="ix_AI_SI_and"/>Last, but not least, AI itself as understood in the context of this book might lead to superintelligence: algorithms, such as neural networks, run on standard or specialized hardware and are trained on available or self-created data. There are a number of good reasons why most researchers and practitioners consider this path to be the most likely one, if superintelligence is achievable at all.</p>

<p>The first major reason is that historically humans have been successful in engineering often by ignoring what nature and evolution have come up with to solve a certain problem. Consider airplanes. Their design makes use of the modern understanding of physics, aerodynamics, thermodynamics, and so on instead of trying to mimic how birds or insects fly. Or consider a calculator. When engineers built the first calculators, they did not analyze how the human brain performs calculations, nor did they even try to replicate the biological approach. They rather relied on mathematical algorithms that they implemented on technical hardware. In both cases, the more important aspect is the functionality or capability itself (flying, calculating). The more efficiently it can be provided, the better. There is no need to mimic nature.</p>

<p><a data-type="indexterm" data-primary="ANI (artificial narrow intelligence)" id="idm45625341748424"/>The second major reason is that the number of AI’s success stories seems ever increasing. For example, the application of neural networks to domains that only a few years ago seemed immune to AI superiority has proven to be a fruitful path to ANIs in many fields. The example of AlphaGo morphing into AlphaZero, mastering multiple board games in a short amount of time, is one that gives hope that the generalization can be pushed much further.</p>

<p>The third major reason is that a superintelligence probably only appears (“singularity”) after many ANIs and maybe even some AGIs have been observed. Since there is no doubt about the power of AI in specific fields and domains, researchers and businesses alike will continue to focus on improving AI algorithms and hardware. For example, large hedge funds will push their efforts to generate alpha—a measure for the outperformance of a fund compared to a market benchmark—with AI methods and agents. Many of them have large dedicated teams working on such efforts. These global efforts across different industries might then together yield the required advancements for a superintelligence.</p>
<div data-type="note" epub:type="note"><h1>Artificial Intelligence</h1>
<p>Of all the possible paths to superintelligence, AI seems to be the most promising one. Recent successes in the field based on reinforcement learning and neural networks have led to another AI spring, after a number of AI winters. Many even now believe that a superintelligence might not be as far away as we thought even a few years ago. The field currently is characterized by much faster advancements than originally predicted by experts only a short while ago.<a data-type="indexterm" data-primary="" data-startref="ix_AI_SI_and" id="idm45625333106600"/><a data-type="indexterm" data-primary="" data-startref="ix_SI_AI_path" id="idm45625333105624"/></p>
</div>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Intelligence Explosion"><div class="sect1" id="si_intelligence_explosion">
<h1>Intelligence Explosion</h1>

<p><a data-type="indexterm" data-primary="technological singularity" id="idm45625333103240"/>The quote from Vinge (1993) mentioned earlier not only depicts a dangerous scenario for humankind after the technological singularity, but also predicts that the dangerous scenario will materialize <em>shortly afterwards</em>. Why so quickly?</p>

<p>If there is one superintelligence, then engineers or the superintelligence itself can create another superintelligence, maybe even a better one, since a superintelligence would have superior engineering know-how and skills compared to the creators of the initial one. The replication of the superintelligence would not be constrained by the duration of biological processes that have evolved over millions of years. It would only be constrained by the technical assembling processes for new hardware, which a superintelligence could improve upon itself and in a significant manner. Software is quickly and easily copied to new hardware. Resources might constrain the replication as well. The superintelligence might come up with better or even new ways to mine and produce the required resources.</p>

<p><a data-type="indexterm" data-primary="SI (superintelligence)" data-secondary="reproduction or expansion of" id="idm45625341501656"/>These and similar arguments support the idea that once the technological singularity is reached, there will be an explosion in intelligence. This might happen similarly to the Big Bang, which started as a (physical) singularity and from which the known universe emerged as from an explosion.</p>

<p>With regard to specific fields and ANIs, similar arguments might apply. Suppose an algorithmic trading AI agent is much more successful and consistent performance-wise than other traders and hedge funds in the markets. Such an AI agent would accumulate ever more funds, both from gains of trade and by attracting outside money. This in turn would increase the available budget to improve upon the hardware, the algorithms, the learning methods, and so forth by, for example, paying above-market salaries and incentives to the brightest minds in AI applied to finance.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Goals and Control"><div class="sect1" id="si_control_problem">
<h1>Goals and Control</h1>

<p>In a normal AI context, say, when an AI agent is supposed to master the simple <code>CartPole</code> game depicted in <a data-type="xref" href="#figure_cart_pole">Figure 2-1</a> or a more complex game such as chess or Go, the goal is in general well defined: “reach at least a reward of 200,” “win 
<span class="keep-together">the chess</span> game through checkmate,” and so on. But what about the goal(s) of a 
<span class="keep-together">superintelligence?</span></p>








<section data-type="sect2" data-pdf-bookmark="Superintelligence and Goals"><div class="sect2" id="idm45625341494648">
<h2>Superintelligence and Goals</h2>

<p><a data-type="indexterm" data-primary="SI (superintelligence)" data-secondary="goals of" id="ix_SI_instru_goals"/><a data-type="indexterm" data-primary="instrumental goals of a superintelligence" id="ix_instru_goals_SI"/>For a superintelligence that has superhuman capabilities, the goal might not be as simple and stable as in the preceding examples. For one, a superintelligence might come up with a new goal for itself that it considers more appropriate than its originally formulated and programmed goal. After all, it has the capabilities to do so in the same way its engineering team could. In general, it would be able to reprogram itself in any respect. Many science fiction novels and movies let us believe that such a change in the main goal is in general to the worse for humankind, which is what Vinge (1993) assumes as well.</p>

<p>Even if one assumes that the main goal of a superintelligence can be programmed and embedded in a nonchangeable way or that a superintelligence might simply stick to its original goal, problems might arise. <a data-type="indexterm" data-primary="Bostrom, Nick" id="ix_bostrom_SI_goals"/>Independent of the main goal, Bostrom (2014, ch. 7) argues, every superintelligence has five instrumental sub-goals:</p>
<dl>
<dt>Self-preservation</dt>
<dd>
<p><a data-type="indexterm" data-primary="self-preservation, SI goal of" id="idm45625341376072"/>A long enough survival of the superintelligence is necessary to achieve its main goal. To this end, the superintelligence might implement different measures, some of them maybe harmful to humans, to ensure its survival.</p>
</dd>
<dt>Goal-content integrity</dt>
<dd>
<p><a data-type="indexterm" data-primary="goal-content integrity, SI goal of" id="idm45625341547080"/>This refers to the idea that a superintelligence will try to preserve its current main goal because this increases the probability that its future self will achieve this very goal. Therefore, present and future main goals are likely to be the same. Consider a chess-playing AI agent that starts with the goal of winning a chess game. It might change its goal to avoiding the capturing of its queen at any cost. This might prevent it from winning the game in the end, and such a change in goals would therefore be inconsistent.</p>
</dd>
<dt>Cognitive enhancement</dt>
<dd>
<p><a data-type="indexterm" data-primary="cognitive enhancement, SI goal of" id="idm45625341544584"/>No matter the main goal of the superintelligence, cognitive enhancements will in general prove beneficial. It might therefore strive to increase its capabilities as fast and as far as possible if this seems to serve its main goal. Cognitive enhancement is therefore a major instrumental goal.</p>
</dd>
<dt>Technological perfection</dt>
<dd>
<p><a data-type="indexterm" data-primary="technological perfection, SI goal of" id="idm45625341542360"/>Another instrumental goal is technological perfection. In the sense of Life 3.0, a superintelligence would not be confined to its current hardware nor to the state of its software. It could rather strive to exist on better hardware that it might design and produce, and to make use of improved software that it has coded. This would in general serve its main goal and probably allow for its faster achievement. In the financial industry, for example, high frequency trading (HFT) is a field that is characterized by a race to technological superiority.</p>
</dd>
<dt>Resource acquisition</dt>
<dd>
<p><a data-type="indexterm" data-primary="resource acquisition, SI goal of" id="idm45625341539832"/>For almost any main goal, more resources in general increase both the probability of achieving the goal and the speed at which it can be achieved. This holds particularly true when there is a competitive situation implicit in the goal. Consider an AI agent with the goal of mining as many Bitcoins as possible as fast as possible. The more resources in the form of hardware, energy, and so on the AI agent has available, the better it will be for achieving its goal. In such a situation, it might even come up with illegal practices to acquire (steal) resources from others in the cryptocurrency markets.</p>
</dd>
</dl>

<p>On the surface, instrumental goals might not seem to pose a threat. After all, they ensure that the main goal of an AI agent is achieved. However, as the widely cited example of Bostrom (2014) shows, issues might easily arise. Bostrom argues that, for example, a superintelligence with the goal of maximizing the production of paper clips might pose a serious threat to humankind. To see this, consider the preceding instrumental goals in the context of such an AI agent.</p>

<p>First, it would try to protect itself by all means, even with weaponry used against its own creators. Second, even though its own cognitive reasoning capabilities might suggest that its main goal is not really sensible, it might stick to it over time to maximize its chances of achieving it. Third, cognitive enhancements for sure are valuable in achieving its goal. Therefore, it would try every measure, probably many of them at the expense and to the harm of human beings, to improve its capabilities. Fourth, the better its technology, both for itself as well as for producing paper clips, the better it is for its main goal. It would therefore acquire all existing technology through buying or stealing, for instance, and build new ones that help with its goal. Finally, the more resources it has available, the more paper clips it can produce—up to the point where it builds space exploration and mining technology when resources on earth are exhausted. In the extreme, such a superintelligence might then exhaust the resources in the solar system, the galaxy, and even the whole universe.</p>
<div data-type="caution"><h1>Instrumental Goals</h1>
<p>It is to be assumed that any form of superintelligence will have instrumental goals that are independent of its main goal. This might lead to a number of unintended consequences, such as the insatiable quest to acquire ever more resources with any means that seem promising.</p>
</div>

<p>The example illustrates two important points with regard to goals for AI agents. First, it might not be possible to formulate complex goals for an AI agent in a way that fully and clearly reflects the intentions of those formulating the goal. For example, a noble goal such as “Preserve and protect the human species” might lead to the killing of three-quarters of it to ensure a higher likelihood of survival of the remaining quarter. The superintelligence decides, after billions of simulations for the future on planet earth and for the human species, that this measure leads to the highest probability of achieving its main goal. Second, a seemingly well-intended and harmlessly formulated goal might lead to unintended consequences due to the instrumental goals. In the paper clip example, one problem with the goal is the phrase “as many as possible.” An easy fix here would be to specify the number to, say, one million. But even this might only be a partial fix because instrumental goals, such as self-preservation, might become primary ones.<a data-type="indexterm" data-primary="" data-startref="ix_instru_goals_SI" id="idm45625341645880"/><a data-type="indexterm" data-primary="" data-startref="ix_SI_instru_goals" id="idm45625341644904"/></p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Superintelligence and Control"><div class="sect2" id="idm45625341643832">
<h2>Superintelligence and Control</h2>

<p><a data-type="indexterm" data-primary="SI (superintelligence)" data-secondary="controls on" id="idm45625341642616"/>If bad or even catastrophic consequences are <em>possible</em> after the technological singularity, it is of paramount importance to devise measures that can at least potentially 
<span class="keep-together">control</span> a superintelligence.</p>

<p><a data-type="indexterm" data-primary="motivation selection methods, SI control" id="idm45625341535576"/>The first set of measures is related to the proper formulation and design of the main goal. The previous section discusses this aspect to some extent. Bostrom (2014, ch. 9) provides more details under the topic <em>motivation selection methods</em>.</p>

<p>The second set of measures is related to controlling the capabilities of a superintelligence. Bostrom (2014, ch. 9) sketches four basic approaches.</p>
<dl>
<dt>Boxing</dt>
<dd>
<p><a data-type="indexterm" data-primary="capability controls over SI" id="idm45625341531992"/><a data-type="indexterm" data-primary="boxing approach, SI control" id="idm45625341531320"/>This is an approach that separates a superintelligence in emergence from the outside world. For example, the AI agent might not be connected to the internet. It might also lack any sensory capabilities. Human interaction can also be excluded. Given this approach to control the capabilities, a large set of interesting goals might not be achievable at all. Consider an algorithmically trading AI agent that is supposed to achieve the ANI level. Without being connected to the outside world, such as to stock trading platforms, the AI agent has no chance of achieving its goal.</p>
</dd>
<dt>Incentives</dt>
<dd>
<p><a data-type="indexterm" data-primary="incentives, SI control" id="idm45625341528808"/>An AI agent might be programmed to maximize its reward function for purposefully designed (electronic) rewards that reward desired behavior and punish undesired behavior. Although this indirect approach gives more freedom in the goal design, it suffers to a large extent from problems similar to those of formulating the goal directly.</p>
</dd>
<dt>Stunting</dt>
<dd>
<p><a data-type="indexterm" data-primary="stunting, SI control" id="idm45625341526472"/>This approach refers to deliberately limiting the capabilities of an AI agent, say, with respect to hardware, computing speed, or memory. However, this is a delicate task. Too much stunting and a superintelligence will never emerge. Too little stunting and the ensuing intelligence explosion will render the measure obsolete.</p>
</dd>
<dt>Tripwires</dt>
<dd>
<p><a data-type="indexterm" data-primary="tripwires, SI control" id="idm45625341524152"/>This refers to measures that should help in identifying any suspicious or unwanted behavior early on such that targeted countermeasures can be initiated. This approach, however, suffers the problem of an alarm system alerting the police of a burglary. The police might take 10 minutes to appear on the scene although the burglars left the scene 5 minutes before. Even surveillance camera footage might not help in figuring out who the burglars are.<a data-type="indexterm" data-primary="" data-startref="ix_bostrom_SI_goals" id="idm45625341522856"/></p>
</dd>
</dl>
<div data-type="caution"><h1>Capability Control</h1>
<p>All in all, it seems questionable whether a superintelligence can be properly and systematically controlled when it has reached that level. After all, its superpowers can at least in principle be used to overcome any human-designed control mechanism.</p>
</div>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Potential Outcomes"><div class="sect1" id="si_potential_outcomes">
<h1>Potential Outcomes</h1>

<p><a data-type="indexterm" data-primary="SI (superintelligence)" data-secondary="potential outcomes of" id="ix_SI_outcomes"/><a data-type="indexterm" data-primary="utopian versus dystopian perspectives on AI" id="ix_utopia_dystopia_ch2"/><a data-type="indexterm" data-primary="AI-first finance" data-secondary="utopian versus dystopian outcomes" id="ix_AI-firstfin_utopia"/>Besides the early prophecy of Vinge (1993) that the emergence of a superintelligence will imply doomsday for humankind, what potential outcomes and scenarios are 
<span class="keep-together">conceivable?</span></p>

<p>More and more AI researchers and practitioners warn about potential threats that uncontrolled AI might bring. Before the emergence of superintelligence, AI can lead to discrimination, social imbalances, financial risks, and so on. (A prominent AI critic in this context is Elon Musk, founder of Tesla, SpaceX, and the aforementioned Neuralink, among others.) Therefore, AI ethics and governance are intensively debated topics among researchers and practitioners. To simplify things, one can say that this group fears an AI-induced <em>dystopia</em>. Others, like Ray Kurzweil (2005, 2012), emphasize that AI might be the only way to utopia.</p>

<p>The problem in this context is that even a relatively low probability for a dystopian outcome is enough to be worried. As the previous section illustrates, appropriate control mechanisms might not be available given the state of the art. Against this background, it is no wonder that at the time of this writing, the first international accord on AI development has been signed by 42 countries.</p>
<div style="page-break-after: always;"/>

<p>As Murgia and Shrikanth (2019) report in the <em>Financial Times</em>:</p>
<blockquote>
<p>In a historic step last week, 42 countries came together to support a global governance framework for one of the most powerful emerging technologies of our times—artificial intelligence.</p>

<p>The accord, signed by OECD countries such as the US, UK and Japan, as well as 
<span class="keep-together">non-members,</span> comes at a moment of reckoning for governments, which have only recently begun to grapple with the ethical and practical consequences of applying AI 
<span class="keep-together">in industry</span>….[T]he rapid development of AI in recent years by companies such as 
<span class="keep-together">Google,</span> Amazon, Baidu, Tencent and ByteDance has far outrun regulation in the area, 
<span class="keep-together">exposing</span> major challenges including biased AI decisions, outright fakery and misinformation, and the dangers of automated military weapons.</p></blockquote>
<div data-type="caution"><h1>Utopia Versus Dystopia</h1>
<p>Even strong proponents of a utopian future based on advancements in AI must agree that a dystopian future after a technological singularity cannot be fully excluded. Since the consequences might be catastrophic, dystopian outcomes must play a role in broader discussions about AI and superintelligence.</p>
</div>

<p><a data-type="indexterm" data-primary="technological singularity" data-seealso="SI" id="idm45625341597000"/>What about the number of superintelligences and the situation after the technological singularity? Three basic scenarios seem possible.</p>
<dl>
<dt>Singleton</dt>
<dd>
<p><a data-type="indexterm" data-primary="singleton outcome of technological singularity" id="idm45625341594216"/>A single superintelligence emerges and gains such power that no other can survive or even emerge. For example, Google dominates the search market and has reached almost a monopoly position in the field. A superintelligence might quickly reach comparable positions in many relevant fields and industries soon after its emergence.</p>
</dd>
<dt>Multipolar</dt>
<dd>
<p><a data-type="indexterm" data-primary="multipolar outcome of technological singularity" id="idm45625341591912"/>Multiple superintelligences emerge about the same time and co-exist for a longer period. The hedge fund industry, for instance, has a few large players that can be considered an oligopoly given their combined market share. Multiple superintelligences could similarly co-exist, at least for a certain time, according to a divide-and-conquer agreement between them.</p>
</dd>
<dt>Atomic</dt>
<dd>
<p><a data-type="indexterm" data-primary="atomic outcome of technological singularity" id="idm45625341589576"/>A very large number of superintelligences emerge shortly after the technological singularity. Economically, this scenario resembles a market with perfect competition. Technologically, the evolution of chess provides an analogy for this 
<span class="keep-together">scenario.</span> While IBM in 1997 built a single machine to dominate both the computer and human chess worlds, chess applications on every smartphone today outperform every human chess player. In 2018, there were already more than three 
<span class="keep-together">billion smartphones</span> in use. In this context, it is noteworthy that a recent hardware trend for smartphones is to add dedicated AI chips in addition to the regular CPUs, steadily increasing the capabilities of these small devices.</p>
</dd>
</dl>

<p>This section does not argue for one or another potential outcome after the technological singularity: dystopia, utopia, singleton, multipolar, or atomic. It rather provides a basic framework to think about the potential impact of  superintelligences or powerful ANIs in their respective fields.<a data-type="indexterm" data-primary="" data-startref="ix_SI_gen" id="idm45625341585864"/><a data-type="indexterm" data-primary="" data-startref="ix_AI-firstfin_utopia" id="idm45625341584888"/><a data-type="indexterm" data-primary="" data-startref="ix_SI_outcomes" id="idm45625341583944"/><a data-type="indexterm" data-primary="" data-startref="ix_utopia_dystopia_ch2" id="idm45625341583000"/></p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Conclusions"><div class="sect1" id="idm45625341699016">
<h1>Conclusions</h1>

<p>Recent success stories such as those of DeepMind and AlphaZero have led to a new AI spring, with new and stronger-than-ever hopes that a superintelligence might be achievable. Currently, AI has come up with ANIs that far surpass human expert levels in different domains. Whether AGIs and superintelligences are even possible is still debated. However, it at least can not be excluded that by one path or another—recent experience points toward AI—it can indeed be achieved. Once the technological singularity has happened, it can also not be excluded that a superintelligence might have unintended, negative, or even catastrophic consequences for humankind. Therefore, appropriate goal and incentive design as well as appropriate control mechanisms might be of paramount importance to keep the emerging, ever more powerful AI agents under control, even long before the technological singularity is in sight. Once the singularity is reached, an intelligence explosion might take the control over a superintelligence quickly out of the hands of its own creators and sponsors.</p>

<p>AI, machine learning, neural networks, superintelligence, and technological singularity are topics that are or will be important for any area of human life. Already today, many fields of research, many industries, and many areas of human existence are undergoing fundamental changes due to AI, machine learning, and deep learning. The same holds true for finance and the financial industry, for which the influence of AI might not be as high yet due to a somewhat slower adoption. But as with other fields, AI will change finance and the way players in financial markets operate fundamentally and for good, as later chapters argue.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="References"><div class="sect1" id="idm45625341676200">
<h1>References</h1>

<p>Books and papers cited in this chapter:</p>

<ul class="author-date-bib">
<li>
<p>Barrat, James. 2013. <em>Our Final Invention: Artificial Intelligence and The End of the Human Era</em>. New York: St. Martin’s Press.</p>
</li>
<li>
<p>Bostrom, Nick. 2014. <em>Superintelligence: Paths, Dangers, Strategies</em>. Oxford: Oxford University Press.</p>
</li>
<li>
<p>Chollet, François. 2017. <em>Deep Learning with Python</em>. Shelter Island: Manning.</p>
</li>
<li>
<p>Domingos, Pedro. 2015. <em>The Master Algorithm: How the Quest for the Ultimate Learning Machine will Remake our World</em>. United Kingdom: Penguin Random House.</p>
</li>
<li>
<p>Doudna, Jennifer and Samuel H. Sternberg. 2017. <em>A Crack in Creation: The New Power to Control Evolution</em>. London: The Bodley Head.</p>
</li>
<li>
<p>Gerrish, Sean. 2018. <em>How Smart Machines Think</em>. Cambridge: MIT Press.</p>
</li>
<li>
<p>Harari, Yuval Noah. 2015. <em>Homo Deus: A Brief History of Tomorrow</em>. London: Harvill Secker.</p>
</li>
<li>
<p>Kasparov, Garry. 2017. <em>Deep Thinking: Where Machine Intelligence Ends</em>. London: John Murray.</p>
</li>
<li>
<p>Kurzweil, Ray. 2005. <em>The Singularity Is Near: When Humans Transcend Biology</em>. New York: Penguin Group.</p>
</li>
<li>
<p>⸻. 2012. <em>How to Create a Mind: The Secret of Human Thought Revealed</em>. New York: Penguin Group.</p>
</li>
<li>
<p>Mnih, Volodymyr et al. 2013. “Playing Atari with Deep Reinforcement Learning.” arXiv. December 19, 2013. <a href="https://oreil.ly/HD20U"><em class="hyperlink">https://oreil.ly/HD20U</em></a>.</p>
</li>
<li>
<p>Murgia, Madhumita and Siddarth Shrikanth. 2019. “How Governments Are Beginning to Regulate AI.” <em>Financial Times</em>, May 30, 2019.</p>
</li>
<li>
<p>Silver, David et al. 2016. “Mastering the Game of Go with Deep Neural Networks and Tree Search.” <em>Nature</em> 529 (January): 484-489.</p>
</li>
<li>
<p>⸻. 2017a. “Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm.” arXiv. December 5, 2017. <a href="https://oreil.ly/SBrWQ"><em class="hyperlink">https://oreil.ly/SBrWQ</em></a>.</p>
</li>
<li>
<p>⸻. 2017b. “Mastering the Game of Go without Human Knowledge.” <em>Nature</em>, 550 (October): 354–359. <a href="https://oreil.ly/lB8DH"><em class="hyperlink">https://oreil.ly/lB8DH</em></a>.</p>
</li>
<li>
<p>Shanahan, Murray. 2015. <em>The Technological Singularity</em>. Cambridge: MIT Press.</p>
</li>
<li>
<p>Tegmark, Max. 2017. <em>Life 3.0: Being Human in the Age of Artificial Intelligence</em>. United Kingdom: Penguin Random House.</p>
</li>
<li>
<p>Vinge, Vernor. 1993. “Vernor Vinge on the Singularity.” <a href="https://oreil.ly/NaorT"><em class="hyperlink">https://oreil.ly/NaorT</em></a>.</p>
</li>
</ul>
</div></section>







<div data-type="footnotes"><p data-type="footnote" id="idm45625339005208"><sup><a href="ch02.xhtml#idm45625339005208-marker">1</a></sup> For background and historical information, see <a href="http://bit.ly/aiif_atari"><em class="hyperlink">http://bit.ly/aiif_atari</em></a>.</p><p data-type="footnote" id="idm45625339299256"><sup><a href="ch02.xhtml#idm45625339299256-marker">2</a></sup> For details, refer to Mnih et al. (2013).</p><p data-type="footnote" id="idm45625339295608"><sup><a href="ch02.xhtml#idm45625339295608-marker">3</a></sup> Among other factors, this is made possible by the availability of the <a href="https://oreil.ly/OqnWk">Arcade Learning Environment (ALE)</a> that allows researchers to train AI agents for Atari 2600 games via a standardized API.</p><p data-type="footnote" id="idm45625339281976"><sup><a href="ch02.xhtml#idm45625339281976-marker">4</a></sup> <a data-type="xref" href="ch09.xhtml#reinforcement_learning">Chapter 9</a> revisits this example in more detail.</p><p data-type="footnote" id="idm45625339278824"><sup><a href="ch02.xhtml#idm45625339278824-marker">5</a></sup> More specifically, an AI agent is considered successful if it reaches an average total reward of 195 or more over 100 consecutive games.</p><p data-type="footnote" id="idm45625341766120"><sup><a href="ch02.xhtml#idm45625341766120-marker">6</a></sup> See <a href="http://bit.ly/aiif_1k_chess"><em class="hyperlink">http://bit.ly/aiif_1k_chess</em></a> for an electronic reprint of the original article published in the February 1983 issue of <em>Your Computer</em> and scans of the original code.</p><p data-type="footnote" id="idm45625341829208"><sup><a href="ch02.xhtml#idm45625341829208-marker">7</a></sup> See <a href="http://bit.ly/aiif_bootchess"><em class="hyperlink">http://bit.ly/aiif_bootchess</em></a> for more background.</p><p data-type="footnote" id="idm45625341895352"><sup><a href="ch02.xhtml#idm45625341895352-marker">8</a></sup> For more on this, see: <a href="https://oreil.ly/im174"><em class="hyperlink">https://oreil.ly/im174</em></a>.</p><p data-type="footnote" id="idm45625341893368"><sup><a href="ch02.xhtml#idm45625341893368-marker">9</a></sup> In the table, <em>GPU</em> stands for graphical processing unit. <em>TPU</em> stands for tensor processing unit, which is a computer chip specifically designed to process so-called tensors and operations on tensors more efficiently. More on tensors, which are the basic building blocks of neural networks and deep learning, appears later in the book and in Chollet (2017, ch. 2). <em>TDP</em> stands for thermal design power (see <a href="http://bit.ly/aiif_tdp"><em class="hyperlink">http://bit.ly/aiif_tdp</em></a>).</p><p data-type="footnote" id="idm45625341802072"><sup><a href="ch02.xhtml#idm45625341802072-marker">10</a></sup> <em>CPU</em> stands for central processing unit, the general purpose processors found in any standard desktop or notebook computer.</p><p data-type="footnote" id="idm45625341512360"><sup><a href="ch02.xhtml#idm45625341512360-marker">11</a></sup> For a description of the GDDR6 GPU memory standard from 2018, refer to <a href="http://bit.ly/aiif_gddr6"><em class="hyperlink">http://bit.ly/aiif_gddr6</em></a>.</p><p data-type="footnote" id="idm45625341733496"><sup><a href="ch02.xhtml#idm45625341733496-marker">12</a></sup> In January 2019, an American science fiction thriller called <em>Replicas</em>, starring Keanu Reeves, was released in the US. The main theme of the movie, which proved to be a commercial failure, is the mapping of the human brain and the transfer of the mapping to machines or even other human bodies grown through cloning and replication. The movie touches on a centuries-old human desire to transcend the human body and to become immortal, at least with regard to mind and soul. Even if WBE might not lead to superintelligence, it might theoretically be a basis for achieving this kind of immortality.</p></div></div></section></div>



  </body></html>