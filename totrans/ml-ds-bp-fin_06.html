<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 4. Supervised Learning: Models and Concepts"><div class="chapter" id="Chapter4">
<h1><span class="label">Chapter 4. </span>Supervised Learning: Models and Concepts</h1>


<p><a data-type="indexterm" data-primary="supervised learning" id="ix_Chapter4-asciidoc0"/>Supervised <a data-type="indexterm" data-primary="supervised learning" data-secondary="defined" id="idm45174933193400"/>learning is an area of machine learning where the chosen algorithm tries to fit a target using the given input. A set of training data that contains labels is supplied to the algorithm. Based on a massive set of data, the algorithm will learn a rule that it uses to predict the labels for new observations. In other words, supervised learning algorithms are provided with historical data and asked to find the relationship that has the best predictive power.</p>

<p><a data-type="indexterm" data-primary="classification" data-secondary="regression versus" id="idm45174933191560"/><a data-type="indexterm" data-primary="regression" data-secondary="classification versus" id="idm45174933190584"/>There are two varieties of supervised learning algorithms: regression and classification algorithms. Regression-based supervised learning methods try to predict outputs based on input variables. Classification-based supervised learning methods identify which category a set of data items belongs to. Classification algorithms are probability-based, meaning the outcome is the category for which the algorithm finds the highest probability that the dataset belongs to it. Regression algorithms, in contrast, estimate the outcome of problems that have an infinite number of solutions (continuous set of possible outcomes).</p>

<p>In the context of finance, supervised learning models represent one of the most-used class of machine learning models. Many algorithms that are widely applied in algorithmic trading rely on supervised learning models because they can be efficiently trained, they are relatively robust to noisy financial data, and they have strong links to the theory of finance.</p>

<p><a data-type="indexterm" data-primary="asset price prediction" id="idm45174933187816"/>Regression-based algorithms have been leveraged by academic and industry researchers to develop numerous asset pricing models. These models are used to predict returns over various time periods and to identify significant factors that drive asset returns. There are many other use cases of regression-based supervised learning in portfolio management and derivatives pricing.</p>

<p>Classification-based algorithms, on the other hand, have been leveraged across many areas within finance that require predicting a categorical response. These include fraud detection, default prediction, credit scoring, directional forecast of asset price movement, and Buy/Sell recommendations. There are many other use cases of classification-based supervised learning in portfolio management and algorithmic trading.</p>

<p>Many use cases of regression-based and classification-based supervised machine learning are presented in Chapters <a href="ch05.xhtml#Chapter5">5</a> and <a href="ch06.xhtml#Chapter6">6</a>.</p>

<p>Python and its libraries provide methods and ways to implement these supervised learning models in few lines of code. Some of these libraries were covered in <a data-type="xref" href="ch02.xhtml#Chapter2">Chapter 2</a>. With easy-to-use machine learning libraries like Scikit-learn and Keras, it is straightforward to fit different machine learning models on a given predictive modeling dataset.</p>

<p>In this chapter, we present a high-level overview of supervised learning models. For a thorough coverage of the topics, the reader is referred to <em>Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow</em>, 2nd Edition, by Aurélien Géron (O’Reilly).</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45174933181096">
<h5/>
<p>The following topics are covered in this chapter:</p>

<ul>
<li>
<p>Basic concepts of supervised learning models (both regression and classification).</p>
</li>
<li>
<p>How to implement different supervised learning models in Python.</p>
</li>
<li>
<p>How to tune the models and identify the optimal parameters of the models using grid search.</p>
</li>
<li>
<p>Overfitting versus underfitting and bias versus variance.</p>
</li>
<li>
<p>Strengths and weaknesses of several supervised learning models.</p>
</li>
<li>
<p>How to use ensemble models, ANN, and deep learning models for both regression and classification.</p>
</li>
<li>
<p>How to select a model on the basis of several factors, including model 
<span class="keep-together">performance</span>.</p>
</li>
<li>
<p>Evaluation metrics for classification and regression models.</p>
</li>
<li>
<p>How to perform cross validation.</p>
</li>
</ul>
</div></aside>






<section data-type="sect1" class="pagebreak-before less_space" data-pdf-bookmark="Supervised Learning Models: An Overview"><div class="sect1" id="idm45174933170376">
<h1>Supervised Learning Models: An Overview</h1>

<p><a data-type="indexterm" data-primary="supervised learning" data-secondary="overview of models" id="ix_Chapter4-asciidoc1"/>Classification predictive modeling problems are different from regression predictive modeling problems, as classification is the task of predicting a discrete class label and regression is the task of predicting a continuous quantity. However, both share the same concept of utilizing known variables to make predictions, and there is a significant overlap between the two models. Hence, the models for classification and regression are presented together in this chapter. <a data-type="xref" href="#ModelsSupervised">Figure 4-1</a> summarizes the list of the  models commonly used for classification and regression.</p>

<p>Some models can be used for both classification and regression with small modifications. These are <em>K</em>-nearest neighbors, decision trees, support vector, ensemble bagging/boosting methods, and ANNs (including deep neural networks), as shown in <a data-type="xref" href="#ModelsSupervised">Figure 4-1</a>. However, some models, such as linear regression and logistic regression, cannot (or cannot easily) be used for both problem types.</p>

<figure><div id="ModelsSupervised" class="figure">
<img src="Images/mlbf_0401.png" alt="mlbf 0401" width="1168" height="881"/>
<h6><span class="label">Figure 4-1. </span>Models for regression and classification</h6>
</div></figure>

<p>This section contains the following details about the models:</p>

<ul>
<li>
<p>Theory of the models.</p>
</li>
<li>
<p>Implementation in Scikit-learn or Keras.</p>
</li>
<li>
<p>Grid search for different models.</p>
</li>
<li>
<p>Pros and cons of the models.</p>
</li>
</ul>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>In finance, a key focus is on models that extract signals from previously observed data in order to predict future values for the same time series. This family of time series models predicts continuous output and is more aligned with the supervised regression models. Time series models are covered separately in the supervised regression chapter (<a data-type="xref" href="ch05.xhtml#Chapter5">Chapter 5</a>).</p>
</div>








<section data-type="sect2" data-pdf-bookmark="Linear Regression (Ordinary Least Squares)"><div class="sect2" id="idm45174933154904">
<h2>Linear Regression (Ordinary Least Squares)</h2>

<p><a data-type="indexterm" data-primary="linear regression" id="ix_Chapter4-asciidoc2"/><a data-type="indexterm" data-primary="ordinary least squares (OLS) regression" data-seealso="linear regression" id="ix_Chapter4-asciidoc3"/><a data-type="indexterm" data-primary="supervised learning" data-secondary="linear regression" id="ix_Chapter4-asciidoc4"/><em>Linear regression</em> (Ordinary Least Squares Regression or OLS Regression) is perhaps one of the most well-known and best-understood algorithms in statistics and machine learning. Linear regression is a linear model, e.g., a model that assumes a linear relationship between the input variables (<em>x</em>) and the single output variable (<em>y</em>). The goal of linear regression is to train a linear model to predict a new <em>y</em> given a previously unseen <em>x</em> with as little error as possible.</p>

<p>Our model will be a function that predicts <em>y</em> given <math alttext="x 1 comma x 2 period period period x Subscript i Baseline">
  <mrow>
    <msub><mi>x</mi> <mn>1</mn> </msub>
    <mo>,</mo>
    <msub><mi>x</mi> <mn>2</mn> </msub>
    <mo>.</mo>
    <mo>.</mo>
    <mo>.</mo>
    <msub><mi>x</mi> <mi>i</mi> </msub>
  </mrow>
</math>:</p>
<div data-type="equation">
<math alttext="y equals beta 0 plus beta 1 x 1 plus period period period plus beta Subscript i Baseline x Subscript i Baseline" display="block">
  <mrow>
    <mi>y</mi>
    <mo>=</mo>
    <msub><mi>β</mi> <mn>0</mn> </msub>
    <mo>+</mo>
    <msub><mi>β</mi> <mn>1</mn> </msub>
    <msub><mi>x</mi> <mn>1</mn> </msub>
    <mo>+</mo>
    <mo>.</mo>
    <mo>.</mo>
    <mo>.</mo>
    <mo>+</mo>
    <msub><mi>β</mi> <mi>i</mi> </msub>
    <msub><mi>x</mi> <mi>i</mi> </msub>
  </mrow>
</math>
</div>

<p>where, <math alttext="beta 0">
  <msub><mi>β</mi> <mn>0</mn> </msub>
</math> is called intercept and <math alttext="beta 1 period period period beta Subscript i Baseline">
  <mrow>
    <msub><mi>β</mi> <mn>1</mn> </msub>
    <mo>.</mo>
    <mo>.</mo>
    <mo>.</mo>
    <msub><mi>β</mi> <mi>i</mi> </msub>
  </mrow>
</math> are the coefficient of the regression.</p>










<section data-type="sect3" data-pdf-bookmark="Implementation in Python"><div class="sect3" id="idm45174933120232">
<h3>Implementation in Python</h3>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="k">import</code> <code class="n">LinearRegression</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">LinearRegression</code><code class="p">()</code>
<code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">Y</code><code class="p">)</code></pre>

<p><a data-type="indexterm" data-primary="linear regression" data-secondary="implementation in Python" id="idm45174933106840"/>In the following section, we cover the training of a linear regression model and grid search of the model. However, the overall concepts and related approaches are applicable to all other supervised learning models.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Training a model"><div class="sect3" id="idm45174933100376">
<h3>Training a model</h3>

<p><a data-type="indexterm" data-primary="linear regression" data-secondary="training a model" id="idm45174933099304"/>As we mentioned in <a data-type="xref" href="ch03.xhtml#Chapter3">Chapter 3</a>, training a model basically means retrieving the model parameters by minimizing the cost (loss) function. The two steps for training a linear regression model are:</p>
<dl>
<dt>Define a cost function (or loss function)</dt>
<dd>
<p><a data-type="indexterm" data-primary="cost functions" id="idm45174933095816"/><a data-type="indexterm" data-primary="loss (cost) functions" id="idm45174933094920"/>Measures how inaccurate the model’s predictions are. <a data-type="indexterm" data-primary="residual sum of squares (RSS)" id="idm45174933094120"/><a data-type="indexterm" data-primary="RSS (residual sum of squares)" id="idm45174933093480"/><a data-type="indexterm" data-primary="sum of squared residuals (RSS)" id="idm45174933092840"/>The <em>sum of squared residuals (RSS)</em> as defined in <a data-type="xref" href="#EquationRSS">Equation 4-1</a> measures the squared sum of the difference between the actual and predicted value and is the cost function for linear 
<span class="keep-together">regression</span>.</p>
<div class="pagebreak-before" data-type="equation" id="EquationRSS">
<h5 class="less_space"><span class="label">Equation 4-1. </span>Sum of squared residuals</h5>
<math display="block">
  <mrow>
    <mi>R</mi>
    <mi>S</mi>
    <mi>S</mi>
    <mo>=</mo>
    <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi> </munderover>
    <msup><mfenced separators="" open="(" close=")"><msub><mi>y</mi> <mi>i</mi> </msub> <mo>–</mo> <msub><mi>β</mi> <mn>0</mn> </msub> <mo>–</mo> <munderover><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi> </munderover> <msub><mi>β</mi> <mi>j</mi> </msub> <msub><mi>x</mi> <mrow><mi>i</mi><mi>j</mi></mrow> </msub></mfenced> <mn>2</mn> </msup>
  </mrow>
</math>
</div>

<p>In this equation, <math alttext="beta 0">
  <msub><mi>β</mi> <mn>0</mn> </msub>
</math> is the intercept; <math alttext="beta Subscript j">
  <msub><mi>β</mi> <mi>j</mi> </msub>
</math> represents the coefficient; <math alttext="beta 1 comma period period comma beta Subscript j Baseline">
  <mrow>
    <msub><mi>β</mi> <mn>1</mn> </msub>
    <mo>,</mo>
    <mo>.</mo>
    <mo>.</mo>
    <mo>,</mo>
    <msub><mi>β</mi> <mi>j</mi> </msub>
  </mrow>
</math> are the coefficients of the regression; and <math alttext="x Subscript i j">
  <msub><mi>x</mi> <mrow><mi>i</mi><mi>j</mi></mrow> </msub>
</math> represents the <math alttext="i Superscript t h">
  <msup><mi>i</mi> <mrow><mi>t</mi><mi>h</mi></mrow> </msup>
</math> observation and <math alttext="j Superscript t h">
  <msup><mi>j</mi> <mrow><mi>t</mi><mi>h</mi></mrow> </msup>
</math> 
<span class="keep-together">variable</span>.</p>
</dd>
<dt>Find the parameters that minimize loss</dt>
<dd>
<p>For example, make our model as accurate as possible. Graphically, in two dimensions, this results in a line of best fit as shown in <a data-type="xref" href="#LinReg">Figure 4-2</a>. In higher dimensions, we would have higher-dimensional hyperplanes.
Mathematically, we look at the difference between each real data point (<em>y</em>) and our model’s prediction (<em>ŷ</em>). Square these differences to avoid negative numbers and penalize larger differences, and then add them up and take the average. This is a measure of how well our data fits the line.</p>

<figure><div id="LinReg" class="figure">
<img src="Images/mlbf_0402.png" alt="mlbf 0402" width="820" height="594"/>
<h6><span class="label">Figure 4-2. </span>Linear regression</h6>
</div></figure>
</dd>
</dl>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Grid search"><div class="sect3" id="idm45174933026056">
<h3>Grid search</h3>

<p><a data-type="indexterm" data-primary="grid search" id="idm45174933024584"/><a data-type="indexterm" data-primary="linear regression" data-secondary="grid search" id="idm45174933023880"/>The overall idea of the grid search is to create a grid of all possible hyperparameter combinations and train the model using each one of them. Hyperparameters are the external characteristic of the model, can be considered the model’s settings, and are not estimated based on data-like model parameters. These hyperparameters are tuned during grid search to achieve better model performance.</p>

<p class="pagebreak-before">Due to its exhaustive search, a grid search is guaranteed to find the optimal parameter within the grid. The drawback is that the size of the grid grows exponentially with the addition of more parameters or more considered values.</p>

<p>The <code>GridSearchCV</code> class in the <code>model_selection</code> module of the sklearn package facilitates the systematic evaluation of all combinations of the hyperparameter values that we would like to test.</p>

<p>The first step is to create a model object. We then define a dictionary where the keywords name the hyperparameters and the values list the parameter settings to be 
<span class="keep-together">tested</span>. For linear regression, the hyperparameter is <code>fit_intercept</code>, which is a boolean variable that determines whether or not to calculate the <em>intercept</em> for this model. If set to <code>False</code>, no intercept will be used in calculations:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">model</code> <code class="o">=</code> <code class="n">LinearRegression</code><code class="p">()</code>
<code class="n">param_grid</code> <code class="o">=</code> <code class="p">{</code><code class="s">'fit_intercept'</code><code class="p">:</code> <code class="p">[</code><code class="k">True</code><code class="p">,</code> <code class="k">False</code><code class="p">]}</code>
<code class="p">}</code></pre>

<p>The second step is to instantiate the <code>GridSearchCV</code> object and provide the estimator object and parameter grid, as well as a scoring method and cross validation choice, to the initialization method. Cross validation is a resampling procedure used to evaluate machine learning models, and scoring parameter is the evaluation metrics of the model:<sup><a data-type="noteref" id="idm45174932998104-marker" href="ch04.xhtml#idm45174932998104">1</a></sup></p>

<p>With all settings in place, we can fit <code>GridSearchCV</code>:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">grid</code> <code class="o">=</code> <code class="n">GridSearchCV</code><code class="p">(</code><code class="n">estimator</code><code class="o">=</code><code class="n">model</code><code class="p">,</code> <code class="n">param_grid</code><code class="o">=</code><code class="n">param_grid</code><code class="p">,</code> <code class="n">scoring</code><code class="o">=</code> <code class="s">'r2'</code><code class="p">,</code> \
  <code class="n">cv</code><code class="o">=</code><code class="n">kfold</code><code class="p">)</code>
<code class="n">grid_result</code> <code class="o">=</code> <code class="n">grid</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">Y</code><code class="p">)</code></pre>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Advantages and disadvantages"><div class="sect3" id="idm45174932995336">
<h3>Advantages and disadvantages</h3>

<p><a data-type="indexterm" data-primary="linear regression" data-secondary="advantages and disadvantages" id="idm45174932950248"/>In terms of advantages, linear regression is easy to understand and interpret. However, it may not work well when there is a nonlinear relationship between predicted and predictor variables. <a data-type="indexterm" data-primary="overfitting" id="idm45174932948776"/>Linear regression is prone to <em>overfitting</em> (which we will discuss in the next section) and when a large number of features are present, it may not handle irrelevant features well. Linear regression also requires the data to follow certain <a href="https://oreil.ly/tNDnc">assumptions</a>, such as the absence of multicollinearity. If the assumptions fail, then we cannot trust the results obtained.<a data-type="indexterm" data-startref="ix_Chapter4-asciidoc4" id="idm45174932946568"/><a data-type="indexterm" data-startref="ix_Chapter4-asciidoc3" id="idm45174932945864"/><a data-type="indexterm" data-startref="ix_Chapter4-asciidoc2" id="idm45174932945192"/></p>
</div></section>



</div></section>













<section data-type="sect2" class="pagebreak-before less_space" data-pdf-bookmark="Regularized Regression"><div class="sect2" id="idm45174932944392">
<h2>Regularized Regression</h2>

<p><a data-type="indexterm" data-primary="regularized regression" id="ix_Chapter4-asciidoc5"/><a data-type="indexterm" data-primary="supervised learning" data-secondary="regularized regression" id="ix_Chapter4-asciidoc6"/>When a linear regression model contains many independent variables, their coefficients will be poorly determined, and the model will have a tendency to fit extremely well to the training data (data used to build the model) but fit poorly to testing data (data used to test how good the model is). This is known as overfitting or high 
<span class="keep-together">variance</span>.</p>

<p><a data-type="indexterm" data-primary="regularization, defined" id="idm45174932939096"/>One popular technique to control overfitting is <em>regularization</em>, which involves the addition of a <em>penalty</em> term to the error or loss function to discourage the coefficients from reaching large values. Regularization, in simple terms, is a penalty mechanism that applies shrinkage to model parameters (driving them closer to zero) in order to build a model with higher prediction accuracy and interpretation. <a data-type="indexterm" data-primary="linear regression" data-secondary="regularized regression versus" id="idm45174932937144"/>Regularized regression has two advantages over linear regression:</p>
<dl>
<dt>Prediction accuracy</dt>
<dd>
<p>The performance of the model working better on the testing data suggests that the model is trying to generalize from training data. A model with too many parameters might try to fit noise specific to the training data. By shrinking or setting some coefficients to zero, we trade off the ability to fit complex models (higher bias) for a more generalizable model (lower variance).</p>
</dd>
<dt>Interpretation</dt>
<dd>
<p>A large number of predictors may complicate the interpretation
or communication of the big picture of the results. It may be preferable to
sacrifice some detail to limit the model to a smaller subset of parameters with the
strongest effects.</p>
</dd>
</dl>

<p>The common ways to regularize a linear regression model are as follows:</p>
<dl>
<dt>L1 regularization or Lasso regression</dt>
<dd>
<p><a data-type="indexterm" data-primary="L1 regularization" id="idm45174932930120"/><a data-type="indexterm" data-primary="lasso regression (LASSO)" id="idm45174932929416"/><em>Lasso regression</em> performs <em>L1 regularization</em> by adding a factor of the sum of the absolute value of coefficients in the cost function (RSS) for linear regression, as mentioned in <a data-type="xref" href="#EquationRSS">Equation 4-1</a>. The equation for lasso regularization can be represented as follows:</p>

<p><math alttext="upper C o s t upper F u n c t i o n equals upper R upper S upper S plus lamda asterisk sigma-summation Underscript j equals 1 Overscript p Endscripts StartAbsoluteValue beta Subscript j Baseline EndAbsoluteValue">
  <mrow>
    <mi>C</mi>
    <mi>o</mi>
    <mi>s</mi>
    <mi>t</mi>
    <mi>F</mi>
    <mi>u</mi>
    <mi>n</mi>
    <mi>c</mi>
    <mi>t</mi>
    <mi>i</mi>
    <mi>o</mi>
    <mi>n</mi>
    <mo>=</mo>
    <mi>R</mi>
    <mi>S</mi>
    <mi>S</mi>
    <mo>+</mo>
    <mi>λ</mi>
    <mo>*</mo>
    <msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>p</mi> </msubsup>
    <mfenced separators="" open="|" close="|">
      <msub><mi>β</mi> <mi>j</mi> </msub>
    </mfenced>
  </mrow>
</math></p>

<p>L1 regularization can lead to zero coefficients (i.e., some of the features are completely neglected for the evaluation of output). The larger the value of <math alttext="lamda">
  <mi>λ</mi>
</math>, the more features are shrunk to zero. This can eliminate some features entirely and give us a subset of predictors, reducing model complexity. So Lasso regression not only helps in reducing overfitting, but also can help in feature selection. Predictors not shrunk toward zero signify that they are important, and thus L1 regularization allows for feature selection (sparse selection). The regularization parameter (<math alttext="lamda">
  <mi>λ</mi>
</math>) can be controlled, and a <code>lambda</code> value of zero produces the basic linear regression equation.</p>

<p>A lasso regression model can be constructed using the <code>Lasso</code> class of the sklearn package of Python, as shown in the code snippet that follows:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="k">import</code> <code class="n">Lasso</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">Lasso</code><code class="p">()</code>
<code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">Y</code><code class="p">)</code></pre>
</dd>
<dt>L2 regularization or Ridge regression</dt>
<dd>
<p><a data-type="indexterm" data-primary="L2 regularization" id="idm45174932856888"/><a data-type="indexterm" data-primary="ridge regression" id="idm45174932856280"/><em>Ridge regression</em> performs <em>L2 regularization</em> by adding a factor of the sum of the square of coefficients in the cost function (RSS) for linear regression, as mentioned in <a data-type="xref" href="#EquationRSS">Equation 4-1</a>. The equation for ridge regularization can be represented as follows:</p>

<p><math>
  <mrow>
    <mi>C</mi>
    <mi>o</mi>
    <mi>s</mi>
    <mi>t</mi>
    <mi>F</mi>
    <mi>u</mi>
    <mi>n</mi>
    <mi>c</mi>
    <mi>t</mi>
    <mi>i</mi>
    <mi>o</mi>
    <mi>n</mi>
    <mo>=</mo>
    <mi>R</mi>
    <mi>S</mi>
    <mi>S</mi>
    <mo>+</mo>
    <mi>λ</mi>
    <mo>*</mo>
    <msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>p</mi> </msubsup>
    <msubsup><mi>β</mi> <mrow><mi>j</mi></mrow> <mn>2</mn> </msubsup>
  </mrow>
</math></p>

<p>Ridge regression puts constraint on the coefficients. The penalty term (<math alttext="lamda">
  <mi>λ</mi>
</math>) regularizes the coefficients such that if the coefficients take large values, the optimization function is penalized. So ridge regression shrinks the coefficients and helps to reduce the model complexity. Shrinking the coefficients leads to a lower variance and a lower error value. Therefore, ridge regression decreases the complexity of a model but does not reduce the number of variables; it just shrinks their effect. When <math alttext="lamda">
  <mi>λ</mi>
</math> is closer to zero, the cost function becomes similar to the linear regression cost function. So the lower the constraint (low <math alttext="lamda">
  <mi>λ</mi>
</math>) on the features, the more the model will resemble the linear regression model.</p>

<p>A ridge regression model can be constructed using the <code>Ridge</code> class of the sklearn package of Python, as shown in the code snippet that follows:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="k">import</code> <code class="n">Ridge</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">Ridge</code><code class="p">()</code>
<code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">Y</code><code class="p">)</code></pre>
</dd>
<dt>Elastic net</dt>
<dd>
<p><a data-type="indexterm" data-primary="elastic net (EN)" id="idm45174932793528"/><em>Elastic nets</em> add regularization terms to the model, which are a combination of both L1 and L2 regularization, as shown in the following equation:</p>

<p>
<math display="inline">
  <mrow>
    <mi>C</mi>
    <mi>o</mi>
    <mi>s</mi>
    <mi>t</mi>
    <mi>F</mi>
    <mi>u</mi>
    <mi>n</mi>
    <mi>c</mi>
    <mi>t</mi>
    <mi>i</mi>
    <mi>o</mi>
    <mi>n</mi>
    <mo>=</mo>
    <mi>R</mi>
    <mi>S</mi>
    <mi>S</mi>
    <mo>+</mo>
    <mi>λ</mi>
    <mo>*</mo>
    <mfenced separators="" open="(" close=")"><mrow><mo>(</mo><mn>1</mn><mo>–</mo><mi>α</mi><mo>)</mo></mrow> <mo>/</mo> <mn>2</mn> <mo>*</mo> <msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>p</mi> </msubsup> <msubsup><mi>β</mi> <mrow><mi>j</mi></mrow> <mn>2</mn> </msubsup> <mo>+</mo> <mi>α</mi> <mo>*</mo> <msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>p</mi> </msubsup> <mfenced separators="" open="|" close="|"><msub><mi>β</mi> <mi>j</mi> </msub></mfenced></mfenced>
  </mrow>
</math></p>

<p>In addition to setting and choosing a <math alttext="lamda">
  <mi>λ</mi>
</math> value, an elastic net also allows us to tune the alpha parameter, where <math alttext="alpha">
  <mi>α</mi>
</math> = <em>0</em> corresponds to ridge and <math alttext="alpha">
  <mi>α</mi>
</math> = <em>1</em> to lasso. Therefore, we can choose an <math alttext="alpha">
  <mi>α</mi>
</math> value between <em>0</em> and <em>1</em> to optimize the elastic net. Effectively, this will shrink some coefficients and set some to <em>0</em> for sparse selection.</p>

<p>An elastic net regression model can be constructed using the <code>ElasticNet</code> class of the sklearn package of Python, as shown in the following code snippet:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="k">import</code> <code class="n">ElasticNet</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">ElasticNet</code><code class="p">()</code>
<code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">Y</code><code class="p">)</code></pre>
</dd>
</dl>

<p>For all the regularized regression, <math alttext="lamda">
  <mi>λ</mi>
</math> is the key parameter to tune during grid search in Python. In an elastic net, <math alttext="alpha">
  <mi>α</mi>
</math> can be an additional parameter to tune.<a data-type="indexterm" data-startref="ix_Chapter4-asciidoc6" id="idm45174932717736"/><a data-type="indexterm" data-startref="ix_Chapter4-asciidoc5" id="idm45174932717032"/></p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Logistic Regression"><div class="sect2" id="idm45174932716232">
<h2>Logistic Regression</h2>

<p><a data-type="indexterm" data-primary="logistic regression" id="idm45174932714664"/><a data-type="indexterm" data-primary="supervised learning" data-secondary="logistic regression" id="idm45174932713960"/><em>Logistic regression</em> is one of the most widely used algorithms for classification. The logistic regression model arises from the desire to model the probabilities of the output classes given a function that is linear in <em>x</em>, at the same time ensuring that output probabilities sum up to one and remain between zero and one as we would expect from probabilities.</p>

<p>If we train a linear regression model on several examples where <em>Y = 0</em> or <em>1</em>, we might end up predicting some probabilities that are less than zero or greater than one, which doesn’t make sense. Instead, we use a logistic regression model (or <em>logit</em> model), which is a modification of linear regression that makes sure to output a probability between zero and one by applying the <code>sigmoid</code> function.<sup><a data-type="noteref" id="idm45174932709384-marker" href="ch04.xhtml#idm45174932709384">2</a></sup></p>

<p><a data-type="xref" href="#LogisticEq">Equation 4-2</a> shows the equation for a logistic regression model. Similar to linear regression, input values (<em>x</em>) are combined linearly using weights or coefficient values to predict an output value (<em>y</em>). The output coming from <a data-type="xref" href="#LogisticEq">Equation 4-2</a> is a probability that is transformed into a binary value (<em>0</em> or <em>1</em>) to get the model prediction.</p>
<div id="LogisticEq" data-type="equation">
<h5><span class="label">Equation 4-2. </span>Logistic regression equation</h5>
<math alttext="y equals StartFraction exp left-parenthesis beta 0 plus beta 1 x 1 plus period period period period plus beta Subscript i Baseline x 1 right-parenthesis Over 1 plus exp left-parenthesis beta 0 plus beta 1 x 1 plus period period period period plus beta Subscript i Baseline x 1 right-parenthesis EndFraction" display="block">
  <mrow>
    <mi>y</mi>
    <mo>=</mo>
    <mfrac><mrow><mo form="prefix">exp</mo><mo>(</mo><msub><mi>β</mi> <mn>0</mn> </msub><mo>+</mo><msub><mi>β</mi> <mn>1</mn> </msub><msub><mi>x</mi> <mn>1</mn> </msub><mo>+</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>+</mo><msub><mi>β</mi> <mi>i</mi> </msub><msub><mi>x</mi> <mn>1</mn> </msub><mo>)</mo></mrow> <mrow><mn>1</mn><mo>+</mo><mo form="prefix">exp</mo><mo>(</mo><msub><mi>β</mi> <mn>0</mn> </msub><mo>+</mo><msub><mi>β</mi> <mn>1</mn> </msub><msub><mi>x</mi> <mn>1</mn> </msub><mo>+</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>+</mo><msub><mi>β</mi> <mi>i</mi> </msub><msub><mi>x</mi> <mn>1</mn> </msub><mo>)</mo></mrow></mfrac>
  </mrow>
</math>
</div>

<p>Where <em>y</em> is the predicted output, <math alttext="beta 0">
  <msub><mi>β</mi> <mn>0</mn> </msub>
</math> is the bias or intercept term and B<sub>1</sub> is the coefficient for the single input value (<em>x</em>). Each column in the input data has an associated <math alttext="beta">
  <mi>β</mi>
</math> coefficient (a constant real value) that must be learned from the training data.</p>

<p>In logistic regression, the cost function is basically a measure of how often we 
<span class="keep-together">predicted</span> one when the true answer was zero, or vice versa. Training the logistic regression coefficients is done using techniques such as maximum likelihood estimation (MLE) to predict values close to <em>1</em> for the default class and close to <em>0</em> for the other class.<sup><a data-type="noteref" id="idm45174932651752-marker" href="ch04.xhtml#idm45174932651752">3</a></sup></p>

<p class="pagebreak-before">A logistic regression model can be constructed using the <code>LogisticRegression</code> class of the sklearn package of Python, as shown in the following code snippet:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="k">import</code> <code class="n">LogisticRegression</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">LogisticRegression</code><code class="p">()</code>
<code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">Y</code><code class="p">)</code></pre>










<section data-type="sect3" data-pdf-bookmark="Hyperparameters"><div class="sect3" id="idm45174932637624">
<h3>Hyperparameters</h3>
<dl>
<dt><a data-type="indexterm" data-primary="logistic regression" data-secondary="hyperparameters" id="idm45174932631272"/>Regularization (<code>penalty</code> in sklearn)</dt>
<dd>
<p>Similar to linear regression, logistic regression can have regularization, which can be <em><em>L1</em></em>, <em><em>L2</em></em>, or <em><em>elasticnet</em></em>. The values in the <em>sklearn</em> library are <em>[<em>l1</em>, <em>l2</em>, <em>elasticnet</em>]</em>.</p>
</dd>
<dt>Regularization strength (<code>C</code> in sklearn)</dt>
<dd>
<p>This parameter controls the regularization strength. Good values of the penalty parameters can be <em>[100, 10, 1.0, 0.1, 0.01]</em>.</p>
</dd>
</dl>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Advantages and disadvantages"><div class="sect3" id="idm45174932622040">
<h3>Advantages and disadvantages</h3>

<p><a data-type="indexterm" data-primary="logistic regression" data-secondary="advantages and disadvantages" id="idm45174932620872"/>In terms of the advantages, the logistic regression model is easy to implement, has good interpretability, and performs very well on linearly separable classes. The output of the model is a probability, which provides more insight and can be used for ranking. The model has small number of hyperparameters. Although there may be risk of overfitting, this may be addressed using <em>L1/L2</em> regularization, similar to the way we addressed overfitting for the linear regression models.</p>

<p>In terms of disadvantages, the model may overfit when provided with large numbers of features. Logistic regression can only learn linear functions and is less suitable to complex relationships between features and the target variable. Also, it may not handle irrelevant features well, especially if the features are strongly correlated.</p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Support Vector Machine"><div class="sect2" id="idm45174932715640">
<h2>Support Vector Machine</h2>

<p><a data-type="indexterm" data-primary="supervised learning" data-secondary="support vector machine" id="ix_Chapter4-asciidoc7"/><a data-type="indexterm" data-primary="support vector machine (SVM)" id="ix_Chapter4-asciidoc8"/>The objective of the <em>support vector machine</em> (SVM) algorithm is to maximize the margin (shown as shaded area in <a data-type="xref" href="#SVM">Figure 4-3</a>), which is defined as the distance between the separating hyperplane (or decision boundary) and the training samples that are closest to this hyperplane, the so-called support vectors. The margin is calculated as the perpendicular distance from the line to only the closest points, as shown in <a data-type="xref" href="#SVM">Figure 4-3</a>. Hence, SVM calculates a maximum-margin boundary that leads to a homogeneous partition of all data points.</p>

<figure><div id="SVM" class="figure">
<img src="Images/mlbf_0403.png" alt="mlbf 0403" width="1023" height="836"/>
<h6><span class="label">Figure 4-3. </span>Support vector machine</h6>
</div></figure>

<p>In practice, the data is messy and cannot be separated perfectly with a hyperplane. The constraint of maximizing the margin of the line that separates the classes must be relaxed. This change allows some points in the training data to violate the separating line. An additional set of coefficients is introduced that give the margin wiggle room in each dimension. A tuning parameter is introduced, simply called <em>C</em>, that defines the magnitude of the wiggle allowed across all dimensions. The larger the value of <em>C</em>, the more violations of the hyperplane are permitted.</p>

<p><a data-type="indexterm" data-primary="kernel" id="idm45174932607624"/>In some cases, it is not possible to find a hyperplane or a linear decision boundary, and kernels are used. A kernel is just a transformation of the input data that allows the SVM algorithm to treat/process the data more easily. Using kernels, the original data is projected into a higher dimension to classify the data better.</p>

<p>SVM is used for both classification and regression. We achieve this by converting the original optimization problem into a dual problem. For regression, the trick is to reverse the objective. Instead of trying to fit the largest possible street between two classes while limiting margin violations, SVM regression tries to fit as many instances as possible on the street (shaded area in <a data-type="xref" href="#SVM">Figure 4-3</a>) while limiting margin violations. The width of the street is controlled by a hyperparameter.</p>

<p>The SVM regression and classification models can be constructed using the sklearn package of Python, as shown in the following code snippets:</p>

<p class="pagebreak-before"><code>Regression</code></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">from</code> <code class="nn">sklearn.svm</code> <code class="k">import</code> <code class="n">SVR</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">SVR</code><code class="p">()</code>
<code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">Y</code><code class="p">)</code></pre>

<p><code>Classification</code></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">from</code> <code class="nn">sklearn.svm</code> <code class="k">import</code> <code class="n">SVC</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">SVC</code><code class="p">()</code>
<code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">Y</code><code class="p">)</code></pre>










<section data-type="sect3" data-pdf-bookmark="Hyperparameters"><div class="sect3" id="idm45174932496088">
<h3>Hyperparameters</h3>

<p><a data-type="indexterm" data-primary="sklearn" id="idm45174932554296"/><a data-type="indexterm" data-primary="support vector machine (SVM)" data-secondary="hyperparameters" id="idm45174932553592"/>The following key parameters are present in the sklearn implementation of SVM and can be tweaked while performing the grid search:</p>
<dl>
<dt>Kernels (<code>kernel</code> in sklearn)</dt>
<dd>
<p>The choice of kernel controls the manner in which the input variables will be projected. There are many kernels to choose from, but <em>linear</em> and <a href="https://oreil.ly/XpBOi"><em>RBF</em></a> are the most common.</p>
</dd>
<dt>Penalty (<code>C</code> in sklearn)</dt>
<dd>
<p>The penalty parameter tells the SVM optimization how much you want to avoid misclassifying each training example. For large values of the penalty parameter, the optimization will choose a smaller-margin hyperplane. Good values might be a log scale from 10 to 1,000.</p>
</dd>
</dl>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Advantages and disadvantages"><div class="sect3" id="idm45174932511864">
<h3>Advantages and disadvantages</h3>

<p><a data-type="indexterm" data-primary="support vector machine (SVM)" data-secondary="advantages and disadvantages" id="idm45174932510696"/>In terms of advantages, SVM is fairly robust against overfitting, especially in higher dimensional space. It handles the nonlinear relationships quite well, with many kernels to choose from. Also, there is no distributional requirement for the data.</p>

<p>In terms of disadvantages, SVM can be inefficient to train and memory-intensive to run and tune. It doesn’t perform well with large datasets. It requires the feature scaling of the data. There are also many hyperparameters, and their meanings are often not intuitive.<a data-type="indexterm" data-startref="ix_Chapter4-asciidoc8" id="idm45174932508712"/><a data-type="indexterm" data-startref="ix_Chapter4-asciidoc7" id="idm45174932508008"/></p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="K-Nearest Neighbors"><div class="sect2" id="idm45174932617416">
<h2>K-Nearest Neighbors</h2>

<p><a data-type="indexterm" data-primary="k-nearest neighbors (KNN)" id="idm45174932505640"/><a data-type="indexterm" data-primary="supervised learning" data-secondary="k-nearest neighbors" id="idm45174932504968"/><em>K-nearest neighbors</em> (KNN) is considered a “lazy learner,” as there is no learning required in the model. For a new data point, predictions are made by searching through the entire training set for the <em>K</em> most similar instances (the neighbors) and summarizing the output variable for those <em>K</em> instances.</p>

<p>To determine which of the <em>K</em> instances in the training dataset are most similar to a new input, a distance measure is used. <a data-type="indexterm" data-primary="Euclidean distance" id="idm45174932501400"/>The most popular distance measure is <em>Euclidean distance</em>, which is calculated as the square root of the sum of the squared differences between a point <em>a</em> and a point <em>b</em> across all input attributes <em><em>i</em></em>, and which is represented as <math display="inline">
  <mrow>
    <mi>d</mi>
    <mrow>
      <mo>(</mo>
      <mi>a</mi>
      <mo>,</mo>
      <mi>b</mi>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <msqrt>
      <mrow>
        <msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi> </msubsup>
        <msup><mrow><mo>(</mo><msub><mi>a</mi> <mi>i</mi> </msub><mo>–</mo><msub><mi>b</mi> <mi>i</mi> </msub><mo>)</mo></mrow> <mn>2</mn> </msup>
      </mrow>
    </msqrt>
  </mrow>
</math>. Euclidean distance is a good distance measure to use if the input variables are similar in type.</p>

<p><a data-type="indexterm" data-primary="Manhattan distance" id="idm45174932466344"/>Another distance metric is <em>Manhattan distance</em>, in which the distance between point <em>a</em> and point <em>b</em> is represented as <math display="inline">
  <mrow>
    <mi>d</mi>
    <mrow>
      <mo>(</mo>
      <mi>a</mi>
      <mo>,</mo>
      <mi>b</mi>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi> </msubsup>
    <mrow>
      <mo>|</mo>
      <msub><mi>a</mi> <mi>i</mi> </msub>
      <mo>–</mo>
      <msub><mi>b</mi> <mi>i</mi> </msub>
      <mo>|</mo>
    </mrow>
  </mrow>
</math>.
Manhattan distance is a good measure to use if the input variables are not similar in type.</p>

<p>The steps of KNN can be summarized as follows:</p>
<ol>
<li>
<p>Choose the number of <em>K</em> and a distance metric.</p>
</li>
<li>
<p>Find the <em>K</em>-nearest neighbors of the sample that we want to classify.</p>
</li>
<li>
<p>Assign the class label by majority vote.</p>
</li>

</ol>

<p>KNN regression and classification models can be constructed using the sklearn package of Python, as shown in the following code:</p>

<p><code>Classification</code></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">from</code> <code class="nn">sklearn.neighbors</code> <code class="k">import</code> <code class="n">KNeighborsClassifier</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">KNeighborsClassifier</code><code class="p">()</code>
<code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">Y</code><code class="p">)</code></pre>

<p><code>Regression</code></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">from</code> <code class="nn">sklearn.neighbors</code> <code class="k">import</code> <code class="n">KNeighborsRegressor</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">KNeighborsRegressor</code><code class="p">()</code>
<code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">Y</code><code class="p">)</code></pre>










<section data-type="sect3" data-pdf-bookmark="Hyperparameters"><div class="sect3" id="idm45174932361480">
<h3>Hyperparameters</h3>

<p><a data-type="indexterm" data-primary="k-nearest neighbors (KNN)" data-secondary="hyperparameters" id="idm45174932421368"/>The following key parameters are present in the sklearn implementation of KNN and can be tweaked while performing the grid search:</p>
<dl>
<dt>Number of neighbors (<code>n_neighbors</code> in sklearn)</dt>
<dd>
<p>The most important hyperparameter for KNN is the number of neighbors (n_neighbors). Good values are between 1 and 20.</p>
</dd>
<dt>Distance metric (<code>metric</code> in sklearn)</dt>
<dd>
<p>It may also be interesting to test different distance metrics for choosing the composition of the neighborhood. Good values are <em>euclidean</em> and <em>manhattan</em>.</p>
</dd>
</dl>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Advantages and disadvantages"><div class="sect3" id="idm45174932379176">
<h3>Advantages and disadvantages</h3>

<p><a data-type="indexterm" data-primary="k-nearest neighbors (KNN)" data-secondary="advantages and disadvantages" id="idm45174932377976"/>In terms of advantages, no training is involved and hence there is no learning phase. Since the algorithm requires no training before making predictions, new data can be added seamlessly without impacting the accuracy of the algorithm. It is intuitive and easy to understand. The model naturally handles multiclass classification and can learn complex decision boundaries. KNN is effective if the training data is large. It is also robust to noisy data, and there is no need to filter the outliers.</p>

<p>In terms of the disadvantages, the distance metric to choose is not obvious and difficult to justify in many cases. KNN performs poorly on high dimensional datasets. It is expensive and slow to predict new instances because the distance to all neighbors must be recalculated. KNN is sensitive to noise in the dataset. We need to manually input missing values and remove outliers. Also, feature scaling (standardization and normalization) is required before applying the KNN algorithm to any dataset; otherwise, KNN may generate wrong predictions.</p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Linear Discriminant Analysis"><div class="sect2" id="idm45174932506616">
<h2>Linear Discriminant Analysis</h2>

<p><a data-type="indexterm" data-primary="LDA (linear discriminant analysis)" id="idm45174932373976"/><a data-type="indexterm" data-primary="linear discriminant analysis (LDA)" id="idm45174932373304"/><a data-type="indexterm" data-primary="supervised learning" data-secondary="linear discriminant analysis" id="idm45174932372616"/>The objective of the <em>linear discriminant analysis</em> (LDA) algorithm is to project the data onto a lower-dimensional space in a way that the class separability is maximized and the variance within a class is minimized.<sup><a data-type="noteref" id="idm45174932370936-marker" href="ch04.xhtml#idm45174932370936">4</a></sup></p>

<p>During the training of the LDA model, the statistical properties (i.e., mean and covariance matrix) of each class are computed. The statistical properties are estimated on the basis of the following assumptions about the data:</p>

<ul>
<li>
<p>Data is <a href="https://oreil.ly/cuc7p">normally distributed</a>, so that each variable is shaped like a bell curve when plotted.</p>
</li>
<li>
<p>Each attribute has the same variance, and the values of each variable vary around the mean by the same amount on average.</p>
</li>
</ul>

<p>To make a prediction, LDA estimates the probability that a new set of inputs belongs to every class. The output class is the one that has the highest probability.</p>










<section data-type="sect3" data-pdf-bookmark="Implementation in Python and hyperparameters"><div class="sect3" id="idm45174932365192">
<h3>Implementation in Python and hyperparameters</h3>

<p>The LDA classification model can be constructed using the sklearn package of Python, as shown in the following code snippet:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">from</code> <code class="nn">sklearn.discriminant_analysis</code> <code class="k">import</code> <code class="n">LinearDiscriminantAnalysis</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">LinearDiscriminantAnalysis</code><code class="p">()</code>
<code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">Y</code><code class="p">)</code></pre>

<p><a data-type="indexterm" data-primary="supervised learning" data-secondary="hyperparameters" id="idm45174932325832"/>The key hyperparameter for the LDA model is <code>number of components</code> for dimensionality reduction, which is represented by <code>n_components</code> in sklearn.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Advantages and disadvantages"><div class="sect3" id="idm45174932323752">
<h3>Advantages and disadvantages</h3>

<p>In terms of advantages, LDA is a relatively simple model with fast implementation and is easy to implement.
In terms of disadvantages, it requires feature scaling and involves complex matrix operations.</p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Classification and Regression Trees"><div class="sect2" id="idm45174932321944">
<h2>Classification and Regression Trees</h2>

<p><a data-type="indexterm" data-primary="classification and regression trees (CART)" id="ix_Chapter4-asciidoc9"/><a data-type="indexterm" data-primary="supervised learning" data-secondary="classification and regression trees" id="ix_Chapter4-asciidoc10"/>In the most general terms, the purpose of an analysis via tree-building algorithms is to determine a set of <em>if–then</em> logical (split) conditions that permit accurate prediction or classification of cases. <em>Classification and regression trees</em> (or <em>CART</em> or <em>decision tree classifiers</em>) are attractive models if we care about interpretability. We can think of this model as breaking down our data and making a decision based on asking a series of questions. This algorithm is the foundation of ensemble methods such as random forest and gradient boosting method.</p>










<section data-type="sect3" data-pdf-bookmark="Representation"><div class="sect3" id="idm45174932315704">
<h3>Representation</h3>

<p><a data-type="indexterm" data-primary="binary (decision) tree" id="idm45174932314536"/><a data-type="indexterm" data-primary="classification and regression trees (CART)" data-secondary="representation" id="idm45174932313608"/><a data-type="indexterm" data-primary="decision (binary) tree" id="idm45174932312632"/>The model can be represented by a <em>binary tree</em> (or <em>decision tree</em>), where each node is an input variable <em>x</em> with a split point and each leaf contains an output variable <em>y</em> for prediction.</p>

<p><a data-type="xref" href="#CART">Figure 4-4</a> shows an example of a simple classification tree to predict whether a person is a male or a female based on two inputs of height (in centimeters) and weight (in kilograms).</p>

<figure><div id="CART" class="figure">
<img src="Images/mlbf_0404.png" alt="mlbf 0404" width="728" height="460"/>
<h6><span class="label">Figure 4-4. </span>Classification and regression tree example</h6>
</div></figure>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Learning a CART model"><div class="sect3" id="idm45174932306696">
<h3>Learning a CART model</h3>

<p><a data-type="indexterm" data-primary="classification and regression trees (CART)" data-secondary="learning a CART model" id="idm45174932305256"/>Creating a binary tree is actually a process of dividing up the input space. <a data-type="indexterm" data-primary="recursive binary splitting" id="idm45174932304120"/>A <em>greedy approach</em> called <em>recursive binary splitting</em> is used to divide the space. This is a numerical procedure in which all the values are lined up and different split points are tried and tested using a cost (loss) function. The split with the best cost (lowest cost, because we minimize cost) is selected.
All input variables and all possible split points are evaluated and chosen in a greedy manner (e.g., the very best split point is chosen each time).</p>

<p><a data-type="indexterm" data-primary="sum squared error" id="idm45174932301816"/>For regression predictive modeling problems, the cost function that is minimized to choose split points is the <em>sum of squared errors</em> across all training samples that fall within the rectangle:</p>
<div data-type="equation">
<math>
  <mrow>
    <msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi> </msubsup>
    <msup><mrow><mo>(</mo><msub><mi>y</mi> <mi>i</mi> </msub><mo>–</mo><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>o</mi><msub><mi>n</mi> <mi>i</mi> </msub><mo>)</mo></mrow> <mn>2</mn> </msup>
  </mrow>
</math>
</div>

<p>where <math alttext="y Subscript i">
  <msub><mi>y</mi> <mi>i</mi> </msub>
</math> is the output for the training sample and prediction is the predicted output for the rectangle. <a data-type="indexterm" data-primary="Gini cost function" id="idm45174932265960"/>For classification, the <em>Gini cost function</em> is used; it provides an indication of how pure the leaf nodes are (i.e., how mixed the training data assigned to each node is) and is defined as:</p>
<div data-type="equation">
<math>
  <mrow>
    <mi>G</mi>
    <mo>=</mo>
    <msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi> </msubsup>
    <msub><mi>p</mi> <mi>k</mi> </msub>
    <mo>*</mo>
    <mrow>
      <mo>(</mo>
      <mn>1</mn>
      <mo>–</mo>
      <msub><mi>p</mi> <mi>k</mi> </msub>
      <mo>)</mo>
    </mrow>
  </mrow>
</math>
</div>

<p>where <em>G</em> is the Gini cost over all classes and <math alttext="p Subscript k">
  <msub><mi>p</mi> <mi>k</mi> </msub>
</math> is the number of training instances with class <em>k</em> in the rectangle of interest. A node that has all classes of the same type (perfect class purity) will have <em>G = 0</em>, while a node that has a <em>50–50</em> split of classes for a binary classification problem (worst purity) will have <em>G = 0.5</em>.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Stopping criterion"><div class="sect3" id="idm45174932249400">
<h3>Stopping criterion</h3>

<p><a data-type="indexterm" data-primary="classification and regression trees (CART)" data-secondary="stopping criterion" id="idm45174932248168"/>The recursive binary splitting procedure described in the preceding section needs to know when to stop splitting as it works its way down the tree with the training data. The most common stopping procedure is to use a minimum count on the number of training instances assigned to each leaf node. If the count is less than some minimum, then the split is not accepted and the node is taken as a final leaf node.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Pruning the tree"><div class="sect3" id="idm45174932246280">
<h3>Pruning the tree</h3>

<p><a data-type="indexterm" data-primary="classification and regression trees (CART)" data-secondary="pruning the tress" id="idm45174932245112"/>The stopping criterion is important as it strongly influences the performance of the tree. Pruning can be used after learning the tree to further lift performance. The complexity of a decision tree is defined as the number of splits in the tree. Simpler trees are preferred as they are faster to run and easy to understand, consume less memory during processing and storage, and are less likely to overfit the data. The fastest and simplest pruning method is to work through each leaf node in the tree and evaluate the effect of removing it using a test set. A leaf node is removed only if doing so results in a drop in the overall cost function on the entire test set. The removal of nodes can be stopped when no further improvements can be made.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Implementation in Python"><div class="sect3" id="idm45174932242952">
<h3>Implementation in Python</h3>

<p><a data-type="indexterm" data-primary="classification and regression trees (CART)" data-secondary="implementation in Python" id="idm45174932241576"/>CART regression and classification models can be constructed using the sklearn package of Python, as shown in the following code snippet:</p>

<p><code>Classification</code></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">from</code> <code class="nn">sklearn.tree</code> <code class="k">import</code> <code class="n">DecisionTreeClassifier</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">DecisionTreeClassifier</code><code class="p">()</code>
<code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">Y</code><code class="p">)</code></pre>

<p><code>Regression</code></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">from</code> <code class="nn">sklearn.tree</code> <code class="k">import</code> <code class="n">DecisionTreeRegressor</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">DecisionTreeRegressor</code> <code class="p">()</code>
<code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">Y</code><code class="p">)</code></pre>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Hyperparameters"><div class="sect3" id="idm45174932154680">
<h3>Hyperparameters</h3>

<p><a data-type="indexterm" data-primary="classification and regression trees (CART)" data-secondary="hyperparameters" id="idm45174932210440"/>CART has many hyperparameters. However, the key hyperparameter is the maximum depth of the tree model, which is the number of components for dimensionality reduction, and which is represented by <code>max_depth</code> in the sklearn package. Good values can range from <em>2</em> to <em>30</em> depending on the number of features in the data.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Advantages and disadvantages"><div class="sect3" id="idm45174932207464">
<h3>Advantages and disadvantages</h3>

<p><a data-type="indexterm" data-primary="classification and regression trees (CART)" data-secondary="advantages and disadvantages" id="idm45174932174520"/>In terms of advantages, CART is easy to interpret and can adapt to learn complex relationships. It requires little data preparation, and data typically does not need to be scaled. Feature importance is built in due to the way decision nodes are built. It performs well on large datasets. It works for both regression and classification problems.</p>

<p>In terms of disadvantages, CART is prone to overfitting unless pruning is used. It can be very nonrobust, meaning that small changes in the training dataset can lead to quite major differences in the hypothesis function that gets learned. CART generally has worse performance than ensemble models, which are covered next.<a data-type="indexterm" data-startref="ix_Chapter4-asciidoc10" id="idm45174932172264"/><a data-type="indexterm" data-startref="ix_Chapter4-asciidoc9" id="idm45174932171560"/></p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Ensemble Models"><div class="sect2" id="idm45174932170632">
<h2>Ensemble Models</h2>

<p><a data-type="indexterm" data-primary="ensemble models (supervised learning)" id="ix_Chapter4-asciidoc11"/><a data-type="indexterm" data-primary="supervised learning" data-secondary="ensemble models" id="ix_Chapter4-asciidoc12"/>The goal of <em>ensemble models</em> is to combine different classifiers into a meta-classifier that has better generalization performance than each individual classifier alone. For example, assuming that we collected predictions from 10 experts, ensemble methods would allow us to strategically combine their predictions to come up with a prediction that is more accurate and robust than the experts’ individual predictions.</p>

<p>The two most popular ensemble methods are bagging and boosting. <a data-type="indexterm" data-primary="bagging (bootstrap aggregation)" id="idm45174932165320"/><em>Bagging</em> (or <em>bootstrap aggregation</em>) is an ensemble technique of training several individual models in a parallel way. Each model is trained by a random subset of the data. <a data-type="indexterm" data-primary="boosting" id="idm45174932163656"/><em>Boosting</em>, on the other hand, is an ensemble technique of training several individual models in a sequential way. This is done by building a model from the training data and then 
<span class="keep-together">creating</span> a second model that attempts to correct the errors of the first model. Models are added until the training set is predicted perfectly or a maximum number of models is added. Each individual model learns from mistakes made by the previous model. Just like the decision trees themselves, bagging and boosting can be used for classification and regression problems.</p>

<p>By combining individual models, the ensemble model tends to be more flexible (less bias) and less data-sensitive (less variance).<sup><a data-type="noteref" id="idm45174932160872-marker" href="ch04.xhtml#idm45174932160872">5</a></sup> Ensemble methods combine multiple, simpler algorithms to obtain better performance.</p>

<p>In this section we will cover random forest, AdaBoost, the gradient boosting method, and extra trees, along with their implementation using sklearn package.</p>












<section data-type="sect4" data-pdf-bookmark="Random forest"><div class="sect4" id="idm45174932159496">
<h4>Random forest</h4>

<p><a data-type="indexterm" data-primary="ensemble models (supervised learning)" data-secondary="random forest" id="ix_Chapter4-asciidoc13"/><a data-type="indexterm" data-primary="random forest" id="ix_Chapter4-asciidoc14"/><em>Random forest</em> is a tweaked version of bagged decision trees. In order to understand a random forest algorithm, let us first understand the <em>bagging algorithm</em>. Assuming we have a dataset of one thousand instances, the steps of bagging are:</p>
<ol>
<li>
<p>Create many (e.g., one hundred) random subsamples of our dataset.</p>
</li>
<li>
<p>Train a CART model on each sample.</p>
</li>
<li>
<p>Given a new dataset, calculate the average prediction from each model and aggregate the prediction by each tree to assign the final label by majority vote.</p>
</li>

</ol>

<p>A problem with decision trees like CART is that they are greedy. They choose the variable to split by using a greedy algorithm that minimizes error. Even after bagging, the decision trees can have a lot of structural similarities and result in high correlation in their predictions. Combining predictions from multiple models in ensembles works better if the predictions from the submodels are uncorrelated, or at best are weakly correlated. Random forest changes the learning algorithm in such a way that the resulting predictions from all of the subtrees have less correlation.</p>

<p>In CART, when selecting a split point, the learning algorithm is allowed to look through all variables and all variable values in order to select the most optimal split point. The random forest algorithm changes this procedure such that each subtree can access only a random sample of features when selecting the split points. The number of features that can be searched at each split point (<em>m</em>) must be specified as a parameter to the algorithm.</p>

<p>As the bagged decision trees are constructed, we can calculate how much the error function drops for a variable at each split point. In regression problems, this may be the drop in sum squared error, and in classification, this might be the Gini cost. The bagged method can provide feature importance by calculating and averaging the error function drop for individual variables.</p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="Implementation in Python"><div class="sect4" id="idm45174932129064">
<h4>Implementation in Python</h4>

<p><a data-type="indexterm" data-primary="random forest" data-secondary="implementation in Python" id="idm45174932127816"/>Random forest regression and classification models can be constructed using the sklearn package of Python, as shown in the following code:</p>

<p><code>Classification</code></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="k">import</code> <code class="n">RandomForestClassifier</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">RandomForestClassifier</code><code class="p">()</code>
<code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">Y</code><code class="p">)</code></pre>

<p><code>Regression</code></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="k">import</code> <code class="n">RandomForestRegressor</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">RandomForestRegressor</code><code class="p">()</code>
<code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">Y</code><code class="p">)</code></pre>
</div></section>













<section data-type="sect4" data-pdf-bookmark="Hyperparameters"><div class="sect4" id="idm45174932040776">
<h4>Hyperparameters</h4>

<p><a data-type="indexterm" data-primary="random forest" data-secondary="hyperparameters" id="idm45174932100632"/><a data-type="indexterm" data-primary="sklearn" id="idm45174932099656"/>Some of the main hyperparameters that are present in the sklearn implementation of random forest and that can be tweaked while performing the grid search are:</p>
<dl>
<dt>Maximum number of features (<code>max_features</code> in sklearn)</dt>
<dd>
<p>This is the most important parameter. It is the number of random features to sample at each split point. You could try a range of integer values, such as 1 to 20, or 1 to half the number of input features.</p>
</dd>
<dt>Number of estimators (<code>n_estimators</code> in sklearn)</dt>
<dd>
<p>This parameter represents the number of trees. Ideally, this should be increased until no further improvement is seen in the model. Good values might be a log scale from 10 to 1,000.</p>
</dd>
</dl>
</div></section>













<section data-type="sect4" data-pdf-bookmark="Advantages and disadvantages"><div class="sect4" id="idm45174932058520">
<h4>Advantages and disadvantages</h4>

<p><a data-type="indexterm" data-primary="random forest" data-secondary="advantages and disadvantages" id="idm45174932057352"/>The random forest algorithm (or model) has gained huge popularity in ML applications during the last decade due to its good performance, scalability, and ease of use. It is flexible and naturally assigns feature importance scores, so it can handle redundant feature columns. It scales to large datasets and is generally robust to overfitting. The algorithm doesn’t need the data to be scaled and can model a nonlinear relationship.</p>

<p>In terms of disadvantages, random forest can feel like a black box approach, as we have very little control over what the model does, and the results may be difficult to interpret. Although random forest does a good job at classification, it may not be good for regression problems, as it does not give a precise continuous nature prediction. In the case of regression, it doesn’t predict beyond the range in the training data and may overfit datasets that are particularly noisy.<a data-type="indexterm" data-startref="ix_Chapter4-asciidoc14" id="idm45174932054952"/><a data-type="indexterm" data-startref="ix_Chapter4-asciidoc13" id="idm45174932054248"/></p>
</div></section>











<section data-type="sect3" data-pdf-bookmark="Extra trees"><div class="sect3" id="idm45174932053320">
<h3>Extra trees</h3>

<p><a data-type="indexterm" data-primary="ensemble models (supervised learning)" data-secondary="extra trees" id="idm45174932051912"/><a data-type="indexterm" data-primary="extra trees (extremely randomized trees)" id="idm45174932050968"/><em>Extra trees</em>, otherwise known as <em>extremely randomized trees</em>, is a variant of a random forest; it builds multiple trees and splits nodes using random subsets of features similar to random forest. However, unlike random forest, where observations are drawn with replacement, the observations are drawn without replacement in extra trees. So there is no repetition of observations.</p>

<p>Additionally, random forest selects the best split to convert the parent into the two most homogeneous child nodes.<sup><a data-type="noteref" id="idm45174932048648-marker" href="ch04.xhtml#idm45174932048648">6</a></sup> However, extra trees selects a random split to divide the parent node into two random child nodes. In extra trees, randomness doesn’t come from bootstrapping the data; it comes from the random splits of all 
<span class="keep-together">observations</span>.</p>

<p>In real-world cases, performance is comparable to an ordinary random forest, sometimes a bit better. The advantages and disadvantages of extra trees are similar to those of random forest.</p>












<section data-type="sect4" data-pdf-bookmark="Implementation in Python"><div class="sect4" id="idm45174932046168">
<h4>Implementation in Python</h4>

<p>Extra trees regression and classification models can be constructed using the sklearn package of Python, as shown in the following code snippet. The hyperparameters of extra trees are similar to random forest, as shown in the previous section:</p>

<p><code>Classification</code></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="k">import</code> <code class="n">ExtraTreesClassifier</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">ExtraTreesClassifier</code><code class="p">()</code>
<code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">Y</code><code class="p">)</code></pre>

<p><code>Regression</code></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="k">import</code> <code class="n">ExtraTreesRegressor</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">ExtraTreesRegressor</code><code class="p">()</code>
<code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">Y</code><code class="p">)</code></pre>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="Adaptive Boosting (AdaBoost)"><div class="sect3" id="idm45174931938552">
<h3>Adaptive Boosting (AdaBoost)</h3>

<p><a data-type="indexterm" data-primary="adaptive boosting (AdaBoost)" id="idm45174931997624"/><a data-type="indexterm" data-primary="ensemble models (supervised learning)" data-secondary="adaptive boosting" id="idm45174931996984"/><em>Adaptive Boosting</em> or <em>AdaBoost</em> is a boosting technique in which the basic idea is to try predictors sequentially, and each subsequent model attempts to fix the errors of its predecessor.  At each iteration, the AdaBoost algorithm changes the sample distribution by modifying the weights attached to each of the instances. It increases the weights of the wrongly predicted instances and decreases the ones of the correctly predicted instances.</p>

<p class="pagebreak-before">The steps of the AdaBoost algorithm are:</p>
<ol>
<li>
<p>Initially, all observations are given equal weights.</p>
</li>
<li>
<p>A model is built on a subset of data, and using this model, predictions are made on the whole dataset. Errors are calculated by comparing the predictions and actual values.</p>
</li>
<li>
<p>While creating the next model, higher weights are given to the data points that were predicted incorrectly. Weights can be determined using the error value. For instance, the higher the error, the more weight is assigned to the observation.</p>
</li>
<li>
<p>This process is repeated until the error function does not change, or until the maximum limit of the number of estimators is reached.</p>
</li>

</ol>












<section data-type="sect4" data-pdf-bookmark="Implementation in Python"><div class="sect4" id="idm45174931954376">
<h4>Implementation in Python</h4>

<p><a data-type="indexterm" data-primary="adaptive boosting (AdaBoost)" data-secondary="implementation in Python" id="idm45174931952968"/>AdaBoost regression and classification models can be constructed using the sklearn package of Python, as shown in the following code snippet:</p>

<p><code>Classification</code></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="k">import</code> <code class="n">AdaBoostClassifier</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">AdaBoostClassifier</code><code class="p">()</code>
<code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">Y</code><code class="p">)</code></pre>

<p><code>Regression</code></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="k">import</code> <code class="n">AdaBoostRegressor</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">AdaBoostRegressor</code><code class="p">()</code>
<code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">Y</code><code class="p">)</code></pre>
</div></section>













<section data-type="sect4" data-pdf-bookmark="Hyperparameters"><div class="sect4" id="idm45174931846200">
<h4>Hyperparameters</h4>

<p><a data-type="indexterm" data-primary="adaptive boosting (AdaBoost)" data-secondary="hyperparameters" id="idm45174931906008"/>Some of the main hyperparameters that are present in the sklearn implementation of <span class="keep-together">AdaBoost</span> and that can be tweaked while performing the grid search are as follows:</p>
<dl>
<dt>Learning rate (<code>learning_rate</code> in sklearn)</dt>
<dd>
<p>Learning rate shrinks the contribution of each classifier/regressor. It can be considered on a log scale. The sample values for grid search can be 0.001, 0.01, and 0.1.</p>
</dd>
<dt>Number of estimators (<code>n_estimators</code> in sklearn)</dt>
<dd>
<p>This parameter represents the number of trees. Ideally, this should be increased until no further improvement is seen in the model. Good values might be a log scale from 10 to 1,000.</p>
</dd>
</dl>
</div></section>













<section data-type="sect4" data-pdf-bookmark="Advantages and disadvantages"><div class="sect4" id="idm45174931864072">
<h4>Advantages and disadvantages</h4>

<p><a data-type="indexterm" data-primary="adaptive boosting (AdaBoost)" data-secondary="advantages and disadvantages" id="idm45174931862904"/>In terms of advantages, AdaBoost has a high degree of precision. AdaBoost can 
<span class="keep-together">achieve</span> similar results to other models with much less tweaking of parameters or settings. The algorithm doesn’t need the data to be scaled and can model a nonlinear relationship.</p>

<p>In terms of disadvantages, the training of AdaBoost is time consuming. AdaBoost can be sensitive to noisy data and outliers, and data imbalance leads to a decrease in classification accuracy</p>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="Gradient boosting method"><div class="sect3" id="idm45174931860136">
<h3>Gradient boosting method</h3>

<p><a data-type="indexterm" data-primary="ensemble models (supervised learning)" data-secondary="gradient boosting method" id="idm45174931858728"/><a data-type="indexterm" data-primary="gradient boosting method (GBM)" id="idm45174931857768"/><em>Gradient boosting method</em> (GBM) is another boosting technique similar to <span class="keep-together">AdaBoost</span>, where the general idea is to try predictors sequentially. Gradient boosting works by sequentially adding the previous underfitted predictions to the ensemble, ensuring the errors made previously are corrected.</p>

<p>The following are the steps of the gradient boosting algorithm:</p>
<ol>
<li>
<p>A model (which can be referred to as the first weak learner) is built on a subset of data. Using this model, predictions are made on the whole dataset.</p>
</li>
<li>
<p>Errors are calculated by comparing the predictions and actual values, and the loss is calculated using the loss function.</p>
</li>
<li>
<p>A new model is created using the errors of the previous step as the target variable. The objective is to find the best split in the data to minimize the error. The predictions made by this new model are combined with the predictions of the previous. New errors are calculated using this predicted value and actual value.</p>
</li>
<li>
<p>This process is repeated until the error function does not change or until the maximum limit of the number of estimators is reached.</p>
</li>

</ol>

<p>Contrary to AdaBoost, which tweaks the instance weights at every interaction, this method tries to fit the new predictor to the residual errors made by the previous 
<span class="keep-together">predictor</span>.</p>












<section data-type="sect4" data-pdf-bookmark="Implementation in Python and hyperparameters"><div class="sect4" id="idm45174931849304">
<h4>Implementation in Python and hyperparameters</h4>

<p><a data-type="indexterm" data-primary="gradient boosting method (GBM)" data-secondary="hyperparameters" id="idm45174931847864"/><a data-type="indexterm" data-primary="gradient boosting method (GBM)" data-secondary="implementation in Python" id="idm45174931846824"/>Gradient boosting method regression and classification models can be constructed using the sklearn package of Python, as shown in the following code snippet. The hyperparameters of gradient boosting method are similar to AdaBoost, as shown in the previous section:</p>

<p><code>Classification</code></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="k">import</code> <code class="n">GradientBoostingClassifier</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">GradientBoostingClassifier</code><code class="p">()</code>
<code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">Y</code><code class="p">)</code></pre>

<p><code>Regression</code></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="k">import</code> <code class="n">GradientBoostingRegressor</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">GradientBoostingRegressor</code><code class="p">()</code>
<code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">Y</code><code class="p">)</code></pre>
</div></section>













<section data-type="sect4" data-pdf-bookmark="Advantages and disadvantages"><div class="sect4" id="idm45174931739752">
<h4>Advantages and disadvantages</h4>

<p><a data-type="indexterm" data-primary="gradient boosting method (GBM)" data-secondary="advantages and disadvantages" id="idm45174931799880"/>In terms of advantages, gradient boosting method is robust to missing data, highly correlated features, and irrelevant features in the same way as random forest. It naturally assigns feature importance scores, with slightly better performance than random forest. The algorithm doesn’t need the data to be scaled and can model a nonlinear relationship.</p>

<p>In terms of disadvantages, it may be more prone to overfitting than random forest, as the main purpose of the boosting approach is to reduce bias and not variance. It has many hyperparameters to tune, so model development may not be as fast. Also, feature importance may not be robust to variation in the training dataset.<a data-type="indexterm" data-startref="ix_Chapter4-asciidoc12" id="idm45174931797784"/><a data-type="indexterm" data-startref="ix_Chapter4-asciidoc11" id="idm45174931797080"/></p>
</div></section>

</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="ANN-Based Models"><div class="sect2" id="idm45174931796024">
<h2>ANN-Based Models</h2>

<p><a data-type="indexterm" data-primary="artificial neural networks (ANNs)" data-secondary="ANN-based supervised learning models" id="idm45174931759400"/><a data-type="indexterm" data-primary="supervised learning" data-secondary="ANN-based models" id="idm45174931758440"/>In <a data-type="xref" href="ch03.xhtml#Chapter3">Chapter 3</a> we covered the basics of ANNs, along with the architecture of ANNs and their training and implementation in Python. The details provided in that chapter are applicable across all areas of machine learning, including supervised learning. However, there are a few additional details from the supervised learning perspective, which we will cover in this section.</p>

<p>Neural networks are reducible to a classification or regression model with the activation function of the node in the output layer. In the case of a regression problem, the output node has linear activation function (or no activation function). A linear function produces a continuous output ranging from <code>-inf</code> to <code>+inf</code>. Hence, the output layer will be the linear function of the nodes in the layer before the output layer, and it will be a regression-based model.</p>

<p>In the case of a classification problem, the output node has a sigmoid or softmax activation function. A sigmoid or softmax function produces an output ranging from zero to one to represent the probability of target value. Softmax function can also be used for multiple groups for classification.</p>










<section data-type="sect3" data-pdf-bookmark="ANN using sklearn"><div class="sect3" id="idm45174931753560">
<h3>ANN using sklearn</h3>

<p>ANN regression and classification models can be constructed using the sklearn package of Python, as shown in the following code snippet:</p>

<p><code>Classification</code></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">from</code> <code class="nn">sklearn.neural_network</code> <code class="k">import</code> <code class="n">MLPClassifier</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">MLPClassifier</code><code class="p">()</code>
<code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">Y</code><code class="p">)</code></pre>

<p><code>Regression</code></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">from</code> <code class="nn">sklearn.neural_network</code> <code class="k">import</code> <code class="n">MLPRegressor</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">MLPRegressor</code><code class="p">()</code>
<code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">Y</code><code class="p">)</code></pre>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Hyperparameters"><div class="sect3" id="idm45174931692600">
<h3>Hyperparameters</h3>

<p>As we saw in <a data-type="xref" href="ch03.xhtml#Chapter3">Chapter 3</a>, ANN has many hyperparameters. Some of the hyperparameters that are present in the sklearn implementation of ANN and can be tweaked while performing the grid search are:</p>
<dl>
<dt>Hidden Layers (<code>hidden_layer_sizes</code> in sklearn)</dt>
<dd>
<p>It represents the number of layers and nodes in the ANN architecture. In sklearn implementation of ANN, the ith  element represents the number of neurons in the ith hidden layer. A sample value for grid search in the sklearn implementation can be [(<em>20</em>,), (<em>50</em>,), (<em>20</em>, <em>20</em>), (<em>20</em>, <em>30</em>, <em>20</em>)].</p>
</dd>
<dt>Activation Function (<code>activation</code> in sklearn)</dt>
<dd>
<p>It represents the activation function of a hidden layer. Some of the activation functions defined in <a data-type="xref" href="ch03.xhtml#Chapter3">Chapter 3</a>, such as <code>sigmoid</code>, <code>relu</code>, or <code>tanh</code>, can be used.</p>
</dd>
</dl>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Deep neural network"><div class="sect3" id="idm45174931658584">
<h3>Deep neural network</h3>

<p><a data-type="indexterm" data-primary="deep neural networks" data-secondary="ANN-based supervised learning models" id="idm45174931657352"/>ANNs with more than a single hidden layer are often called deep networks. We prefer using the library Keras to implement such networks, given the flexibility of the library. The detailed implementation of a deep neural network in Keras was shown in <a data-type="xref" href="ch03.xhtml#Chapter3">Chapter 3</a>. Similar to <code>MLPClassifier</code> and <code>MLPRegressor</code> in sklearn for classification and regression, Keras has modules called <code>KerasClassifier</code> and <code>KerasRegressor</code> that can be used for creating classification and regression models with deep network.</p>

<p><a data-type="indexterm" data-primary="time series prediction" id="idm45174931653160"/>A popular problem in finance is time series prediction, which is predicting the next value of a time series based on a historical overview. Some of the deep neural networks, such as recurrent neural network (RNN), can be directly used for time series prediction. The details of this approach are provided in <a data-type="xref" href="ch05.xhtml#Chapter5">Chapter 5</a>.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Advantages and disadvantages"><div class="sect3" id="idm45174931650952">
<h3>Advantages and disadvantages</h3>

<p>The main advantage of an ANN is that it captures the nonlinear relationship between the variables quite well. ANN can more easily learn rich representations and is good with a large number of input features with a large dataset. ANN is flexible in how it can be used. This is evident from its use across a wide variety of areas in machine learning and AI, including reinforcement learning and NLP, as discussed in 
<span class="keep-together"><a data-type="xref" href="ch03.xhtml#Chapter3">Chapter 3</a></span>.</p>

<p>The main disadvantage of ANN is the interpretability of the model, which is a drawback that often cannot be ignored and is sometimes the determining factor when choosing a model. ANN is not good with small datasets and requires a lot of tweaking and guesswork. Choosing the right topology/algorithms to solve a problem is difficult. Also, ANN is computationally expensive and can take a lot of time to train.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Using ANNs for supervised learning in finance"><div class="sect3" id="idm45174931646888">
<h3>Using ANNs for supervised learning in finance</h3>

<p>If a simple model such as linear or logistic regression perfectly fits your problem, don’t bother with ANN. However, if you are modeling a complex dataset and feel a need for better prediction power, give ANN a try. ANN is one of the most flexible models in adapting itself to the shape of the data, and using it for supervised learning problems can be an interesting and valuable exercise.<a data-type="indexterm" data-startref="ix_Chapter4-asciidoc1" id="idm45174931626520"/></p>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Model Performance"><div class="sect1" id="idm45174933169752">
<h1>Model Performance</h1>

<p><a data-type="indexterm" data-primary="supervised learning" data-secondary="model performance" id="ix_Chapter4-asciidoc15"/>In the previous section, we discussed grid search as a way to find the right hyperparameter to achieve better performance. In this section, we will expand on that process by discussing the key components of evaluating the model performance, which are overfitting, cross validation, and evaluation metrics.</p>








<section data-type="sect2" data-pdf-bookmark="Overfitting and Underfitting"><div class="sect2" id="idm45174931622488">
<h2>Overfitting and Underfitting</h2>

<p><a data-type="indexterm" data-primary="overfitting" id="idm45174931621112"/><a data-type="indexterm" data-primary="supervised learning" data-secondary="overfitting/underfitting" id="idm45174931620184"/>A common problem in machine learning is <em>overfitting</em>, which is defined by learning a function that perfectly explains the training data that the model learned from but doesn’t generalize well to unseen test data. Overfitting happens when a model overlearns from the training data to the point that it starts picking up idiosyncrasies that aren’t representative of patterns in the real world. This becomes especially problematic as we make our models increasingly more complex. <a data-type="indexterm" data-primary="underfitting" id="idm45174931618312"/><em>Underfitting</em> is a related issue in which the model is not complex enough to capture the underlying trend in the data. <a data-type="xref" href="#OverfittingUnderfitting">Figure 4-5</a> illustrates overfitting and underfitting. The left-hand panel of <a data-type="xref" href="#OverfittingUnderfitting">Figure 4-5</a> shows a linear regression model; a straight line clearly underfits the true function. The middle panel shows that a high degree polynomial approximates the true relationship reasonably well. On the other hand, a polynomial of a very high degree fits the small sample almost perfectly, and performs best on the training data, but this doesn’t generalize, and it would do a horrible job at explaining a new data point.</p>

<p><a data-type="indexterm" data-primary="bias-variance trade-off" id="idm45174931614968"/>The concepts of overfitting and underfitting are closely linked to <em>bias-variance trade-off</em>. <a data-type="indexterm" data-primary="bias, defined" id="idm45174931613592"/><em>Bias</em> refers to the error due to overly simplistic assumptions or faulty assumptions in the learning algorithm. Bias results in underfitting of the data, as shown in the left-hand panel of <a data-type="xref" href="#OverfittingUnderfitting">Figure 4-5</a>. A high bias means our learning algorithm is missing important trends among the features. <a data-type="indexterm" data-primary="variance, defined" id="idm45174931611352"/><em>Variance</em> refers to the error due to an overly complex model that tries to fit the training data as closely as possible. In high variance cases, the model’s predicted values are extremely close to the actual values from the training set. High variance gives rise to overfitting, as shown in the right-hand panel of <a data-type="xref" href="#OverfittingUnderfitting">Figure 4-5</a>. Ultimately, in order to have a good model, we need low bias and low variance.</p>

<figure><div id="OverfittingUnderfitting" class="figure">
<img src="Images/mlbf_0405.png" alt="mlbf 0405" width="1249" height="373"/>
<h6><span class="label">Figure 4-5. </span>Overfitting and underfitting</h6>
</div></figure>

<p>There can be two ways to combat overfitting:</p>
<dl>
<dt>Using more training data</dt>
<dd>
<p>The more training data we have, the harder it is to overfit the data by learning too much from any single training example.</p>
</dd>
<dt>Using regularization</dt>
<dd>
<p>Adding a penalty in the loss function for building a model that assigns too much explanatory power to any one feature, or allows too many features to be taken into account.</p>
</dd>
</dl>

<p>The concept of overfitting and the ways to combat it are applicable across all the supervised learning models. For example, regularized regressions address overfitting in linear regression, as discussed earlier in this chapter.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Cross Validation"><div class="sect2" id="idm45174931602728">
<h2>Cross Validation</h2>

<p><a data-type="indexterm" data-primary="cross validation" id="idm45174931601288"/><a data-type="indexterm" data-primary="supervised learning" data-secondary="cross validation" id="idm45174931600360"/>One of the challenges of machine learning is training models that are able to generalize well to unseen data (overfitting versus underfitting or a bias-variance trade-off). The main idea behind <em>cross validation</em> is to split the data one time or several times so that each split is used once as a validation set and the remainder is used as a training set: part of the data (the training sample) is used to train the algorithm, and the remaining part (the validation sample) is used for estimating the risk of the algorithm. Cross validation allows us to obtain reliable estimates of the model’s generalization error. It is easiest to understand it with an example. When doing <em>k</em>-fold cross validation, we randomly split the training data into <em>k</em> folds. Then we train the model using <em>k-1</em> folds and evaluate the performance on the <em>k</em>th fold. We repeat this process <em>k</em> times and average the resulting scores.</p>

<p><a data-type="xref" href="#CrossValidation">Figure 4-6</a> shows an example of cross validation, where the data is split into five sets and in each round one of the sets is used for validation.</p>

<figure><div id="CrossValidation" class="figure">
<img src="Images/mlbf_0406.png" alt="mlbf 0406" width="991" height="479"/>
<h6><span class="label">Figure 4-6. </span>Cross validation</h6>
</div></figure>

<p>A potential drawback of cross validation is the computational cost, especially when paired with a grid search for hyperparameter tuning. Cross validation can be performed in a couple of lines using the sklearn package; we will perform cross validation in the supervised learning case studies.</p>

<p>In the next section, we cover the evaluation metrics for the supervised learning models that are used to measure and compare the models’ performance.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Evaluation Metrics"><div class="sect2" id="idm45174931591544">
<h2>Evaluation Metrics</h2>

<p><a data-type="indexterm" data-primary="supervised learning" data-secondary="evaluation metrics" id="ix_Chapter4-asciidoc16"/>The metrics used to evaluate the machine learning algorithms are very important. The choice of metrics to use influences how the performance of machine learning algorithms is measured and compared. The metrics influence both how you weight the importance of different characteristics in the results and your ultimate choice of algorithm.</p>

<p>The main evaluation metrics for regression and classification are illustrated in <a data-type="xref" href="#EvaluationMetrics">Figure 4-7</a>.</p>

<figure><div id="EvaluationMetrics" class="figure">
<img src="Images/mlbf_0407.png" alt="mlbf 0407" width="1107" height="338"/>
<h6><span class="label">Figure 4-7. </span>Evaluation metrics for regression and classification</h6>
</div></figure>

<p>Let us first look at the evaluation metrics for supervised regression.</p>










<section data-type="sect3" data-pdf-bookmark="Mean absolute error"><div class="sect3" id="idm45174931584488">
<h3>Mean absolute error</h3>

<p><a data-type="indexterm" data-primary="MAE (mean absolute error)" id="idm45174931583288"/><a data-type="indexterm" data-primary="mean absolute error (MAE)" id="idm45174931582568"/>The <em>mean absolute error</em> (MAE) is the sum of the absolute differences between predictions and actual values. The MAE is a linear score, which means that all the individual differences are weighted equally in the average. It gives an idea of how wrong the predictions were. The measure gives an idea of the magnitude of the error, but no idea of the direction (e.g., over- or underpredicting).</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Mean squared error"><div class="sect3" id="idm45174931580696">
<h3>Mean squared error</h3>

<p><a data-type="indexterm" data-primary="mean squared error (MSE)" id="idm45174931579496"/><a data-type="indexterm" data-primary="MSE (mean squared error)" id="idm45174931578776"/>The <em>mean squared error</em> (MSE)  represents the sample standard deviation of the differences between predicted values and observed values (called residuals). This is much like the mean absolute error in that it provides a gross idea of the magnitude of the error. <a data-type="indexterm" data-primary="root mean squared error (RMSE)" id="idm45174931577288"/>Taking the square root of the mean squared error converts the units back to the original units of the output variable and can be meaningful for description and presentation. This is called the <em>root mean squared error</em> (RMSE).</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="R² metric"><div class="sect3" id="idm45174931575592">
<h3>R² metric</h3>

<p><a data-type="indexterm" data-primary="R² metric" id="idm45174931574360"/>The <em>R² metric</em> provides an indication of the “goodness of fit” of the predictions to actual value. In statistical literature this measure is called the coefficient of determination. This is a value between zero and one, for no-fit and perfect fit, respectively.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Adjusted R² metric"><div class="sect3" id="idm45174931572584">
<h3>Adjusted R² metric</h3>

<p><a data-type="indexterm" data-primary="adjusted R² metric" id="idm45174931571384"/>Just like <em>R²</em>, <em>adjusted R²</em> also shows how well terms fit a curve or line but adjusts for the number of terms in a model. It is given in the following formula:</p>
<math display="block">
  <mrow>
    <msubsup><mi>R</mi> <mrow><mi>a</mi><mi>d</mi><mi>j</mi></mrow> <mn>2</mn> </msubsup>
    <mo>=</mo>
    <mn>1</mn>
    <mo>–</mo>
    <mfenced separators="" open="[" close="]">
      <mfrac><mrow><mrow><mo>(</mo><mn>1</mn><mo>–</mo><msup><mi>R</mi> <mn>2</mn> </msup><mo>)</mo></mrow><mrow><mo>(</mo><mi>n</mi><mo>–</mo><mn>1</mn><mo>)</mo></mrow><mrow><mo>)</mo></mrow></mrow> <mrow><mi>n</mi><mo>–</mo><mi>k</mi><mo>–</mo><mn>1</mn></mrow></mfrac>
    </mfenced>
  </mrow>
</math>

<p>where <em>n</em> is the total number of observations and <em>k</em> is the number of predictors. 
<span class="keep-together">Adjusted</span> <em>R²</em> will always be less than or equal to <em>R²</em>.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Selecting an evaluation metric for supervised regression"><div class="sect3" id="idm45174931553544">
<h3>Selecting an evaluation metric for supervised regression</h3>

<p><a data-type="indexterm" data-primary="supervised learning" data-secondary="selecting evaluation metric for supervised regression" id="idm45174931552344"/>In terms of a preference among these evaluation metrics, if the main goal is predictive accuracy, then RMSE is best. It is computationally simple and is easily differentiable. The loss is symmetric, but larger errors weigh more in the calculation. The MAEs are symmetric but do not weigh larger errors more. <em>R²</em> and adjusted <em>R²</em> are often used for explanatory purposes by indicating how well the selected independent variable(s) explains the variability in the dependent variable(s).</p>

<p>Let us first look at the evaluation metrics for supervised classification.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Classification"><div class="sect3" id="idm45174931549144">
<h3>Classification</h3>

<p><a data-type="indexterm" data-primary="classification" data-secondary="evaluation metrics" id="idm45174931547704"/><a data-type="indexterm" data-primary="supervised learning" data-secondary="classification metrics" id="idm45174931546728"/>For simplicity, we will mostly discuss things in terms of a binary classification problem (i.e., only two outcomes, such as true or false); some common terms are:</p>
<dl>
<dt>True positives (TP)</dt>
<dd>
<p>Predicted positive and are actually positive.</p>
</dd>
<dt>False positives (FP)</dt>
<dd>
<p>Predicted positive and are actually negative.</p>
</dd>
<dt>True negatives (TN)</dt>
<dd>
<p>Predicted negative and are actually negative.</p>
</dd>
<dt>False negatives (FN)</dt>
<dd>
<p>Predicted negative and are actually positive.</p>
</dd>
</dl>

<p>The difference between three commonly used evaluation metrics for classification, accuracy, precision, and recall, is illustrated in <a data-type="xref" href="#AccuracyPrecisionRecall">Figure 4-8</a>.</p>

<figure><div id="AccuracyPrecisionRecall" class="figure">
<img src="Images/mlbf_0408.png" alt="mlbf 0408" width="1341" height="457"/>
<h6><span class="label">Figure 4-8. </span>Accuracy, precision, and recall</h6>
</div></figure>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Accuracy"><div class="sect3" id="idm45174931536088">
<h3>Accuracy</h3>

<p><a data-type="indexterm" data-primary="accuracy (evaluation metric)" id="idm45174931534888"/>As shown in <a data-type="xref" href="#AccuracyPrecisionRecall">Figure 4-8</a>, <em>accuracy</em> is the number of correct predictions made as a ratio of all predictions made. This is the most common evaluation metric for classification problems and is also the most misused. It is most suitable when there are an equal number of observations in each class (which is rarely the case) and when all predictions and the related prediction errors are equally important, which is often not the case.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Precision"><div class="sect3" id="idm45174931532104">
<h3>Precision</h3>

<p><a data-type="indexterm" data-primary="precision (evaluation metric)" id="idm45174931530904"/><em>Precision</em> is the percentage of positive instances out of the total predicted positive instances. Here, the denominator is the model prediction done as positive from the whole given dataset. Precision is a good measure to determine when the cost of false positives is high (e.g., email spam detection).</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Recall"><div class="sect3" id="idm45174931529240">
<h3>Recall</h3>

<p><a data-type="indexterm" data-primary="recall (evaluation metric)" id="idm45174931528040"/><a data-type="indexterm" data-primary="sensitivity (recall)" id="idm45174931527320"/><a data-type="indexterm" data-primary="true positive rate (recall)" id="idm45174931526648"/><em>Recall</em> (or <em>sensitivity</em> or <em>true positive rate</em>) is the percentage of positive instances out of the total actual positive instances. Therefore, the denominator (true positive + false negative) is the actual number of positive instances present in the dataset. Recall is a good measure when there is a high cost associated with false negatives (e.g., fraud detection).</p>

<p>In addition to accuracy, precision, and recall, some of the other commonly used evaluation metrics for classification are discussed in the following sections.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Area under ROC curve"><div class="sect3" id="idm45174931523496">
<h3>Area under ROC curve</h3>

<p><a data-type="indexterm" data-primary="area under ROC curve (AUC)" id="idm45174931522296"/><a data-type="indexterm" data-primary="AUC (area under ROC curve)" id="idm45174931521576"/><em>Area under ROC curve</em> (AUC) is an evaluation metric for binary classification problems. ROC is a probability curve, and AUC represents degree or measure of separability. It tells how much the model is capable of distinguishing between classes. The higher the AUC, the better the model is at predicting zeros as zeros and ones as ones. An AUC of <em>0.5</em> means that the model has no class separation capacity whatsoever. The probabilistic interpretation of the AUC score is that if you randomly choose a positive case and a negative case, the probability that the positive case outranks the negative case according to the classifier is given by the AUC.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Confusion matrix"><div class="sect3" id="idm45174931519176">
<h3>Confusion matrix</h3>

<p><a data-type="indexterm" data-primary="confusion matrix" id="idm45174931517976"/>A confusion matrix lays out the performance of a learning algorithm. The confusion matrix is simply a square matrix that reports the counts of the true positive (TP), true negative (TN), false positive (FP), and false negative (FN) predictions of a classifier, as shown in <a data-type="xref" href="#ConfusionMetrix">Figure 4-9</a>.</p>

<figure><div id="ConfusionMetrix" class="figure">
<img src="Images/mlbf_0409.png" alt="mlbf 0409" width="538" height="355"/>
<h6><span class="label">Figure 4-9. </span>Confusion matrix</h6>
</div></figure>

<p>The confusion matrix is a handy presentation of the accuracy of a model with two or more classes. The table presents predictions on the <em>x-axis</em> and accuracy outcomes on the <em>y-axis</em>. The cells of the table are the number of predictions made by the model. For example, a model can predict zero or one, and each prediction may actually have been a zero or a one. Predictions for zero that were actually zero appear in the cell for prediction = 0 and actual = 0, whereas predictions for zero that were actually one appear in the cell for prediction = 0 and actual = 1.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Selecting an evaluation metric for supervised classification"><div class="sect3" id="idm45174931511992">
<h3>Selecting an evaluation metric for supervised classification</h3>

<p><a data-type="indexterm" data-primary="supervised classification" data-secondary="selecting an evaluation metric for" id="idm45174931510744"/>The evaluation metric for classification depends heavily on the task at hand. For example, recall is a good measure when there is a high cost associated with false negatives such as fraud detection. We will further examine these evaluation metrics in the case studies<a data-type="indexterm" data-startref="ix_Chapter4-asciidoc16" id="idm45174931509272"/>.<a data-type="indexterm" data-startref="ix_Chapter4-asciidoc15" id="idm45174931508472"/></p>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Model Selection"><div class="sect1" id="idm45174931590920">
<h1>Model Selection</h1>

<p><a data-type="indexterm" data-primary="supervised learning" data-secondary="model selection" id="ix_Chapter4-asciidoc17"/>Selecting the perfect machine learning model is both an art and a science. Looking at machine learning models, there is no one solution or approach that fits all. There are several factors that can affect your choice of a machine learning model. The main criteria in most of the cases is the model performance that we discussed in the previous section. However, there are many other factors to consider while performing model selection. In the following section, we will go over all such factors, followed by a discussion of model trade-offs.</p>








<section data-type="sect2" data-pdf-bookmark="Factors for Model Selection"><div class="sect2" id="idm45174931504408">
<h2>Factors for Model Selection</h2>

<p><a data-type="indexterm" data-primary="supervised learning" data-secondary="factors for model selection" id="idm45174931502872"/>The factors considered for the model selection process are as follows:</p>
<dl>
<dt>Simplicity</dt>
<dd>
<p>The degree of simplicity of the model. Simplicity usually results in quicker, more scalable, and easier to understand models and results.</p>
</dd>
<dt>Training time</dt>
<dd>
<p>Speed, performance, memory usage and overall time taken for model training.</p>
</dd>
<dt>Handle nonlinearity in the data</dt>
<dd>
<p>The ability of the model to handle the nonlinear relationship between the 
<span class="keep-together">variables</span>.</p>
</dd>
<dt>Robustness to overfitting</dt>
<dd>
<p>The ability of the model to handle overfitting.</p>
</dd>
<dt>Size of the dataset</dt>
<dd>
<p>The ability of the model to handle large number of training examples in the 
<span class="keep-together">dataset</span>.</p>
</dd>
<dt>Number of features</dt>
<dd>
<p>The ability of the model to handle high dimensionality of the feature space.</p>
</dd>
<dt>Model interpretation</dt>
<dd>
<p>How explainable is the model? Model interpretability is important because it allows us to take concrete actions to solve the underlying problem.</p>
</dd>
<dt>Feature scaling</dt>
<dd>
<p>Does the model require variables to be scaled or normally distributed?</p>
</dd>
</dl>

<p><a data-type="xref" href="#ModelSelection">Figure 4-10</a> compares the supervised learning models on the factors mentioned previously and outlines a general rule-of-thumb to narrow down the search for the best machine learning algorithm<sup><a data-type="noteref" id="idm45174931487736-marker" href="ch04.xhtml#idm45174931487736">7</a></sup> for a given problem. The table is based on the advantages and disadvantages of different models discussed in the individual model section in this chapter.</p>

<figure><div id="ModelSelection" class="figure">
<img src="Images/mlbf_0410.png" alt="mlbf 0410" width="1477" height="779"/>
<h6><span class="label">Figure 4-10. </span>Model selection</h6>
</div></figure>

<p>We can see from the table that relatively simple models include linear and logistic regression and as we move towards the ensemble and ANN, the complexity increases. In terms of the training time, the linear models and CART are relatively faster to train as compared to ensemble methods and ANN.</p>

<p>Linear and logistic regression can’t handle nonlinear relationships, while all other models can. SVM can handle the nonlinear relationship between dependent and independent variables with nonlinear kernels.</p>

<p>SVM and random forest tend to overfit less as compared to the linear regression, logistic regression, gradient boosting, and ANN. The degree of overfitting also depends on other parameters, such as size of the data and model tuning, and can be checked by looking at the results of the test set for each model. Also, the boosting methods such as gradient boosting have higher overfitting risk compared to the bagging methods, such as random forest. Recall the focus of gradient boosting is to minimize the bias and not variance.</p>

<p>Linear and logistic regressions are not able to handle large datasets and large number of features well. However, CART, ensemble methods, and ANN are capable of handling large datasets and many features quite well. The linear and logistic regression generally perform better than other models in case the size of the dataset is small. Application of variable reduction techniques (shown in <a data-type="xref" href="ch07.xhtml#Chapter7">Chapter 7</a>) enables the linear models to handle large datasets. The performance of ANN increases with an increase in the size of the dataset.</p>

<p>Given linear regression, logistic regression, and CART are relatively simpler models, they have better model interpretation as compared to the ensemble models and ANN.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Model Trade-off"><div class="sect2" id="idm45174931503816">
<h2>Model Trade-off</h2>

<p><a data-type="indexterm" data-primary="supervised learning" data-secondary="model trade-off" id="idm45174931476936"/>Often, it’s a trade-off between different factors when selecting a model. ANN, SVM, and some ensemble methods can be used to create very accurate predictive models, but they may lack simplicity and interpretability and may take a significant amount of resources to train.</p>

<p>In terms of selecting the final model, models with lower interpretability may be preferred when predictive performance is the most important goal, and it’s not necessary to explain how the model works and makes predictions. In some cases, however, model interpretability is mandatory.</p>

<p>Interpretability-driven examples are often seen in the financial industry. In many cases, choosing a machine learning algorithm has less to do with the optimization or the technical aspects of the algorithm and more to do with business decisions. Suppose a machine learning algorithm is used to accept or reject an individual’s credit card application. If the applicant is rejected and decides to file a complaint or take legal action, the financial institution will need to explain how that decision was made. While that can be nearly impossible for ANN, it’s relatively straightforward for decision tree–based models.</p>

<p>Different classes of models are good at modeling different types of underlying patterns in data. So a good first step is to quickly test out a few different classes of models to know which ones capture the underlying structure of the dataset most efficiently. We will follow this approach while performing model selection in all our supervised learning–based case studies.<a data-type="indexterm" data-startref="ix_Chapter4-asciidoc17" id="idm45174931473000"/></p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Chapter Summary"><div class="sect1" id="idm45174931472040">
<h1>Chapter Summary</h1>

<p>In this chapter, we discussed the importance of supervised learning models in finance, followed by a brief introduction to several supervised learning models, including linear and logistic regression, SVM, decision trees, ensemble, KNN, LDA, and ANN. We demonstrated training and tuning of these models in a few lines of code using sklearn and Keras libraries.</p>

<p>We discussed the most common error metrics for regression and classification models, explained the bias-variance trade-off, and illustrated the various tools for managing the model selection process using cross validation.</p>

<p>We introduced the strengths and weaknesses of each model and discussed the factors to consider when selecting the best model. We also discussed the trade-off between model performance and interpretability.<a data-type="indexterm" data-startref="ix_Chapter4-asciidoc0" id="idm45174931468984"/></p>

<p>In the following chapter, we will dive into the case studies for regression and classification. All case studies in the next two chapters leverage the concepts presented in this chapter and in the previous two chapters.</p>
</div></section>







<div data-type="footnotes"><p data-type="footnote" id="idm45174932998104"><sup><a href="ch04.xhtml#idm45174932998104-marker">1</a></sup> Cross validation will be covered in detail later in this chapter.</p><p data-type="footnote" id="idm45174932709384"><sup><a href="ch04.xhtml#idm45174932709384-marker">2</a></sup> See the activation function section of <a data-type="xref" href="ch03.xhtml#Chapter3">Chapter 3</a> for details on the <code>sigmoid</code> function.</p><p data-type="footnote" id="idm45174932651752"><sup><a href="ch04.xhtml#idm45174932651752-marker">3</a></sup> <a href="https://oreil.ly/y9atF">MLE</a> is a method of estimating the parameters of a probability distribution so that under the assumed statistical model the observed data is most probable.</p><p data-type="footnote" id="idm45174932370936"><sup><a href="ch04.xhtml#idm45174932370936-marker">4</a></sup> The approach of projecting data is similar to the PCA algorithm discussed in <a data-type="xref" href="ch07.xhtml#Chapter7">Chapter 7</a>.</p><p data-type="footnote" id="idm45174932160872"><sup><a href="ch04.xhtml#idm45174932160872-marker">5</a></sup> Bias and variance are described in detail later in this chapter.</p><p data-type="footnote" id="idm45174932048648"><sup><a href="ch04.xhtml#idm45174932048648-marker">6</a></sup> Split is the process of converting a nonhomogeneous parent node into two homogeneous child nodes (best possible).</p><p data-type="footnote" id="idm45174931487736"><sup><a href="ch04.xhtml#idm45174931487736-marker">7</a></sup> In this table we do not include <em>AdaBoost</em> and <em>extra trees</em> as their overall behavior across all the parameters are similar to <em>Gradient Boosting</em> and <em>Random Forest</em>, respectively.</p></div></div></section></div>



  </body></html>