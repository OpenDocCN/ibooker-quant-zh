- en: Chapter 9\. Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like a human, our agents learn for themselves to achieve successful strategies
    that lead to the greatest long-term rewards. This paradigm of learning by trial-and-error,
    solely from rewards or punishments, is known as reinforcement learning.^([1](ch09.xhtml#idm45625285042408))
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: DeepMind (2016)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The learning algorithms applied in Chapters [7](ch07.xhtml#dense_networks)
    and [8](ch08.xhtml#recurrent_networks) fall into the category of *supervised learning*.
    These methods require that there is a data set available with features and labels
    that allows the algorithms to learn relationships between the features and labels
    to succeed at estimation or classification tasks. As the simple example in [Chapter 1](ch01.xhtml#artificial_intelligence)
    illustrates, *reinforcement learning* (RL) works differently. To begin with, there
    is no need for a comprehensive data set of features and labels to be given up
    front. The data is rather generated by the learning agent while interacting with
    the environment of interest. This chapter covers RL in some detail and introduces
    fundamental notions, as well as one of the most popular algorithms used in the
    field: *Q-learning* (QL). Neural networks are not replaced by RL algorithms; they
    generally play an important role in this context as well.'
  prefs: []
  type: TYPE_NORMAL
- en: '[“Fundamental Notions”](#rl_notions) explains fundamental notions in RL, such
    as environments, states, and agents. [“OpenAI Gym”](#rl_oai_gym) introduces the
    OpenAI Gym suite of RL environments of which the `CartPole` environment is used
    as an example. In this environment, which [Chapter 2](ch02.xhtml#superintelligence)
    introduces and discusses briefly, agents must learn how to balance a pole on a
    cart by moving the cart to the left or to the right. [“Monte Carlo Agent”](#rl_mc_agent)
    shows how to solve the `CartPole` problem by the use of dimensionality reduction
    and Monte Carlo simulation. Standard supervised learning algorithms such as DNNs
    are in general not suited to solve problems such as the `CartPole` one since they
    lack a notion of delayed reward. This problem is illustrated in [“Neural Network
    Agent”](#rl_nn_agent). [“DQL Agent”](#rl_dql_agent) discusses a QL agent that
    explicitly takes into account delayed rewards and is able to solve the `CartPole`
    problem. The same agent is applied in [“Simple Finance Gym”](#rl_sf_gym) to a
    simple financial market environment. Although the agent does not perform too well
    in this setting, the example shows that QL agents can also learn to trade and
    to become what is often called a *trading bot*. To improve the learning of QL
    agents, [“Better Finance Gym”](#rl_bf_gym) presents an improved financial market
    environment that, among other benefits, allows the use of more than one type of
    feature to describe the state of the environment. Based on this improved environment,
    [“FQL Agent”](#rl_fql_agent) introduces and applies an improved financial QL agent
    that performs better as a trading bot.'
  prefs: []
  type: TYPE_NORMAL
- en: Fundamental Notions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section gives a brief overview of the fundamental notions in RL. Among
    them are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Environment
  prefs: []
  type: TYPE_NORMAL
- en: The *environment* defines the problem at hand. This can be a computer game to
    be played or a financial market to be traded in.
  prefs: []
  type: TYPE_NORMAL
- en: State
  prefs: []
  type: TYPE_NORMAL
- en: A *state* subsumes all relevant parameters that describe the current state of
    the environment. In a computer game, this might be the whole screen with all its
    pixels. In a financial market, this might include current and historical price
    levels or financial indicators such as moving averages, macroeconomic variables,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Agent
  prefs: []
  type: TYPE_NORMAL
- en: The term *agent* subsumes all elements of the RL algorithm that interacts with
    the environment and that learns from these interactions. In a gaming context,
    the agent might represent a player playing the game. In a financial context, the
    agent could represent a trader placing bets on rising or falling markets.
  prefs: []
  type: TYPE_NORMAL
- en: Action
  prefs: []
  type: TYPE_NORMAL
- en: An agent can choose one *action* from a (limited) set of allowed actions. In
    a computer game, movements to the left or right might be allowed actions, whereas
    in a financial market, going long or short could be admissible actions.
  prefs: []
  type: TYPE_NORMAL
- en: Step
  prefs: []
  type: TYPE_NORMAL
- en: Given an action of an agent, the state of the environment is updated. One such
    update is generally called a *step*. The concept of a step is general enough to
    encompass both heterogeneous and homogeneous time intervals between two steps.
    While in computer games, real-time interaction with the game environment is simulated
    by rather short, homogeneous time intervals (“game clock”), a trading bot interacting
    with a financial market environment could take actions at longer, heterogeneous
    time intervals, for instance.
  prefs: []
  type: TYPE_NORMAL
- en: Reward
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the action an agent chooses, a *reward* (or penalty) is awarded.
    For a computer game, points are a typical reward. In a financial context, profit
    (or loss) is a standard reward (or penalty).
  prefs: []
  type: TYPE_NORMAL
- en: Target
  prefs: []
  type: TYPE_NORMAL
- en: The *target* specifies what the agent tries to maximize. In a computer game,
    this in general is the score reached by the agent. For a financial trading bot,
    this might be the accumulated trading profit.
  prefs: []
  type: TYPE_NORMAL
- en: Policy
  prefs: []
  type: TYPE_NORMAL
- en: The *policy* defines which action an agent takes given a certain state of the
    environment. Given a certain state of a computer game, represented by all the
    pixels that make up the current scene, the policy might specify that the agent
    chooses “move right” as the action. A trading bot that observes three price increases
    in a row might decide, according to its policy, to short the market.
  prefs: []
  type: TYPE_NORMAL
- en: Episode
  prefs: []
  type: TYPE_NORMAL
- en: An *episode* is a set of steps from the initial state of the environment until
    success is achieved or failure is observed. In a game, this is from the start
    of the game until a win or loss. In the financial world, for example, this is
    from the beginning of the year to the end of the year or to bankruptcy.
  prefs: []
  type: TYPE_NORMAL
- en: Sutton and Barto (2018) provide a detailed introduction to the RL field. The
    book discusses the preceding notions in detail and illustrates them on the basis
    of a multitude of concrete examples. The following sections again choose a practical,
    implementation-oriented approach to RL. The examples discussed illustrate all
    of the preceding notions on the basis of Python code.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI Gym
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In most of the success stories as presented in [Chapter 2](ch02.xhtml#superintelligence),
    RL plays a dominant role. This has spurred widespread interest in RL as an algorithm.
    OpenAI is an organization that strives to facilitate research in AI in general
    and in RL in particular. OpenAI has developed and open sourced a suite of environments,
    called [`OpenAI Gym`](https://gym.openai.com), that allows the training of RL
    agents via a standardized API.
  prefs: []
  type: TYPE_NORMAL
- en: 'Among the many environments, there is the [`CartPole`](https://oreil.ly/f6tAK)
    environment (or game) that simulates a classical RL problem. A pole is standing
    upright on a cart, and the goal is to learn a policy to balance the pole on the
    cart by moving the cart either to the right or to the left. The state of the environment
    is given by four parameters, describing the following physical measurements: cart
    position, cart velocity, pole angle, and pole velocity (at tip). [Figure 9-1](#figure_rl_cp)
    shows a visualization of the environment.'
  prefs: []
  type: TYPE_NORMAL
- en: '![aiif 0901](Images/aiif_0901.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-1\. CartPole environment of OpenAI Gym
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Consider the following Python code that instantiates an environment object
    for `CartPole` and inspects the *observation space*. The observation space is
    a model for the state of the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_reinforcement_learning_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The environment object, with fixed seed values
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_reinforcement_learning_CO1-4)'
  prefs: []
  type: TYPE_NORMAL
- en: The observation space with minimal and maximal values
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_reinforcement_learning_CO1-7)'
  prefs: []
  type: TYPE_NORMAL
- en: Reset of the environment
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_reinforcement_learning_CO1-8)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Initial state: cart position, cart velocity, pole angle, pole angular velocity'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following environment, the allowed actions are described by the *action
    space*. In this case there are two, and they are represented by `0` (push cart
    to the left) and `1` (push cart to the right):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_reinforcement_learning_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The action space
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_reinforcement_learning_CO2-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Random actions sampled from the action space
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_reinforcement_learning_CO2-7)'
  prefs: []
  type: TYPE_NORMAL
- en: Step forward based on random action
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_reinforcement_learning_CO2-8)'
  prefs: []
  type: TYPE_NORMAL
- en: New state of the environment, reward, success/failure, additional information
  prefs: []
  type: TYPE_NORMAL
- en: As long as `done=False`, the agent is still in the game and can choose another
    action. Success is achieved when the agent reaches a total of 200 steps in a row
    or a total reward of 200 (reward of 1.0 per step). A failure is observed when
    the pole on the cart reaches a certain angle that would lead to the pole falling
    from the cart. In that case, `done=True` is returned.
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple agent is one that follows a completely random policy: no matter what
    state is observed, the agent chooses a random action. This is what the following
    code implements. The number of steps the agent can go only depends in such a case
    on how lucky it is. No learning in the form of updating the policy is taking place:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_reinforcement_learning_CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Random action policy
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_reinforcement_learning_CO3-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Stepping forward one step
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_reinforcement_learning_CO3-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Failure if less than 200 steps
  prefs: []
  type: TYPE_NORMAL
- en: Data Through Interaction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Whereas in supervised learning the training, validation, and test data sets
    are assumed to exist before the training begins, in RL the agent generates its
    data itself by interacting with the environment. In many contexts, such as in
    games, this is a huge simplification. Consider the game of chess: instead of loading
    thousands of historical human-played chess games into a computer, an RL agent
    can generate thousands or millions of games itself by playing against another
    chess engine or another version of itself, for instance.'
  prefs: []
  type: TYPE_NORMAL
- en: Monte Carlo Agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `CartPole` problem does not necessarily require a full-fledged RL approach
    nor some neural network to be solved. This section presents a simple solution
    to the problem based on Monte Carlo simulation. To this end, a specific policy
    is defined that makes use of *dimensionality reduction*. In that case, the four
    parameters defining a state of the environment are collapsed, via a linear combination,
    into a single real-valued parameter.^([2](ch09.xhtml#idm45625283606888)) The following
    Python code implements this idea:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_reinforcement_learning_CO4-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Random weights for fixed seed value
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_reinforcement_learning_CO4-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Initial state of the environment
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_reinforcement_learning_CO4-6)'
  prefs: []
  type: TYPE_NORMAL
- en: Dot product of state and weights
  prefs: []
  type: TYPE_NORMAL
- en: 'The policy is then defined based on the sign of the single state parameter
    `s`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This policy can then be used to play an episode of the `CartPole` game. Given
    the random nature of the weights applied, the results are in general not better
    than those of the random action policy of the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Therefore, Monte Carlo simulation is applied to test a large number of different
    weights. The following code simulates a large number of weights, checks them for
    success or failure, and then chooses the weights that yield success:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_reinforcement_learning_CO5-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Random weights.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_reinforcement_learning_CO5-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Total reward for these weights.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_reinforcement_learning_CO5-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Improvement observed?
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_reinforcement_learning_CO5-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Replace best total reward.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](Images/5.png)](#co_reinforcement_learning_CO5-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Replace best weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `CartPole` problem is considered solved by an agent if the average total
    reward over 100 consecutive episodes is 195 or higher. As the following code demonstrates,
    this is indeed the case for the Monte Carlo agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This is, of course, a strong benchmark that other, more sophisticated approaches
    are up against.
  prefs: []
  type: TYPE_NORMAL
- en: Neural Network Agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `CartPole` game can be cast into a classification setting as well. The state
    of an environment consists of four feature values. The correct action given the
    feature values is the label. By interacting with the environment, a neural network
    agent can collect a data set consisting of combinations of feature values and
    labels. Given this incrementally growing data set, a neural network can be trained
    to learn the correct action given a state of the environment. The neural network
    represents the policy in this case. The agent updates the policy based on new
    experiences.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, some imports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Second is the `NNAgent` class that combines the major elements of the agent:
    the neural network model for the policy, choosing an action given the policy,
    updating the policy (training the neural network), and the learning process itself
    over a number of episodes. The agent uses both *exploration* and *exploitation*
    to choose an action. Exploration refers to a random action, independent of the
    current policy. Exploitation refers to an action as derived from the current policy.
    The idea is that some degree of exploration ensures a richer experience and thereby
    improved learning for the agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_reinforcement_learning_CO6-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The maximum total reward
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_reinforcement_learning_CO6-2)'
  prefs: []
  type: TYPE_NORMAL
- en: The DNN classification model for the policy
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_reinforcement_learning_CO6-3)'
  prefs: []
  type: TYPE_NORMAL
- en: The method to choose an action (exploration and exploitation)
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_reinforcement_learning_CO6-4)'
  prefs: []
  type: TYPE_NORMAL
- en: The method to update the policy (train the neural network)
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](Images/5.png)](#co_reinforcement_learning_CO6-5)'
  prefs: []
  type: TYPE_NORMAL
- en: The method to learn from interacting with the environment
  prefs: []
  type: TYPE_NORMAL
- en: 'The neural network agent does not solve the problem for the configuration shown.
    The maximum total reward of 200 is not achieved even once:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_reinforcement_learning_CO7-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Average total reward over all episodes
  prefs: []
  type: TYPE_NORMAL
- en: Something seems to be missing with this approach. One major missing element
    is the idea of looking beyond the current state and action to be chosen. The approach
    implemented does not, by any means, take into account that success is only achieved
    when the agent survives 200 consecutive steps. Simply speaking, the agent avoids
    taking the wrong action but does not learn to win the game.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the collected history of states (features) and actions (labels) reveals
    that the neural network reaches an accuracy of around 75%.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, this does not translate into a winning policy as seen before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_reinforcement_learning_CO8-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Features (states) from all episodes
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_reinforcement_learning_CO8-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Labels (actions) from all episodes
  prefs: []
  type: TYPE_NORMAL
- en: DQL Agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Q-learning (QL) is an algorithm that takes into account delayed rewards in addition
    to immediate rewards from an action. The algorithm is due to Watkins (1989) and
    Watkins and Dayan (1992) and is explained in detail in Sutton and Barto (2018,
    ch. 6). QL addresses the problem of looking beyond the immediate next reward as
    encountered with the neural network agent.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm works roughly as follows. There is an *action-value* policy <math
    alttext="upper Q"><mi>Q</mi></math> , which assigns a value to every combination
    of a state and an action. The higher the value is, the better the action from
    the point of view of the agent will be. If the agent uses the policy <math alttext="upper
    Q"><mi>Q</mi></math> to choose an action, it selects the action with the highest
    value.
  prefs: []
  type: TYPE_NORMAL
- en: 'How is the value of an action derived? The value of an action is composed of
    its *direct reward* and the *discounted value* of the optimal action in the next
    state. The following is the formal expression:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper Q left-parenthesis upper S Subscript t Baseline comma upper
    A Subscript t Baseline right-parenthesis equals upper R Subscript t plus 1 Baseline
    plus gamma max Underscript a Endscripts upper Q left-parenthesis upper S Subscript
    t plus 1 Baseline comma a right-parenthesis" display="block"><mrow><mi>Q</mi>
    <mrow><mo>(</mo> <msub><mi>S</mi> <mi>t</mi></msub> <mo>,</mo> <msub><mi>A</mi>
    <mi>t</mi></msub> <mo>)</mo></mrow> <mo>=</mo> <msub><mi>R</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub>
    <mo>+</mo> <mi>γ</mi> <munder><mo movablelimits="true" form="prefix">max</mo>
    <mi>a</mi></munder> <mi>Q</mi> <mrow><mo>(</mo> <msub><mi>S</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub>
    <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Here, <math alttext="upper S Subscript t"><msub><mi>S</mi> <mi>t</mi></msub></math>
    is the state at step (time) <math alttext="t"><mi>t</mi></math> , <math alttext="upper
    A Subscript t"><msub><mi>A</mi> <mi>t</mi></msub></math> is the action taken at
    state <math alttext="upper S Subscript t"><msub><mi>S</mi> <mi>t</mi></msub></math>
    , <math alttext="upper R Subscript t plus 1"><msub><mi>R</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></math>
    is the direct reward of action <math alttext="upper A Subscript t"><msub><mi>A</mi>
    <mi>t</mi></msub></math> , <math alttext="0 less-than gamma less-than 1"><mrow><mn>0</mn>
    <mo><</mo> <mi>γ</mi> <mo><</mo> <mn>1</mn></mrow></math> is a discount factor,
    and <math alttext="max Underscript a Endscripts upper Q left-parenthesis upper
    S Subscript t plus 1 Baseline comma a right-parenthesis"><mrow><msub><mo movablelimits="true"
    form="prefix">max</mo> <mi>a</mi></msub> <mi>Q</mi> <mrow><mo>(</mo> <msub><mi>S</mi>
    <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub> <mo>,</mo> <mi>a</mi> <mo>)</mo></mrow></mrow></math>
    is the maximum delayed reward given the optimal action from the current policy
    <math alttext="upper Q"><mi>Q</mi></math> .
  prefs: []
  type: TYPE_NORMAL
- en: In a simple environment, with only a limited number of possible states, <math
    alttext="upper Q"><mi>Q</mi></math> can, for example, be represented by a *table*,
    listing for every state-action combination the corresponding value. However, in
    more interesting or complex settings, such as the `CartPole` environment, the
    number of states is too large for <math alttext="upper Q"><mi>Q</mi></math> to
    be written out comprehensively. Therefore, <math alttext="upper Q"><mi>Q</mi></math>
    is in general understood to be a *function*.
  prefs: []
  type: TYPE_NORMAL
- en: This is where neural networks come into play. In realistic settings and environments,
    a closed-form solution for the function <math alttext="upper Q"><mi>Q</mi></math>
    might not exist or might be too hard to derive, say, based on dynamic programming.
    Therefore, QL algorithms generally target *approximations* only. Neural networks,
    with their universal approximation capabilities, are a natural choice to accomplish
    the approximation of <math alttext="upper Q"><mi>Q</mi></math> .
  prefs: []
  type: TYPE_NORMAL
- en: Another critical element of QL is *replay*. The QL agent replays a number of
    experiences (state-action combinations) to update the policy function <math alttext="upper
    Q"><mi>Q</mi></math> regularly. This can improve the learning considerably. Furthermore,
    the QL agent presented in the following—`DQLAgent`—also alternates between exploration
    and exploitation during the learning. The alternation is done in a systematic
    way in that the agent starts with exploration only—in the beginning it could not
    have learned anything—and slowly but steadily decreases the exploration rate <math
    alttext="epsilon"><mi>ϵ</mi></math> until it reaches a minimum level:^([3](ch09.xhtml#idm45625283311016))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_reinforcement_learning_CO9-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Initial exploration rate
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_reinforcement_learning_CO9-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Minimum exploration rate
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_reinforcement_learning_CO9-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Decay rate for exploration rate
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_reinforcement_learning_CO9-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Discount factor for delayed reward
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](Images/5.png)](#co_reinforcement_learning_CO9-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Batch size for replay
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](Images/6.png)](#co_reinforcement_learning_CO9-6)'
  prefs: []
  type: TYPE_NORMAL
- en: '`deque` collection for limited history'
  prefs: []
  type: TYPE_NORMAL
- en: '[![7](Images/7.png)](#co_reinforcement_learning_CO9-7)'
  prefs: []
  type: TYPE_NORMAL
- en: Random selection of history batch for replay
  prefs: []
  type: TYPE_NORMAL
- en: '[![8](Images/8.png)](#co_reinforcement_learning_CO9-8)'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper Q"><mi>Q</mi></math> value for state-action pair
  prefs: []
  type: TYPE_NORMAL
- en: '[![9](Images/9.png)](#co_reinforcement_learning_CO9-9)'
  prefs: []
  type: TYPE_NORMAL
- en: Update of the neural network for the new action-value pairs
  prefs: []
  type: TYPE_NORMAL
- en: '[![10](Images/10.png)](#co_reinforcement_learning_CO9-10)'
  prefs: []
  type: TYPE_NORMAL
- en: Update of the exploration rate
  prefs: []
  type: TYPE_NORMAL
- en: '[![11](Images/11.png)](#co_reinforcement_learning_CO9-11)'
  prefs: []
  type: TYPE_NORMAL
- en: Storing the new data
  prefs: []
  type: TYPE_NORMAL
- en: '[![12](Images/12.png)](#co_reinforcement_learning_CO9-12)'
  prefs: []
  type: TYPE_NORMAL
- en: Replay to update the policy based on past experiences
  prefs: []
  type: TYPE_NORMAL
- en: 'How does the QL agent perform? As the code that follows shows, it reaches a
    winning state for `CartPole` of a total reward of 200\. [Figure 9-2](#figure_rl_01)
    shows the moving average of scores and how it increases over time, although not
    monotonically. To the contrary, the performance of the agent can significantly
    decrease at times, as [Figure 9-2](#figure_rl_01) shows. Among other things, the
    exploration that is taking place throughout leads to random actions that might
    not necessarily lead to good results in terms of total rewards but may lead to
    beneficial experiences for updating the policy network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![aiif 0902](Images/aiif_0902.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-2\. Average total rewards of `DQLAgent` for `CartPole`
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Does the QL agent solve the `CartPole` problem? In this particular case, it
    does, given the definition of success by OpenAI Gym:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Simple Finance Gym
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To transfer the QL approach to the financial domain, this section provides a
    class that mimics an OpenAI Gym environment, but for a financial market as represented
    by financial time series data. The idea is that, similar to the `CartPole` environment,
    four historical prices represent the state of the financial market. An agent can
    decide, when presented with the state, whether to go long or to go short. In that
    case, the two environments are comparable since a state is given by four parameters
    and an agent can take two different actions.
  prefs: []
  type: TYPE_NORMAL
- en: 'To mimic the OpenAI Gym API, two helper classes are needed—one for the observation
    space, and one for the action space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The following Python code defines the `Finance` class. It retrieves end-of-day
    historical prices for a number of symbols. The major methods of the class are
    `.reset()` and `.step()`. The `.step()` method checks whether the right action
    has been taken, defines the reward accordingly, and checks for success or failure.
    A success is achieved when the agent is able to correctly trade through the whole
    data set. This can, of course, be defined differently (say, a success is achieved
    when the agent trades successfully for 1,000 steps). A failure is defined as an
    accuracy ratio of less than 50% (total rewards divided by total number of steps).
    However, this is only checked for after a certain number of steps to avoid the
    high initial variance of this metric:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_reinforcement_learning_CO10-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Defines the minimum accuracy required.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_reinforcement_learning_CO10-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Selects the data defining the state of the financial market.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_reinforcement_learning_CO10-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Resets the environment to its initial values.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_reinforcement_learning_CO10-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Checks whether the agent has chosen the right action (successful trade).
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](Images/5.png)](#co_reinforcement_learning_CO10-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Defines the reward the agent receives.
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](Images/6.png)](#co_reinforcement_learning_CO10-6)'
  prefs: []
  type: TYPE_NORMAL
- en: Adds the reward to the total reward.
  prefs: []
  type: TYPE_NORMAL
- en: '[![7](Images/7.png)](#co_reinforcement_learning_CO10-7)'
  prefs: []
  type: TYPE_NORMAL
- en: Moves the environment one step forward.
  prefs: []
  type: TYPE_NORMAL
- en: '[![8](Images/8.png)](#co_reinforcement_learning_CO10-8)'
  prefs: []
  type: TYPE_NORMAL
- en: Calculates the accuracy of successful actions (trades) given all steps (trades).
  prefs: []
  type: TYPE_NORMAL
- en: '[![9](Images/9.png)](#co_reinforcement_learning_CO10-9)'
  prefs: []
  type: TYPE_NORMAL
- en: If the agent reaches the end of the data set, success is achieved.
  prefs: []
  type: TYPE_NORMAL
- en: '[![10](Images/10.png)](#co_reinforcement_learning_CO10-10)'
  prefs: []
  type: TYPE_NORMAL
- en: If the agent takes the right action, it can move on.
  prefs: []
  type: TYPE_NORMAL
- en: '[![11](Images/11.png)](#co_reinforcement_learning_CO10-11)'
  prefs: []
  type: TYPE_NORMAL
- en: If, after some initial steps, the accuracy drops under the minimum level, the
    episode ends (failure).
  prefs: []
  type: TYPE_NORMAL
- en: '[![12](Images/12.png)](#co_reinforcement_learning_CO10-12)'
  prefs: []
  type: TYPE_NORMAL
- en: For the remaining cases, the agent can move on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instances of the `Finance` class behave like an environment of the OpenAI Gym.
    In particular, in this base case, the instance behaves exactly like the `CartPole`
    environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_reinforcement_learning_CO11-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Specifies which symbol and which type of feature (symbol or log return) to be
    used to define the data representing the state
  prefs: []
  type: TYPE_NORMAL
- en: 'Can the `DQLAgent`, as developed for the `CartPole` game, learn to trade in
    a financial market? Yes, it can, as the following code illustrates. However, although
    the agent improves its trading skill (on average) over the training episodes,
    the results are not too impressive (see [Figure 9-3](#figure_rl_02)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![aiif 0903](Images/aiif_0903.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-3\. Average total rewards of `DQLAgent` for `Finance`
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: General RL Agents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section provides a class for a financial market environment that mimics
    the API of an OpenAI Gym environment. It also applies, without any changes to
    the agent itself, the QL agent to the new financial market environment. Although
    the performance of the agent in this new environment might not be impressive,
    it illustrates that the approach of RL, as introduced in this chapter, is rather
    general. RL agents can in general learn from different environments they interact
    with. This explains to some extent why AlphaZero from DeepMind is able to master
    not only the game of Go but also chess and shogi, as discussed in [Chapter 2](ch02.xhtml#superintelligence).
  prefs: []
  type: TYPE_NORMAL
- en: Better Finance Gym
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The idea in the previous section is to develop a simple class that allows RL
    within a financial market setting. The major goal in that section is to replicate
    the API of an OpenAI Gym environment. However, there is no need to restrict such
    an environment to a single type of feature to describe the state of the financial
    market nor to use only four lags. This section introduces an improved `Finance`
    class that allows for multiple features, a flexible number of lags, and specific
    start and end points for the base data set used. This, among other things, allows
    the use of one part of the data set for learning and another one for validation
    or testing. The Python code presented in the following also allows the use of
    leverage. This might be helpful when intraday data is considered with relatively
    small absolute returns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_reinforcement_learning_CO12-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The features to define the state
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_reinforcement_learning_CO12-3)'
  prefs: []
  type: TYPE_NORMAL
- en: The number of lags to be used
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_reinforcement_learning_CO12-4)'
  prefs: []
  type: TYPE_NORMAL
- en: The minimum gross performance required
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_reinforcement_learning_CO12-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Additional financial features (simple moving average, momentum, rolling volatility)
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](Images/5.png)](#co_reinforcement_learning_CO12-8)'
  prefs: []
  type: TYPE_NORMAL
- en: Gaussian normalization of the data
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](Images/6.png)](#co_reinforcement_learning_CO12-11)'
  prefs: []
  type: TYPE_NORMAL
- en: The leveraged return for the step
  prefs: []
  type: TYPE_NORMAL
- en: '[![7](Images/7.png)](#co_reinforcement_learning_CO12-12)'
  prefs: []
  type: TYPE_NORMAL
- en: The return-based reward for the step
  prefs: []
  type: TYPE_NORMAL
- en: '[![8](Images/8.png)](#co_reinforcement_learning_CO12-13)'
  prefs: []
  type: TYPE_NORMAL
- en: The gross performance after the step
  prefs: []
  type: TYPE_NORMAL
- en: 'The new `Finance` class gives more flexibility for the modeling of the financial
    market environment. The following code shows an example for two features and five
    lags:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: FQL Agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Relying on the new `Finance` environment, this section improves on the simple
    DQL agent to improve the performance in the financial market context. The `FQLAgent`
    class is able to handle multiple features and a flexible number of lags. It also
    distinguishes the learning environment (`learn_env`) from the validation environment
    (`valid_env`). This allows one to gain a more realistic picture of the out-of-sample
    performance of the agent during training. The basic structure of the class and
    the RL/QL learning approach is the same for both the `DQLAgent` class and the
    `FQLAgent` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The following Python code shows that the performance of the `FQLAgent` is substantially
    better than that of the simple `DQLAgent` that solves the `CartPole` problem.
    This trading bot seems to learn about trading rather consistently through interacting
    with the financial market environment (see [Figure 9-4](#figure_rl_03)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![aiif 0904](Images/aiif_0904.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-4\. Average total rewards of `FQLAgent` for `Finance`
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'An interesting picture also arises for the training and validation performances,
    as shown in [Figure 9-5](#figure_rl_04). The training performance shows a high
    variance, which is due, for example, to the exploration that is going on in addition
    to the exploitation of the currently optimal policy. In comparison, the validation
    performance has a much lower variance because it only relies on the exploitation
    of the currently optimal policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '![aiif 0905](Images/aiif_0905.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-5\. Training and validation performance of the `FQLAgent` per episode
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Conclusions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter discusses reinforcement learning as one of the most successful
    algorithm classes that AI has to offer. Most of the advances and success stories
    discussed in [Chapter 2](ch02.xhtml#superintelligence) have their origin in improvements
    in the field of RL. In this context, neural networks are not rendered useless.
    To the contrary, they play an important role in approximating the optimal action
    policy, usually in the form of a policy <math alttext="upper Q"><mi>Q</mi></math>
    that, given a certain state, assigns each action a value. The higher the value
    is, the better the action will be, taking into account both immediate and delayed
    rewards.
  prefs: []
  type: TYPE_NORMAL
- en: The inclusion of delayed rewards, of course, is relevant in many important contexts.
    In a gaming context, with multiple actions available in general, it is optimal
    to choose the one that promises the highest total reward—and probably not just
    the highest immediate reward. The final total score is what is to be maximized.
    The same holds true in a financial context. The long-term performance is in general
    the appropriate goal for trading and investing, not a quick short-term profit
    that might come at an increased risk of going bankrupt.
  prefs: []
  type: TYPE_NORMAL
- en: The examples in this chapter also demonstrate that the RL approach is rather
    flexible and general in that it can be applied to different settings equally well.
    The DQL agent that solves the `CartPole` problem can also learn how to trade in
    a financial market, although not too well. Based on improvements of the `Finance`
    environment and the FQL agent, the FQL trading bot shows a respectable performance
    both in-sample (on the training data) and out-of-sample (on the validation data).
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Books and papers cited in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sutton, Richard S. and Andrew G. Barto. 2018\. *Reinforcement Learning: An
    Introduction*. Cambridge and London: MIT Press.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Watkins, Christopher. 1989\. *Learning from Delayed Rewards*. Ph.D. thesis,
    University of Cambridge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Watkins, Christopher and Peter Dayan. 1992\. “Q-Learning.” *Machine Learning*
    8 (May): 279-282.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ^([1](ch09.xhtml#idm45625285042408-marker)) See [Deep Reinforcement Learning](https://oreil.ly/h-EFL).
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch09.xhtml#idm45625283606888-marker)) See, for example, this [blog post](https://oreil.ly/84RwE).
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch09.xhtml#idm45625283311016-marker)) The implementation is similar to
    the one found in this [blog post](https://oreil.ly/8mI4m).
  prefs: []
  type: TYPE_NORMAL
