<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 6. Supervised Learning: Classification"><div class="chapter" id="Chapter6">
<h1><span class="label">Chapter 6. </span>Supervised Learning: Classification</h1>


<p><a data-type="indexterm" data-primary="supervised classification" id="ix_Chapter6-asciidoc0"/><a data-type="indexterm" data-primary="supervised learning" data-secondary="classification" data-seealso="classification" id="ix_Chapter6-asciidoc1"/>Here are some of the key questions that financial analysts attempt to solve:</p>

<ul>
<li>
<p>Is a borrower going to repay their loan or default on it?</p>
</li>
<li>
<p>Will the instrument price go up or down?</p>
</li>
<li>
<p>Is this credit card transaction a fraud or not?</p>
</li>
</ul>

<p>All of these problem statements, in which the goal is to predict the categorical class labels, are inherently suitable for classification-based machine learning.</p>

<p>Classification-based algorithms have been used across many areas within finance that require predicting a qualitative response. These include fraud detection, default prediction, credit scoring, directional forecasting of asset price movement, and buy/sell recommendations. There are many other use cases of classification-based supervised learning in portfolio management and algorithmic trading.</p>

<p>In this chapter we cover three such classification-based case studies that span a diverse set of areas, including fraud detection, loan default probability, and formulating a trading strategy.</p>

<p>In <a data-type="xref" href="#CaseStudy1SC">“Case Study 1: Fraud Detection”</a>, we use a classification-based algorithm to predict whether a transaction is fraudulent. The focus of this case study is also to deal with an unbalanced dataset, given that the fraud dataset is highly unbalanced with a small number of fraudulent observations.</p>

<p>In <a data-type="xref" href="#CaseStudy2SC">“Case Study 2: Loan Default Probability”</a>, we use a classification-based 
<span class="keep-together">algorithm to</span> predict whether a loan will default. The case study focuses on various techniques and concepts of data processing, feature selection, and exploratory 
<span class="keep-together">analysis</span>.</p>

<p>In <a data-type="xref" href="#CaseStudy3SC">“Case Study 3: Bitcoin Trading Strategy”</a>, we use classification-based algorithms to predict whether the current trading signal of bitcoin is to buy or sell depending on the relationship between the short-term and long-term price. We predict the trend of bitcoin’s price using technical indicators. The prediction model can easily be transformed into a trading bot that can perform buy, sell, or hold actions without human intervention.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45174924226136">
<h5/>
<p>In addition to focusing on different problem statements in finance, these case studies will help you understand:</p>

<ul>
<li>
<p>How to develop new features such as technical indicators for an investment strategy using feature engineering, and how to improve model performance.</p>
</li>
<li>
<p>How to use data preparation and data transformation, and how to perform feature reduction and use feature importance.</p>
</li>
<li>
<p>How to use data visualization and exploratory data analysis for feature reduction and to improve model performance.</p>
</li>
<li>
<p>How to use algorithm tuning and grid search across various classification-based models to improve model performance.</p>
</li>
<li>
<p>How to handle unbalanced data.</p>
</li>
<li>
<p>How to use the appropriate evaluation metrics for classification.</p>
</li>
</ul>
</div></aside>
<div data-type="note" epub:type="note"><h1>This Chapter’s Code Repository</h1>
<p>A Python-based master template for supervised classification model, along with the Jupyter notebook for the case studies presented in this chapter, is included in the folder <a href="https://oreil.ly/y19Yc"><em>Chapter 6 - Sup. Learning - Classification models</em></a> in the code repository for this book. All of the case studies presented in this chapter use the standardized seven-step model development process presented in <a data-type="xref" href="ch02.xhtml#Chapter2">Chapter 2</a>.<sup><a data-type="noteref" id="idm45174924215160-marker" href="ch06.xhtml#idm45174924215160">1</a></sup></p>

<p>For any new classification-based problem, the master template from the code repository can be modified with the elements specific to the problem. The templates are designed to run on cloud infrastructure (e.g., Kaggle, Google Colab, or AWS). In order to run the template on the local machine, all the packages used within the template must be installed successfully.</p>
</div>






<section data-type="sect1" data-pdf-bookmark="Case Study 1: Fraud Detection"><div class="sect1" id="CaseStudy1SC">
<h1>Case Study 1: Fraud Detection</h1>

<p><a data-type="indexterm" data-primary="fraud detection" id="ix_Chapter6-asciidoc2"/>Fraud is one of the most significant issues the finance sector faces. It is incredibly costly. According to one study, it is estimated that the typical organization loses 5% of its annual revenue to fraud each year. When applied to the 2017 estimated Gross World Product of $79.6 trillion, this translates to potential global losses of up to $4 trillion.</p>

<p>Fraud detection is a task inherently suitable for machine learning, as machine learning–based models can scan through huge transactional datasets, detect unusual activity, and identify all cases that might be prone to fraud. Also, the computations of these models are faster compared to traditional rule-based approaches. By collecting data from various sources and then mapping them to trigger points, machine learning solutions are able to discover the rate of defaulting or fraud propensity for each potential customer and transaction, providing key alerts and insights for the financial institutions.</p>

<p>In this case study, we will use various classification-based models to detect whether a transaction is a normal payment or a fraud.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45174924208584">
<h5/>
<p>The focuses of this case study are:</p>

<ul>
<li>
<p>Handling unbalanced data by downsampling/upsampling the data.</p>
</li>
<li>
<p>Selecting the right evaluation metric, given that one of the main goals is to reduce false negatives (cases in which fraudulent transactions incorrectly go unnoticed).</p>
</li>
</ul>
</div></aside>
<img class="blueprint top" src="Images/bracket_top.png" width="1625" height="32"/>








<section data-type="sect2" class="blueprint" data-pdf-bookmark="Blueprint for Using Classification Models to Determine Whether a Transaction Is Fraudulent"><div class="sect2" id="idm45174924203928">
<h2>Blueprint for Using Classification Models to Determine Whether a Transaction Is Fraudulent</h2>










<section data-type="sect3" data-pdf-bookmark="1. Problem definition"><div class="sect3" id="idm45174924202296">
<h3>1. Problem definition</h3>

<p>In the classification framework defined for this case study, the response (or target) variable has the column name “Class.” This column has a value of 1 in the case of fraud and a value of 0 otherwise.</p>

<p>The dataset used is obtained from <a href="https://oreil.ly/CeFRs">Kaggle</a>. This dataset holds transactions by European cardholders that occurred over two days in September 2013, with 492 cases of fraud out of 284,807 transactions.</p>

<p>The dataset has been anonymized for privacy reasons. Given that certain feature names are not provided (i.e., they are called V1, V2, V3, etc.), the visualization and feature importance will not give much insight into the behavior of the model.</p>

<p>By the end of this case study, readers will be familiar with a general approach to fraud modeling, from gathering and cleaning data to building and tuning a classifier.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="2. Getting started—loading the data and Python packages"><div class="sect3" id="idm45174924197768">
<h3>2. Getting started—loading the data and Python packages</h3>












<section data-type="sect4" data-pdf-bookmark="2.1. Loading the Python packages"><div class="sect4" id="idm45174924196872">
<h4>2.1. Loading the Python packages</h4>

<p><a data-type="indexterm" data-primary="fraud detection" data-secondary="loading data and Python packages" id="idm45174924195256"/>The list of the libraries used for data loading, data analysis, data preparation, model evaluation, and model tuning are shown below. The packages used for different purposes have been separated in the Python code below. The details of most of these packages and functions have been provided in <a data-type="xref" href="ch02.xhtml#Chapter2">Chapter 2</a> and <a data-type="xref" href="ch04.xhtml#Chapter4">Chapter 4</a>:</p>

<p><code>Packages for data loading, data analysis, and data preparation</code></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">import</code> <code class="nn">pandas</code> <code class="k">as</code> <code class="nn">pd</code>
<code class="kn">import</code> <code class="nn">seaborn</code> <code class="k">as</code> <code class="nn">sns</code>
<code class="kn">from</code> <code class="nn">matplotlib</code> <code class="k">import</code> <code class="n">pyplot</code>

<code class="kn">from</code> <code class="nn">pandas</code> <code class="k">import</code> <code class="n">read_csv</code><code class="p">,</code> <code class="n">set_option</code>
<code class="kn">from</code> <code class="nn">pandas.plotting</code> <code class="k">import</code> <code class="n">scatter_matrix</code>
<code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="k">import</code> <code class="n">StandardScaler</code></pre>

<p><code>Packages for model evaluation and classification models</code></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="k">import</code> <code class="n">train_test_split</code><code class="p">,</code> <code class="n">KFold</code><code class="p">,</code>\
 <code class="n">cross_val_score</code><code class="p">,</code> <code class="n">GridSearchCV</code>
<code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="k">import</code> <code class="n">LogisticRegression</code>
<code class="kn">from</code> <code class="nn">sklearn.tree</code> <code class="k">import</code> <code class="n">DecisionTreeClassifier</code>
<code class="kn">from</code> <code class="nn">sklearn.neighbors</code> <code class="k">import</code> <code class="n">KNeighborsClassifier</code>
<code class="kn">from</code> <code class="nn">sklearn.discriminant_analysis</code> <code class="k">import</code> <code class="n">LinearDiscriminantAnalysis</code>
<code class="kn">from</code> <code class="nn">sklearn.naive_bayes</code> <code class="k">import</code> <code class="n">GaussianNB</code>
<code class="kn">from</code> <code class="nn">sklearn.svm</code> <code class="k">import</code> <code class="n">SVC</code>
<code class="kn">from</code> <code class="nn">sklearn.neural_network</code> <code class="k">import</code> <code class="n">MLPClassifier</code>
<code class="kn">from</code> <code class="nn">sklearn.pipeline</code> <code class="k">import</code> <code class="n">Pipeline</code>
<code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="k">import</code> <code class="n">AdaBoostClassifier</code><code class="p">,</code> <code class="n">GradientBoostingClassifier</code><code class="p">,</code>
<code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="k">import</code> <code class="n">RandomForestClassifier</code><code class="p">,</code> <code class="n">ExtraTreesClassifier</code>
<code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="k">import</code> <code class="n">classification_report</code><code class="p">,</code> <code class="n">confusion_matrix</code><code class="p">,</code>\
  <code class="n">accuracy_score</code></pre>

<p><code>Packages for deep learning models</code></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">from</code> <code class="nn">keras.models</code> <code class="k">import</code> <code class="n">Sequential</code>
<code class="kn">from</code> <code class="nn">keras.layers</code> <code class="k">import</code> <code class="n">Dense</code>
<code class="kn">from</code> <code class="nn">keras.wrappers.scikit_learn</code> <code class="k">import</code> <code class="n">KerasClassifier</code></pre>

<p class="pagebreak-before"><code>Packages for saving the model</code></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">from</code> <code class="nn">pickle</code> <code class="k">import</code> <code class="n">dump</code>
<code class="kn">from</code> <code class="nn">pickle</code> <code class="k">import</code> <code class="n">load</code></pre>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="3. Exploratory data analysis"><div class="sect3" id="idm45174924196248">
<h3>3. Exploratory data analysis</h3>

<p><a data-type="indexterm" data-primary="fraud detection" data-secondary="exploratory data analysis" id="idm45174923936520"/>The following sections walk through some high-level data inspection.</p>












<section data-type="sect4" data-pdf-bookmark="3.1. Descriptive statistics"><div class="sect4" id="idm45174923935352">
<h4>3.1. Descriptive statistics</h4>

<p>The first thing we must do is gather a basic sense of our data. Remember, except for the transaction and amount, we do not know the names of other columns. The only thing we know is that the values of those columns have been scaled. Let’s look at the shape and columns of the data:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c"># shape</code>
<code class="n">dataset</code><code class="o">.</code><code class="n">shape</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">(284807, 31)</pre>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c">#peek at data</code>
<code class="n">set_option</code><code class="p">(</code><code class="s">'display.width'</code><code class="p">,</code> <code class="mi">100</code><code class="p">)</code>
<code class="n">dataset</code><code class="o">.</code><code class="n">head</code><code class="p">(</code><code class="mi">5</code><code class="p">)</code></pre>

<p><code>Output</code></p>

<figure><div class="figure">
<img src="Images/mlbf_06in01.png" alt="mlbf 06in01" width="1078" height="155"/>
<h6/>
</div></figure>

<p><code>5 rows × 31 columns</code></p>

<p>As shown, the variable names are nondescript (<em>V1</em>, <em>V2</em>, etc.). Also, the data type for the entire dataset is <code>float</code>, except <code>Class</code>, which is of type integer.</p>

<p>How many are fraud and how many are not fraud? Let us check:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">class_names</code> <code class="o">=</code> <code class="p">{</code><code class="mi">0</code><code class="p">:</code><code class="s">'Not Fraud'</code><code class="p">,</code> <code class="mi">1</code><code class="p">:</code><code class="s">'Fraud'</code><code class="p">}</code>
<code class="nb">print</code><code class="p">(</code><code class="n">dataset</code><code class="o">.</code><code class="n">Class</code><code class="o">.</code><code class="n">value_counts</code><code class="p">()</code><code class="o">.</code><code class="n">rename</code><code class="p">(</code><code class="n">index</code> <code class="o">=</code> <code class="n">class_names</code><code class="p">))</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">Not Fraud    284315
Fraud           492
Name: Class, dtype: int64</pre>

<p>Notice the stark imbalance of the data labels. Most of the transactions are nonfraud. If we use this dataset as the base for our modeling, most models will not place enough emphasis on the fraud signals; the nonfraud data points will drown out any weight the fraud signals provide. As is, we may encounter difficulties modeling the prediction of fraud, with this imbalance leading the models to simply assume <em>all</em> transactions are nonfraud. This would be an unacceptable result. We will explore some ways of dealing with this issue in the subsequent sections.</p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="3.2. Data visualization"><div class="sect4" id="idm45174923934760">
<h4>3.2. Data visualization</h4>

<p>Since the feature descriptions are not provided, visualizing the data will not lead to much insight. This step will be skipped in this case study.</p>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="4. Data preparation"><div class="sect3" id="idm45174923797160">
<h3>4. Data preparation</h3>

<p><a data-type="indexterm" data-primary="fraud detection" data-secondary="data preparation" id="idm45174923795960"/>This data is from Kaggle and is already in a cleaned format without any empty rows or columns. Data cleaning or categorization is unnecessary.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="5. Evaluate models"><div class="sect3" id="idm45174923794440">
<h3>5. Evaluate models</h3>

<p><a data-type="indexterm" data-primary="fraud detection" data-secondary="evaluation of models" id="ix_Chapter6-asciidoc3"/>Now we are ready to split the data and evaluate the models.</p>












<section data-type="sect4" data-pdf-bookmark="5.1. Train-test split and evaluation metrics"><div class="sect4" id="idm45174923791768">
<h4>5.1. Train-test split and evaluation metrics</h4>

<p>As described in <a data-type="xref" href="ch02.xhtml#Chapter2">Chapter 2</a>, it is a good idea to partition the original dataset into training and test sets. The test set is a sample of the data that we hold back from our analysis and modeling. We use it at the end of our project to confirm the accuracy of our final model. It is the final test that gives us confidence in our estimates of accuracy on unseen data. We will use 80% of the dataset for model training and 20% for testing:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">Y</code><code class="o">=</code> <code class="n">dataset</code><code class="p">[</code><code class="s">"Class"</code><code class="p">]</code>
<code class="n">X</code> <code class="o">=</code> <code class="n">dataset</code><code class="o">.</code><code class="n">loc</code><code class="p">[:,</code> <code class="n">dataset</code><code class="o">.</code><code class="n">columns</code> <code class="o">!=</code> <code class="s">'Class'</code><code class="p">]</code>
<code class="n">validation_size</code> <code class="o">=</code> <code class="mf">0.2</code>
<code class="n">seed</code> <code class="o">=</code> <code class="mi">7</code>
<code class="n">X_train</code><code class="p">,</code> <code class="n">X_validation</code><code class="p">,</code> <code class="n">Y_train</code><code class="p">,</code> <code class="n">Y_validation</code> <code class="o">=</code>\
<code class="n">train_test_split</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">Y</code><code class="p">,</code> <code class="n">test_size</code><code class="o">=</code><code class="n">validation_size</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="n">seed</code><code class="p">)</code></pre>
</div></section>













<section data-type="sect4" data-pdf-bookmark="5.2. Checking models"><div class="sect4" id="idm45174923671880">
<h4>5.2. Checking models</h4>

<p>In this step, we will evaluate different machine learning models. To optimize the various hyperparameters of the models, we use ten-fold cross validation and recalculate the results ten times to account for the inherent randomness in some of the models and the CV process. All of these models, including cross validation, are described in <a data-type="xref" href="ch04.xhtml#Chapter4">Chapter 4</a>.</p>

<p>Let us design our test harness. We will evaluate algorithms using the <em>accuracy metric</em>. This is a gross metric that will give us a quick idea of how correct a given model is. It is useful on binary classification problems.</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c"># test options for classification</code>
<code class="n">num_folds</code> <code class="o">=</code> <code class="mi">10</code>
<code class="n">scoring</code> <code class="o">=</code> <code class="s">'accuracy'</code></pre>

<p>Let’s create a baseline of performance for this problem and spot-check a number of different algorithms. The selected algorithms include:</p>
<dl>
<dt>Linear algorithms</dt>
<dd>
<p>Logistic regression (LR) and linear discriminant analysis (LDA).</p>
</dd>
<dt>Nonlinear algorithms</dt>
<dd>
<p>Classification and regression trees (CART) and <em>K</em>-nearest neighbors (KNN).</p>
</dd>
</dl>

<p>There are good reasons for selecting these models. These models are simpler and faster models with good interpretation for problems with large datasets. CART and KNN will be able to discern any nonlinear relationships between the variables. The key problem here is using an unbalanced sample. Unless we resolve that, more complex models, such as ensemble and ANNs, will have poor prediction. We will focus on addressing this later in the case study and then will evaluate the performance of these types of models.</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c"># spot-check basic Classification algorithms</code>
<code class="n">models</code> <code class="o">=</code> <code class="p">[]</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'LR'</code><code class="p">,</code> <code class="n">LogisticRegression</code><code class="p">()))</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'LDA'</code><code class="p">,</code> <code class="n">LinearDiscriminantAnalysis</code><code class="p">()))</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'KNN'</code><code class="p">,</code> <code class="n">KNeighborsClassifier</code><code class="p">()))</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'CART'</code><code class="p">,</code> <code class="n">DecisionTreeClassifier</code><code class="p">()))</code></pre>

<p>All the algorithms use default tuning parameters. We will display the mean and standard deviation of accuracy for each algorithm as we calculate and collect the results for use later.</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">results</code> <code class="o">=</code> <code class="p">[]</code>
<code class="n">names</code> <code class="o">=</code> <code class="p">[]</code>
<code class="k">for</code> <code class="n">name</code><code class="p">,</code> <code class="n">model</code> <code class="ow">in</code> <code class="n">models</code><code class="p">:</code>
    <code class="n">kfold</code> <code class="o">=</code> <code class="n">KFold</code><code class="p">(</code><code class="n">n_splits</code><code class="o">=</code><code class="n">num_folds</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="n">seed</code><code class="p">)</code>
    <code class="n">cv_results</code> <code class="o">=</code> <code class="n">cross_val_score</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">X_train</code><code class="p">,</code> <code class="n">Y_train</code><code class="p">,</code> <code class="n">cv</code><code class="o">=</code><code class="n">kfold</code><code class="p">,</code> \
      <code class="n">scoring</code><code class="o">=</code><code class="n">scoring</code><code class="p">)</code>
    <code class="n">results</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">cv_results</code><code class="p">)</code>
    <code class="n">names</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">name</code><code class="p">)</code>
    <code class="n">msg</code> <code class="o">=</code> <code class="s">"%s: %f (%f)"</code> <code class="o">%</code> <code class="p">(</code><code class="n">name</code><code class="p">,</code> <code class="n">cv_results</code><code class="o">.</code><code class="n">mean</code><code class="p">(),</code> <code class="n">cv_results</code><code class="o">.</code><code class="n">std</code><code class="p">())</code>
    <code class="nb">print</code><code class="p">(</code><code class="n">msg</code><code class="p">)</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">LR: 0.998942 (0.000229)
LDA: 0.999364 (0.000136)
KNN: 0.998310 (0.000290)
CART: 0.999175 (0.000193)</pre>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c"># compare algorithms</code>
<code class="n">fig</code> <code class="o">=</code> <code class="n">pyplot</code><code class="o">.</code><code class="n">figure</code><code class="p">()</code>
<code class="n">fig</code><code class="o">.</code><code class="n">suptitle</code><code class="p">(</code><code class="s">'Algorithm Comparison'</code><code class="p">)</code>
<code class="n">ax</code> <code class="o">=</code> <code class="n">fig</code><code class="o">.</code><code class="n">add_subplot</code><code class="p">(</code><code class="mi">111</code><code class="p">)</code>
<code class="n">pyplot</code><code class="o">.</code><code class="n">boxplot</code><code class="p">(</code><code class="n">results</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_xticklabels</code><code class="p">(</code><code class="n">names</code><code class="p">)</code>
<code class="n">fig</code><code class="o">.</code><code class="n">set_size_inches</code><code class="p">(</code><code class="mi">8</code><code class="p">,</code><code class="mi">4</code><code class="p">)</code>
<code class="n">pyplot</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>

<figure class="width-90"><div class="figure">
<img src="Images/mlbf_06in02.png" alt="mlbf 06in02" width="724" height="357"/>
<h6/>
</div></figure>

<p>The accuracy of the overall result is quite high. But let us check how well it predicts the fraud cases. Choosing one of the model CART from the results above and looking at the result on the test set:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c"># prepare model</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">DecisionTreeClassifier</code><code class="p">()</code>
<code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">Y_train</code><code class="p">)</code>

<code class="c"># estimate accuracy on validation set</code>
<code class="n">predictions</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_validation</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">accuracy_score</code><code class="p">(</code><code class="n">Y_validation</code><code class="p">,</code> <code class="n">predictions</code><code class="p">))</code>
<code class="nb">print</code><code class="p">(</code><code class="n">classification_report</code><code class="p">(</code><code class="n">Y_validation</code><code class="p">,</code> <code class="n">predictions</code><code class="p">))</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">0.9992275552122467
              precision    recall  f1-score   support

           0       1.00      1.00      1.00     56862
           1       0.77      0.79      0.78       100

    accuracy                           1.00     56962
   macro avg       0.89      0.89      0.89     56962
weighted avg       1.00      1.00      1.00     56962</pre>

<p>And producing the confusion matrix yields:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">df_cm</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">confusion_matrix</code><code class="p">(</code><code class="n">Y_validation</code><code class="p">,</code> <code class="n">predictions</code><code class="p">),</code> \
<code class="n">columns</code><code class="o">=</code><code class="n">np</code><code class="o">.</code><code class="n">unique</code><code class="p">(</code><code class="n">Y_validation</code><code class="p">),</code> <code class="n">index</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">unique</code><code class="p">(</code><code class="n">Y_validation</code><code class="p">))</code>
<code class="n">df_cm</code><code class="o">.</code><code class="n">index</code><code class="o">.</code><code class="n">name</code> <code class="o">=</code> <code class="s">'Actual'</code>
<code class="n">df_cm</code><code class="o">.</code><code class="n">columns</code><code class="o">.</code><code class="n">name</code> <code class="o">=</code> <code class="s">'Predicted'</code>
<code class="n">sns</code><code class="o">.</code><code class="n">heatmap</code><code class="p">(</code><code class="n">df_cm</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s">"Blues"</code><code class="p">,</code> <code class="n">annot</code><code class="o">=</code><code class="k">True</code><code class="p">,</code><code class="n">annot_kws</code><code class="o">=</code><code class="p">{</code><code class="s">"size"</code><code class="p">:</code> <code class="mi">16</code><code class="p">})</code></pre>

<figure><div class="figure">
<img src="Images/mlbf_06in03.png" alt="mlbf 06in03" width="390" height="257"/>
<h6/>
</div></figure>

<p>Overall accuracy is strong, but the confusion metrics tell a different story. Despite the high accuracy level, 21 out of 100 instances of fraud are missed and incorrectly predicted as nonfraud. The <em>false negative</em> rate is substantial.</p>

<p>The intention of a fraud detection model is to minimize these false negatives. To do so, the first step would be to choose the right evaluation metric.</p>

<p>In <a data-type="xref" href="ch04.xhtml#Chapter4">Chapter 4</a>, we covered the evaluation metrics, such as accuracy, precision, and recall, for a classification-based problem. Accuracy is the number of correct predictions made as a ratio of all predictions made. Precision is the number of items correctly identified as positive out of total items identified as positive by the model. Recall is the total number of items correctly identified as positive out of total true positives.</p>

<p>For this type of problem, we should focus on recall, the ratio of true positives to the sum of true positives and false negatives. So if false negatives are high, then the value of recall will be low.</p>

<p>In the next step, we perform model tuning, select the model using the recall metric, and perform under-sampling.<a data-type="indexterm" data-startref="ix_Chapter6-asciidoc3" id="idm45174923244920"/></p>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="6. Model tuning"><div class="sect3" id="idm45174923715976">
<h3>6. Model tuning</h3>

<p><a data-type="indexterm" data-primary="fraud detection" data-secondary="model tuning" id="ix_Chapter6-asciidoc4"/>The purpose of the model tuning step is to perform the grid search on the model selected in the previous step. However, since we encountered poor model performance in the previous section due to the unbalanced dataset, we will focus our attention on that. We will analyze the impact of choosing the correct evaluation metric and see the impact of using an adjusted, balanced sample.</p>












<section data-type="sect4" data-pdf-bookmark="6.1. Model tuning by choosing the correct evaluation metric"><div class="sect4" id="idm45174923241416">
<h4>6.1. Model tuning by choosing the correct evaluation metric</h4>

<p>As mentioned in the preceding step, if false negatives are high, then the value of recall will be low. Models are ranked according to this metric:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">scoring</code> <code class="o">=</code> <code class="s">'recall'</code></pre>

<p>Let us spot-check some basic classification algorithms for recall:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">models</code> <code class="o">=</code> <code class="p">[]</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'LR'</code><code class="p">,</code> <code class="n">LogisticRegression</code><code class="p">()))</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'LDA'</code><code class="p">,</code> <code class="n">LinearDiscriminantAnalysis</code><code class="p">()))</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'KNN'</code><code class="p">,</code> <code class="n">KNeighborsClassifier</code><code class="p">()))</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'CART'</code><code class="p">,</code> <code class="n">DecisionTreeClassifier</code><code class="p">()))</code></pre>

<p>Running cross validation:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">results</code> <code class="o">=</code> <code class="p">[]</code>
<code class="n">names</code> <code class="o">=</code> <code class="p">[]</code>
<code class="k">for</code> <code class="n">name</code><code class="p">,</code> <code class="n">model</code> <code class="ow">in</code> <code class="n">models</code><code class="p">:</code>
    <code class="n">kfold</code> <code class="o">=</code> <code class="n">KFold</code><code class="p">(</code><code class="n">n_splits</code><code class="o">=</code><code class="n">num_folds</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="n">seed</code><code class="p">)</code>
    <code class="n">cv_results</code> <code class="o">=</code> <code class="n">cross_val_score</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">X_train</code><code class="p">,</code> <code class="n">Y_train</code><code class="p">,</code> <code class="n">cv</code><code class="o">=</code><code class="n">kfold</code><code class="p">,</code> \
      <code class="n">scoring</code><code class="o">=</code><code class="n">scoring</code><code class="p">)</code>
    <code class="n">results</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">cv_results</code><code class="p">)</code>
    <code class="n">names</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">name</code><code class="p">)</code>
    <code class="n">msg</code> <code class="o">=</code> <code class="s">"%s: %f (%f)"</code> <code class="o">%</code> <code class="p">(</code><code class="n">name</code><code class="p">,</code> <code class="n">cv_results</code><code class="o">.</code><code class="n">mean</code><code class="p">(),</code> <code class="n">cv_results</code><code class="o">.</code><code class="n">std</code><code class="p">())</code>
    <code class="nb">print</code><code class="p">(</code><code class="n">msg</code><code class="p">)</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">LR: 0.595470 (0.089743)
LDA: 0.758283 (0.045450)
KNN: 0.023882 (0.019671)
CART: 0.735192 (0.073650)</pre>

<p>We see that the LDA model has the best recall of the four models. We continue by evaluating the test set using the trained LDA:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c"># prepare model</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">LinearDiscriminantAnalysis</code><code class="p">()</code>
<code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">Y_train</code><code class="p">)</code>
<code class="c"># estimate accuracy on validation set</code>

<code class="n">predictions</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_validation</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">accuracy_score</code><code class="p">(</code><code class="n">Y_validation</code><code class="p">,</code> <code class="n">predictions</code><code class="p">))</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">0.9995435553526912</pre>

<figure><div class="figure">
<img src="Images/mlbf_06in04.png" alt="mlbf 06in04" width="390" height="257"/>
<h6/>
</div></figure>

<p>LDA performs better, missing only 18 out of 100 cases of fraud. Additionally, we find fewer false positives as well. However, there is still improvement to be made.</p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="6.2. Model tuning—balancing the sample by random under-sampling"><div class="sect4" id="idm45174923240824">
<h4>6.2. Model tuning—balancing the sample by random under-sampling</h4>

<p><a data-type="indexterm" data-primary="random under-sampling" id="ix_Chapter6-asciidoc5"/><a data-type="indexterm" data-primary="under-sampling" id="ix_Chapter6-asciidoc6"/>The current data exhibits a significant class imbalance, where there are very few data points labeled “fraud.” The issue of such class imbalance can result in a serious bias toward the majority class, reducing the classification performance and increasing the number of false negatives.</p>

<p>One of the remedies to handle such situations is to <em>under-sample</em> the data. A simple technique is to under-sample the majority class randomly and uniformly. This might lead to a loss of information, but it may yield strong results by modeling the minority class well.</p>

<p>Next, we will implement random under-sampling, which consists of removing data to have a more balanced dataset. This will help ensure that our models avoid 
<span class="keep-together">overfitting</span>.</p>

<p>The steps to implement random under-sampling are:</p>
<ol>
<li>
<p>First, we determine the severity of the class imbalance by using <code>value_counts()</code> on the class column. We determine how many instances are considered fraud transactions (<em>fraud = 1</em>).</p>
</li>
<li>
<p>We bring the nonfraud transaction observation count to the same amount as fraud transactions.
Assuming we want a 50/50 ratio, this will be equivalent to 492 cases of fraud and 492 cases of nonfraud transactions.</p>
</li>
<li>
<p>We now have a subsample of our dataframe with a 50/50 ratio
with regards to our classes. We train the models on this subsample. Then we perform this iteration again to shuffle the nonfraud observations in the training sample. We keep track of the model performance to see whether our models can maintain a certain accuracy every time we repeat this process:</p>
</li>

</ol>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">concat</code><code class="p">([</code><code class="n">X_train</code><code class="p">,</code> <code class="n">Y_train</code><code class="p">],</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
<code class="c"># amount of fraud classes 492 rows.</code>
<code class="n">fraud_df</code> <code class="o">=</code> <code class="n">df</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="n">df</code><code class="p">[</code><code class="s">'Class'</code><code class="p">]</code> <code class="o">==</code> <code class="mi">1</code><code class="p">]</code>
<code class="n">non_fraud_df</code> <code class="o">=</code> <code class="n">df</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="n">df</code><code class="p">[</code><code class="s">'Class'</code><code class="p">]</code> <code class="o">==</code> <code class="mi">0</code><code class="p">][:</code><code class="mi">492</code><code class="p">]</code>

<code class="n">normal_distributed_df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">concat</code><code class="p">([</code><code class="n">fraud_df</code><code class="p">,</code> <code class="n">non_fraud_df</code><code class="p">])</code>

<code class="c"># Shuffle dataframe rows</code>
<code class="n">df_new</code> <code class="o">=</code> <code class="n">normal_distributed_df</code><code class="o">.</code><code class="n">sample</code><code class="p">(</code><code class="n">frac</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="c"># split out validation dataset for the end</code>
<code class="n">Y_train_new</code><code class="o">=</code> <code class="n">df_new</code><code class="p">[</code><code class="s">"Class"</code><code class="p">]</code>
<code class="n">X_train_new</code> <code class="o">=</code> <code class="n">df_new</code><code class="o">.</code><code class="n">loc</code><code class="p">[:,</code> <code class="n">dataset</code><code class="o">.</code><code class="n">columns</code> <code class="o">!=</code> <code class="s">'Class'</code><code class="p">]</code></pre>

<p>Let us look at the distribution of the classes in the dataset:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="nb">print</code><code class="p">(</code><code class="s">'Distribution of the Classes in the subsample dataset'</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">df_new</code><code class="p">[</code><code class="s">'Class'</code><code class="p">]</code><code class="o">.</code><code class="n">value_counts</code><code class="p">()</code><code class="o">/</code><code class="nb">len</code><code class="p">(</code><code class="n">df_new</code><code class="p">))</code>
<code class="n">sns</code><code class="o">.</code><code class="n">countplot</code><code class="p">(</code><code class="s">'Class'</code><code class="p">,</code> <code class="n">data</code><code class="o">=</code><code class="n">df_new</code><code class="p">)</code>
<code class="n">pyplot</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="s">'Equally Distributed Classes'</code><code class="p">,</code> <code class="n">fontsize</code><code class="o">=</code><code class="mi">14</code><code class="p">)</code>
<code class="n">pyplot</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">Distribution of the Classes in the subsample dataset
1    0.5
0    0.5
Name: Class, dtype: float64</pre>

<figure><div class="figure">
<img src="Images/mlbf_06in05.png" alt="mlbf 06in05" width="376" height="264"/>
<h6/>
</div></figure>

<p class="pagebreak-before">The data is now balanced, with close to 1,000 observations. We will train all the models again, including an ANN. Now that the data is balanced, we will focus on accuracy as our main evaluation metric, since it considers both false positives and false negatives. Recall can always be produced if needed:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c">#setting the evaluation metric</code>
<code class="n">scoring</code><code class="o">=</code><code class="s">'accuracy'</code>
<code class="c"># spot-check the algorithms</code>
<code class="n">models</code> <code class="o">=</code> <code class="p">[]</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'LR'</code><code class="p">,</code> <code class="n">LogisticRegression</code><code class="p">()))</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'LDA'</code><code class="p">,</code> <code class="n">LinearDiscriminantAnalysis</code><code class="p">()))</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'KNN'</code><code class="p">,</code> <code class="n">KNeighborsClassifier</code><code class="p">()))</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'CART'</code><code class="p">,</code> <code class="n">DecisionTreeClassifier</code><code class="p">()))</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'NB'</code><code class="p">,</code> <code class="n">GaussianNB</code><code class="p">()))</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'SVM'</code><code class="p">,</code> <code class="n">SVC</code><code class="p">()))</code>
<code class="c">#Neural Network</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'NN'</code><code class="p">,</code> <code class="n">MLPClassifier</code><code class="p">()))</code>
<code class="c"># Ensemble Models</code>
<code class="c"># Boosting methods</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'AB'</code><code class="p">,</code> <code class="n">AdaBoostClassifier</code><code class="p">()))</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'GBM'</code><code class="p">,</code> <code class="n">GradientBoostingClassifier</code><code class="p">()))</code>
<code class="c"># Bagging methods</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'RF'</code><code class="p">,</code> <code class="n">RandomForestClassifier</code><code class="p">()))</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'ET'</code><code class="p">,</code> <code class="n">ExtraTreesClassifier</code><code class="p">()))</code></pre>

<p>The steps to define and compile an ANN-based deep learning model in Keras, along with all the terms (neurons, activation, momentum, etc.) mentioned in the following code, have been described in <a data-type="xref" href="ch03.xhtml#Chapter3">Chapter 3</a>. This code can be leveraged for any deep learning–based classification model.</p>

<p><code>Keras-based deep learning model:</code></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c"># Function to create model, required for KerasClassifier</code>
<code class="k">def</code> <code class="nf">create_model</code><code class="p">(</code><code class="n">neurons</code><code class="o">=</code><code class="mi">12</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s">'relu'</code><code class="p">,</code> <code class="n">learn_rate</code> <code class="o">=</code> <code class="mf">0.01</code><code class="p">,</code> <code class="n">momentum</code><code class="o">=</code><code class="mi">0</code><code class="p">):</code>
    <code class="c"># create model</code>
    <code class="n">model</code> <code class="o">=</code> <code class="n">Sequential</code><code class="p">()</code>
    <code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dense</code><code class="p">(</code><code class="n">X_train</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code> <code class="n">input_dim</code><code class="o">=</code><code class="n">X_train</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code> \
      <code class="n">activation</code><code class="o">=</code><code class="n">activation</code><code class="p">))</code>
    <code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dense</code><code class="p">(</code><code class="mi">32</code><code class="p">,</code><code class="n">activation</code><code class="o">=</code><code class="n">activation</code><code class="p">))</code>
    <code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dense</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s">'sigmoid'</code><code class="p">))</code>
    <code class="c"># Compile model</code>
    <code class="n">optimizer</code> <code class="o">=</code> <code class="n">SGD</code><code class="p">(</code><code class="n">lr</code><code class="o">=</code><code class="n">learn_rate</code><code class="p">,</code> <code class="n">momentum</code><code class="o">=</code><code class="n">momentum</code><code class="p">)</code>
    <code class="n">model</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">loss</code><code class="o">=</code><code class="s">'binary_crossentropy'</code><code class="p">,</code> <code class="n">optimizer</code><code class="o">=</code><code class="s">'adam'</code><code class="p">,</code> \
    <code class="n">metrics</code><code class="o">=</code><code class="p">[</code><code class="s">'accuracy'</code><code class="p">])</code>
    <code class="k">return</code> <code class="n">model</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'DNN'</code><code class="p">,</code> <code class="n">KerasClassifier</code><code class="p">(</code><code class="n">build_fn</code><code class="o">=</code><code class="n">create_model</code><code class="p">,</code>\
<code class="n">epochs</code><code class="o">=</code><code class="mi">50</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">verbose</code><code class="o">=</code><code class="mi">0</code><code class="p">)))</code></pre>

<p>Running the cross validation on the new set of models results in the following:</p>

<figure><div class="figure">
<img src="Images/mlbf_06in06.png" alt="mlbf 06in06" width="806" height="429"/>
<h6/>
</div></figure>

<p>Although a couple of models, including random forest (RF) and logistic regression (LR), perform well, GBM slightly edges out the other models. We select this for further analysis. Note that the result of the deep learning model using Keras (i.e., “DNN”) is poor.</p>

<p>A grid search is performed for the GBM model by varying the number of estimators and maximum depth. The details of the GBM model and the parameters to tune for this model are described in <a data-type="xref" href="ch04.xhtml#Chapter4">Chapter 4</a>.</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c"># Grid Search: GradientBoosting Tuning</code>
<code class="n">n_estimators</code> <code class="o">=</code> <code class="p">[</code><code class="mi">20</code><code class="p">,</code><code class="mi">180</code><code class="p">,</code><code class="mi">1000</code><code class="p">]</code>
<code class="n">max_depth</code><code class="o">=</code> <code class="p">[</code><code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code><code class="mi">5</code><code class="p">]</code>
<code class="n">param_grid</code> <code class="o">=</code> <code class="nb">dict</code><code class="p">(</code><code class="n">n_estimators</code><code class="o">=</code><code class="n">n_estimators</code><code class="p">,</code> <code class="n">max_depth</code><code class="o">=</code><code class="n">max_depth</code><code class="p">)</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">GradientBoostingClassifier</code><code class="p">()</code>
<code class="n">kfold</code> <code class="o">=</code> <code class="n">KFold</code><code class="p">(</code><code class="n">n_splits</code><code class="o">=</code><code class="n">num_folds</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="n">seed</code><code class="p">)</code>
<code class="n">grid</code> <code class="o">=</code> <code class="n">GridSearchCV</code><code class="p">(</code><code class="n">estimator</code><code class="o">=</code><code class="n">model</code><code class="p">,</code> <code class="n">param_grid</code><code class="o">=</code><code class="n">param_grid</code><code class="p">,</code> <code class="n">scoring</code><code class="o">=</code><code class="n">scoring</code><code class="p">,</code> \
  <code class="n">cv</code><code class="o">=</code><code class="n">kfold</code><code class="p">)</code>
<code class="n">grid_result</code> <code class="o">=</code> <code class="n">grid</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train_new</code><code class="p">,</code> <code class="n">Y_train_new</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="s">"Best: %f using %s"</code> <code class="o">%</code> <code class="p">(</code><code class="n">grid_result</code><code class="o">.</code><code class="n">best_score_</code><code class="p">,</code> <code class="n">grid_result</code><code class="o">.</code><code class="n">best_params_</code><code class="p">))</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">Best: 0.936992 using {'max_depth': 5, 'n_estimators': 1000}</pre>

<p>In the next step, the final model is prepared, and the result on the test set is checked:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c"># prepare model</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">GradientBoostingClassifier</code><code class="p">(</code><code class="n">max_depth</code><code class="o">=</code> <code class="mi">5</code><code class="p">,</code> <code class="n">n_estimators</code> <code class="o">=</code> <code class="mi">1000</code><code class="p">)</code>
<code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train_new</code><code class="p">,</code> <code class="n">Y_train_new</code><code class="p">)</code>
<code class="c"># estimate accuracy on Original validation set</code>
<code class="n">predictions</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_validation</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">accuracy_score</code><code class="p">(</code><code class="n">Y_validation</code><code class="p">,</code> <code class="n">predictions</code><code class="p">))</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">0.9668199852533268</pre>

<p>The accuracy of the model is high. Let’s look at the confusion matrix:</p>

<p><code>Output</code></p>

<figure><div class="figure">
<img src="Images/mlbf_06in07.png" alt="mlbf 06in07" width="365" height="247"/>
<h6/>
</div></figure>

<p>The results on the test set are impressive, with a high accuracy and, importantly, no false negatives. However, we see that an outcome of using our under-sampled data is a propensity for false positives—cases in which nonfraud transactions are misclassified as fraudulent. This is a trade-off the financial institution would have to consider. There is an inherent cost balance between the operational overhead, and possible customer experience impact, from processing false positives and the financial loss resulting from missing fraud cases through false negatives.</p>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="Conclusion"><div class="sect3" id="idm45174922932344">
<h3>Conclusion</h3>

<p>In this case study, we performed fraud detection on credit card transactions. We illustrated how different classification machine learning models stack up against each other and demonstrated that choosing the right metric can make an important difference in model evaluation. Under-sampling was shown to lead to a significant improvement, as all fraud cases in the test set were correctly identified after applying under-sampling. This came with a trade-off, though. The reduction in false negatives came with an increase in false positives.</p>

<p>Overall, by using different machine learning models, choosing the right evaluation metrics, and handling unbalanced data, we demonstrated how the implementation of a simple classification-based model can produce robust results for <a data-type="indexterm" data-startref="ix_Chapter6-asciidoc6" id="idm45174922430920"/><a data-type="indexterm" data-startref="ix_Chapter6-asciidoc5" id="idm45174922430216"/>fraud detection<a data-type="indexterm" data-startref="ix_Chapter6-asciidoc4" id="idm45174922429416"/>.<a data-type="indexterm" data-startref="ix_Chapter6-asciidoc2" id="idm45174922428584"/></p>
<img class="blueprint bottom" src="Images/bracket_bottom.png" width="1625" height="32"/>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Case Study 2: Loan Default Probability"><div class="sect1" id="CaseStudy2SC">
<h1>Case Study 2: Loan Default Probability</h1>

<p><a data-type="indexterm" data-primary="loan default probability" id="ix_Chapter6-asciidoc7"/>Lending is one of the most important activities of the finance industry. Lenders provide loans to borrowers in exchange for the promise of repayment with interest. That means the lender makes a profit only if the borrower pays off the loan. Hence, the two most critical questions in the lending industry are:</p>
<ol>
<li>
<p>How risky is the borrower?</p>
</li>
<li>
<p>Given the borrower’s risk, should we lend to them?</p>
</li>

</ol>

<p>Default prediction could be described as a perfect job for machine learning, as the algorithms can be trained on millions of examples of consumer data. Algorithms can perform automated tasks such as matching data records, identifying exceptions, and calculating whether an applicant qualifies for a loan. The underlying trends can be assessed with algorithms and continuously analyzed to detect trends that might influence lending and underwriting risk in the future.</p>

<p>The goal of this case study is to build a machine learning model to predict the probability that a loan will default.</p>

<p>In most real-life cases, including loan default modeling, we are unable to work with clean, complete data. Some of the potential problems we are bound to encounter are missing values, incomplete categorical data, and irrelevant features. Although data cleaning may not be mentioned often, it is critical for the success of machine learning applications. The algorithms that we use can be powerful, but without the relevant or appropriate data, the system may fail to yield ideal results. So one of the focus areas of this case study will be data preparation and cleaning. Various techniques and concepts of data processing, feature selection, and exploratory analysis are used for data cleaning and organizing the feature space.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45174922418872">
<h5/>
<p>In this case study, we will focus on:</p>

<ul>
<li>
<p>Data preparation, data cleaning, and handling a large number of features.</p>
</li>
<li>
<p>Data discretization and handling categorical data.</p>
</li>
<li>
<p>Feature selection and data transformation.</p>
</li>
</ul>
</div></aside>
<img class="blueprint top" src="Images/bracket_top.png" width="1625" height="32"/>








<section data-type="sect2" class="blueprint" data-pdf-bookmark="Blueprint for Creating a Machine Learning Model for Predicting Loan Default Probability"><div class="sect2" id="idm45174922413448">
<h2>Blueprint for Creating a Machine Learning Model for Predicting Loan Default Probability</h2>










<section data-type="sect3" data-pdf-bookmark="1. Problem definition"><div class="sect3" id="idm45174922411768">
<h3>1. Problem definition</h3>

<p><a data-type="indexterm" data-primary="charge-off" id="idm45174922410568"/>In the classification framework for this case study, the predicted variable is <em>charge-off</em>, a debt that a creditor has given up trying to collect on after a borrower has missed payments for several months. The predicted variable takes a value of 1 in case of charge-off and a value of 0 otherwise.</p>

<p>We will analyze data for loans from 2007 to 2017Q3 from Lending Club, <a href="https://oreil.ly/DG9j5">available on Kaggle</a>. Lending Club is a US peer-to-peer lending company. It operates an online lending platform that enables borrowers to obtain a loan and investors to purchase notes backed by payments made on these loans. The dataset contains more than 887,000 observations with 150 variables containing complete loan data for all loans issued over the specified time period. The features include income, age, credit scores, home ownership, borrower location, collections, and many others. We will investigate these 150 predictor variables for feature selection.</p>

<p>By the end of this case study, readers will be familiar with a general approach to loan default modeling, from gathering and cleaning data to building and tuning a 
<span class="keep-together">classifier</span>.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="2. Getting started—loading the data and Python packages"><div class="sect3" id="idm45174922405928">
<h3>2. Getting started—loading the data and Python packages</h3>












<section data-type="sect4" data-pdf-bookmark="2.1. Loading the Python packages"><div class="sect4" id="idm45174922404888">
<h4>2.1. Loading the Python packages</h4>

<p><a data-type="indexterm" data-primary="loan default probability" data-secondary="loading data and Python packages" id="idm45174922403672"/>The standard Python packages are loaded in this step. The details have been presented in the previous case studies. Please refer to the Jupyter notebook for this case study for more details.</p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="2.2. Loading the data"><div class="sect4" id="idm45174922402024">
<h4>2.2. Loading the data</h4>

<p>The loan data for the time period from 2007 to 2017Q3 is loaded:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c"># load dataset</code>
<code class="n">dataset</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">read_csv</code><code class="p">(</code><code class="s">'LoansData.csv.gz'</code><code class="p">,</code> <code class="n">compression</code><code class="o">=</code><code class="s">'gzip'</code><code class="p">,</code> \
<code class="n">low_memory</code><code class="o">=</code><code class="k">True</code><code class="p">)</code></pre>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="3. Data preparation and feature selection"><div class="sect3" id="idm45174922154024">
<h3>3. Data preparation and feature selection</h3>

<p><a data-type="indexterm" data-primary="loan default probability" data-secondary="data preparation" id="ix_Chapter6-asciidoc8"/>In the first step, let us look at the size of the data:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">dataset</code><code class="o">.</code><code class="n">shape</code></pre>

<p class="pagebreak-before"><code>Output</code></p>

<pre data-type="programlisting">(1646801, 150)</pre>

<p>Given that there are 150 features for each loan, we will first focus on limiting the feature space, followed by the exploratory analysis.</p>












<section data-type="sect4" data-pdf-bookmark="3.1. Preparing the predicted variable"><div class="sect4" id="idm45174922145208">
<h4>3.1. Preparing the predicted variable</h4>

<p>Here, we look at the details of the predicted variable and prepare it. The predicted variable will be derived from the <code>loan_status</code> column. Let’s check the value distributions:<sup><a data-type="noteref" id="idm45174922143032-marker" href="ch06.xhtml#idm45174922143032">2</a></sup></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">dataset</code><code class="p">[</code><code class="s">'loan_status'</code><code class="p">]</code><code class="o">.</code><code class="n">value_counts</code><code class="p">(</code><code class="n">dropna</code><code class="o">=</code><code class="k">False</code><code class="p">)</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">Current                                                788950
Fully Paid                                             646902
Charged Off                                            168084
Late (31-120 days)                                      23763
In Grace Period                                         10474
Late (16-30 days)                                        5786
Does not meet the credit policy. Status:Fully Paid       1988
Does not meet the credit policy. Status:Charged Off       761
Default                                                    70
NaN                                                        23
Name: loan_status, dtype: int64</pre>

<p>From the data definition documentation:</p>
<dl>
<dt>Fully Paid</dt>
<dd>
<p>Loans that have been fully repaid.</p>
</dd>
<dt>Default</dt>
<dd>
<p>Loans that have not been current for 121 days or more.</p>
</dd>
<dt>Charged Off</dt>
<dd>
<p>Loans for which there is no longer a reasonable expectation of further payments.</p>
</dd>
</dl>

<p>A large proportion of observations show a status of <code>Current</code>, and we do not know whether those will be <code>Charged Off</code>, <code>Fully Paid</code>, or <code>Default</code> in the future. The observations for <code>Default</code> are tiny in number compared to <code>Fully Paid</code> or <code>Charged Off</code> and are not considered. The remaining categories of <code>loan status</code> are not of prime importance for this analysis. So, in order to convert this to a binary classification problem and to analyze in detail the effect of important variables on the loan status, we will consider only two major categories—Charged Off and Fully Paid:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">dataset</code> <code class="o">=</code> <code class="n">dataset</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="n">dataset</code><code class="p">[</code><code class="s">'loan_status'</code><code class="p">]</code><code class="o">.</code><code class="n">isin</code><code class="p">([</code><code class="s">'Fully Paid'</code><code class="p">,</code> <code class="s">'Charged Off'</code><code class="p">])]</code>
<code class="n">dataset</code><code class="p">[</code><code class="s">'loan_status'</code><code class="p">]</code><code class="o">.</code><code class="n">value_counts</code><code class="p">(</code><code class="n">normalize</code><code class="o">=</code><code class="k">True</code><code class="p">,</code> <code class="n">dropna</code><code class="o">=</code><code class="k">False</code><code class="p">)</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">Fully Paid     0.793758
Charged Off    0.206242
Name: loan_status, dtype: float64</pre>

<p>About 79% of the remaining loans have been fully paid and 21% have been charged off, so we have a somewhat unbalanced classification problem, but it is not as unbalanced as the dataset of fraud detection we saw in the previous case study.</p>

<p>In the next step, we create a new binary column in the dataset, where we categorize Fully Paid as 0 and Charged Off as 1. This column represents the predicted variable for this classification problem. A value of 1 in this column indicates the borrower has defaulted:<a data-type="indexterm" data-startref="ix_Chapter6-asciidoc8" id="idm45174922029880"/></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">dataset</code><code class="p">[</code><code class="s">'charged_off'</code><code class="p">]</code> <code class="o">=</code> <code class="p">(</code><code class="n">dataset</code><code class="p">[</code><code class="s">'loan_status'</code><code class="p">]</code> <code class="o">==</code> <code class="s">'Charged Off'</code><code class="p">)</code><code class="o">.</code><code class="n">apply</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">uint8</code><code class="p">)</code>
<code class="n">dataset</code><code class="o">.</code><code class="n">drop</code><code class="p">(</code><code class="s">'loan_status'</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">inplace</code><code class="o">=</code><code class="k">True</code><code class="p">)</code></pre>
</div></section>













<section data-type="sect4" data-pdf-bookmark="3.2. Feature selection—limit the feature space"><div class="sect4" id="idm45174922144584">
<h4>3.2. Feature selection—limit the feature space</h4>

<p><a data-type="indexterm" data-primary="loan default probability" data-secondary="feature selection" id="ix_Chapter6-asciidoc9"/>The full dataset has 150 features for each loan, but not all features contribute to the prediction variable. Removing features of low importance can improve accuracy and reduce both model complexity and overfitting. Training time can also be reduced for very large datasets. We’ll eliminate features in the following steps using three different approaches:</p>

<ul>
<li>
<p>Eliminating features that have more than 30% missing values.</p>
</li>
<li>
<p>Eliminating features that are unintuitive based on subjective judgment.</p>
</li>
<li>
<p>Eliminating features with low correlation with the predicted variable.</p>
</li>
</ul>
</div></section>













<section data-type="sect4" data-pdf-bookmark="3.2.1. Feature elimination based on significant missing values"><div class="sect4" id="idm45174921954872">
<h4>3.2.1. Feature elimination based on significant missing values</h4>

<p>First, we calculate the percentage of missing data for each feature:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">missing_fractions</code> <code class="o">=</code> <code class="n">dataset</code><code class="o">.</code><code class="n">isnull</code><code class="p">()</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code><code class="o">.</code><code class="n">sort_values</code><code class="p">(</code><code class="n">ascending</code><code class="o">=</code><code class="k">False</code><code class="p">)</code>

<code class="c">#Drop the missing fraction</code>
<code class="n">drop_list</code> <code class="o">=</code> <code class="nb">sorted</code><code class="p">(</code><code class="nb">list</code><code class="p">(</code><code class="n">missing_fractions</code><code class="p">[</code><code class="n">missing_fractions</code> <code class="o">&gt;</code> <code class="mf">0.3</code><code class="p">]</code><code class="o">.</code><code class="n">index</code><code class="p">))</code>
<code class="n">dataset</code><code class="o">.</code><code class="n">drop</code><code class="p">(</code><code class="n">labels</code><code class="o">=</code><code class="n">drop_list</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">inplace</code><code class="o">=</code><code class="k">True</code><code class="p">)</code>
<code class="n">dataset</code><code class="o">.</code><code class="n">shape</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">(814986, 92)</pre>

<p>This dataset has 92 columns remaining once some of the columns with a significant number of missing values are dropped.</p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="3.2.2. Feature elimination based on intuitiveness"><div class="sect4" id="idm45174921881240">
<h4>3.2.2. Feature elimination based on intuitiveness</h4>

<p>To filter the features further we check the description in the data dictionary and keep the features that intuitively contribute to the prediction of default. We keep features that contain the relevant credit detail of the borrower, including annual income, FICO score, and debt-to-income ratio. We also keep those features that are available to investors when considering an investment in the loan. These include features in the loan application and any features added by Lending Club when the loan listing was accepted, such as loan grade and interest rate.</p>

<p>The list of the features retained are shown in the following code snippet:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">keep_list</code> <code class="o">=</code> <code class="p">[</code><code class="s">'charged_off'</code><code class="p">,</code><code class="s">'funded_amnt'</code><code class="p">,</code><code class="s">'addr_state'</code><code class="p">,</code> <code class="s">'annual_inc'</code><code class="p">,</code> \
<code class="s">'application_type'</code><code class="p">,</code><code class="s">'dti'</code><code class="p">,</code> <code class="s">'earliest_cr_line'</code><code class="p">,</code> <code class="s">'emp_length'</code><code class="p">,</code>\
<code class="s">'emp_title'</code><code class="p">,</code> <code class="s">'fico_range_high'</code><code class="p">,</code>\
<code class="s">'fico_range_low'</code><code class="p">,</code> <code class="s">'grade'</code><code class="p">,</code> <code class="s">'home_ownership'</code><code class="p">,</code> <code class="s">'id'</code><code class="p">,</code> <code class="s">'initial_list_status'</code><code class="p">,</code> \
<code class="s">'installment'</code><code class="p">,</code> <code class="s">'int_rate'</code><code class="p">,</code> <code class="s">'loan_amnt'</code><code class="p">,</code> <code class="s">'loan_status'</code><code class="p">,</code>\
<code class="s">'mort_acc'</code><code class="p">,</code> <code class="s">'open_acc'</code><code class="p">,</code> <code class="s">'pub_rec'</code><code class="p">,</code> <code class="s">'pub_rec_bankruptcies'</code><code class="p">,</code> \
<code class="s">'purpose'</code><code class="p">,</code> <code class="s">'revol_bal'</code><code class="p">,</code> <code class="s">'revol_util'</code><code class="p">,</code> \
<code class="s">'sub_grade'</code><code class="p">,</code> <code class="s">'term'</code><code class="p">,</code> <code class="s">'title'</code><code class="p">,</code> <code class="s">'total_acc'</code><code class="p">,</code>\
<code class="s">'verification_status'</code><code class="p">,</code> <code class="s">'zip_code'</code><code class="p">,</code><code class="s">'last_pymnt_amnt'</code><code class="p">,</code>\
<code class="s">'num_actv_rev_tl'</code><code class="p">,</code> <code class="s">'mo_sin_rcnt_rev_tl_op'</code><code class="p">,</code>\
<code class="s">'mo_sin_old_rev_tl_op'</code><code class="p">,</code><code class="s">"bc_util"</code><code class="p">,</code><code class="s">"bc_open_to_buy"</code><code class="p">,</code>\
<code class="s">"avg_cur_bal"</code><code class="p">,</code><code class="s">"acc_open_past_24mths"</code> <code class="p">]</code>

<code class="n">drop_list</code> <code class="o">=</code> <code class="p">[</code><code class="n">col</code> <code class="k">for</code> <code class="n">col</code> <code class="ow">in</code> <code class="n">dataset</code><code class="o">.</code><code class="n">columns</code> <code class="k">if</code> <code class="n">col</code> <code class="ow">not</code> <code class="ow">in</code> <code class="n">keep_list</code><code class="p">]</code>
<code class="n">dataset</code><code class="o">.</code><code class="n">drop</code><code class="p">(</code><code class="n">labels</code><code class="o">=</code><code class="n">drop_list</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">inplace</code><code class="o">=</code><code class="k">True</code><code class="p">)</code>
<code class="n">dataset</code><code class="o">.</code><code class="n">shape</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">(814986, 39)</pre>

<p>After removing the features in this step, 39 columns remain.</p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="3.2.3. Feature elimination based on the correlation"><div class="sect4" id="idm45174921875208">
<h4>3.2.3. Feature elimination based on the correlation</h4>

<p>The next step is to check the correlation with the predicted variable. Correlation gives us the interdependence between the predicted variable and the feature. We select features with a moderate-to-strong relationship with the target variable and drop those that have a correlation of less than 3% with the predicted variable:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">correlation</code> <code class="o">=</code> <code class="n">dataset</code><code class="o">.</code><code class="n">corr</code><code class="p">()</code>
<code class="n">correlation_chargeOff</code> <code class="o">=</code> <code class="nb">abs</code><code class="p">(</code><code class="n">correlation</code><code class="p">[</code><code class="s">'charged_off'</code><code class="p">])</code>
<code class="n">drop_list_corr</code> <code class="o">=</code> <code class="nb">sorted</code><code class="p">(</code><code class="nb">list</code><code class="p">(</code><code class="n">correlation_chargeOff</code>\
  <code class="p">[</code><code class="n">correlation_chargeOff</code> <code class="o">&lt;</code> <code class="mf">0.03</code><code class="p">]</code><code class="o">.</code><code class="n">index</code><code class="p">))</code>
<code class="nb">print</code><code class="p">(</code><code class="n">drop_list_corr</code><code class="p">)</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">['pub_rec', 'pub_rec_bankruptcies', 'revol_bal', 'total_acc']</pre>

<p>The columns with low correlation are dropped from the dataset, and we are left with only 35 columns:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">dataset</code><code class="o">.</code><code class="n">drop</code><code class="p">(</code><code class="n">labels</code><code class="o">=</code><code class="n">drop_list_corr</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">inplace</code><code class="o">=</code><code class="k">True</code><code class="p">)</code></pre>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="4. Feature selection and exploratory analysis"><div class="sect3" id="idm45174922153560">
<h3>4. Feature selection and exploratory analysis</h3>

<p>In this step, we perform the exploratory data analysis of the feature selection. Given that many features had to be eliminated, it is preferable that we perform the exploratory data analysis after feature selection to better visualize the relevant features. We will also continue the feature elimination by visually screening and dropping those features deemed irrelevant.</p>












<section data-type="sect4" data-pdf-bookmark="4.1. Feature analysis and exploration"><div class="sect4" id="idm45174921573288">
<h4>4.1. Feature analysis and exploration</h4>

<p>In the following sections, we take a deeper dive into the dataset features.</p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="4.1.1. Analyzing the categorical features"><div class="sect4" id="idm45174921570680">
<h4>4.1.1. Analyzing the categorical features</h4>

<p>Let us look at the some of the categorical features
in the dataset.</p>

<p>First, let’s look at the <code>id</code>, <code>emp_title</code>, <code>title</code>, and <code>zip_code</code> features:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">dataset</code><code class="p">[[</code><code class="s">'id'</code><code class="p">,</code><code class="s">'emp_title'</code><code class="p">,</code><code class="s">'title'</code><code class="p">,</code><code class="s">'zip_code'</code><code class="p">]]</code><code class="o">.</code><code class="n">describe</code><code class="p">()</code></pre>

<p><code>Output</code></p>
<table>

<thead>
<tr>
<th/>
<th>id</th>
<th>emp_title</th>
<th>title</th>
<th>zip_code</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>count</p></td>
<td><p>814986</p></td>
<td><p>766415</p></td>
<td><p>807068</p></td>
<td><p>814986</p></td>
</tr>
<tr>
<td><p>unique</p></td>
<td><p>814986</p></td>
<td><p>280473</p></td>
<td><p>60298</p></td>
<td><p>925</p></td>
</tr>
<tr>
<td><p>top</p></td>
<td><p>14680062</p></td>
<td><p>Teacher</p></td>
<td><p>Debt consolidation</p></td>
<td><p>945xx</p></td>
</tr>
<tr>
<td><p>freq</p></td>
<td><p>1</p></td>
<td><p>11351</p></td>
<td><p>371874</p></td>
<td><p>9517</p></td>
</tr>
</tbody>
</table>

<p>IDs are all unique and irrelevant for modeling. There are too many unique values for employment titles and titles. Occupation and job title may provide some information for default modeling; however, we assume much of this information is embedded in the verified income of the customer. Moreover, additional cleaning steps on these features, such as standardizing or grouping the titles, would be necessary to extract any marginal information. This work is outside the scope of this case study but could be explored in subsequent iterations of the model.</p>

<p>Geography could play a role in credit determination, and zip codes provide a granular view of this dimension. Again, additional work would be necessary to prepare this feature for modeling and was deemed outside the scope of this case study.</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">dataset</code><code class="o">.</code><code class="n">drop</code><code class="p">([</code><code class="s">'id'</code><code class="p">,</code><code class="s">'emp_title'</code><code class="p">,</code><code class="s">'title'</code><code class="p">,</code><code class="s">'zip_code'</code><code class="p">],</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">inplace</code><code class="o">=</code><code class="k">True</code><code class="p">)</code></pre>

<p>Let’s look at the <code>term</code> feature.</p>

<p><em>Term</em> refers to the number of payments on the loan. Values are in months and can be either 36 or 60. The 60-month loans are more likely to charge off.</p>

<p class="pagebreak-before">Let’s convert term to integers and group by the term for further analysis:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">dataset</code><code class="p">[</code><code class="s">'term'</code><code class="p">]</code> <code class="o">=</code> <code class="n">dataset</code><code class="p">[</code><code class="s">'term'</code><code class="p">]</code><code class="o">.</code><code class="n">apply</code><code class="p">(</code><code class="k">lambda</code> <code class="n">s</code><code class="p">:</code> <code class="n">np</code><code class="o">.</code><code class="n">int8</code><code class="p">(</code><code class="n">s</code><code class="o">.</code><code class="n">split</code><code class="p">()[</code><code class="mi">0</code><code class="p">]))</code>
<code class="n">dataset</code><code class="o">.</code><code class="n">groupby</code><code class="p">(</code><code class="s">'term'</code><code class="p">)[</code><code class="s">'charged_off'</code><code class="p">]</code><code class="o">.</code><code class="n">value_counts</code><code class="p">(</code><code class="n">normalize</code><code class="o">=</code><code class="k">True</code><code class="p">)</code><code class="o">.</code><code class="n">loc</code><code class="p">[:,</code><code class="mi">1</code><code class="p">]</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">term
36    0.165710
60    0.333793
Name: charged_off, dtype: float64</pre>

<p>Loans with five-year periods are more than twice as likely to charge-off as loans with three-year periods. This feature seems to be important for the prediction.</p>

<p>Let’s look at the <code>emp_length</code> feature:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">dataset</code><code class="p">[</code><code class="s">'emp_length'</code><code class="p">]</code><code class="o">.</code><code class="n">replace</code><code class="p">(</code><code class="n">to_replace</code><code class="o">=</code><code class="s">'10+ years'</code><code class="p">,</code> <code class="n">value</code><code class="o">=</code><code class="s">'10 years'</code><code class="p">,</code>\
  <code class="n">inplace</code><code class="o">=</code><code class="k">True</code><code class="p">)</code>

<code class="n">dataset</code><code class="p">[</code><code class="s">'emp_length'</code><code class="p">]</code><code class="o">.</code><code class="n">replace</code><code class="p">(</code><code class="s">'&lt; 1 year'</code><code class="p">,</code> <code class="s">'0 years'</code><code class="p">,</code> <code class="n">inplace</code><code class="o">=</code><code class="k">True</code><code class="p">)</code>

<code class="k">def</code> <code class="nf">emp_length_to_int</code><code class="p">(</code><code class="n">s</code><code class="p">):</code>
    <code class="k">if</code> <code class="n">pd</code><code class="o">.</code><code class="n">isnull</code><code class="p">(</code><code class="n">s</code><code class="p">):</code>
        <code class="k">return</code> <code class="n">s</code>
    <code class="k">else</code><code class="p">:</code>
        <code class="k">return</code> <code class="n">np</code><code class="o">.</code><code class="n">int8</code><code class="p">(</code><code class="n">s</code><code class="o">.</code><code class="n">split</code><code class="p">()[</code><code class="mi">0</code><code class="p">])</code>

<code class="n">dataset</code><code class="p">[</code><code class="s">'emp_length'</code><code class="p">]</code> <code class="o">=</code> <code class="n">dataset</code><code class="p">[</code><code class="s">'emp_length'</code><code class="p">]</code><code class="o">.</code><code class="n">apply</code><code class="p">(</code><code class="n">emp_length_to_int</code><code class="p">)</code>
<code class="n">charge_off_rates</code> <code class="o">=</code> <code class="n">dataset</code><code class="o">.</code><code class="n">groupby</code><code class="p">(</code><code class="s">'emp_length'</code><code class="p">)[</code><code class="s">'charged_off'</code><code class="p">]</code><code class="o">.</code><code class="n">value_counts</code>\
  <code class="p">(</code><code class="n">normalize</code><code class="o">=</code><code class="k">True</code><code class="p">)</code><code class="o">.</code><code class="n">loc</code><code class="p">[:,</code><code class="mi">1</code><code class="p">]</code>
<code class="n">sns</code><code class="o">.</code><code class="n">barplot</code><code class="p">(</code><code class="n">x</code><code class="o">=</code><code class="n">charge_off_rates</code><code class="o">.</code><code class="n">index</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="n">charge_off_rates</code><code class="o">.</code><code class="n">values</code><code class="p">)</code></pre>

<p><code>Output</code></p>

<figure><div class="figure">
<img src="Images/mlbf_06in08.png" alt="mlbf 06in08" width="371" height="251"/>
<h6/>
</div></figure>

<p class="pagebreak-before">Loan status does not appear to vary much with employment length (on average); hence this feature is dropped:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">dataset</code><code class="o">.</code><code class="n">drop</code><code class="p">([</code><code class="s">'emp_length'</code><code class="p">],</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">inplace</code><code class="o">=</code><code class="k">True</code><code class="p">)</code></pre>

<p>Let’s look at the <code>sub_grade</code> feature:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">charge_off_rates</code> <code class="o">=</code> <code class="n">dataset</code><code class="o">.</code><code class="n">groupby</code><code class="p">(</code><code class="s">'sub_grade'</code><code class="p">)[</code><code class="s">'charged_off'</code><code class="p">]</code><code class="o">.</code><code class="n">value_counts</code>\
<code class="p">(</code><code class="n">normalize</code><code class="o">=</code><code class="k">True</code><code class="p">)</code><code class="o">.</code><code class="n">loc</code><code class="p">[:,</code><code class="mi">1</code><code class="p">]</code>
<code class="n">sns</code><code class="o">.</code><code class="n">barplot</code><code class="p">(</code><code class="n">x</code><code class="o">=</code><code class="n">charge_off_rates</code><code class="o">.</code><code class="n">index</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="n">charge_off_rates</code><code class="o">.</code><code class="n">values</code><code class="p">)</code></pre>

<p><code>Output</code></p>

<figure><div class="figure">
<img src="Images/mlbf_06in09.png" alt="mlbf 06in09" width="694" height="304"/>
<h6/>
</div></figure>

<p>As shown in the chart, there’s a clear trend of higher probability of charge-off as the sub-grade worsens, and so it is considered to be a key feature.</p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="4.1.2. Analyzing the continuous features"><div class="sect4" id="idm45174921569736">
<h4>4.1.2. Analyzing the continuous features</h4>

<p>Let’s look at the <code>annual_inc</code> feature:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">dataset</code><code class="p">[[</code><code class="s">'annual_inc'</code><code class="p">]]</code><code class="o">.</code><code class="n">describe</code><code class="p">()</code></pre>

<p><code>Output</code></p>
<table>

<thead>
<tr>
<th/>
<th>annual_inc</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>count</p></td>
<td><p>8.149860e+05</p></td>
</tr>
<tr>
<td><p>mean</p></td>
<td><p>7.523039e+04</p></td>
</tr>
<tr>
<td><p>std</p></td>
<td><p>6.524373e+04</p></td>
</tr>
<tr>
<td><p>min</p></td>
<td><p>0.000000e+00</p></td>
</tr>
<tr>
<td><p>25%</p></td>
<td><p>4.500000e+04</p></td>
</tr>
<tr>
<td><p>50%</p></td>
<td><p>6.500000e+04</p></td>
</tr>
<tr>
<td><p>75%</p></td>
<td><p>9.000000e+04</p></td>
</tr>
<tr>
<td><p>max</p></td>
<td><p>9.550000e+06</p></td>
</tr>
</tbody>
</table>

<p>Annual income ranges from $0 to $9,550,000, with a median of $65,000. Because of the large range of incomes, we use a log transform of the annual income variable:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">dataset</code><code class="p">[</code><code class="s">'log_annual_inc'</code><code class="p">]</code> <code class="o">=</code> <code class="n">dataset</code><code class="p">[</code><code class="s">'annual_inc'</code><code class="p">]</code><code class="o">.</code><code class="n">apply</code><code class="p">(</code><code class="k">lambda</code> <code class="n">x</code><code class="p">:</code> <code class="n">np</code><code class="o">.</code><code class="n">log10</code><code class="p">(</code><code class="n">x</code><code class="o">+</code><code class="mi">1</code><code class="p">))</code>
<code class="n">dataset</code><code class="o">.</code><code class="n">drop</code><code class="p">(</code><code class="s">'annual_inc'</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">inplace</code><code class="o">=</code><code class="k">True</code><code class="p">)</code></pre>

<p>Let’s look at the FICO score (<code>fico_range_low</code>, <code>fico_range_high</code>) feature:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">dataset</code><code class="p">[[</code><code class="s">'fico_range_low'</code><code class="p">,</code><code class="s">'fico_range_high'</code><code class="p">]]</code><code class="o">.</code><code class="n">corr</code><code class="p">()</code></pre>

<p><code>Output</code></p>
<table>

<thead>
<tr>
<th/>
<th>fico_range_low</th>
<th>fico_range_high</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>fico_range_low</p></td>
<td><p>1.0</p></td>
<td><p>1.0</p></td>
</tr>
<tr>
<td><p>fico_range_high</p></td>
<td><p>1.0</p></td>
<td><p>1.0</p></td>
</tr>
</tbody>
</table>

<p>Given that the correlation between FICO low and high is 1, it is preferred that we keep only one feature, which we take as the average of FICO scores:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">dataset</code><code class="p">[</code><code class="s">'fico_score'</code><code class="p">]</code> <code class="o">=</code> <code class="mf">0.5</code><code class="o">*</code><code class="n">dataset</code><code class="p">[</code><code class="s">'fico_range_low'</code><code class="p">]</code> <code class="o">+</code>\
 <code class="mf">0.5</code><code class="o">*</code><code class="n">dataset</code><code class="p">[</code><code class="s">'fico_range_high'</code><code class="p">]</code>

<code class="n">dataset</code><code class="o">.</code><code class="n">drop</code><code class="p">([</code><code class="s">'fico_range_high'</code><code class="p">,</code> <code class="s">'fico_range_low'</code><code class="p">],</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">inplace</code><code class="o">=</code><code class="k">True</code><code class="p">)</code></pre>
</div></section>













<section data-type="sect4" data-pdf-bookmark="4.2. Encoding categorical data"><div class="sect4" id="idm45174921204952">
<h4>4.2. Encoding categorical data</h4>

<p>In order to use a feature in the classification models, we need to convert the categorical data (i.e., text features) to its numeric representation. This process is called encoding. There can be different ways of encoding. However, for this case study we will use a <em>label encoder</em>, which encodes labels with a value between 0 and <em>n</em>, where <em>n</em> is the number of distinct labels. The <code>LabelEncoder</code> function from sklearn is used in the following step, and all the categorical columns are encoded at once:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="k">import</code> <code class="n">LabelEncoder</code>
<code class="c"># Categorical boolean mask</code>
<code class="n">categorical_feature_mask</code> <code class="o">=</code> <code class="n">dataset</code><code class="o">.</code><code class="n">dtypes</code><code class="o">==</code><code class="nb">object</code>
<code class="c"># filter categorical columns using mask and turn it into a list</code>
<code class="n">categorical_cols</code> <code class="o">=</code> <code class="n">dataset</code><code class="o">.</code><code class="n">columns</code><code class="p">[</code><code class="n">categorical_feature_mask</code><code class="p">]</code><code class="o">.</code><code class="n">tolist</code><code class="p">()</code></pre>

<p>Let us look at the categorical columns:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">categorical_cols</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">['grade',
 'sub_grade',
 'home_ownership',
 'verification_status',
 'purpose',
 'addr_state',
 'initial_list_status',
 'application_type']</pre>
</div></section>













<section data-type="sect4" data-pdf-bookmark="4.3. Sampling data"><div class="sect4" id="idm45174920858008">
<h4>4.3. Sampling data</h4>

<p>Given that the loan data is skewed, it is sampled to have an equal number of charge-off and no charge-off observations. Sampling leads to a more balanced dataset and avoids overfitting:<sup><a data-type="noteref" id="idm45174920856536-marker" href="ch06.xhtml#idm45174920856536">3</a></sup></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">loanstatus_0</code> <code class="o">=</code> <code class="n">dataset</code><code class="p">[</code><code class="n">dataset</code><code class="p">[</code><code class="s">"charged_off"</code><code class="p">]</code><code class="o">==</code><code class="mi">0</code><code class="p">]</code>
<code class="n">loanstatus_1</code> <code class="o">=</code> <code class="n">dataset</code><code class="p">[</code><code class="n">dataset</code><code class="p">[</code><code class="s">"charged_off"</code><code class="p">]</code><code class="o">==</code><code class="mi">1</code><code class="p">]</code>
<code class="n">subset_of_loanstatus_0</code> <code class="o">=</code> <code class="n">loanstatus_0</code><code class="o">.</code><code class="n">sample</code><code class="p">(</code><code class="n">n</code><code class="o">=</code><code class="mi">5500</code><code class="p">)</code>
<code class="n">subset_of_loanstatus_1</code> <code class="o">=</code> <code class="n">loanstatus_1</code><code class="o">.</code><code class="n">sample</code><code class="p">(</code><code class="n">n</code><code class="o">=</code><code class="mi">5500</code><code class="p">)</code>
<code class="n">dataset</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">concat</code><code class="p">([</code><code class="n">subset_of_loanstatus_1</code><code class="p">,</code> <code class="n">subset_of_loanstatus_0</code><code class="p">])</code>
<code class="n">dataset</code> <code class="o">=</code> <code class="n">dataset</code><code class="o">.</code><code class="n">sample</code><code class="p">(</code><code class="n">frac</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code><code class="o">.</code><code class="n">reset_index</code><code class="p">(</code><code class="n">drop</code><code class="o">=</code><code class="k">True</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="s">"Current shape of dataset :"</code><code class="p">,</code><code class="n">dataset</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code></pre>

<p>Although sampling may have its advantages, there might be some disadvantages as well. Sampling may exclude some data that might not be homogeneous to the data that is taken. This affects the level of accuracy in the results. Also, selection of the proper size of samples is a difficult job. Hence, sampling should be performed with caution and should generally be avoided in the case of a relatively balanced dataset.<a data-type="indexterm" data-startref="ix_Chapter6-asciidoc9" id="idm45174920925400"/></p>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="5. Evaluate algorithms and models"><div class="sect3" id="idm45174920744728">
<h3>5. Evaluate algorithms and models</h3>












<section data-type="sect4" data-pdf-bookmark="5.1. Train-test split"><div class="sect4" id="idm45174920743816">
<h4>5.1. Train-test split</h4>

<p><a data-type="indexterm" data-primary="loan default probability" data-secondary="evaluation of algorithms and models" id="idm45174920742616"/>Splitting out the validation dataset for the model evaluation is the next step:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">Y</code><code class="o">=</code> <code class="n">dataset</code><code class="p">[</code><code class="s">"charged_off"</code><code class="p">]</code>
<code class="n">X</code> <code class="o">=</code> <code class="n">dataset</code><code class="o">.</code><code class="n">loc</code><code class="p">[:,</code> <code class="n">dataset</code><code class="o">.</code><code class="n">columns</code> <code class="o">!=</code> <code class="s">'charged_off'</code><code class="p">]</code>
<code class="n">validation_size</code> <code class="o">=</code> <code class="mf">0.2</code>
<code class="n">seed</code> <code class="o">=</code> <code class="mi">7</code>
<code class="n">X_train</code><code class="p">,</code> <code class="n">X_validation</code><code class="p">,</code> <code class="n">Y_train</code><code class="p">,</code> <code class="n">Y_validation</code> <code class="o">=</code> \
<code class="n">train_test_split</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">Y</code><code class="p">,</code> <code class="n">test_size</code><code class="o">=</code><code class="n">validation_size</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="n">seed</code><code class="p">)</code></pre>
</div></section>













<section data-type="sect4" data-pdf-bookmark="5.2. Test options and evaluation metrics"><div class="sect4" id="idm45174920739816">
<h4>5.2. Test options and evaluation metrics</h4>

<p>In this step, the test options and evaluation metrics are selected. The <code>roc_auc</code> evaluation metric is selected for this classification. The details of this metric were provided in <a data-type="xref" href="ch04.xhtml#Chapter4">Chapter 4</a>. This metric represents a model’s ability to discriminate between positive and negative classes. An <code>roc_auc</code> of 1.0 represents a model that made all predictions perfectly, and a value of 0.5 represents a model that is as good as random.</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">num_folds</code> <code class="o">=</code> <code class="mi">10</code>
<code class="n">scoring</code> <code class="o">=</code> <code class="s">'roc_auc'</code></pre>

<p>The model cannot afford to have a high amount of false negatives as that leads to a negative impact on the investors and the credibility of the company. So we can use recall as we did in the fraud detection use case.</p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="5.3. Compare models and algorithms"><div class="sect4" id="idm45174920696440">
<h4>5.3. Compare models and algorithms</h4>

<p>Let us spot-check the classification algorithms. We include ANN and ensemble models in the list of models to be checked:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">models</code> <code class="o">=</code> <code class="p">[]</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'LR'</code><code class="p">,</code> <code class="n">LogisticRegression</code><code class="p">()))</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'LDA'</code><code class="p">,</code> <code class="n">LinearDiscriminantAnalysis</code><code class="p">()))</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'KNN'</code><code class="p">,</code> <code class="n">KNeighborsClassifier</code><code class="p">()))</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'CART'</code><code class="p">,</code> <code class="n">DecisionTreeClassifier</code><code class="p">()))</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'NB'</code><code class="p">,</code> <code class="n">GaussianNB</code><code class="p">()))</code>
<code class="c"># Neural Network</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'NN'</code><code class="p">,</code> <code class="n">MLPClassifier</code><code class="p">()))</code>
<code class="c"># Ensemble Models</code>
<code class="c"># Boosting methods</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'AB'</code><code class="p">,</code> <code class="n">AdaBoostClassifier</code><code class="p">()))</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'GBM'</code><code class="p">,</code> <code class="n">GradientBoostingClassifier</code><code class="p">()))</code>
<code class="c"># Bagging methods</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'RF'</code><code class="p">,</code> <code class="n">RandomForestClassifier</code><code class="p">()))</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'ET'</code><code class="p">,</code> <code class="n">ExtraTreesClassifier</code><code class="p">()))</code></pre>

<p>After performing the <em>k</em>-fold cross validation on the models shown above, the overall performance is as follows:</p>

<figure><div class="figure">
<img src="Images/mlbf_06in10.png" alt="mlbf 06in10" width="1175" height="621"/>
<h6/>
</div></figure>

<p>The gradient boosting method (GBM) model performs best, and we select it for grid search in the next step. The details of GBM along with the model parameters are described in <a data-type="xref" href="ch04.xhtml#Chapter4">Chapter 4</a>.</p>
</div></section>

</div></section>













<section data-type="sect3" class="pagebreak-before less_space" data-pdf-bookmark="6. Model tuning and grid search"><div class="sect3" id="idm45174920494392">
<h3>6. Model tuning and grid search</h3>

<p><a data-type="indexterm" data-primary="loan default probability" data-secondary="model tuning and grid search" id="idm45174920492744"/>We tune the number of estimator and maximum depth hyperparameters, which were discussed in <a data-type="xref" href="ch04.xhtml#Chapter4">Chapter 4</a>:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c"># Grid Search: GradientBoosting Tuning</code>
<code class="n">n_estimators</code> <code class="o">=</code> <code class="p">[</code><code class="mi">20</code><code class="p">,</code><code class="mi">180</code><code class="p">]</code>
<code class="n">max_depth</code><code class="o">=</code> <code class="p">[</code><code class="mi">3</code><code class="p">,</code><code class="mi">5</code><code class="p">]</code>
<code class="n">param_grid</code> <code class="o">=</code> <code class="nb">dict</code><code class="p">(</code><code class="n">n_estimators</code><code class="o">=</code><code class="n">n_estimators</code><code class="p">,</code> <code class="n">max_depth</code><code class="o">=</code><code class="n">max_depth</code><code class="p">)</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">GradientBoostingClassifier</code><code class="p">()</code>
<code class="n">kfold</code> <code class="o">=</code> <code class="n">KFold</code><code class="p">(</code><code class="n">n_splits</code><code class="o">=</code><code class="n">num_folds</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="n">seed</code><code class="p">)</code>
<code class="n">grid</code> <code class="o">=</code> <code class="n">GridSearchCV</code><code class="p">(</code><code class="n">estimator</code><code class="o">=</code><code class="n">model</code><code class="p">,</code> <code class="n">param_grid</code><code class="o">=</code><code class="n">param_grid</code><code class="p">,</code> <code class="n">scoring</code><code class="o">=</code><code class="n">scoring</code><code class="p">,</code> \
  <code class="n">cv</code><code class="o">=</code><code class="n">kfold</code><code class="p">)</code>
<code class="n">grid_result</code> <code class="o">=</code> <code class="n">grid</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">Y_train</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="s">"Best: %f using %s"</code> <code class="o">%</code> <code class="p">(</code><code class="n">grid_result</code><code class="o">.</code><code class="n">best_score_</code><code class="p">,</code> <code class="n">grid_result</code><code class="o">.</code><code class="n">best_params_</code><code class="p">))</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">Best: 0.952950 using {'max_depth': 5, 'n_estimators': 180}</pre>

<p>A GBM model with <code>max_depth</code> of 5 and number of estimators of 150 results in the best model.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="7. Finalize the model"><div class="sect3" id="idm45174920376056">
<h3>7. Finalize the model</h3>

<p>Now, we perform the final steps for selecting a model.</p>












<section data-type="sect4" data-pdf-bookmark="7.1. Results on the test dataset"><div class="sect4" id="idm45174920374600">
<h4>7.1. Results on the test dataset</h4>

<p><a data-type="indexterm" data-primary="loan default probability" data-secondary="finalizing the model" id="idm45174920373192"/>Let us prepare the GBM model with the parameters found during the grid search step and check the results on the test dataset:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">model</code> <code class="o">=</code> <code class="n">GradientBoostingClassifier</code><code class="p">(</code><code class="n">max_depth</code><code class="o">=</code> <code class="mi">5</code><code class="p">,</code> <code class="n">n_estimators</code><code class="o">=</code> <code class="mi">180</code><code class="p">)</code>
<code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">Y_train</code><code class="p">)</code>

<code class="c"># estimate accuracy on validation set</code>
<code class="n">predictions</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_validation</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">accuracy_score</code><code class="p">(</code><code class="n">Y_validation</code><code class="p">,</code> <code class="n">predictions</code><code class="p">))</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">0.889090909090909</pre>

<p class="pagebreak-before">The accuracy of the model is a reasonable 89% on the test set. Let us examine the confusion matrix:</p>

<figure><div class="figure">
<img src="Images/mlbf_06in11.png" alt="mlbf 06in11" width="359" height="247"/>
<h6/>
</div></figure>

<p>Looking at the confusion matrix and the overall result of the test set, both the rate of false positives and the rate of false negatives are lower; the overall model performance looks good and is in line with the training set results.</p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="7.2. Variable intuition/feature importance"><div class="sect4" id="idm45174920250712">
<h4>7.2. Variable intuition/feature importance</h4>

<p>In this step, we compute and display the variable importance of our trained model:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="nb">print</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">feature_importances_</code><code class="p">)</code> <code class="c">#use inbuilt class feature_importances</code>
<code class="n">feat_importances</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">Series</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">feature_importances_</code><code class="p">,</code> <code class="n">index</code><code class="o">=</code><code class="n">X</code><code class="o">.</code><code class="n">columns</code><code class="p">)</code>
<code class="c">#plot graph of feature importances for better visualization</code>
<code class="n">feat_importances</code><code class="o">.</code><code class="n">nlargest</code><code class="p">(</code><code class="mi">10</code><code class="p">)</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">kind</code><code class="o">=</code><code class="s">'barh'</code><code class="p">)</code>
<code class="n">pyplot</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>

<p><code>Output</code></p>

<figure><div class="figure">
<img src="Images/mlbf_06in12.png" alt="mlbf 06in12" width="458" height="233"/>
<h6/>
</div></figure>

<p>The results of the model importance are intuitive. The last payment amount seems to be the most important feature, followed by loan term and sub-grade.</p>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="Conclusion"><div class="sect3" id="idm45174920226440">
<h3>Conclusion</h3>

<p>In this case study, we introduced the classification-based tree algorithm applied to loan default prediction. We showed that data preparation is one of the most important steps. We addressed this by performing feature elimination using different techniques, such as feature intuition, correlation analysis, visualization, and data quality checks of the features. We illustrated that there can be different ways of handling and analyzing the categorical data and converting categorical data into a usable format for the models.</p>

<p>We emphasized that performing data processing and establishing an understanding of variable importance is key in the model development process. A focus on these steps led to the implementation of a simple classification-based model that produced robust results for default prediction.<a data-type="indexterm" data-startref="ix_Chapter6-asciidoc7" id="idm45174920223848"/></p>
<img class="blueprint bottom" src="Images/bracket_bottom.png" width="1625" height="32"/>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Case Study 3: Bitcoin Trading Strategy"><div class="sect1" id="CaseStudy3SC">
<h1>Case Study 3: Bitcoin Trading Strategy</h1>

<p><a data-type="indexterm" data-primary="bitcoin trading: strategy" id="ix_Chapter6-asciidoc10"/>First released as open source in 2009 by the pseudonymous Satoshi Nakamoto, bitcoin is the longest-running and most well-known cryptocurrency.</p>

<p>A major drawback of cryptocurrency trading is the volatility of the market. Since cryptocurrency markets trade 24/7, tracking cryptocurrency positions against quickly changing market dynamics can rapidly become an impossible task to manage. This is where automated trading algorithms and trading bots can assist.</p>

<p>Various machine learning algorithms can be used to generate trading signals in an attempt to predict the market’s movement. One could use machine learning algorithms to classify the next day’s movement into three categories: market will rise (take a long position), market will fall (take a short position), or market will move sideways (take no position). Since we know the market direction, we can decide the optimum entry and exit points.</p>

<p><a data-type="indexterm" data-primary="feature engineering" id="idm45174920217496"/>Machine learning has one key aspect called <em>feature engineering</em>. It means that we can create new, intuitive features and feed them to a machine learning algorithm in order to achieve better results. We can introduce different technical indicators as features to help predict future prices of an asset. These technical indicators are derived from market variables such as price or volume and have additional information or signals embedded in them. There are many different categories of technical indicators, including trend, volume, volatility, and momentum indicators.</p>

<p>In this case study, we will use various classification-based models to predict whether the current position signal is buy or sell. We will create additional trend and momentum indicators from market prices to leverage as additional features in the prediction.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45174920214936">
<h5/>
<p>In this case study, we will focus on:</p>

<ul>
<li>
<p>Building a trading strategy using classification (classification of long/short signals).</p>
</li>
<li>
<p>Feature engineering and constructing technical indicators of trend, momentum, and mean reversion.</p>
</li>
<li>
<p>Building a framework for backtesting results of a trading strategy.</p>
</li>
<li>
<p>Choosing the right evaluation metric to assess a trading strategy.</p>
</li>
</ul>
</div></aside>
<img class="blueprint top" src="Images/bracket_top.png" width="1625" height="32"/>








<section data-type="sect2" class="blueprint" data-pdf-bookmark="Blueprint for Using Classification-Based Models to Predict Whether to Buy or Sell in the Bitcoin Market"><div class="sect2" id="idm45174920208552">
<h2>Blueprint for Using Classification-Based Models to Predict Whether to Buy or Sell in the Bitcoin Market</h2>










<section data-type="sect3" data-pdf-bookmark="1. Problem definition"><div class="sect3" id="idm45174920206760">
<h3>1. Problem definition</h3>

<p>The problem of predicting a buy or sell signal for a trading strategy is defined in the classification framework, where the predicted variable has a value of 1 for buy and 0 for sell. This signal is decided through the comparison of the short-term and long-term price trends.</p>

<p>The data used is from one of the largest bitcoin exchanges in terms of average daily volume, <a href="https://www.bitstamp.com">Bitstamp</a>. The data covers prices from January 2012 to May 2017. Different trend and momentum indicators are created from the data and are added as features to enhance the performance of the prediction model.</p>

<p>By the end of this case study, readers will be familiar with a general approach to building a trading strategy, from cleaning data and feature engineering to model tuning and developing a backtesting framework.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="2. Getting started—loading the data and Python packages"><div class="sect3" id="idm45174920202808">
<h3>2. Getting started—loading the data and Python packages</h3>

<p>Let’s load the packages and the data.</p>












<section data-type="sect4" data-pdf-bookmark="2.1. Loading the Python packages"><div class="sect4" id="idm45174920201384">
<h4>2.1. Loading the Python packages</h4>

<p><a data-type="indexterm" data-primary="bitcoin trading: strategy" data-secondary="loading data and Python packages" id="idm45174920200168"/>The standard Python packages are loaded in this step. The details have been presented in the previous case studies. Refer to the Jupyter notebook for this case study for more details.</p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="2.2. Loading the data"><div class="sect4" id="idm45174920198536">
<h4>2.2. Loading the data</h4>

<p>The bitcoin data fetched from the Bitstamp website is loaded in this step:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c"># load dataset</code>
<code class="n">dataset</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">read_csv</code><code class="p">(</code><code class="s">'BitstampData.csv'</code><code class="p">)</code></pre>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="3. Exploratory data analysis"><div class="sect3" id="data_analysis_ch6">
<h3>3. Exploratory data analysis</h3>

<p>In this step, we will take a detailed look at this data.</p>












<section data-type="sect4" data-pdf-bookmark="3.1. Descriptive statistics"><div class="sect4" id="idm45174920189992">
<h4>3.1. Descriptive statistics</h4>

<p><a data-type="indexterm" data-primary="bitcoin trading: strategy" data-secondary="exploratory data analysis" id="idm45174920188584"/>First, let us look at the shape of the data:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">dataset</code><code class="o">.</code><code class="n">shape</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">(2841377, 8)</pre>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c"># peek at data</code>
<code class="n">set_option</code><code class="p">(</code><code class="s">'display.width'</code><code class="p">,</code> <code class="mi">100</code><code class="p">)</code>
<code class="n">dataset</code><code class="o">.</code><code class="n">tail</code><code class="p">(</code><code class="mi">2</code><code class="p">)</code></pre>

<p><code>Output</code></p>
<table>

<thead>
<tr>
<th/>
<th>Timestamp</th>
<th>Open</th>
<th>High</th>
<th>Low</th>
<th>Close</th>
<th>Volume_(BTC)</th>
<th>Volume_(Currency)</th>
<th>Weighted_Price</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>2841372</p></td>
<td><p>1496188560</p></td>
<td><p>2190.49</p></td>
<td><p>2190.49</p></td>
<td><p>2181.37</p></td>
<td><p>2181.37</p></td>
<td><p>1.700166</p></td>
<td><p>3723.784755</p></td>
<td><p>2190.247337</p></td>
</tr>
<tr>
<td><p>2841373</p></td>
<td><p>1496188620</p></td>
<td><p>2190.50</p></td>
<td><p>2197.52</p></td>
<td><p>2186.17</p></td>
<td><p>2195.63</p></td>
<td><p>6.561029</p></td>
<td><p>14402.811961</p></td>
<td><p>2195.206304</p></td>
</tr>
</tbody>
</table>

<p>The dataset has minute-by-minute data of OHLC (Open, High, Low, Close) and 
<span class="keep-together">traded</span> volume of bitcoin. The dataset is large, with approximately 2.8 million total observations.</p>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="4. Data preparation"><div class="sect3" id="idm45174920073960">
<h3>4. Data preparation</h3>

<p>In this part, we will clean the data to prepare for modeling.</p>












<section data-type="sect4" data-pdf-bookmark="4.1. Data cleaning"><div class="sect4" id="idm45174920072264">
<h4>4.1. Data cleaning</h4>

<p><a data-type="indexterm" data-primary="bitcoin trading: strategy" data-secondary="data preparation" id="ix_Chapter6-asciidoc11"/>We clean the data by filling the NaNs with the last available values:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">dataset</code><code class="p">[</code><code class="n">dataset</code><code class="o">.</code><code class="n">columns</code><code class="o">.</code><code class="n">values</code><code class="p">]</code> <code class="o">=</code> <code class="n">dataset</code><code class="p">[</code><code class="n">dataset</code><code class="o">.</code><code class="n">columns</code><code class="o">.</code><code class="n">values</code><code class="p">]</code><code class="o">.</code><code class="n">ffill</code><code class="p">()</code></pre>

<p>The <code>Timestamp</code> column is not useful for modeling and is dropped from the dataset:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">dataset</code><code class="o">=</code><code class="n">dataset</code><code class="o">.</code><code class="n">drop</code><code class="p">(</code><code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s">'Timestamp'</code><code class="p">])</code></pre>
</div></section>













<section data-type="sect4" data-pdf-bookmark="4.2. Preparing the data for classification"><div class="sect4" id="idm45174920071928">
<h4>4.2. Preparing the data for classification</h4>

<p>As a first step, we will create the target variable for our model. This is the column that will indicate whether the trading signal is buy or sell. We define the short-term price as the 10-day rolling average and the long-term price as the 60-day rolling average. We attach a label of <code>1</code> <code>(0)</code> if the short-term price is higher (lower) than the long-term price:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c"># Create short simple moving average over the short window</code>
<code class="n">dataset</code><code class="p">[</code><code class="s">'short_mavg'</code><code class="p">]</code> <code class="o">=</code> <code class="n">dataset</code><code class="p">[</code><code class="s">'Close'</code><code class="p">]</code><code class="o">.</code><code class="n">rolling</code><code class="p">(</code><code class="n">window</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">min_periods</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code>\
<code class="n">center</code><code class="o">=</code><code class="k">False</code><code class="p">)</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code>

<code class="c"># Create long simple moving average over the long window</code>
<code class="n">dataset</code><code class="p">[</code><code class="s">'long_mavg'</code><code class="p">]</code> <code class="o">=</code> <code class="n">dataset</code><code class="p">[</code><code class="s">'Close'</code><code class="p">]</code><code class="o">.</code><code class="n">rolling</code><code class="p">(</code><code class="n">window</code><code class="o">=</code><code class="mi">60</code><code class="p">,</code> <code class="n">min_periods</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code>\
<code class="n">center</code><code class="o">=</code><code class="k">False</code><code class="p">)</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code>

<code class="c"># Create signals</code>
<code class="n">dataset</code><code class="p">[</code><code class="s">'signal'</code><code class="p">]</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">where</code><code class="p">(</code><code class="n">dataset</code><code class="p">[</code><code class="s">'short_mavg'</code><code class="p">]</code> <code class="o">&gt;</code>
<code class="n">dataset</code><code class="p">[</code><code class="s">'long_mavg'</code><code class="p">],</code> <code class="mf">1.0</code><code class="p">,</code> <code class="mf">0.0</code><code class="p">)</code></pre>
</div></section>













<section data-type="sect4" data-pdf-bookmark="4.3. Feature engineering"><div class="sect4" id="idm45174919982680">
<h4>4.3. Feature engineering</h4>

<p><a data-type="indexterm" data-primary="bitcoin trading: strategy" data-secondary="feature engineering" id="ix_Chapter6-asciidoc12"/>We begin feature engineering by analyzing the features we expect may influence the performance of the prediction model. Based on a conceptual understanding of key factors that drive investment strategies, the task at hand is to identify and construct new features that may capture the risks or characteristics embodied by these return drivers. <a data-type="indexterm" data-primary="momentum technical indicators" id="idm45174919888184"/>For this case study, we will explore the efficacy of specific momentum technical indicators.</p>

<p>The current data of the bitcoin consists of date, open, high, low, close, and volume. Using this data, we calculate the following momentum indicators:</p>
<dl>
<dt>Moving average</dt>
<dd>
<p><a data-type="indexterm" data-primary="moving average" id="idm45174919885000"/>A moving average provides an indication of a price trend by cutting down the amount of noise in the series.</p>
</dd>
<dt>Stochastic oscillator %K</dt>
<dd>
<p><a data-type="indexterm" data-primary="stochastic oscillators" id="idm45174919882920"/>A stochastic oscillator is a momentum indicator that compares the closing price of a security to a range of its previous prices over a certain period of time. <em>%K</em> and <em>%D</em> are slow and fast indicators. The fast indicator is more sensitive than the slow indicator to changes in the price of the underlying security and will likely result in many transaction signals.</p>
</dd>
<dt>Relative strength index (RSI)</dt>
<dd>
<p><a data-type="indexterm" data-primary="relative strength index (RSI)" id="idm45174919879720"/><a data-type="indexterm" data-primary="RSI (relative strength index)" id="idm45174919879048"/>This is a momentum indicator that measures the magnitude of recent price changes to evaluate overbought or oversold conditions in the price of a stock or other asset. The RSI ranges from 0 to 100. An asset is deemed to be overbought once the RSI approaches 70, meaning that the asset may be getting overvalued and is a good candidate for a pullback. Likewise, if the RSI approaches 30, it is an indication that the asset may be getting oversold and is therefore likely to become undervalued.</p>
</dd>
<dt>Rate of change (ROC)</dt>
<dd>
<p><a data-type="indexterm" data-primary="rate of change (ROC)" id="idm45174919876616"/><a data-type="indexterm" data-primary="ROC (rate of change)" id="idm45174919875912"/>This is a momentum oscillator that measures the percentage change between the current price and the <em>n</em> period past prices. Assets with higher ROC values are considered more likely to be overbought; those with lower ROC, more likely to be oversold.</p>
</dd>
<dt>Momentum (MOM)</dt>
<dd>
<p><a data-type="indexterm" data-primary="momentum (MOM)" id="idm45174919873272"/>This is the rate of acceleration of a security’s price or volume—that is, the speed at which the price is changing.</p>
</dd>
</dl>

<p>The following steps show how to generate some useful features for prediction. The features for trend and momentum can be leveraged for other trading strategy models:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c">#calculation of exponential moving average</code>
<code class="k">def</code> <code class="nf">EMA</code><code class="p">(</code><code class="n">df</code><code class="p">,</code> <code class="n">n</code><code class="p">):</code>
    <code class="n">EMA</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">Series</code><code class="p">(</code><code class="n">df</code><code class="p">[</code><code class="s">'Close'</code><code class="p">]</code><code class="o">.</code><code class="n">ewm</code><code class="p">(</code><code class="n">span</code><code class="o">=</code><code class="n">n</code><code class="p">,</code> <code class="n">min_periods</code><code class="o">=</code><code class="n">n</code><code class="p">)</code><code class="o">.</code><code class="n">mean</code><code class="p">(),</code> <code class="n">name</code><code class="o">=</code><code class="s">'EMA_'</code>\
     <code class="o">+</code> <code class="nb">str</code><code class="p">(</code><code class="n">n</code><code class="p">))</code>
    <code class="k">return</code> <code class="n">EMA</code>
<code class="n">dataset</code><code class="p">[</code><code class="s">'EMA10'</code><code class="p">]</code> <code class="o">=</code> <code class="n">EMA</code><code class="p">(</code><code class="n">dataset</code><code class="p">,</code> <code class="mi">10</code><code class="p">)</code>
<code class="n">dataset</code><code class="p">[</code><code class="s">'EMA30'</code><code class="p">]</code> <code class="o">=</code> <code class="n">EMA</code><code class="p">(</code><code class="n">dataset</code><code class="p">,</code> <code class="mi">30</code><code class="p">)</code>
<code class="n">dataset</code><code class="p">[</code><code class="s">'EMA200'</code><code class="p">]</code> <code class="o">=</code> <code class="n">EMA</code><code class="p">(</code><code class="n">dataset</code><code class="p">,</code> <code class="mi">200</code><code class="p">)</code>
<code class="n">dataset</code><code class="o">.</code><code class="n">head</code><code class="p">()</code>

<code class="c">#calculation of rate of change</code>
<code class="k">def</code> <code class="nf">ROC</code><code class="p">(</code><code class="n">df</code><code class="p">,</code> <code class="n">n</code><code class="p">):</code>
    <code class="n">M</code> <code class="o">=</code> <code class="n">df</code><code class="o">.</code><code class="n">diff</code><code class="p">(</code><code class="n">n</code> <code class="o">-</code> <code class="mi">1</code><code class="p">)</code>
    <code class="n">N</code> <code class="o">=</code> <code class="n">df</code><code class="o">.</code><code class="n">shift</code><code class="p">(</code><code class="n">n</code> <code class="o">-</code> <code class="mi">1</code><code class="p">)</code>
    <code class="n">ROC</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">Series</code><code class="p">(((</code><code class="n">M</code> <code class="o">/</code> <code class="n">N</code><code class="p">)</code> <code class="o">*</code> <code class="mi">100</code><code class="p">),</code> <code class="n">name</code> <code class="o">=</code> <code class="s">'ROC_'</code> <code class="o">+</code> <code class="nb">str</code><code class="p">(</code><code class="n">n</code><code class="p">))</code>
    <code class="k">return</code> <code class="n">ROC</code>
<code class="n">dataset</code><code class="p">[</code><code class="s">'ROC10'</code><code class="p">]</code> <code class="o">=</code> <code class="n">ROC</code><code class="p">(</code><code class="n">dataset</code><code class="p">[</code><code class="s">'Close'</code><code class="p">],</code> <code class="mi">10</code><code class="p">)</code>
<code class="n">dataset</code><code class="p">[</code><code class="s">'ROC30'</code><code class="p">]</code> <code class="o">=</code> <code class="n">ROC</code><code class="p">(</code><code class="n">dataset</code><code class="p">[</code><code class="s">'Close'</code><code class="p">],</code> <code class="mi">30</code><code class="p">)</code>

<code class="c">#calculation of price momentum</code>
<code class="k">def</code> <code class="nf">MOM</code><code class="p">(</code><code class="n">df</code><code class="p">,</code> <code class="n">n</code><code class="p">):</code>
    <code class="n">MOM</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">Series</code><code class="p">(</code><code class="n">df</code><code class="o">.</code><code class="n">diff</code><code class="p">(</code><code class="n">n</code><code class="p">),</code> <code class="n">name</code><code class="o">=</code><code class="s">'Momentum_'</code> <code class="o">+</code> <code class="nb">str</code><code class="p">(</code><code class="n">n</code><code class="p">))</code>
    <code class="k">return</code> <code class="n">MOM</code>
<code class="n">dataset</code><code class="p">[</code><code class="s">'MOM10'</code><code class="p">]</code> <code class="o">=</code> <code class="n">MOM</code><code class="p">(</code><code class="n">dataset</code><code class="p">[</code><code class="s">'Close'</code><code class="p">],</code> <code class="mi">10</code><code class="p">)</code>
<code class="n">dataset</code><code class="p">[</code><code class="s">'MOM30'</code><code class="p">]</code> <code class="o">=</code> <code class="n">MOM</code><code class="p">(</code><code class="n">dataset</code><code class="p">[</code><code class="s">'Close'</code><code class="p">],</code> <code class="mi">30</code><code class="p">)</code>

<code class="c">#calculation of relative strength index</code>
<code class="k">def</code> <code class="nf">RSI</code><code class="p">(</code><code class="n">series</code><code class="p">,</code> <code class="n">period</code><code class="p">):</code>
 <code class="n">delta</code> <code class="o">=</code> <code class="n">series</code><code class="o">.</code><code class="n">diff</code><code class="p">()</code><code class="o">.</code><code class="n">dropna</code><code class="p">()</code>
 <code class="n">u</code> <code class="o">=</code> <code class="n">delta</code> <code class="o">*</code> <code class="mi">0</code>
 <code class="n">d</code> <code class="o">=</code> <code class="n">u</code><code class="o">.</code><code class="n">copy</code><code class="p">()</code>
 <code class="n">u</code><code class="p">[</code><code class="n">delta</code> <code class="o">&gt;</code> <code class="mi">0</code><code class="p">]</code> <code class="o">=</code> <code class="n">delta</code><code class="p">[</code><code class="n">delta</code> <code class="o">&gt;</code> <code class="mi">0</code><code class="p">]</code>
 <code class="n">d</code><code class="p">[</code><code class="n">delta</code> <code class="o">&lt;</code> <code class="mi">0</code><code class="p">]</code> <code class="o">=</code> <code class="o">-</code><code class="n">delta</code><code class="p">[</code><code class="n">delta</code> <code class="o">&lt;</code> <code class="mi">0</code><code class="p">]</code>
 <code class="n">u</code><code class="p">[</code><code class="n">u</code><code class="o">.</code><code class="n">index</code><code class="p">[</code><code class="n">period</code><code class="o">-</code><code class="mi">1</code><code class="p">]]</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code> <code class="n">u</code><code class="p">[:</code><code class="n">period</code><code class="p">]</code> <code class="p">)</code> <code class="c">#first value is sum of avg gains</code>
 <code class="n">u</code> <code class="o">=</code> <code class="n">u</code><code class="o">.</code><code class="n">drop</code><code class="p">(</code><code class="n">u</code><code class="o">.</code><code class="n">index</code><code class="p">[:(</code><code class="n">period</code><code class="o">-</code><code class="mi">1</code><code class="p">)])</code>
 <code class="n">d</code><code class="p">[</code><code class="n">d</code><code class="o">.</code><code class="n">index</code><code class="p">[</code><code class="n">period</code><code class="o">-</code><code class="mi">1</code><code class="p">]]</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code> <code class="n">d</code><code class="p">[:</code><code class="n">period</code><code class="p">]</code> <code class="p">)</code> <code class="c">#first value is sum of avg losses</code>
 <code class="n">d</code> <code class="o">=</code> <code class="n">d</code><code class="o">.</code><code class="n">drop</code><code class="p">(</code><code class="n">d</code><code class="o">.</code><code class="n">index</code><code class="p">[:(</code><code class="n">period</code><code class="o">-</code><code class="mi">1</code><code class="p">)])</code>
 <code class="n">rs</code> <code class="o">=</code> <code class="n">u</code><code class="o">.</code><code class="n">ewm</code><code class="p">(</code><code class="n">com</code><code class="o">=</code><code class="n">period</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="n">adjust</code><code class="o">=</code><code class="k">False</code><code class="p">)</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code> <code class="o">/</code> \
 <code class="n">d</code><code class="o">.</code><code class="n">ewm</code><code class="p">(</code><code class="n">com</code><code class="o">=</code><code class="n">period</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="n">adjust</code><code class="o">=</code><code class="k">False</code><code class="p">)</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code>
 <code class="k">return</code> <code class="mi">100</code> <code class="o">-</code> <code class="mi">100</code> <code class="o">/</code> <code class="p">(</code><code class="mi">1</code> <code class="o">+</code> <code class="n">rs</code><code class="p">)</code>
<code class="n">dataset</code><code class="p">[</code><code class="s">'RSI10'</code><code class="p">]</code> <code class="o">=</code> <code class="n">RSI</code><code class="p">(</code><code class="n">dataset</code><code class="p">[</code><code class="s">'Close'</code><code class="p">],</code> <code class="mi">10</code><code class="p">)</code>
<code class="n">dataset</code><code class="p">[</code><code class="s">'RSI30'</code><code class="p">]</code> <code class="o">=</code> <code class="n">RSI</code><code class="p">(</code><code class="n">dataset</code><code class="p">[</code><code class="s">'Close'</code><code class="p">],</code> <code class="mi">30</code><code class="p">)</code>
<code class="n">dataset</code><code class="p">[</code><code class="s">'RSI200'</code><code class="p">]</code> <code class="o">=</code> <code class="n">RSI</code><code class="p">(</code><code class="n">dataset</code><code class="p">[</code><code class="s">'Close'</code><code class="p">],</code> <code class="mi">200</code><code class="p">)</code>

<code class="c">#calculation of stochastic osillator.</code>

<code class="k">def</code> <code class="nf">STOK</code><code class="p">(</code><code class="n">close</code><code class="p">,</code> <code class="n">low</code><code class="p">,</code> <code class="n">high</code><code class="p">,</code> <code class="n">n</code><code class="p">):</code>
 <code class="n">STOK</code> <code class="o">=</code> <code class="p">((</code><code class="n">close</code> <code class="o">-</code> <code class="n">low</code><code class="o">.</code><code class="n">rolling</code><code class="p">(</code><code class="n">n</code><code class="p">)</code><code class="o">.</code><code class="n">min</code><code class="p">())</code> <code class="o">/</code> <code class="p">(</code><code class="n">high</code><code class="o">.</code><code class="n">rolling</code><code class="p">(</code><code class="n">n</code><code class="p">)</code><code class="o">.</code><code class="n">max</code><code class="p">()</code> <code class="o">-</code> \
 <code class="n">low</code><code class="o">.</code><code class="n">rolling</code><code class="p">(</code><code class="n">n</code><code class="p">)</code><code class="o">.</code><code class="n">min</code><code class="p">()))</code> <code class="o">*</code> <code class="mi">100</code>
 <code class="k">return</code> <code class="n">STOK</code>

<code class="k">def</code> <code class="nf">STOD</code><code class="p">(</code><code class="n">close</code><code class="p">,</code> <code class="n">low</code><code class="p">,</code> <code class="n">high</code><code class="p">,</code> <code class="n">n</code><code class="p">):</code>
 <code class="n">STOK</code> <code class="o">=</code> <code class="p">((</code><code class="n">close</code> <code class="o">-</code> <code class="n">low</code><code class="o">.</code><code class="n">rolling</code><code class="p">(</code><code class="n">n</code><code class="p">)</code><code class="o">.</code><code class="n">min</code><code class="p">())</code> <code class="o">/</code> <code class="p">(</code><code class="n">high</code><code class="o">.</code><code class="n">rolling</code><code class="p">(</code><code class="n">n</code><code class="p">)</code><code class="o">.</code><code class="n">max</code><code class="p">()</code> <code class="o">-</code> \
 <code class="n">low</code><code class="o">.</code><code class="n">rolling</code><code class="p">(</code><code class="n">n</code><code class="p">)</code><code class="o">.</code><code class="n">min</code><code class="p">()))</code> <code class="o">*</code> <code class="mi">100</code>
 <code class="n">STOD</code> <code class="o">=</code> <code class="n">STOK</code><code class="o">.</code><code class="n">rolling</code><code class="p">(</code><code class="mi">3</code><code class="p">)</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code>
 <code class="k">return</code> <code class="n">STOD</code>

<code class="n">dataset</code><code class="p">[</code><code class="s">'%K10'</code><code class="p">]</code> <code class="o">=</code> <code class="n">STOK</code><code class="p">(</code><code class="n">dataset</code><code class="p">[</code><code class="s">'Close'</code><code class="p">],</code> <code class="n">dataset</code><code class="p">[</code><code class="s">'Low'</code><code class="p">],</code> <code class="n">dataset</code><code class="p">[</code><code class="s">'High'</code><code class="p">],</code> <code class="mi">10</code><code class="p">)</code>
<code class="n">dataset</code><code class="p">[</code><code class="s">'%D10'</code><code class="p">]</code> <code class="o">=</code> <code class="n">STOD</code><code class="p">(</code><code class="n">dataset</code><code class="p">[</code><code class="s">'Close'</code><code class="p">],</code> <code class="n">dataset</code><code class="p">[</code><code class="s">'Low'</code><code class="p">],</code> <code class="n">dataset</code><code class="p">[</code><code class="s">'High'</code><code class="p">],</code> <code class="mi">10</code><code class="p">)</code>
<code class="n">dataset</code><code class="p">[</code><code class="s">'%K30'</code><code class="p">]</code> <code class="o">=</code> <code class="n">STOK</code><code class="p">(</code><code class="n">dataset</code><code class="p">[</code><code class="s">'Close'</code><code class="p">],</code> <code class="n">dataset</code><code class="p">[</code><code class="s">'Low'</code><code class="p">],</code> <code class="n">dataset</code><code class="p">[</code><code class="s">'High'</code><code class="p">],</code> <code class="mi">30</code><code class="p">)</code>
<code class="n">dataset</code><code class="p">[</code><code class="s">'%D30'</code><code class="p">]</code> <code class="o">=</code> <code class="n">STOD</code><code class="p">(</code><code class="n">dataset</code><code class="p">[</code><code class="s">'Close'</code><code class="p">],</code> <code class="n">dataset</code><code class="p">[</code><code class="s">'Low'</code><code class="p">],</code> <code class="n">dataset</code><code class="p">[</code><code class="s">'High'</code><code class="p">],</code> <code class="mi">30</code><code class="p">)</code>
<code class="n">dataset</code><code class="p">[</code><code class="s">'%K200'</code><code class="p">]</code> <code class="o">=</code> <code class="n">STOK</code><code class="p">(</code><code class="n">dataset</code><code class="p">[</code><code class="s">'Close'</code><code class="p">],</code> <code class="n">dataset</code><code class="p">[</code><code class="s">'Low'</code><code class="p">],</code> <code class="n">dataset</code><code class="p">[</code><code class="s">'High'</code><code class="p">],</code> <code class="mi">200</code><code class="p">)</code>
<code class="n">dataset</code><code class="p">[</code><code class="s">'%D200'</code><code class="p">]</code> <code class="o">=</code> <code class="n">STOD</code><code class="p">(</code><code class="n">dataset</code><code class="p">[</code><code class="s">'Close'</code><code class="p">],</code> <code class="n">dataset</code><code class="p">[</code><code class="s">'Low'</code><code class="p">],</code> <code class="n">dataset</code><code class="p">[</code><code class="s">'High'</code><code class="p">],</code> <code class="mi">200</code><code class="p">)</code>

<code class="c">#calculation of moving average</code>
<code class="k">def</code> <code class="nf">MA</code><code class="p">(</code><code class="n">df</code><code class="p">,</code> <code class="n">n</code><code class="p">):</code>
    <code class="n">MA</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">Series</code><code class="p">(</code><code class="n">df</code><code class="p">[</code><code class="s">'Close'</code><code class="p">]</code><code class="o">.</code><code class="n">rolling</code><code class="p">(</code><code class="n">n</code><code class="p">,</code> <code class="n">min_periods</code><code class="o">=</code><code class="n">n</code><code class="p">)</code><code class="o">.</code><code class="n">mean</code><code class="p">(),</code> <code class="n">name</code><code class="o">=</code><code class="s">'MA_'</code>\
     <code class="o">+</code> <code class="nb">str</code><code class="p">(</code><code class="n">n</code><code class="p">))</code>
    <code class="k">return</code> <code class="n">MA</code>
<code class="n">dataset</code><code class="p">[</code><code class="s">'MA21'</code><code class="p">]</code> <code class="o">=</code> <code class="n">MA</code><code class="p">(</code><code class="n">dataset</code><code class="p">,</code> <code class="mi">10</code><code class="p">)</code>
<code class="n">dataset</code><code class="p">[</code><code class="s">'MA63'</code><code class="p">]</code> <code class="o">=</code> <code class="n">MA</code><code class="p">(</code><code class="n">dataset</code><code class="p">,</code> <code class="mi">30</code><code class="p">)</code>
<code class="n">dataset</code><code class="p">[</code><code class="s">'MA252'</code><code class="p">]</code> <code class="o">=</code> <code class="n">MA</code><code class="p">(</code><code class="n">dataset</code><code class="p">,</code> <code class="mi">200</code><code class="p">)</code></pre>

<p>With our features completed, we’ll prepare them for use.<a data-type="indexterm" data-startref="ix_Chapter6-asciidoc12" id="idm45174919609688"/></p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="4.4. Data visualization"><div class="sect4" id="idm45174919608952">
<h4>4.4. Data visualization</h4>

<p><a data-type="indexterm" data-primary="bitcoin trading: strategy" data-secondary="data visualization" id="idm45174919607416"/>In this step, we visualize different properties of the features and the predicted variable:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">dataset</code><code class="p">[[</code><code class="s">'Weighted_Price'</code><code class="p">]]</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">grid</code><code class="o">=</code><code class="k">True</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>

<p><code>Output</code></p>

<figure><div class="figure">
<img src="Images/mlbf_06in13.png" alt="mlbf 06in13" width="367" height="233"/>
<h6/>
</div></figure>

<p>The chart illustrates a sharp rise in the price of bitcoin, increasing from close to $0 to around $2,500 in 2017. Also, high price volatility is readily visible.</p>

<p>Let us look at the distribution of the predicted variable:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">fig</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">figure</code><code class="p">()</code>
<code class="n">plot</code> <code class="o">=</code> <code class="n">dataset</code><code class="o">.</code><code class="n">groupby</code><code class="p">([</code><code class="s">'signal'</code><code class="p">])</code><code class="o">.</code><code class="n">size</code><code class="p">()</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">kind</code><code class="o">=</code><code class="s">'barh'</code><code class="p">,</code> <code class="n">color</code><code class="o">=</code><code class="s">'red'</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>

<p><code>Output</code></p>

<figure><div class="figure">
<img src="Images/mlbf_06in14.png" alt="mlbf 06in14" width="373" height="233"/>
<h6/>
</div></figure>

<p>The predicted variable is 1 more than 52% of the time, meaning there are more buy signals than sell signals. The predicted variable is relatively balanced, especially as compared to the fraud dataset we saw in the first case study.<a data-type="indexterm" data-startref="ix_Chapter6-asciidoc11" id="idm45174918684072"/></p>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="5. Evaluate algorithms and models"><div class="sect3" id="idm45174919608360">
<h3>5. Evaluate algorithms and models</h3>

<p><a data-type="indexterm" data-primary="bitcoin trading: strategy" data-secondary="evaluation of algorithms and models" id="ix_Chapter6-asciidoc13"/>In this step, we will evaluate different algorithms.</p>












<section data-type="sect4" data-pdf-bookmark="5.1. Train-test split"><div class="sect4" id="idm45174918680552">
<h4>5.1. Train-test split</h4>

<p>We first split the dataset into training (80%) and test (20%) sets. For this case study we use 100,000 observations for a faster calculation. The next steps would be same in the event we want to use the entire dataset:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c"># split out validation dataset for the end</code>
<code class="n">subset_dataset</code><code class="o">=</code> <code class="n">dataset</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="o">-</code><code class="mi">100000</code><code class="p">:]</code>
<code class="n">Y</code><code class="o">=</code> <code class="n">subset_dataset</code><code class="p">[</code><code class="s">"signal"</code><code class="p">]</code>
<code class="n">X</code> <code class="o">=</code> <code class="n">subset_dataset</code><code class="o">.</code><code class="n">loc</code><code class="p">[:,</code> <code class="n">dataset</code><code class="o">.</code><code class="n">columns</code> <code class="o">!=</code> <code class="s">'signal'</code><code class="p">]</code>
<code class="n">validation_size</code> <code class="o">=</code> <code class="mf">0.2</code>
<code class="n">seed</code> <code class="o">=</code> <code class="mi">1</code>
<code class="n">X_train</code><code class="p">,</code> <code class="n">X_validation</code><code class="p">,</code> <code class="n">Y_train</code><code class="p">,</code> <code class="n">Y_validation</code> <code class="o">=</code>\
<code class="n">train_test_split</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">Y</code><code class="p">,</code> <code class="n">test_size</code><code class="o">=</code><code class="n">validation_size</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code></pre>
</div></section>













<section data-type="sect4" class="pagebreak-before less_space" data-pdf-bookmark="5.2. Test options and evaluation metrics"><div class="sect4" id="idm45174918676936">
<h4>5.2. Test options and evaluation metrics</h4>

<p>Accuracy can be used as the evaluation metric since there is not a significant class imbalance in the data:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c"># test options for classification</code>
<code class="n">num_folds</code> <code class="o">=</code> <code class="mi">10</code>
<code class="n">scoring</code> <code class="o">=</code> <code class="s">'accuracy'</code></pre>
</div></section>













<section data-type="sect4" data-pdf-bookmark="5.3. Compare models and algorithms"><div class="sect4" id="idm45174918597608">
<h4>5.3. Compare models and algorithms</h4>

<p>In order to know which algorithm is best for our strategy, we evaluate the linear, nonlinear, and ensemble models.</p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="5.3.1. Models"><div class="sect4" id="idm45174918595208">
<h4>5.3.1. Models</h4>

<p>Checking the classification algorithms:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">models</code> <code class="o">=</code> <code class="p">[]</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'LR'</code><code class="p">,</code> <code class="n">LogisticRegression</code><code class="p">(</code><code class="n">n_jobs</code><code class="o">=-</code><code class="mi">1</code><code class="p">)))</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'LDA'</code><code class="p">,</code> <code class="n">LinearDiscriminantAnalysis</code><code class="p">()))</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'KNN'</code><code class="p">,</code> <code class="n">KNeighborsClassifier</code><code class="p">()))</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'CART'</code><code class="p">,</code> <code class="n">DecisionTreeClassifier</code><code class="p">()))</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'NB'</code><code class="p">,</code> <code class="n">GaussianNB</code><code class="p">()))</code>
<code class="c">#Neural Network</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'NN'</code><code class="p">,</code> <code class="n">MLPClassifier</code><code class="p">()))</code>
<code class="c"># Ensemble Models</code>
<code class="c"># Boosting methods</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'AB'</code><code class="p">,</code> <code class="n">AdaBoostClassifier</code><code class="p">()))</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'GBM'</code><code class="p">,</code> <code class="n">GradientBoostingClassifier</code><code class="p">()))</code>
<code class="c"># Bagging methods</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'RF'</code><code class="p">,</code> <code class="n">RandomForestClassifier</code><code class="p">(</code><code class="n">n_jobs</code><code class="o">=-</code><code class="mi">1</code><code class="p">)))</code></pre>

<p>After performing the <em>k</em>-fold cross validation, the comparison of the models is as 
<span class="keep-together">follows</span>:</p>

<figure><div class="figure">
<img src="Images/mlbf_06in15.png" alt="mlbf 06in15" width="1170" height="618"/>
<h6/>
</div></figure>

<p>Although some of the models show promising results, we prefer an ensemble model given the huge size of the dataset, the large number of features, and an expected nonlinear relationship between the predicted variable and the features. Random forest has the best performance among the ensemble models.<a data-type="indexterm" data-startref="ix_Chapter6-asciidoc13" id="idm45174918444136"/></p>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="6. Model tuning and grid search"><div class="sect3" id="idm45174918443304">
<h3>6. Model tuning and grid search</h3>

<p><a data-type="indexterm" data-primary="bitcoin trading: strategy" data-secondary="model tuning and grid search" id="idm45174918441928"/>A grid search is performed for the random forest model by varying the number of estimators and maximum depth. The details of the random forest model and the parameters to tune are discussed in <a data-type="xref" href="ch04.xhtml#Chapter4">Chapter 4</a>:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">n_estimators</code> <code class="o">=</code> <code class="p">[</code><code class="mi">20</code><code class="p">,</code><code class="mi">80</code><code class="p">]</code>
<code class="n">max_depth</code><code class="o">=</code> <code class="p">[</code><code class="mi">5</code><code class="p">,</code><code class="mi">10</code><code class="p">]</code>
<code class="n">criterion</code> <code class="o">=</code> <code class="p">[</code><code class="s">"gini"</code><code class="p">,</code><code class="s">"entropy"</code><code class="p">]</code>
<code class="n">param_grid</code> <code class="o">=</code> <code class="nb">dict</code><code class="p">(</code><code class="n">n_estimators</code><code class="o">=</code><code class="n">n_estimators</code><code class="p">,</code> <code class="n">max_depth</code><code class="o">=</code><code class="n">max_depth</code><code class="p">,</code> \
  <code class="n">criterion</code> <code class="o">=</code> <code class="n">criterion</code> <code class="p">)</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">RandomForestClassifier</code><code class="p">(</code><code class="n">n_jobs</code><code class="o">=-</code><code class="mi">1</code><code class="p">)</code>
<code class="n">kfold</code> <code class="o">=</code> <code class="n">KFold</code><code class="p">(</code><code class="n">n_splits</code><code class="o">=</code><code class="n">num_folds</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="n">seed</code><code class="p">)</code>
<code class="n">grid</code> <code class="o">=</code> <code class="n">GridSearchCV</code><code class="p">(</code><code class="n">estimator</code><code class="o">=</code><code class="n">model</code><code class="p">,</code> <code class="n">param_grid</code><code class="o">=</code><code class="n">param_grid</code><code class="p">,</code> \
  <code class="n">scoring</code><code class="o">=</code><code class="n">scoring</code><code class="p">,</code> <code class="n">cv</code><code class="o">=</code><code class="n">kfold</code><code class="p">)</code>
<code class="n">grid_result</code> <code class="o">=</code> <code class="n">grid</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">Y_train</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="s">"Best: %f using %s"</code> <code class="o">%</code> <code class="p">(</code><code class="n">grid_result</code><code class="o">.</code><code class="n">best_score_</code><code class="p">,</code>\
  <code class="n">grid_result</code><code class="o">.</code><code class="n">best_params_</code><code class="p">))</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">Best: 0.903438 using {'criterion': 'gini', 'max_depth': 10, 'n_estimators': 80}</pre>
</div></section>













<section data-type="sect3" data-pdf-bookmark="7. Finalize the model"><div class="sect3" id="idm45174918437512">
<h3>7. Finalize the model</h3>

<p><a data-type="indexterm" data-primary="bitcoin trading: strategy" data-secondary="finalizing the model" id="ix_Chapter6-asciidoc14"/>Let us finalize the model with the best parameters found during the tuning step and perform the variable intuition.</p>












<section data-type="sect4" data-pdf-bookmark="7.1. Results on the test dataset"><div class="sect4" id="idm45174918303112">
<h4>7.1. Results on the test dataset</h4>

<p>In this step, we evaluate the selected model on the test set:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c"># prepare model</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">RandomForestClassifier</code><code class="p">(</code><code class="n">criterion</code><code class="o">=</code><code class="s">'gini'</code><code class="p">,</code> <code class="n">n_estimators</code><code class="o">=</code><code class="mi">80</code><code class="p">,</code><code class="n">max_depth</code><code class="o">=</code><code class="mi">10</code><code class="p">)</code>

<code class="c">#model = LogisticRegression()</code>
<code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">Y_train</code><code class="p">)</code>

<code class="c"># estimate accuracy on validation set</code>
<code class="n">predictions</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_validation</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">accuracy_score</code><code class="p">(</code><code class="n">Y_validation</code><code class="p">,</code> <code class="n">predictions</code><code class="p">))</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">0.9075</pre>

<p class="pagebreak-before">The selected model performs quite well, with an accuracy of 90.75%. Let us look at the confusion matrix:</p>

<figure><div class="figure">
<img src="Images/mlbf_06in16.png" alt="mlbf 06in16" width="365" height="247"/>
<h6/>
</div></figure>

<p>The overall model performance is reasonable and is in line with the training set results.</p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="7.2. Variable intuition/feature importance"><div class="sect4" id="idm45174918151224">
<h4>7.2. Variable intuition/feature importance</h4>

<p>Let us look into the feature importance of the model:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">Importance</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">({</code><code class="s">'Importance'</code><code class="p">:</code><code class="n">model</code><code class="o">.</code><code class="n">feature_importances_</code><code class="o">*</code><code class="mi">100</code><code class="p">},</code>\
 <code class="n">index</code><code class="o">=</code><code class="n">X</code><code class="o">.</code><code class="n">columns</code><code class="p">)</code>
<code class="n">Importance</code><code class="o">.</code><code class="n">sort_values</code><code class="p">(</code><code class="s">'Importance'</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">ascending</code><code class="o">=</code><code class="k">True</code><code class="p">)</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">kind</code><code class="o">=</code><code class="s">'barh'</code><code class="p">,</code> \
<code class="n">color</code><code class="o">=</code><code class="s">'r'</code> <code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s">'Variable Importance'</code><code class="p">)</code></pre>

<p><code>Output</code></p>

<figure><div class="figure">
<img src="Images/mlbf_06in17.png" alt="mlbf 06in17" width="420" height="249"/>
<h6/>
</div></figure>

<p>The result of the variable importance looks intuitive, and the momentum indicators of RSI and MOM over the last 30 days seem to be the two most important features. The feature importance chart corroborates the fact that introducing new features leads to an improvement in the model performance.</p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="7.3. Backtesting results"><div class="sect4" id="idm45174918124424">
<h4>7.3. Backtesting results</h4>

<p>In this additional step, we perform a backtest on the model we’ve developed. We create a column for strategy returns by multiplying the daily returns by the position that was held at the close of business the previous day and compare it against the actual returns.</p>
<div data-type="tip"><h1>Backtesting Trading Strategies</h1>
<p>A backtesting approach similar to the one presented in this case study can be used to quickly backtest any trading strategy.</p>
</div>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">backtestdata</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">index</code><code class="o">=</code><code class="n">X_validation</code><code class="o">.</code><code class="n">index</code><code class="p">)</code>
<code class="n">backtestdata</code><code class="p">[</code><code class="s">'signal_pred'</code><code class="p">]</code> <code class="o">=</code> <code class="n">predictions</code>
<code class="n">backtestdata</code><code class="p">[</code><code class="s">'signal_actual'</code><code class="p">]</code> <code class="o">=</code> <code class="n">Y_validation</code>
<code class="n">backtestdata</code><code class="p">[</code><code class="s">'Market Returns'</code><code class="p">]</code> <code class="o">=</code> <code class="n">X_validation</code><code class="p">[</code><code class="s">'Close'</code><code class="p">]</code><code class="o">.</code><code class="n">pct_change</code><code class="p">()</code>
<code class="n">backtestdata</code><code class="p">[</code><code class="s">'Actual Returns'</code><code class="p">]</code> <code class="o">=</code> <code class="n">backtestdata</code><code class="p">[</code><code class="s">'Market Returns'</code><code class="p">]</code> <code class="o">*</code>\
<code class="n">backtestdata</code><code class="p">[</code><code class="s">'signal_actual'</code><code class="p">]</code><code class="o">.</code><code class="n">shift</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code>
<code class="n">backtestdata</code><code class="p">[</code><code class="s">'Strategy Returns'</code><code class="p">]</code> <code class="o">=</code> <code class="n">backtestdata</code><code class="p">[</code><code class="s">'Market Returns'</code><code class="p">]</code> <code class="o">*</code> \
<code class="n">backtestdata</code><code class="p">[</code><code class="s">'signal_pred'</code><code class="p">]</code><code class="o">.</code><code class="n">shift</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code>
<code class="n">backtestdata</code><code class="o">=</code><code class="n">backtestdata</code><code class="o">.</code><code class="n">reset_index</code><code class="p">()</code>
<code class="n">backtestdata</code><code class="o">.</code><code class="n">head</code><code class="p">()</code>
<code class="n">backtestdata</code><code class="p">[[</code><code class="s">'Strategy Returns'</code><code class="p">,</code><code class="s">'Actual Returns'</code><code class="p">]]</code><code class="o">.</code><code class="n">cumsum</code><code class="p">()</code><code class="o">.</code><code class="n">hist</code><code class="p">()</code>
<code class="n">backtestdata</code><code class="p">[[</code><code class="s">'Strategy Returns'</code><code class="p">,</code><code class="s">'Actual Returns'</code><code class="p">]]</code><code class="o">.</code><code class="n">cumsum</code><code class="p">()</code><code class="o">.</code><code class="n">plot</code><code class="p">()</code></pre>

<p><code>Output</code></p>

<figure><div class="figure">
<img src="Images/mlbf_06in18.png" alt="mlbf 06in18" width="371" height="249"/>
<h6/>
</div></figure>

<figure class="width-75"><div class="figure">
<img src="Images/mlbf_06in19.png" alt="mlbf 06in19" width="375" height="233"/>
<h6/>
</div></figure>

<p>Looking at the backtesting results, we do not deviate significantly from the actual market return. Indeed, the achieved momentum trading strategy made us better at predicting the price direction to buy or sell in order to make profits. However, as our accuracy is not 100% (but more than 96%), we made relatively few losses compared to the actual returns<a data-type="indexterm" data-startref="ix_Chapter6-asciidoc14" id="idm45174917970136"/>.</p>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="Conclusion"><div class="sect3" id="idm45174917969176">
<h3>Conclusion</h3>

<p>This case study demonstrated that framing the problem is a key step when tackling a finance problem with machine learning. In doing so, it was detemined that transforming the labels according to an investment objective and performing feature engineering were required for this trading strategy. We demonstrated the efficiency of using intuitive features related to the trend and momentum of the price movement. This helped increase the predictive power of the model. Finally, we introduced a backtesting framework, which allowed us to simulate a trading strategy using historical data. This enabled us to generate results and analyze risk and profitability before risking any actual capital.<a data-type="indexterm" data-startref="ix_Chapter6-asciidoc10" id="idm45174917967144"/></p>
<img class="blueprint bottom" src="Images/bracket_bottom.png" width="1625" height="32"/>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Chapter Summary"><div class="sect1" id="idm45174920222120">
<h1>Chapter Summary</h1>

<p>In <a data-type="xref" href="#CaseStudy1SC">“Case Study 1: Fraud Detection”</a>, we explored the issue of an unbalanced dataset and the importance of having the right evaluation metric. In <a data-type="xref" href="#CaseStudy2SC">“Case Study 2: Loan Default Probability”</a>, various techniques and concepts of data processing, feature selection, and exploratory analysis were covered. In <a data-type="xref" href="#CaseStudy3SC">“Case Study 3: Bitcoin Trading Strategy”</a>, we looked at ways to create technical indicators as features in order to use them for model enhancement. We also prepared a backtesting framework for a trading strategy.</p>

<p>Overall, the concepts in Python, machine learning, and finance presented in this chapter can used as a blueprint for any other classification-based problem in finance.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Exercises"><div class="sect1" id="idm45174917960632">
<h1>Exercises</h1>

<ul>
<li>
<p>Predict whether a stock price will go up or down using the features related to the stock or macroeconomic variables (use the ideas from the bitcoin-based case study presented in this chapter).</p>
</li>
<li>
<p>Create a model to detect money laundering using the features of a transaction. A sample dataset for this exercise can be obtained from <a href="https://oreil.ly/GcinN">Kaggle</a>.</p>
</li>
<li>
<p>Perform a credit rating analysis of corporations using the features related to creditworthiness.<a data-type="indexterm" data-startref="ix_Chapter6-asciidoc1" id="idm45174917955752"/><a data-type="indexterm" data-startref="ix_Chapter6-asciidoc0" id="idm45174917955048"/></p>
</li>
</ul>
</div></section>







<div data-type="footnotes"><p data-type="footnote" id="idm45174924215160"><sup><a href="ch06.xhtml#idm45174924215160-marker">1</a></sup> There may be reordering or renaming of the steps or substeps based on the appropriateness and intuitiveness of the steps/substeps.</p><p data-type="footnote" id="idm45174922143032"><sup><a href="ch06.xhtml#idm45174922143032-marker">2</a></sup> The predicted variable is further used for correlation-based feature reduction.</p><p data-type="footnote" id="idm45174920856536"><sup><a href="ch06.xhtml#idm45174920856536-marker">3</a></sup> Sampling is covered in detail in <a data-type="xref" href="#CaseStudy1SC">“Case Study 1: Fraud Detection”</a>.</p></div></div></section></div>



  </body></html>