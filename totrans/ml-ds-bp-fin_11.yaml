- en: 'Chapter 8\. Unsupervised Learning: Clustering'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we explored dimensionality reduction, which is one
    type of unsupervised learning. In this chapter, we will explore *clustering*,
    a category of unsupervised learning techniques that allows us to discover hidden
    structures in data.
  prefs: []
  type: TYPE_NORMAL
- en: Both clustering and dimensionality reduction summarize the data. Dimensionality
    reduction compresses the data by representing it using new, fewer features while
    still capturing the most relevant information. Similarly, clustering is a way
    to reduce the volume of data and find patterns. However, it does so by categorizing
    the original data and not by creating new variables. Clustering algorithms assign
    observations to subgroups that consist of similar data points. The goal of clustering
    is to find a natural grouping in data so that items in a given cluster are more
    similar to each other than to those of different clusters. Clustering serves to
    better understand the data through the lens of several categories or groups created.
    It also permits the automatic categorization of new objects according to the learned
    criteria.
  prefs: []
  type: TYPE_NORMAL
- en: In the field of finance, clustering has been used by traders and investment
    managers to find homogeneous groups of assets, classes, sectors, and countries
    based on similar characteristics. Clustering analysis augments trading strategies
    by providing insights into categories of trading signals. The technique has been
    used to segment customers or investors into a number of groups to better understand
    their behavior and to perform additional analysis.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will discuss fundamental clustering techniques and introduce
    three case studies in the areas of portfolio management and trading strategy development.
  prefs: []
  type: TYPE_NORMAL
- en: 'In [“Case Study 1: Clustering for Pairs Trading”](#CaseStudy1CL), we use clustering
    methods to select pairs of stocks for a trading strategy. A *pairs trading strategy*
    involves matching a long position with a short position in two financial instruments
    that are closely related. Finding appropriate pairs can be a challenge when the
    number of instruments is high. In this case study, we demonstrate how clustering
    can be a useful technique in trading strategy development and other similar situations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In [“Case Study 2: Portfolio Management: Clustering Investors”](#CaseStudy2CL),
    we identify clusters of investors with similar abilities and willingness to take
    risks. We show how clustering techniques can be used for effective asset allocation
    and portfolio rebalancing. This illustrates how part of the portfolio management
    process can be automated, which is immensely useful for investment managers and
    robo-advisors alike.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In [“Case Study 3: Hierarchical Risk Parity”](#CaseStudy3CL), we use a clustering-based
    algorithm to allocate capital into different asset classes and compare the results
    against other portfolio allocation techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: This Chapter’s Code Repository
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Python-based master template for clustering, along with the Jupyter notebook
    for the case studies presented in this chapter are in [Chapter 8 - Unsup. Learning
    - Clustering](https://oreil.ly/uzbaH) in the code repository for this book. To
    work through any machine learning problems in Python involving the models for
    clustering (such as *k*-means, hierarchical clustering, etc.) presented in this
    chapter, readers simply need to modify the template to align with their problem
    statement. Similar to the previous chapters, the case studies presented in this
    chapter use the standard Python master template with the standardized model development
    steps presented in [Chapter 2](ch02.xhtml#Chapter2). For the clustering case studies,
    steps 6 (Model Tuning and Grid Search) and 7 (Finalizing the Model) have merged
    with step 5 (Evaluate Algorithms and Models).
  prefs: []
  type: TYPE_NORMAL
- en: Clustering Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are many types of clustering techniques, and they differ with respect
    to their strategy of identifying groupings. Choosing which technique to apply
    depends on the nature and structure of the data. In this chapter, we will cover
    the following three clustering techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: '*k*-means clustering'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hierarchical clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Affinity propagation clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following section summarizes these clustering techniques, including their
    strengths and weaknesses. Additional details for each of the clustering methods
    are provided in the case studies.
  prefs: []
  type: TYPE_NORMAL
- en: k-means Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*k*-means is the most well-known clustering technique. The algorithm of *k*-means
    aims to find and group data points into classes that have high similarity between
    them. This similarity is understood as the opposite of the distance between data
    points. The closer the data points are, the more likely they are to belong to
    the same cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm finds *k* centroids and assigns each data point to exactly one
    cluster with the goal of minimizing the within-cluster variance (called *inertia*).
    It typically uses the Euclidean distance (ordinary distance between two points),
    but other distance metrics can be used. The *k*-means algorithm delivers a local
    optimum for a given *k* and proceeds as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: This algorithm specifies the number of clusters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data points are randomly selected as cluster centers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each data point is assigned to the cluster center it is nearest to.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cluster centers are updated to the mean of the assigned points.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Steps 3–4 are repeated until all cluster centers remain unchanged.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In simple terms, we randomly move around the specified number of centroids in
    each iteration, assigning each data point to the closest centroid. Once we have
    done that, we calculate the mean distance of all points in each centroid. Then,
    once we can no longer reduce the minimum distance from data points to their respective
    centroids, we have found our clusters.
  prefs: []
  type: TYPE_NORMAL
- en: k-means hyperparameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The *k*-means hyperparameters include:'
  prefs: []
  type: TYPE_NORMAL
- en: Number of clusters
  prefs: []
  type: TYPE_NORMAL
- en: The number of clusters and centroids to generate.
  prefs: []
  type: TYPE_NORMAL
- en: Maximum iterations
  prefs: []
  type: TYPE_NORMAL
- en: Maximum iterations of the algorithm for a single run.
  prefs: []
  type: TYPE_NORMAL
- en: Number initial
  prefs: []
  type: TYPE_NORMAL
- en: The number of times the algorithm will be run with different centroid seeds.
    The final result will be the best output of the defined number of consecutive
    runs, in terms of inertia.
  prefs: []
  type: TYPE_NORMAL
- en: With *k*-means, different random starting points for the cluster centers often
    result in very different clustering solutions. Therefore, the *k*-means algorithm
    is run in sklearn with at least 10 different random initializations, and the solution
    occurring the greatest number of times is chosen.
  prefs: []
  type: TYPE_NORMAL
- en: The strengths of *k*-means include its simplicity, wide range of applicability,
    fast convergence, and linear scalability to large data while producing clusters
    of an even size. It is most useful when we know the exact number of clusters,
    *k*, beforehand. In fact, a main weakness of *k*-means is having to tune this
    hyperparameter. Additional drawbacks include the lack of a guarantee to find a
    global optimum and its sensitivity to outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation in Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Python’s sklearn library offers a powerful implementation of *k*-means. The
    following code snippet illustrates how to apply *k*-means clustering on a dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The number of clusters is the key hyperparameter to be tuned. We will look at
    the *k*-means clustering technique in case studies 1 and 2 of this chapter, in
    which further details on choosing the right number of clusters and detailed visualizations
    are provided.
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Hierarchical clustering* involves creating clusters that have a predominant
    ordering from top to bottom. The main advantage of hierarchical clustering is
    that we do not need to specify the number of clusters; the model determines that
    by itself. This clustering technique is divided into two types: agglomerative
    hierarchical clustering and divisive hierarchical clustering.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Agglomerative hierarchical clustering* is the most common type of hierarchical
    clustering and is used to group objects based on their similarity. It is a “bottom-up”
    approach where each observation starts in its own cluster, and pairs of clusters
    are merged as one moves up the hierarchy. The agglomerative hierarchical clustering
    algorithm delivers a *local optimum* and proceeds as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Make each data point a single-point cluster and form *N* clusters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take the two closest data points and combine them, leaving *N-1* clusters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take the two closest clusters and combine them, forming *N-2* clusters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat step 3 until left with only one cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Divisive hierarchical clustering* works “top-down” and sequentially splits
    the remaining clusters to produce the most distinct subgroups.'
  prefs: []
  type: TYPE_NORMAL
- en: Both produce *N-1* hierarchical levels and facilitate the clustering creation
    at the level that best partitions data into homogeneous groups. We will focus
    on the more common agglomerative clustering approach.
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical clustering enables the plotting of *dendrograms*, which are visualizations
    of a binary hierarchical clustering. A dendrogram is a type of tree diagram showing
    hierarchical relationships between different sets of data. They provide an interesting
    and informative visualization of hierarchical clustering results. A dendrogram
    contains the memory of the hierarchical clustering algorithm, so you can tell
    how the cluster is formed simply by inspecting the chart.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 8-1](#HC) shows an example of dendrograms based on hierarchical clustering.
    The distance between data points represents dissimilarities, and the height of
    the blocks represents the distance between clusters.'
  prefs: []
  type: TYPE_NORMAL
- en: Observations that fuse at the bottom are similar, while those at the top are
    quite different. With dendrograms, conclusions are made based on the location
    of the vertical axis rather than on the horizontal one.
  prefs: []
  type: TYPE_NORMAL
- en: The advantages of hierarchical clustering are that it is easy to implement it,
    does not require one to specify the number of clusters, and it produces dendrograms
    that are very useful in understanding the data. However, the time complexity for
    hierarchical clustering can result in long computation times relative to other
    algorithms, such as *k*-means. If we have a large dataset, it can be difficult
    to determine the correct number of clusters by looking at the dendrogram. Hierarchical
    clustering is very sensitive to outliers, and in their presence, model performance
    decreases significantly.
  prefs: []
  type: TYPE_NORMAL
- en: '![mlbf 0801](Images/mlbf_0801.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-1\. Hierarchical clustering
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Implementation in Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following code snippet illustrates how to apply agglomerative hierarchical
    clustering with four clusters on a dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: More details regarding the hyperparameters of agglomerative hierarchical clustering
    can be found on the [sklearn website](https://scikit-learn.org). We will look
    at the hierarchical clustering technique in case studies 1 and 3 in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Affinity Propagation Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Affinity propagation* creates clusters by sending messages between data points
    until convergence. Unlike clustering algorithms such as *k*-means, affinity propagation
    does not require the number of clusters to be determined or estimated before running
    the algorithm. Two important parameters are used in affinity propagation to determine
    the number of clusters: the *preference*, which controls how many *exemplars*
    (or prototypes) are used; and the *damping factor*, which dampens the responsibility
    and availability of messages to avoid numerical oscillations when updating these
    messages.'
  prefs: []
  type: TYPE_NORMAL
- en: A dataset is described using a small number of exemplars. These are members
    of the input set that are representative of clusters. The affinity propagation
    algorithm takes in a set of pairwise similarities between data points and finds
    clusters by maximizing the total similarity between data points and their exemplars.
    The messages sent between pairs represent the suitability of one sample to be
    the exemplar of the other, which is updated in response to the values from other
    pairs. This updating happens iteratively until convergence, at which point the
    final exemplars are chosen, and we obtain the final clustering.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of strengths, affinity propagation does not require the number of clusters
    to be determined before running the algorithm. The algorithm is fast and can be
    applied to large similarity matrices. However, the algorithm often converges to
    suboptimal solutions, and at times it can fail to converge.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation in Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following code snippet illustrates how to implement the affinity propagation
    algorithm for a dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: More details regarding the hyperparameters of affinity propagation clustering
    can be found on the [sklearn website](https://scikit-learn.org). We will look
    at the affinity propagation technique in case studies 1 and 2 in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Case Study 1: Clustering for Pairs Trading'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A pairs trading strategy constructs a portfolio of correlated assets with similar
    market risk factor exposure. Temporary price discrepancies in these assets can
    create opportunities to profit through a long position in one instrument and a
    short position in another. A pairs trading strategy is designed to eliminate market
    risk and exploit these temporary discrepancies in the relative returns of stocks.
  prefs: []
  type: TYPE_NORMAL
- en: The fundamental premise in pairs trading is that *mean reversion* is an expected
    dynamic of the assets. This mean reversion should lead to a long-run equilibrium
    relationship, which we try to approximate through statistical methods. When moments
    of (presumably temporary) divergence from this long-term trend arise, one can
    possibly profit. The key to successful pairs trading is the ability to select
    the right pairs of assets to be used.
  prefs: []
  type: TYPE_NORMAL
- en: Traditionally, trial and error was used for pairs selection. Stocks or instruments
    that were merely in the same sector or industry were grouped together. The idea
    was that if these stocks were for companies in similar industries, their stocks
    should move similarly as well. However, this was and is not necessarily the case.
    Additionally, with a large universe of stocks, finding a suitable pair is a difficult
    task, given that there are a total of *n(n–1)/2* possible pairs, where *n* is
    the number of instruments. Clustering can be a useful technique here.
  prefs: []
  type: TYPE_NORMAL
- en: In this case study, we will use clustering algorithms to select pairs of stocks
    for a pairs trading strategy.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/bracket_top.png)'
  prefs: []
  type: TYPE_IMG
- en: Blueprint for Using Clustering to Select Pairs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 1\. Problem definition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our goal in this case study is to perform clustering analysis on the stocks
    in the S&P 500 to come up with pairs for a pairs trading strategy. S&P 500 stock
    data was obtained using `pandas_datareader` from Yahoo Finance. It includes price
    data from 2018 onwards.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Getting started—loading the data and Python packages
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The list of the libraries used for data loading, data analysis, data preparation,
    and model evaluation are shown below.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1\. Loading the Python packages
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The details of most of these packages and functions have been provided in Chapters
    [2](ch02.xhtml#Chapter2) and [4](ch04.xhtml#Chapter4). The use of these packages
    will be demonstrated in different steps of the model development process.
  prefs: []
  type: TYPE_NORMAL
- en: '`Packages for clustering`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '`Packages for data processing and visualization`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 2.2\. Loading the data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The stock data is loaded below.^([1](ch08.xhtml#idm45174912844296))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 3\. Exploratory data analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We take a quick look at the data in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1\. Descriptive statistics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let us look at the shape of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '`Output`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The data contains 502 columns and 448 observations.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. Data visualization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We will take a detailed look into the visualization postclustering.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Data preparation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We prepare the data for modeling in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1\. Data cleaning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this step, we check for NAs in the rows and either drop them or fill them
    with the mean of the column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '`Output`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us get rid of the columns with more than 30% missing values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '`Output`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Given that there are null values, we drop some rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The data cleaning steps identified those with missing values and populated them.
    This step is important for creating a meaningful, reliable, and clean dataset
    that can be used without any errors in the clustering.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2\. Data transformation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For the purpose of clustering, we will be using *annual returns* and *variance*
    as the variables, as they are primary indicators of stock performance and volatility.
    The following code prepares these variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'All the variables should be on the same scale before applying clustering; otherwise,
    a feature with large values will dominate the result. We use `StandardScaler`
    in sklearn to standardize the dataset features onto unit scale (mean = 0 and variance
    = 1):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '`Output`'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Returns | Volatility |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| ABT | 0.794067 –0.702741 | ABBV |'
  prefs: []
  type: TYPE_TB
- en: With the data prepared, we can now explore the clustering algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Evaluate algorithms and models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will look at the following models:'
  prefs: []
  type: TYPE_NORMAL
- en: '*k*-means'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hierarchical clustering (agglomerative clustering)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Affinity propagation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 5.1\. k-means clustering
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Here, we model using *k*-means and evaluate two ways to find the optimal number
    of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.1\. Finding the optimal number of clusters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We know that *k*-means initially assigns data points to clusters randomly and
    then calculates centroids or mean values. Further, it calculates the distances
    within each cluster, squares these, and sums them to get the sum of squared errors.
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic idea is to define *k* clusters so that the total within-cluster variation
    (or error) is minimized. The following two methods are useful in finding the number
    of clusters in *k*-means:'
  prefs: []
  type: TYPE_NORMAL
- en: Elbow method
  prefs: []
  type: TYPE_NORMAL
- en: Based on the sum of squared errors (SSE) within clusters
  prefs: []
  type: TYPE_NORMAL
- en: Silhouette method
  prefs: []
  type: TYPE_NORMAL
- en: Based on the silhouette score
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s examine the elbow method. The SSE for each point is the square
    of the distance of the point from its representation (i.e., its predicted cluster
    center). The sum of squared errors is plotted for a range of values for the number
    of clusters. The first cluster will add much information (explain a lot of variance),
    but eventually the marginal gain will drop, giving an angle in the graph. The
    number of clusters is chosen at this point; hence it is referred to as the “elbow
    criterion.”
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us implement this in Python using the sklearn library and plot the SSE
    for a range of values for *k*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '`Output`'
  prefs: []
  type: TYPE_NORMAL
- en: '![mlbf 08in01](Images/mlbf_08in01.png)'
  prefs: []
  type: TYPE_IMG
- en: Inspecting the sum of squared errors chart, it appears the elbow kink occurs
    around five or six clusters for this data. Certainly we can see that as the number
    of clusters increases past six, the SSE within clusters begins to plateau.
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s look at the silhouette method. The silhouette score measures how similar
    a point is to its own cluster (*cohesion*) compared to other clusters (*separation*).
    The range of the silhouette value is between 1 and –1\. A high value is desirable
    and indicates that the point is placed in the correct cluster. If many points
    have a negative silhouette value, that may indicate that we have created too many
    or too few clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us implement this in Python using the sklearn library and plot the silhouette
    score for a range of values for *k*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '`Output`'
  prefs: []
  type: TYPE_NORMAL
- en: '![mlbf 08in02](Images/mlbf_08in02.png)'
  prefs: []
  type: TYPE_IMG
- en: Looking at the silhouette score chart, we can see that there are various parts
    of the graph at which a kink can be seen. Since there is not much of a difference
    in the SSE after six clusters, it implies that six clusters is a preferred choice
    in this *k*-means model.
  prefs: []
  type: TYPE_NORMAL
- en: Combining information from both methods, we infer the optimum number of clusters
    to be six.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.2\. Clustering and visualization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let us build the *k*-means model with six clusters and visualize the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Visualizing how clusters are formed is no easy task when the number of variables
    in the dataset is very large. A basic scatterplot is one method for visualizing
    a cluster in a two-dimensional space. We create one below to identify the relationships
    inherent in our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '`Output`'
  prefs: []
  type: TYPE_NORMAL
- en: '![mlbf 08in03](Images/mlbf_08in03.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding plot, we can somewhat see that there are distinct clusters
    separated by different colors (full-color version available on [GitHub](https://oreil.ly/8RvSp)).
    The grouping of data in the plot seems to be separated quite well. There is also
    a degree of separation in the centroids of the clusters, represented by square
    dots.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us look at the number of stocks in each of the clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '`Output`'
  prefs: []
  type: TYPE_NORMAL
- en: '![mlbf 08in04](Images/mlbf_08in04.png)'
  prefs: []
  type: TYPE_IMG
- en: The number of stocks per cluster ranges from around 40 to 120\. Although the
    distribution is not equal, we have a significant number of stocks in each cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at the hierarchical clustering.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2\. Hierarchical clustering (agglomerative clustering)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the first step, we look at the hierarchy graph and check for the number of
    clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.1\. Building hierarchy graph/dendrogram
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The hierarchy class has a dendrogram method that takes the value returned by
    the *linkage method* of the same class. The linkage method takes the dataset and
    the method to minimize distances as parameters. We use *ward* as the method since
    it minimizes the variance of distances between the clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '`Output`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The best way to visualize an agglomerative clustering algorithm is through
    a dendrogram, which displays a cluster tree, the leaves being the individual stocks
    and the root being the final single cluster. The distance between each cluster
    is shown on the y-axis. The longer the branches are, the less correlated the two
    clusters are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '`Output`'
  prefs: []
  type: TYPE_NORMAL
- en: '![mlbf 08in05](Images/mlbf_08in05.png)'
  prefs: []
  type: TYPE_IMG
- en: This chart can be used to visually inspect the number of clusters that would
    be created for a selected distance threshold (although the names of the stocks
    on the horizontal axis are not very clear, we can see that they are grouped into
    several clusters). The number of vertical lines a hypothetical straight, horizontal
    line will pass through is the number of clusters created for that distance threshold
    value. For example, at a value of 20, the horizontal line would pass through two
    vertical branches of the dendrogram, implying two clusters at that distance threshold.
    All data points (leaves) from that branch would be labeled as that cluster that
    the horizontal line passed through.
  prefs: []
  type: TYPE_NORMAL
- en: 'Choosing a threshold cut at 13 yields four clusters, as confirmed in the following
    Python code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '`Output`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 5.2.2\. Clustering and visualization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let us build the hierarchical clustering model with four clusters and visualize
    the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Similar to the plot of *k*-means clustering, we see that there are some distinct
    clusters separated by different colors (full-size version available on [GitHub](https://oreil.ly/8RvSp)).
  prefs: []
  type: TYPE_NORMAL
- en: '`Output`'
  prefs: []
  type: TYPE_NORMAL
- en: '![mlbf 08in06](Images/mlbf_08in06.png)'
  prefs: []
  type: TYPE_IMG
- en: Now let us look at affinity propagation clustering.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3\. Affinity propagation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let us build the affinity propagation model and visualize the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '`Output`'
  prefs: []
  type: TYPE_NORMAL
- en: '![mlbf 08in07](Images/mlbf_08in07.png)'
  prefs: []
  type: TYPE_IMG
- en: The affinity propagation model with the chosen hyperparameters produced many
    more clusters than *k*-means and hierarchical clustering. There is some clear
    grouping, but also more overlap due to the larger number of clusters (full-size
    version available on [GitHub](https://oreil.ly/8RvSp)). In the next step, we will
    evaluate the clustering techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4\. Cluster evaluation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If the ground truth labels are not known, evaluation must be performed using
    the model itself. The silhouette coefficient (`sklearn.metrics.silhouette_score`)
    is one example we can use. A higher silhouette coefficient score implies a model
    with better defined clusters. The silhouette coefficient is computed for each
    of the clustering methods defined above:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '`Output`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Given that affinity propagation performs the best, we proceed with affinity
    propagation and use 27 clusters as specified by this clustering method.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the return within a cluster
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We have the clustering technique and the number of clusters finalized, but
    we need to check whether the clustering leads to a sensible output. To do this,
    we visualize the historical behavior of the stocks in a few clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '`Output`'
  prefs: []
  type: TYPE_NORMAL
- en: '![mlbf 08in08](Images/mlbf_08in08.png)![mlbf 08in09](Images/mlbf_08in09.png)'
  prefs: []
  type: TYPE_IMG
- en: Looking at the charts above, across all the clusters with small number of stocks,
    we see similar movement of the stocks under different clusters, which corroborates
    the effectiveness of the clustering technique.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Pairs selection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once the clusters are created, several cointegration-based statistical techniques
    can be applied on the stocks within a cluster to create the pairs. Two or more
    time series are considered to be cointegrated if they are nonstationary and tend
    to move together.^([2](ch08.xhtml#idm45174910884168)) The presence of cointegration
    between time series can be validated through several statistical techniques, including
    the [Augmented Dickey-Fuller test](https://oreil.ly/5xKZy) and the [Johansen test](https://oreil.ly/9zbnC).
  prefs: []
  type: TYPE_NORMAL
- en: In this step, we scan through a list of securities within a cluster and test
    for cointegration between the pairs. First, we write a function that returns a
    cointegration test score matrix, a p-value matrix, and any pairs for which the
    p-value was less than 0.05.
  prefs: []
  type: TYPE_NORMAL
- en: Cointegration and pair selection function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we check the cointegration of different pairs within several clusters
    using the function created above and return the pairs found:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '`Output`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Let us visualize the results of the pair selection process now. Refer to the
    Jupyter notebook of this case study for the details of the steps related to the
    pair visualization using the t-SNE technique.
  prefs: []
  type: TYPE_NORMAL
- en: The following chart shows the strength of *k*-means for finding nontraditional
    pairs (pointed out with an arrow in the visualization). DXC is the ticker symbol
    for DXC Technology, and XEC is the ticker symbol for Cimarex Energy. These two
    stocks are from different sectors and appear to have nothing in common on the
    surface, but they are identified as pairs using *k*-means clustering and cointegration
    testing. This implies that a long-run stable relationship exists between their
    stock price movements.
  prefs: []
  type: TYPE_NORMAL
- en: '![mlbf 08in10](Images/mlbf_08in10.png)'
  prefs: []
  type: TYPE_IMG
- en: Once the pairs are created, they can be used in a pairs trading strategy. When
    the share prices of the pair deviate from the identified long-run relationship,
    an investor would seek to take a long position in the underperforming security
    and sell short the outperforming security. If the securities return to their historical
    relationship, a profit is made from the convergence of the prices.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this case study, we demonstrated the efficiency of clustering techniques
    by finding small pools of stocks in which to identify pairs to be used in a pairs
    trading strategy. A next step beyond this case study would be to explore and backtest
    various long/short trading strategies with pairs of stocks from the groupings
    of stocks.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering can be used for dividing stocks and other types of assets into groups
    with similar characteristics for several other kinds of trading strategies. It
    can also be effective in portfolio construction, helping to ensure we choose a
    pool of assets with sufficient diversification between them.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/bracket_bottom.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Case Study 2: Portfolio Management: Clustering Investors'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Asset management and investment allocation is a tedious and time-consuming process
    in which investment managers often must design customized approaches for each
    client or investor.
  prefs: []
  type: TYPE_NORMAL
- en: What if we were able to organize these clients into particular investor profiles,
    or clusters, wherein each group is indicative of investors with similar characteristics?
  prefs: []
  type: TYPE_NORMAL
- en: Clustering investors based on similar characteristics can lead to simplicity
    and standardization in the investment management process. These algorithms can
    group investors based on different factors, such as age, income, and risk tolerance.
    It can help investment managers identify distinct groups within their investors
    base. Additionally, by using these techniques, managers can avoid introducing
    any biases that otherwise could adversely impact decision making. The factors
    analyzed through clustering can have a big impact on asset allocation and rebalancing,
    making it an invaluable tool for faster and effective investment management.
  prefs: []
  type: TYPE_NORMAL
- en: In this case study, we will use clustering methods to identify different types
    of investors.
  prefs: []
  type: TYPE_NORMAL
- en: 'The data used for this case study is from the Survey of Consumer Finances,
    which is conducted by the Federal Reserve Board. The same dataset was used in
    [“Case Study 3: Investor Risk Tolerance and Robo-Advisors”](ch05.xhtml#CaseStudy3SR)
    in [Chapter 5](ch05.xhtml#Chapter5).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/bracket_top.png)'
  prefs: []
  type: TYPE_IMG
- en: Blueprint for Using Clustering for Grouping Investors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 1\. Problem definition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The goal of this case study is to build a clustering model to group individuals
    or investors based on parameters related to the ability and willingness to take
    risk. We will focus on using common demographic and financial characteristics
    to accomplish this.
  prefs: []
  type: TYPE_NORMAL
- en: The survey data we’re using includes responses from 10,000+ individuals in 2007
    (precrisis) and 2009 (postcrisis). There are over 500 features. Since the data
    has many variables, we will first reduce the number of variables and select the
    most intuitive features directly linked to an investor’s ability and willingness
    to take risk.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Getting started—loading the data and Python packages
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 2.1\. Loading the Python packages
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The packages loaded for this case study are similar to those loaded in the
    case study presented in [Chapter 5](ch05.xhtml#Chapter5). However, some additional
    packages related to the clustering techniques are shown in the following code
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 2.2\. Loading the data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The data (again, previously used in [Chapter 5](ch05.xhtml#Chapter5)) is further
    processed to give the following attributes that represent an individual’s ability
    and willingness to take risk. This preprocessed data is for the 2007 survey and
    is loaded below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 3\. Exploratory data analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Next, we take a closer look at the different columns and features found in the
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1\. Descriptive statistics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'First, looking at the shape of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '`Output`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The data has information for 3,886 individuals across 13 columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '![mlbf 08in11](Images/mlbf_08in11.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see in the table above, there are 12 attributes for each of the individuals.
    These attributes can be categorized as demographic, financial, and behavioral
    attributes. They are summarized in [Figure 8-2](#AttrCluster).
  prefs: []
  type: TYPE_NORMAL
- en: '![mlbf 0802](Images/mlbf_0802.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-2\. Attributes for clustering individuals
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Many of these were previously used and defined in the [Chapter 5](ch05.xhtml#Chapter5)
    case study. A few additional attributes (LIFECYCL, HHOUSES, and SPENDMOR) are
    used in this case study and are defined below:'
  prefs: []
  type: TYPE_NORMAL
- en: LIFECYCL
  prefs: []
  type: TYPE_NORMAL
- en: This is a lifecycle variable, used to approximate a person’s ability to take
    on risk. There are six categories in increasing level of ability to take risk.
    A value of 1 represents “age under 55, not married, and no kids,” and a value
    of 6 represents “age over 55 and not working.”
  prefs: []
  type: TYPE_NORMAL
- en: HHOUSES
  prefs: []
  type: TYPE_NORMAL
- en: This is a flag indicating whether the individual is a homeowner. A value of
    1 (0) implies the individual does (does not) own a home.
  prefs: []
  type: TYPE_NORMAL
- en: SPENDMOR
  prefs: []
  type: TYPE_NORMAL
- en: This represents higher spending preference if assets appreciated on a scale
    of 1 to 5.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. Data visualization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We will take a detailed look into the visualization postclustering.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Data preparation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here, we perform any necessary changes to the data in preparation for modeling.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1\. Data cleaning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this step, we check for NAs in the rows and either drop them or fill them
    with the mean of the column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '`Output`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Given that there is not any missing data, and the data is already in categorical
    format, no further data cleaning was performed. The *ID* column is unnecessary
    and is dropped:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 4.2\. Data transformation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As we saw in Section 3.1, all the columns represent categorical data with similar
    numeric scale, with no outliers. Hence, no data transformation will be required
    for clustering.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Evaluate algorithms and models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will analyze the performance of *k*-means and affinity propagation.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1\. k-means clustering
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We look at the details of the *k*-means clustering in this step. First, we find
    the optimal number of clusters, followed by the creation of a model.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.1\. Finding the optimal number of clusters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We look at the following two metrics to evaluate the number of clusters in
    the *k*-means model. The Python code to get these two metrics is the same as in
    case study 1:'
  prefs: []
  type: TYPE_NORMAL
- en: Sum of squared errors (SSE)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Silhouette score
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Sum of squared errors (SSE) within clusters`'
  prefs: []
  type: TYPE_NORMAL
- en: '![mlbf 08in12](Images/mlbf_08in12.png)'
  prefs: []
  type: TYPE_IMG
- en: '`Silhouette score`'
  prefs: []
  type: TYPE_NORMAL
- en: '![mlbf 08in13](Images/mlbf_08in13.png)'
  prefs: []
  type: TYPE_IMG
- en: Looking at both of the preceding charts, the optimum number of clusters seems
    to be around 7\. We can see that as the number of clusters increases past 6, the
    SSE within clusters begins to plateau. From the second graph, we can see that
    there are various parts of the graph where a kink can be seen. Since there is
    not much of a difference in the SSE after 7 clusters, we proceed with using 7
    clusters in the *k*-means model below.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.2\. Clustering and visualization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let us create a *k*-means model with 7 clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us assign a target cluster to each individual in the dataset. This assignment
    is used further for exploratory data analysis to understand the behavior of each
    cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 5.2\. Affinity propagation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Here, we build an affinity propagation model and look at the number of clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '`Output`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: The affinity propagation resulted in over 150 clusters. Such a large number
    will likely make it difficult to ascertain proper differentiation between them.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3\. Cluster evaluation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this step, we check the performance of the clusters using silhouette coefficient
    (*sklearn.metrics.silhouette_score*). Recall that a higher silhouette coefficient
    score relates to a model with better defined clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '`Output`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: The *k*-means model has a much higher silhouette coefficient compared to the
    affinity propagation. Additionally, the large number of clusters resulting from
    the affinity propagation is untenable. In the context of the problem at hand,
    having fewer clusters, or categorizations of investors, helps build simplicity
    and standardization in the investment management process. It gives the users of
    this information (e.g., financial advisors) some manageable intuition around the
    representation of the clusters. Comprehending and being able to speak to six to
    eight investor types is much more practical than maintaining a meaningful understanding
    of over 100 different profiles. With this in mind, we proceed with *k*-means as
    the preferred clustering technique.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Cluster intuition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the next step, we will analyze the clusters and attempt to draw conclusions
    from them. We do that by plotting the average of each variable of the cluster
    and summarizing the findings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '`Demographics Features: Plot for each of the clusters`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '`Output`'
  prefs: []
  type: TYPE_NORMAL
- en: '![mlbf 08in14](Images/mlbf_08in14.png)'
  prefs: []
  type: TYPE_IMG
- en: The plot here shows the average values of the attributes for each of the clusters
    (full size version available on [GitHub](https://oreil.ly/61d9_)). For example,
    in comparing clusters 0 and 1, cluster 0 has *lower* average age, yet *higher*
    average education. However, these two clusters are more similar in marital status
    and number of children. So, based on the demographic attributes, the individuals
    in cluster 0 will, on average, have higher risk tolerance compared to those in
    cluster 1.
  prefs: []
  type: TYPE_NORMAL
- en: '`Financial and Behavioral Attributes: Plot for each of the clusters`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '`Output`'
  prefs: []
  type: TYPE_NORMAL
- en: '![mlbf 08in15](Images/mlbf_08in15.png)'
  prefs: []
  type: TYPE_IMG
- en: The plot here shows the average values of the financial and behavior attributes
    for each of the clusters (full size version available on [GitHub](https://oreil.ly/61d9_)).
    Again, comparing clusters 0 and 1, the former has higher average house ownership,
    higher average net worth and income, and a lower willingness to take risk compared
    to the latter. In terms of saving versus income comparison and willingness to
    save, the two clusters are comparable. Therefore, we can posit that the individuals
    in cluster 0 will, on average, have a higher ability and yet a lower willingness
    to take risks compared to the individuals in cluster 1.
  prefs: []
  type: TYPE_NORMAL
- en: Combining the information from the demographics, financial, and behavioral attributes
    for these two clusters, the overall ability to take risks for an individual in
    cluster 0 is higher than someone in cluster 1\. Performing similar analyses across
    all other clusters, we summarize the results in the table below. The risk tolerance
    column represents the subjective assessment of the risk tolerance of each cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '| Cluster | Features | Risk capacity |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Cluster 0 | Low age, high net worth and income, less risky life category,
    willingness to spend more | High |'
  prefs: []
  type: TYPE_TB
- en: '| Cluster 1 | High age, low net worth and income, highly risky life category,
    willingness to take risk, low education | Low |'
  prefs: []
  type: TYPE_TB
- en: '| Cluster 2 | High age, high net worth and income, highly risky life category,
    willingness to take risk, owns home | Medium |'
  prefs: []
  type: TYPE_TB
- en: '| Cluster 3 | Low age, very low income and net worth, high willingness to take
    risk, many kids | Low |'
  prefs: []
  type: TYPE_TB
- en: '| Cluster 4 | Medium age, very high income and net worth, high willingness
    to take risk, many kids, owns home | High |'
  prefs: []
  type: TYPE_TB
- en: '| Cluster 5 | Low age, very low income and net worth, high willingness to take
    risk, no kids | Medium |'
  prefs: []
  type: TYPE_TB
- en: '| Cluster 6 | Low age, medium income and net worth, high willingness to take
    risk, many kids, owns home | Low |'
  prefs: []
  type: TYPE_TB
- en: Conclusion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the key takeaways from this case study is the approach to understanding
    the cluster intuition. We used visualization techniques to understand the expected
    behavior of a cluster member by qualitatively interpreting mean values of the
    variables in each cluster. We demonstrated the efficiency of clustering in discovering
    the natural groups of different investors based on their risk tolerance.
  prefs: []
  type: TYPE_NORMAL
- en: Given that clustering algorithms can successfully group investors based on different
    factors (such as age, income, and risk tolerance), they can be further used by
    portfolio managers to standardize portfolio allocation and rebalance strategies
    across the clusters, making the investment management process faster and more
    effective.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/bracket_bottom.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Case Study 3: Hierarchical Risk Parity'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Markowitz’s *mean-variance portfolio optimization* is the most commonly used
    technique for portfolio construction and asset allocation. In this technique,
    we need to estimate the covariance matrix and expected returns of assets to be
    used as inputs. As discussed in [“Case Study 1: Portfolio Management: Finding
    an Eigen Portfolio”](ch07.xhtml#CaseStudy1DR) in [Chapter 7](ch07.xhtml#Chapter7),
    the erratic nature of financial returns causes estimation errors in the expected
    returns and the covariance matrix, especially when the number of assets is large
    compared to the sample size. These errors greatly jeopardize the optimality of
    the resulting portfolios, which leads to erroneous and unstable results. Additionally,
    small changes in the assumed asset returns, volatilities, or covariances can lead
    to large effects on the output of the optimization procedure. In this sense, the
    Markowitz mean-variance optimization is an ill-posed (or ill-conditioned) inverse
    problem.'
  prefs: []
  type: TYPE_NORMAL
- en: In [“Building Diversified Portfolios That Outperform Out-of-Sample”](https://oreil.ly/2BmW5)
    by Marcos López de Prado (2016), the author proposes a portfolio allocation method
    based on clustering called *hierarchical risk parity*. The main idea of hierarchical
    risk parity is to run hierarchical clustering on the covariance matrix of stock
    returns and then find a diversified weighting by distributing capital equally
    to each cluster hierarchy (so that many correlated strategies will receive the
    same total allocation as a single uncorrelated one). This alleviates some of the
    issues (highlighted above) found in Markowitz’s mean-variance optimization and
    improves numerical stability.
  prefs: []
  type: TYPE_NORMAL
- en: In this case study, we will implement hierarchical risk parity based on clustering
    methods and compare it against Markowitz’s mean-variance optimization method.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset used for this case study is price data for stocks in the S&P 500
    from 2018 onwards. The dataset can be downloaded from Yahoo Finance. It is the
    same dataset as was used in case study 1.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/bracket_top.png)'
  prefs: []
  type: TYPE_IMG
- en: Blueprint for Using Clustering to Implement Hierarchical Risk Parity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 1\. Problem definition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our goal in this case study is to use a clustering-based algorithm on a dataset
    of stocks to allocate capital into different asset classes. In order to backtest
    and compare the portfolio allocation against the traditional Markowitz mean-variance
    optimization, we will perform visualization and use performance metrics, such
    as the Sharpe ratio.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Getting started—loading the data and Python packages
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 2.1\. Loading the Python packages
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The packages loaded for this case study are similar to those loaded in the
    previous case study. However, some additional packages related to the clustering
    techniques are shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Since this case study uses the same data as case study 1, some of the next steps
    (i.e., loading the data) have been skipped to avoid repetition. As a reminder,
    the data contains around 500 stocks and 448 observations.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Exploratory data analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will take a detailed look into the visualization postclustering later in
    this case study.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Data preparation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 4.1\. Data cleaning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Refer to case study 1 for data cleaning steps.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2\. Data transformation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We will be using annual returns for clustering. Additionally, we will train
    the data and then test the data. Here, we prepare the dataset for training and
    testing by separating 20% of the dataset for testing, and we generate the return
    series:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 5\. Evaluate algorithms and models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this step, we will look at hierarchical clustering and perform further analysis
    and visualization.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1\. Building a hierarchy graph/dendrogram
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The first step is to look for clusters of correlations using the agglomerative
    hierarchical clustering technique. The hierarchy class has a dendrogram method
    that takes the value returned by the linkage method of the same class. The linkage
    method takes the dataset and the method to minimize distances as parameters. There
    are different options for measurement of the distance. The option we will choose
    is ward, since it minimizes the variance of distances between the clusters. Other
    possible measures of distance include single and centroid.
  prefs: []
  type: TYPE_NORMAL
- en: 'Linkage does the actual clustering in one line of code and returns a list of
    the clusters joined in the format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'As a precursor, we define a function to convert correlation into distances:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Now we convert the correlation of the returns of the stocks into distances,
    followed by the computation of linkages in the step below. Computation of linkages
    is followed by the visualization of the clusters through a dendrogram. Again,
    the leaves are the individual stocks, and the root is the final single cluster.
    The distance between each cluster is shown on the y-axis; the longer the branches
    are, the less correlated two clusters are.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: In the following chart, the horizontal axis represents the clusters. Although
    the names of the stocks on the horizontal axis are not very clear (not surprising,
    given that there are 500 stocks), we can see that they are grouped into several
    clusters. The appropriate number of clusters appears to be 2, 3, or 6, depending
    on the desired distance threshold level. Next, we will leverage the linkages computed
    from this step to compute the asset allocation based on hierarchical risk parity.
  prefs: []
  type: TYPE_NORMAL
- en: '`Output`'
  prefs: []
  type: TYPE_NORMAL
- en: '![mlbf 08in16](Images/mlbf_08in16.png)'
  prefs: []
  type: TYPE_IMG
- en: 5.2\. Steps for hierarchical risk parity
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The hierarchical risk parity (HRP) algorithm works in three stages, as outlined
    in Prado’s paper:'
  prefs: []
  type: TYPE_NORMAL
- en: Tree clustering
  prefs: []
  type: TYPE_NORMAL
- en: Grouping similar investments into clusters based on their correlation matrix.
    Having a hierarchical structure helps us improve stability issues of quadratic
    optimizers when inverting the covariance matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Quasi-diagonalization
  prefs: []
  type: TYPE_NORMAL
- en: Reorganizing the covariance matrix so similar investments will be placed together.
    This matrix diagonalization allows us to distribute weights optimally following
    an inverse-variance allocation.
  prefs: []
  type: TYPE_NORMAL
- en: Recursive bisection
  prefs: []
  type: TYPE_NORMAL
- en: Distributing the allocation through recursive bisection based on cluster covariance.
  prefs: []
  type: TYPE_NORMAL
- en: Having performed the first stage in the previous section, where we identified
    clusters based on the distance metrics, we proceed to quasi-diagonalization.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.1\. Quasi-diagonalization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Quasi-diagonalization is a process known as *matrix seriation*, which reorganizes
    the rows and columns of a covariance matrix so that the largest values lie along
    the diagonal. As shown in the following code, the process reorganizes the covariance
    matrix so similar investments are placed together. This matrix diagonalization
    allows us to distribute weights optimally following an inverse-variance allocation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 5.2.2\. Recursive bisection
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the next step, we perform recursive bisection, which is a top-down approach
    to splitting portfolio weights between subsets based on the inverse proportion
    to their aggregated variances. The function `getClusterVar` computes the cluster
    variance, and in this process, it requires the inverse-variance portfolio from
    the function `getIVP`. The output of the function `getClusterVar` is used by the
    function `getRecBipart` to compute the final allocation through recursive bisection
    based on cluster covariance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'The following function `getHRP` combines the three stages—clustering, quasi-diagonalization,
    and recursive bisection—to produce the final weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 5.3\. Comparison against other asset allocation methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A main focus of this case study is to develop an alternative to Markowitz’s
    mean-variance portfolio optimization using clustering. In this step, we define
    a function to compute the allocation of a portfolio based on Markowitz’s mean-variance
    technique. This function (`getMVP`) takes the covariance matrix of the assets
    as an input, performs the mean-variance optimization, and produces the portfolio
    allocations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 5.4\. Getting the portfolio weights for all types of asset allocation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this step, we use the functions above to compute the asset allocation using
    the two asset allocation methods. We then visualize the asset allocation results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: The following pie charts show the asset allocation of MVP versus HRP. We clearly
    see more diversification in HRP. Now let us look at the backtesting results.
  prefs: []
  type: TYPE_NORMAL
- en: '`Output`'
  prefs: []
  type: TYPE_NORMAL
- en: '![mlbf 08in17](Images/mlbf_08in17.png)'
  prefs: []
  type: TYPE_IMG
- en: 6\. Backtesting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will now backtest the performance of portfolios produced by the algorithms,
    looking at both in-sample and out-of-sample results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '`Output`'
  prefs: []
  type: TYPE_NORMAL
- en: '![mlbf 08in18](Images/mlbf_08in18.png)![mlbf 08in19](Images/mlbf_08in19.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Looking at the charts, MVP underperforms for a significant amount of time in
    the in-sample test. In the out-of-sample test, MVP performed better than HRP for
    a brief period of time from August 2019 to mid-September 2019\. In the next step,
    we examine the Sharpe ratio for the two allocation methods:'
  prefs: []
  type: TYPE_NORMAL
- en: In-sample results
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '`Output`'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | stdev | sharp_ratio |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| MVP | 0.086 | 0.785 |'
  prefs: []
  type: TYPE_TB
- en: '| HRP | 0.127 | 0.524 |'
  prefs: []
  type: TYPE_TB
- en: Out-of-sample results
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '`Output`'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | stdev_oos | sharp_ratio_oos |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| MVP | 0.103 | 0.787 |'
  prefs: []
  type: TYPE_TB
- en: '| HRP | 0.126 | 0.836 |'
  prefs: []
  type: TYPE_TB
- en: Although the in-sample results of MVP look promising, the out-of-sample Sharpe
    ratio and overall return of the portfolio constructed using the hierarchical clustering
    approach are better. The diversification that HRP achieves across uncorrelated
    assets makes the methodology more robust against shocks.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this case study, we saw that portfolio allocation based on hierarchical clustering
    offers better separation of assets into clusters with similar characteristics
    without relying on classical correlation analysis used in Markowitz’s mean-variance
    portfolio optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Using Markowitz’s technique yields a less diverse portfolio, concentrated in
    a few stocks. The HRP approach, leveraging hierarchical clustering–based allocation,
    results in a more diverse and distributed portfolio. This approach presented the
    best out-of-sample performance and offers better tail risk management due to the
    diversification.
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, the corresponding hierarchical risk parity strategies address the shortcomings
    of minimum-variance-based portfolio allocation. It is visual and flexible, and
    it seems to offer a robust methodology for portfolio allocation and portfolio
    management.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/bracket_bottom.png)'
  prefs: []
  type: TYPE_IMG
- en: Chapter Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about different clustering techniques and used them
    to capture the natural structure of data to enhance decision making across several
    areas of finance. Through the case studies, we demonstrated that clustering techniques
    can be useful in enhancing trading strategies and portfolio management.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to offering an approach to different finance problems, the case
    studies focused on understanding the concepts of clustering models, developing
    intuition, and visualizing clusters. Overall, the concepts in Python, machine
    learning, and finance presented in this chapter through the case studies can used
    as a blueprint for any other clustering-based problem in finance.
  prefs: []
  type: TYPE_NORMAL
- en: Having covered supervised and unsupervised learning, we will explore another
    type of machine learning, reinforcement learning, in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Use hierarchical clustering to form clusters of investments in a different asset
    class, such as forex or commodities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply clustering analysis for pairs trading in the interest rate market on the
    universe of bonds.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ^([1](ch08.xhtml#idm45174912844296-marker)) Refer to the Jupyter notebook to
    understand fetching price data using `pandas_datareader`.
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch08.xhtml#idm45174910884168-marker)) Refer to [Chapter 5](ch05.xhtml#Chapter5)
    for more details.
  prefs: []
  type: TYPE_NORMAL
