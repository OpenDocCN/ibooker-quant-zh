- en: Chapter 3\. Artificial Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many different types of models used in machine learning. However,
    one class of machine learning models that stands out is artificial neural networks
    (ANNs). Given that artificial neural networks are used across all machine learning
    types, this chapter will cover the basics of ANNs.
  prefs: []
  type: TYPE_NORMAL
- en: ANNs are computing systems based on a collection of connected units or nodes
    called artificial neurons, which loosely model the neurons in a biological brain.
    Each connection, like the synapses in a biological brain, can transmit a signal
    from one artificial neuron to another. An artificial neuron that receives a signal
    can process it and then signal additional artificial neurons connected to it.
  prefs: []
  type: TYPE_NORMAL
- en: '*Deep learning* involves the study of complex ANN-related algorithms. The complexity
    is attributed to elaborate patterns of how information flows throughout the model.
    Deep learning has the ability to represent the world as a nested hierarchy of
    concepts, with each concept defined in relation to a simpler concept. Deep learning
    techniques are extensively used in reinforcement learning and natural language
    processing applications that we will look at in Chapters [9](ch09.xhtml#Chapter9)
    and [10](ch10.xhtml#Chapter10).'
  prefs: []
  type: TYPE_NORMAL
- en: 'ANNs: Architecture, Training, and Hyperparameters'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ANNs contain multiple neurons arranged in layers. An ANN goes through a training
    phase by comparing the modeled output to the desired output, where it learns to
    recognize patterns in data. Let us go through the components of ANNs.
  prefs: []
  type: TYPE_NORMAL
- en: Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An ANN architecture comprises neurons, layers, and weights.
  prefs: []
  type: TYPE_NORMAL
- en: Neurons
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The building blocks for ANNs are neurons (also known as artificial neurons,
    nodes, or perceptrons). Neurons have one or more inputs and one output. It is
    possible to build a network of neurons to compute complex logical propositions.
    Activation functions in these neurons create complicated, nonlinear functional
    mappings between the inputs and the output.^([2](ch03.xhtml#idm45174933836904))
  prefs: []
  type: TYPE_NORMAL
- en: As shown in [Figure 3-1](#SingleNeuron), a neuron takes an input (*x[1]*, *x[2]*…*x[n]*),
    applies the learning parameters to generate a weighted sum (*z*), and then passes
    that sum to an activation function (*f*) that computes the output *f(z)*.
  prefs: []
  type: TYPE_NORMAL
- en: '![mlbf 0301](Images/mlbf_0301.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-1\. An artificial neuron
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Layers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The output *f(z)* from a single neuron (as shown in [Figure 3-1](#SingleNeuron))
    will not be able to model complex tasks. So, in order to handle more complex structures,
    we have multiple layers of such neurons. As we keep stacking neurons horizontally
    and vertically, the class of functions we can get becomes increasing complex.
    [Figure 3-2](#Layers) shows an architecture of an ANN with an input layer, an
    output layer, and a hidden layer.
  prefs: []
  type: TYPE_NORMAL
- en: '![mlbf 0302](Images/mlbf_0302.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-2\. Neural network architecture
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Input layer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The input layer takes input from the dataset and is the exposed part of the
    network. A neural network is often drawn with an input layer of one neuron per
    input value (or column) in the dataset. The neurons in the input layer simply
    pass the input value though to the next layer.
  prefs: []
  type: TYPE_NORMAL
- en: Hidden layers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Layers after the input layer are called hidden layers because they are not directly
    exposed to the input. The simplest network structure is to have a single neuron
    in the hidden layer that directly outputs the value.
  prefs: []
  type: TYPE_NORMAL
- en: A multilayer ANN is capable of solving more complex machine learning–related
    tasks due to its hidden layer(s). Given increases in computing power and efficient
    libraries, neural networks with many layers can be constructed. ANNs with many
    hidden layers (more than three) are known as *deep neural networks*. Multiple
    hidden layers allow deep neural networks to learn features of the data in a so-called
    feature hierarchy, because simple features recombine from one layer to the next
    to form more complex features. ANNs with many layers pass input data (features)
    through more mathematical operations than do ANNs with few layers and are therefore
    more computationally intensive to train.
  prefs: []
  type: TYPE_NORMAL
- en: Output layer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The final layer is called the output layer; it is responsible for outputting
    a value or vector of values that correspond to the format required to solve the
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: Neuron weights
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A neuron weight represents the strength of the connection between units and
    measures the influence the input will have on the output. If the weight from neuron
    one to neuron two has greater magnitude, it means that neuron one has a greater
    influence over neuron two. Weights near zero mean changing this input will not
    change the output. Negative weights mean increasing this input will decrease the
    output.
  prefs: []
  type: TYPE_NORMAL
- en: Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Training a neural network basically means calibrating all of the weights in
    the ANN. This optimization is performed using an iterative approach involving
    forward propagation and backpropagation steps.
  prefs: []
  type: TYPE_NORMAL
- en: Forward propagation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Forward propagation is a process of feeding input values to the neural network
    and getting an output, which we call *predicted value*. When we feed the input
    values to the neural network’s first layer, it goes without any operations. The
    second layer takes values from the first layer and applies multiplication, addition,
    and activation operations before passing this value to the next layer. The same
    process repeats for any subsequent layers until an output value from the last
    layer is received.
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After forward propagation, we get a predicted value from the ANN. Suppose the
    desired output of a network is *Y* and the predicted value of the network from
    forward propagation is *Y′*. The difference between the predicted output and the
    desired output (*Y*–*Y′* ) is converted into the loss (or cost) function *J(w)*,
    where *w* represents the weights in ANN.^([3](ch03.xhtml#idm45174933705544)) The
    goal is to optimize the loss function (i.e., make the loss as small as possible)
    over the training set.
  prefs: []
  type: TYPE_NORMAL
- en: The optimization method used is *gradient descent*. The goal of the gradient
    descent method is to find the gradient of *J(w)* with respect to *w* at the current
    point and take a small step in the direction of the negative gradient until the
    minimum value is reached, as shown in [Figure 3-3](#GradDesc).
  prefs: []
  type: TYPE_NORMAL
- en: '![mlbf 0303](Images/mlbf_0303.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-3\. Gradient descent
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In an ANN, the function *J(w)* is essentially a composition of multiple layers,
    as explained in the preceding text. So, if layer one is represented as function
    *p()*, layer two as *q()*, and layer three as *r()*, then the overall function
    is *J(w)=r(q(p())).* *w* consists of all weights in all three layers. We want
    to find the gradient of *J(w)* with respect to each component of *w*.
  prefs: []
  type: TYPE_NORMAL
- en: Skipping the mathematical details, the above essentially implies that the gradient
    of a component *w* in the first layer would depend on the gradients in the second
    and third layers. Similarly, the gradients in the second layer will depend on
    the gradients in the third layer. Therefore, we start computing the derivatives
    in the reverse direction, starting with the last layer, and use backpropagation
    to compute gradients of the previous layer.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, in the process of backpropagation, the model error (difference between
    predicted and desired output) is propagated back through the network, one layer
    at a time, and the weights are updated according to the amount they contributed
    to the error.
  prefs: []
  type: TYPE_NORMAL
- en: Almost all ANNs use gradient descent and backpropagation. Backpropagation is
    one of the cleanest and most efficient ways to find the gradient.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Hyperparameters* are the variables that are set before the training process,
    and they cannot be learned during training. ANNs have a large number of hyperparameters,
    which makes them quite flexible. However, this flexibility makes the model tuning
    process difficult. Understanding the hyperparameters and the intuition behind
    them helps give an idea of what values are reasonable for each hyperparameter
    so we can restrict the search space. Let’s start with the number of hidden layers
    and nodes.'
  prefs: []
  type: TYPE_NORMAL
- en: Number of hidden layers and nodes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: More hidden layers or nodes per layer means more parameters in the ANN, allowing
    the model to fit more complex functions. To have a trained network that generalizes
    well, we need to pick an optimal number of hidden layers, as well as of the nodes
    in each hidden layer. Too few nodes and layers will lead to high errors for the
    system, as the predictive factors might be too complex for a small number of nodes
    to capture. Too many nodes and layers will overfit to the training data and not
    generalize well.
  prefs: []
  type: TYPE_NORMAL
- en: There is no hard-and-fast rule to decide the number of layers and nodes.
  prefs: []
  type: TYPE_NORMAL
- en: The number of hidden layers primarily depends on the complexity of the task.
    Very complex tasks, such as large image classification or speech recognition,
    typically require networks with dozens of layers and a huge amount of training
    data. For the majority of the problems, we can start with just one or two hidden
    layers and then gradually ramp up the number of hidden layers until we start overfitting
    the training set.
  prefs: []
  type: TYPE_NORMAL
- en: The number of hidden nodes should have a relationship to the number of input
    and output nodes, the amount of training data available, and the complexity of
    the function being modeled. As a rule of thumb, the number of hidden nodes in
    each layer should be somewhere between the size of the input layer and the size
    of the output layer, ideally the mean. The number of hidden nodes shouldn’t exceed
    twice the number of input nodes in order to avoid overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Learning rate
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When we train ANNs, we use many iterations of forward propagation and backpropagation
    to optimize the weights. At each iteration we calculate the derivative of the
    loss function with respect to each weight and subtract it from that weight. The
    learning rate determines how quickly or slowly we want to update our weight (parameter)
    values. This learning rate should be high enough so that it converges in a reasonable
    amount of time. Yet it should be low enough so that it finds the minimum value
    of the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: Activation functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Activation functions (as shown in [Figure 3-1](#SingleNeuron)) refer to the
    functions used over the weighted sum of inputs in ANNs to get the desired output.
    Activation functions allow the network to combine the inputs in more complex ways,
    and they provide a richer capability in the relationship they can model and the
    output they can produce. They decide which neurons will be activated—that is,
    what information is passed to further layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Without activation functions, ANNs lose a bulk of their representation learning
    power. There are several activation functions. The most widely used are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Linear (identity) function
  prefs: []
  type: TYPE_NORMAL
- en: Represented by the equation of a straight line (i.e., <math alttext="f left-parenthesis
    x right-parenthesis equals m x plus c"><mrow><mi>f</mi> <mo>(</mo> <mi>x</mi>
    <mo>)</mo> <mo>=</mo> <mi>m</mi> <mi>x</mi> <mo>+</mo> <mi>c</mi></mrow></math>
    ), where activation is proportional to the input. If we have many layers, and
    all the layers are linear in nature, then the final activation function of the
    last layer is the same as the linear function of the first layer. The range of
    a linear function is *–inf* to *+inf*.
  prefs: []
  type: TYPE_NORMAL
- en: Sigmoid function
  prefs: []
  type: TYPE_NORMAL
- en: Refers to a function that is projected as an S-shaped graph (as shown in [Figure 3-4](#ActFunc)).
    It is represented by the mathematical equation <math display="inline"><mrow><mi>f</mi>
    <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <mn>1</mn> <mo>/</mo>
    <mrow><mo>(</mo> <mn>1</mn> <mo>+</mo> <msup><mi>e</mi> <mrow><mo>–</mo><mi>x</mi></mrow></msup>
    <mo>)</mo></mrow></mrow></math> and ranges from 0 to 1\. A large positive input
    results in a large positive output; a large negative input results in a large
    negative output. It is also referred to as logistic activation function.
  prefs: []
  type: TYPE_NORMAL
- en: Tanh function
  prefs: []
  type: TYPE_NORMAL
- en: Similar to sigmoid activation function with a mathematical equation <math><mrow><mi>T</mi>
    <mi>a</mi> <mi>n</mi> <mi>h</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo> <mo>=</mo> <mn>2</mn>
    <mi>S</mi> <mi>i</mi> <mi>g</mi> <mi>m</mi> <mi>o</mi> <mi>i</mi> <mi>d</mi> <mo>(</mo>
    <mn>2</mn> <mi>x</mi> <mo>)</mo> <mo>–</mo> <mn>1</mn></mrow></math> , where *Sigmoid*
    represents the `sigmoid` function discussed above. The output of this function
    ranges from –1 to 1, with an equal mass on both sides of the zero-axis, as shown
    in [Figure 3-4](#ActFunc).
  prefs: []
  type: TYPE_NORMAL
- en: ReLU function
  prefs: []
  type: TYPE_NORMAL
- en: ReLU stands for the Rectified Linear Unit and is represented as <math alttext="f
    left-parenthesis x right-parenthesis equals m a x left-parenthesis x comma 0 right-parenthesis"><mrow><mi>f</mi>
    <mo>(</mo> <mi>x</mi> <mo>)</mo> <mo>=</mo> <mi>m</mi> <mi>a</mi> <mi>x</mi> <mo>(</mo>
    <mi>x</mi> <mo>,</mo> <mn>0</mn> <mo>)</mo></mrow></math> . So, if the input is
    a positive number, the function returns the number itself, and if the input is
    a negative number, then the function returns zero. It is the most commonly used
    function because of its simplicity.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 3-4](#ActFunc) shows a summary of the activation functions discussed
    in this section.'
  prefs: []
  type: TYPE_NORMAL
- en: '![mlbf 0304](Images/mlbf_0304.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-4\. Activation functions
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There is no hard-and-fast rule for activation function selection. The decision
    completely relies on the properties of the problem and the relationships being
    modeled. We can try different activation functions and select the one that helps
    provide faster convergence and a more efficient training process. The choice of
    activation function in the output layer is strongly constrained by the type of
    problem that is modeled.^([4](ch03.xhtml#idm45174933620152))
  prefs: []
  type: TYPE_NORMAL
- en: Cost functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Cost functions (also known as loss functions) are a measure of the ANN performance,
    measuring how well the ANN fits empirical data. The two most common cost functions
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: Mean squared error (MSE)
  prefs: []
  type: TYPE_NORMAL
- en: This is the cost function used primarily for regression problems, where output
    is a continuous value. MSE is measured as the average of the squared difference
    between predictions and actual observation. MSE is described further in [Chapter 4](ch04.xhtml#Chapter4).
  prefs: []
  type: TYPE_NORMAL
- en: Cross-entropy (or *log loss*)
  prefs: []
  type: TYPE_NORMAL
- en: This cost function is used primarily for classification problems, where output
    is a probability value between zero and one. Cross-entropy loss increases as the
    predicted probability diverges from the actual label. A perfect model would have
    a cross-entropy of zero.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Optimizers update the weight parameters to minimize the loss function.^([5](ch03.xhtml#idm45174933603352))
    Cost function acts as a guide to the terrain, telling the optimizer if it is moving
    in the right direction to reach the global minimum. Some of the common optimizers
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Momentum
  prefs: []
  type: TYPE_NORMAL
- en: The *momentum optimizer* looks at previous gradients in addition to the current
    step. It will take larger steps if the previous updates and the current update
    move the weights in the same direction (gaining momentum). It will take smaller
    steps if the direction of the gradient is opposite. A clever way to visualize
    this is to think of a ball rolling down a valley—it will gain momentum as it approaches
    the valley bottom.
  prefs: []
  type: TYPE_NORMAL
- en: AdaGrad (Adaptive Gradient Algorithm)
  prefs: []
  type: TYPE_NORMAL
- en: '*AdaGrad* adapts the learning rate to the parameters, performing smaller updates
    for parameters associated with frequently occurring features, and larger updates
    for parameters associated with infrequent features.'
  prefs: []
  type: TYPE_NORMAL
- en: RMSProp
  prefs: []
  type: TYPE_NORMAL
- en: '*RMSProp* stands for Root Mean Square Propagation. In RMSProp, the learning
    rate gets adjusted automatically, and it chooses a different learning rate for
    each parameter.'
  prefs: []
  type: TYPE_NORMAL
- en: Adam (Adaptive Moment Estimation)
  prefs: []
  type: TYPE_NORMAL
- en: '*Adam* combines the best properties of the AdaGrad and RMSProp algorithms to
    provide an optimization and is one of the most popular gradient descent optimization
    algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: Epoch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One round of updating the network for the entire training dataset is called
    an *epoch*. A network may be trained for tens, hundreds, or many thousands of
    epochs depending on the data size and computational constraints.
  prefs: []
  type: TYPE_NORMAL
- en: Batch size
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The batch size is the number of training examples in one forward/backward pass.
    A batch size of 32 means that 32 samples from the training dataset will be used
    to estimate the error gradient before the model weights are updated. The higher
    the batch size, the more memory space is needed.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an Artificial Neural Network Model in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 2](ch02.xhtml#Chapter2) we discussed the steps for end-to-end model
    development in Python. In this section, we dig deeper into the steps involved
    in building an ANN-based model in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Our first step will be to look at Keras, the Python package specifically built
    for ANN and deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Keras and Machine Learning Packages
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are several Python libraries that allow building ANN and deep learning
    models easily and quickly without getting into the details of underlying algorithms.
    Keras is one of the most user-friendly packages that enables an efficient numerical
    computation related to ANNs. Using Keras, complex deep learning models can be
    defined and implemented in a few lines of code. We will primarily be using Keras
    packages for implementing deep learning models in several of the book’s case studies.
  prefs: []
  type: TYPE_NORMAL
- en: '[Keras](https://keras.io) is simply a wrapper around more complex numerical
    computation engines such as [TensorFlow](https://www.tensorflow.org) and [Theano](https://oreil.ly/-XFJP).
    In order to install Keras, TensorFlow or Theano needs to be installed first.'
  prefs: []
  type: TYPE_NORMAL
- en: This section describes the steps to define and compile an ANN-based model in
    Keras, with a focus on the following steps.^([6](ch03.xhtml#idm45174933565816))
  prefs: []
  type: TYPE_NORMAL
- en: Importing the packages
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before you can start to build an ANN model, you need to import two modules
    from the Keras package: `Sequential` and `Dense`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Loading data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This example makes use of the `random` module of NumPy to quickly generate
    some data and labels to be used by ANN that we build in the next step. Specifically,
    an array with size *(1000,10)* is first constructed. Next, we create a labels
    array that consists of zeros and ones with a size *(1000,1)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Model construction—defining the neural network architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A quick way to get started is to use the Keras Sequential model, which is a
    linear stack of layers. We create a Sequential model and add layers one at a time
    until the network topology is finalized. The first thing to get right is to ensure
    the input layer has the right number of inputs. We can specify this when creating
    the first layer. We then select a dense or fully connected layer to indicate that
    we are dealing with an input layer by using the argument `input_dim`.
  prefs: []
  type: TYPE_NORMAL
- en: We add a layer to the model with the `add()` function, and the number of nodes
    in each layer is specified. Finally, another dense layer is added as an output
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The architecture for the model shown in [Figure 3-5](#ANNArchitecture) is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The model expects rows of data with 10 variables (`input_dim_=10` argument).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first hidden layer has 32 nodes and uses the `relu` activation function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second hidden layer has 32 nodes and uses the `relu` activation function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output layer has one node and uses the `sigmoid` activation function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![mlbf 0305](Images/mlbf_0305.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-5\. An ANN architecture
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The Python code for the network in [Figure 3-5](#ANNArchitecture) is shown
    below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Compiling the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the model constructed, it can be compiled with the help of the `compile()`
    function. Compiling the model leverages the efficient numerical libraries in the
    Theano or TensorFlow packages. When compiling, it is important to specify the
    additional properties required when training the network. Training a network means
    finding the best set of weights to make predictions for the problem at hand. So
    we must specify the loss function used to evaluate a set of weights, the optimizer
    used to search through different weights for the network, and any optional metrics
    we would like to collect and report during training.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we use `cross-entropy` loss function, which is defined
    in Keras as `binary_crossentropy`. We will also use the adam optimizer, which
    is the default option. Finally, because it is a classification problem, we will
    collect and report the classification accuracy as the metric.^([7](ch03.xhtml#idm45174933399704))
    The Python code follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Fitting the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With our model defined and compiled, it is time to execute it on data. We can
    train or fit our model on our loaded data by calling the `fit()` function on the
    model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The training process will run for a fixed number of iterations (epochs) through
    the dataset, specified using the `nb_epoch` argument. We can also set the number
    of instances that are evaluated before a weight update in the network is performed.
    This is set using the `batch_size` argument. For this problem we will run a small
    number of epochs (10) and use a relatively small batch size of 32\. Again, these
    can be chosen experimentally through trial and error. The Python code follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Evaluating the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have trained our neural network on the entire dataset and can evaluate the
    performance of the network on the same dataset. This will give us an idea of how
    well we have modeled the dataset (e.g., training accuracy) but will not provide
    insight on how well the algorithm will perform on new data. For this, we separate
    the data into training and test datasets. The model is evaluated on the training
    dataset using the `evaluation()` function. This will generate a prediction for
    each input and output pair and collect scores, including the average loss and
    any metrics configured, such as accuracy. The Python code follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Running an ANN Model Faster: GPU and Cloud Services'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For training ANNs (especially deep neural networks with many layers), a large
    amount of computation power is required. Available CPUs, or Central Processing
    Units, are responsible for processing and executing instructions on a local machine.
    Since CPUs are limited in the number of cores and take up the job sequentially,
    they cannot do rapid matrix computations for the large number of matrices required
    for training deep learning models. Hence, the training of deep learning models
    can be extremely slow on the CPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following alternatives are useful for running ANNs that generally require
    a significant amount of time to run on a CPU:'
  prefs: []
  type: TYPE_NORMAL
- en: Running notebooks locally on a GPU.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running notebooks on Kaggle Kernels or Google Colaboratory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Amazon Web Services.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPU
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A GPU is composed of hundreds of cores that can handle thousands of threads
    simultaneously. Running ANNs and deep learning models can be accelerated by the
    use of GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: GPUs are particularly adept at processing complex matrix operations. The GPU
    cores are highly specialized, and they massively accelerate processes such as
    deep learning training by offloading the processing from CPUs to the cores in
    the GPU subsystem.
  prefs: []
  type: TYPE_NORMAL
- en: All the Python packages related to machine learning, including Tensorflow, Theano,
    and Keras, can be configured for the use of GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud services such as Kaggle and Google Colab
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you have a GPU-enabled computer, you can run ANNs locally. If you do not,
    we recommend you use a service such as Kaggle Kernels, Google Colab, or AWS:'
  prefs: []
  type: TYPE_NORMAL
- en: Kaggle
  prefs: []
  type: TYPE_NORMAL
- en: A popular data science website owned by Google that hosts Jupyter service and
    is also referred to as [Kaggle Kernels](https://www.kaggle.com). Kaggle Kernels
    are free to use and come with the most frequently used packages preinstalled.
    You can connect a kernel to any dataset hosted on Kaggle, or alternatively, you
    can just upload a new dataset on the fly.
  prefs: []
  type: TYPE_NORMAL
- en: Google Colaboratory
  prefs: []
  type: TYPE_NORMAL
- en: A free Jupyter Notebook environment provided by Google where you can use free
    GPUs. The features of [Google Colaboratory](https://oreil.ly/keqHk) are the same
    as Kaggle.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Web Services (AWS)
  prefs: []
  type: TYPE_NORMAL
- en: '[AWS Deep Learning](https://oreil.ly/gU84O) provides an infrastructure to accelerate
    deep learning in the cloud, at any scale. You can quickly launch AWS server instances
    preinstalled with popular deep learning frameworks and interfaces to train sophisticated,
    custom AI models, experiment with new algorithms, or learn new skills and techniques.
    These web servers can run longer than Kaggle Kernels. So for big projects, it
    might be worth using an AWS instead of a kernel.'
  prefs: []
  type: TYPE_NORMAL
- en: Chapter Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ANNs comprise a family of algorithms used across all types of machine learning.
    These models are inspired by the biological neural networks containing neurons
    and layers of neurons that constitute animal brains. ANNs with many layers are
    referred to as deep neural networks. Several steps, including forward propagation
    and backpropagation, are required for training these ANNs. Python packages such
    as Keras make the training of these ANNs possible in a few lines of code. The
    training of these deep neural networks require more computational power, and CPUs
    alone might not be enough. Alternatives include using a GPU or cloud service such
    as Kaggle Kernels, Google Colaboratory, or Amazon Web Services for training deep
    neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Next Steps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As a next step, we will be going into the details of the machine learning concepts
    for supervised learning, followed by case studies using the concepts covered in
    this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch03.xhtml#idm45174933851448-marker)) Readers are encouraged to refer
    to the book *Deep Learning* by Aaron Courville, Ian Goodfellow, and Yoshua Bengio
    (MIT Press) for more details on ANN and deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch03.xhtml#idm45174933836904-marker)) Activation functions are described
    in detail later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch03.xhtml#idm45174933705544-marker)) There are many available loss functions
    discussed in the next section. The nature of our problem dictates our choice of
    loss function.
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch03.xhtml#idm45174933620152-marker)) Deriving a regression or classification
    output by changing the activation function of the output layer is described further
    in [Chapter 4](ch04.xhtml#Chapter4).
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch03.xhtml#idm45174933603352-marker)) Refer to [*https://oreil.ly/FSt-8*](https://oreil.ly/FSt-8)
    for more details on optimization.
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch03.xhtml#idm45174933565816-marker)) The steps and Python code related
    to implementing deep learning models using Keras, as demonstrated in this section,
    are used in several case studies in the subsequent chapters.
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch03.xhtml#idm45174933399704-marker)) A detailed discussion of the evaluation
    metrics for classification models is presented in [Chapter 4](ch04.xhtml#Chapter4).
  prefs: []
  type: TYPE_NORMAL
