- en: Chapter 6\. The Dangers of Conventional AI Systems
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章。传统人工智能系统的危险
- en: A man’s got to know his limitations.
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一个人必须知道自己的限制。
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: —Detective “Dirty” Harry in the movie Magnum Force, as he watches an overconfident
    criminal mastermind’s car explode
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ——《铁证悬案》中的侦探“脏哈利”，他目睹一个自信过度的犯罪头目的汽车爆炸
- en: A model’s got to know its limitations. This is worth emphasizing because of
    the importance of this characteristic for models in finance and investing. The
    corollary is that an AI’s got to know its limitations. The most serious limitation
    of all AI systems is that they lack common sense. This stems from their inability
    to understand causal relationships. AI systems only learn statistical relationships
    during training that are hard to generalize to new situations without comprehending
    causality.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 一个模型必须知道自己的限制。这点很重要，因为这是金融和投资模型特性的重要性所在。其推论是，一个人工智能必须知道自己的限制。所有人工智能系统最严重的限制是它们缺乏常识。这源于它们无法理解因果关系。人工智能系统在训练期间只学习了难以推广到新情况的统计关系，而没有理解因果关系。
- en: In [Chapter 1](ch01.html#the_need_for_probabilistic_machine_lear), we examined
    the three ways in which financial markets can humble you even when you apply our
    best models cautiously and thoughtfully. Markets will almost surely humiliate
    you when your models are based on flawed financial and statistical theories such
    as those discussed in the first half of the book. That’s actually not such a bad
    outcome, because a humiliating financial loss can often lead to personal insights
    and growth. A worse outcome is getting fired from your job or your career coming
    to an ignoble end. The worst outcome is personal financial ruin, where the wisdom
    gained from such an experience may not be timely enough to be useful.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第1章](ch01.html#the_need_for_probabilistic_machine_lear) 中，我们探讨了金融市场如何在你谨慎和深思熟虑地应用最佳模型时，仍然可能使你感到羞辱的三种方式。当你的模型基于有缺陷的金融和统计理论（例如本书上半部分讨论的内容）时，市场几乎肯定会让你感到羞辱。实际上，这并不是那么糟糕的结果，因为羞辱性的财务损失通常会带来个人的洞察和成长。更糟糕的结果是被解雇或者职业生涯以不光彩的方式结束。最糟糕的结果是个人财务破产，这种经历带来的智慧可能来不及发挥作用。
- en: When traditional ML models (such as deep learning networks and logistic regression)
    are trained, they generally use the maximum likelihood estimation (MLE) method
    to learn the model parameters from in-sample data. Consequently, these ML systems
    have three deep flaws that severely limit their use in finance and investing.
    First, the parameter estimates of their models are erroneous when used with small
    datasets, especially when they learn from noisy financial data. Second, these
    ML models are awful at extrapolating beyond the data ranges and classes on which
    they have been trained and tested. Third, the probability scores of MLE models
    have to be calibrated into valid probabilities by using a function such as a Sigmoid
    or Softmax function. However, these calibrations are not guaranteed to represent
    the underlying probabilities accurately leading to poor uncertainty quantifications.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 当传统的机器学习模型（如深度学习网络和逻辑回归）被训练时，它们通常使用最大似然估计（MLE）方法从样本数据中学习模型参数。因此，这些机器学习系统有三个严重缺陷，严重限制了它们在金融和投资领域的应用。首先，在使用小数据集时，尤其是在学习嘈杂的金融数据时，它们的模型参数估计是错误的。其次，这些机器学习模型在超出它们已经训练和测试过的数据范围和类别时，表现糟糕。第三，MLE
    模型的概率分数必须通过使用 Sigmoid 或 Softmax 等函数进行校准，以将其校准为有效的概率。然而，这些校准不能保证准确地表示潜在的概率，导致了糟糕的不确定性量化。
- en: What makes all these flaws egregious is that the conventional statistical models
    on which these ML systems are based make erroneous estimates and predictions with
    appallingly high confidence, making them very dangerous in an uncertain world.
    Just like in the movie *Magnum Force*, these overconfident AI models have the
    potential of blowing up investment accounts, companies, financial institutions,
    and economies if they are implemented without understanding their severe limitations.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些缺陷的严重性在于，这些机器学习系统所依赖的传统统计模型在错误估计和预测时表现出令人震惊的高置信度，使它们在不确定的世界中变得非常危险。就像电影《铁证悬案》中一样，这些自信过满的人工智能模型有潜力引爆投资账户、公司、金融机构和经济，如果在不了解它们严重限制的情况下实施。
- en: In [Chapter 4](ch04.html#the_dangers_of_conventional_statistical), we exposed
    the fallacious inferential reasoning of popular statistical methods such as NHST,
    p-values, and confidence intervals. In this chapter, we examine the severe limitations
    and flaws of the popular MLE method and why it fails in finance and investing.
    We do this by examining a case where we want to project whether a newly listed
    public company we have invested in will beat its quarterly earnings expectations,
    based on a short track record. By comparing the results of a traditional MLE model
    with that of a probabilistic model, we demonstrate why probabilistic models are
    better suited for finance and investing in general, especially when datasets are
    sparse.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4章](ch04.html#the_dangers_of_conventional_statistical)中，我们揭示了流行统计方法如NHST、p值和置信区间的错误推理推断。在本章中，我们检视了流行的MLE方法的严重限制和缺陷，以及其在金融和投资领域的失败原因。我们通过研究一个案例，试图预测我们投资的新上市公司是否能超过其季度盈利预期，基于一个短期记录。通过比较传统MLE模型与概率模型的结果，我们展示了为什么概率模型更适合于金融和投资，特别是在数据稀缺时。
- en: As discussed earlier, most real-world probabilistic inference problems cannot
    be solved analytically because of the intractable complexity of the summations/integrals
    in the marginal probability distribution. Instead of using flawed probability
    calibration methods used by MLE models, we settle for approximate numerical solutions
    to probabilistic inference problems. Even though the earnings expectation problem
    can be solved analytically using basic calculus, we apply grid approximation to
    solve it to show how this simple, powerful technique works and makes probabilistic
    inference much easier to understand.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前讨论的，由于边缘概率分布中求和/积分的复杂性无法解析解决大多数现实世界的概率推断问题。与MLE模型使用的有缺陷的概率校准方法不同，我们只能采用近似数值解来解决概率推断问题。即使通过基本微积分可以解析解决盈利预期问题，我们也应用网格近似来解决它，以展示这种简单而强大的技术是如何运作的，使概率推断更容易理解。
- en: Markov chain Monte Carlo (MCMC) simulation is a breakthrough numerical method
    that has transformed the usability of probabilistic inference by estimating analytically
    intractable, high dimensional posterior probability distributions. MCMC simulates
    complex probability distributions using dependent random sampling algorithms.
    We explore the fundamental concepts underlying this powerful, scalable simulation
    method. As a proof-of-concept of the MCMC method, we use the famous Metropolis
    sampling algorithm to simulate a Student’s t-distribution with fat tails.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫链蒙特卡洛（MCMC）模拟是一种突破性的数值方法，通过估计解析上难以处理的高维后验概率分布，改变了概率推断的可用性。MCMC使用依赖随机抽样算法模拟复杂的概率分布。我们探索了支持这种强大、可扩展模拟方法的基本概念。作为MCMC方法的概念验证，我们使用著名的Metropolis抽样算法模拟具有重尾的学生t分布。
- en: 'AI Systems: A Dangerous Lack of Common Sense'
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AI系统：一个危险的缺乏常识
- en: 'Humans are endowed with a very important quality that no AI has been able to
    learn so far: a commonsensical ability to generalize our learnings reasonably
    well to unseen, out-of-sample related classes or ranges, even if we have not been
    specifically trained on them. Unlike AI systems, almost all humans can easily
    deduce, infer, and adjust their knowledge to new circumstances based on common
    sense. For instance, a deep neural network trained to recognize live elephants
    in the wilderness was unable to recognize a taxidermy elephant on display in a
    museum.^([1](ch06.html#ch06fn1)) Even a toddler could do this task easily by just
    using their common sense. As others have pointed out, the AI system literally
    could not see the elephant in the room!'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 人类天生具有一种非常重要的质量，迄今为止没有任何AI能够学会：一种具有常识的能力，合理地将我们的学习推广到未见过的、样本外的相关类别或范围，即使我们没有专门接受过这些训练。与AI系统不同的是，几乎所有人类都能够轻松推断、推理，并根据常识调整自己的知识以应对新的情况。例如，一个训练用于在荒野中识别活象的深度神经网络无法识别展览馆中展示的一只标本象。[^1]
    一个幼儿只需运用他们的常识就能轻松完成这项任务。正如其他人指出的那样，AI系统实际上无法看到房间里的大象！
- en: The primary reason for such common failures is that AI models only compute correlations
    and don’t have the tools to comprehend causation. Furthermore, humans are able
    to abstract concepts from specific examples and think in terms of generalization
    of objects and causal relationships among them, while AI systems are just unable
    to do that. This is a major problem when dealing with noisy, big datasets as they
    present abundant opportunities for correlating variables that have no plausible
    physical or causal relationship. With large datasets, spurious correlations among
    variables are the rule, not the exception.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这种常见失败的主要原因是AI模型只计算相关性，而没有工具来理解因果关系。此外，人类能够从具体例子中抽象概念，并以对象的泛化和它们之间的因果关系来思考，而AI系统却无法做到这一点。当处理嘈杂的大数据集时，这是一个主要问题，因为它们提供了丰富的机会，可以关联那些没有合理物理或因果关系的变量。在大数据集中，变量之间的虚假关联是常规，而不是例外。
- en: For instance, [Figure 6-1](#spurious_correlations_are_the_rule_in_b) shows that
    between 1999 and 2009, there was a 99.8% correlation between US spending on science,
    space and technology, and suicides by hanging, strangulation, and suffocation.^([2](ch06.html#ch06fn2))
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，[图6-1](#spurious_correlations_are_the_rule_in_b)显示在1999年至2009年期间，美国在科学、太空和技术上的支出与自杀（包括绞刑、勒颈和窒息）之间存在99.8%的相关性。^([2](ch06.html#ch06fn2))
- en: '![Spurious correlations are the rule in big datasets.](assets/pmlf_0601.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![大数据集中常见的虚假关联。](assets/pmlf_0601.png)'
- en: Figure 6-1\. Spurious correlations are the rule in big datasets^([3](ch06.html#ch06fn3))
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-1\. 大数据集中常见的虚假关联^([3](ch06.html#ch06fn3))
- en: Clearly this relationship is nonsensical and underscores the adage that correlation
    does not imply causation. Humans would understand the absurdity of such spurious
    correlations quite easily, but not AI systems. This also makes AI systems easy
    to fool by humans who understand such weaknesses and can exploit them.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，这种关系是荒谬的，并且强调了相关性并不意味着因果关系的格言。人类可以很容易理解这种虚假关联的荒谬之处，但AI系统却无法做到。这也使得那些理解这些弱点并能利用它们的人可以轻易愚弄AI系统。
- en: While artificial neural networks were inspired by the structure and function
    of the human brain, our understanding of how human neurons learn and work is still
    incomplete. As a result, artificial neural networks are not exact replicas of
    biological neurons, and there are still many unsolved mysteries surrounding the
    workings of the human brain. The term “deep neural networks” is a misleading marketing
    term to describe artificial neural networks with more than two hidden layers between
    the input and output layers. There is nothing deep about a deep neural network
    that lacks the common sense of a toddler.^([4](ch06.html#ch06fn4))
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然人工神经网络灵感来自人脑的结构和功能，但我们对人类神经元如何学习和工作的理解仍然不完整。因此，人工神经网络并非生物神经元的精确复制品，人类大脑运作的许多未解之谜仍然存在。术语“深度神经网络”是一个误导性的营销术语，用于描述在输入层和输出层之间具有两个以上隐藏层的人工神经网络。对于一个缺乏幼儿通识的深度神经网络来说，并没有什么“深刻”的含义。^([4](ch06.html#ch06fn4))
- en: Why MLE Models Fail in Finance
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么MLE模型在金融领域失败
- en: The MLE statistical method is used by all conventional parametric ML systems,
    from simple linear models to complex deep learning neural networks. The MLE method
    is used to compute the optimal parameters that best fit the data of an assumed
    statistical distribution. The MLE algorithm is useful when the model is dealing
    with only aleatory uncertainty of large datasets that have time-invariant statistical
    distributions where optimization makes sense.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 所有传统的参数化ML系统都使用MLE统计方法，从简单的线性模型到复杂的深度学习神经网络。MLE方法用于计算最优参数，以最佳拟合假定统计分布的数据。当模型只处理具有时间不变统计分布的大数据集的随机不确定性时，MLE算法是有用的。
- en: 'Much valuable information and assessment of uncertainty are lost when a statistical
    distribution is summarized by a point estimate, even if it is an optimal estimate.
    By definition and design, a point estimate cannot capture the epistemic uncertainty
    of model parameters because they are not probability distributions. This has serious
    consequences in finance and investing, where we are dealing with complex, dynamic
    social systems that are steeped in all three dimensions of uncertainty: aleatory,
    epistemic, and ontological. In [Chapter 1](ch01.html#the_need_for_probabilistic_machine_lear),
    we discussed why it is dangerous and foolish to use point estimates in finance
    and investing given that we are continually dealing with erroneous measurements,
    incomplete information, and three-dimensional uncertainty. In other words, MLE-based
    traditional ML systems operate only along one dimension in the three-dimensional
    space of uncertainty as illustrated in [Figure 2-7](ch02.html#human_intelligence_supported_by_probabi).
    What is even more alarming is that many of these ML systems are generally black
    boxes operating confidently at high speeds with flawed probability calibrations.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当统计分布被点估计总结时，尽管这是最优估计，但会丢失许多有价值的信息和不确定性评估。根据定义和设计，点估计不能捕捉模型参数的认知不确定性，因为它们不是概率分布。在金融和投资领域，这会带来严重后果，因为我们正在处理的是深陷于三种不确定性维度的复杂、动态社会系统：偶然性、认知性和本体论性。在[第1章](ch01.html#the_need_for_probabilistic_machine_lear)中，我们讨论了在金融和投资中使用点估计是危险和愚蠢的原因，因为我们持续处理错误的测量、不完整的信息和三维不确定性。换句话说，基于MLE的传统机器学习系统仅在不确定性三维空间的一个维度上运行，如[图2-7](ch02.html#human_intelligence_supported_by_probabi)所示。更令人担忧的是，许多这些机器学习系统通常是黑匣子，以高速自信地运行，但概率校准有缺陷。
- en: Furthermore, MLE ignores prior domain knowledge in the form of base rates or
    prior probabilities, which can lead to base-rate fallacies, as discussed in [Chapter 4](ch04.html#the_dangers_of_conventional_statistical).
    This is especially true when MLE is applied to small datasets. Let’s actually
    see why this is indeed the case by applying the MLE method to a real-world problem
    of estimating the probability that a company will actually beat the market’s expectation
    of its earnings estimates based on a short track record. This example has been
    inspired by the coin tossing example illustrated in the book referred to in the
    references.^([5](ch06.html#ch06fn5))
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，MLE忽略了基于先验领域知识的基础率或先验概率，这可能导致基础率谬误，正如[第4章](ch04.html#the_dangers_of_conventional_statistical)所讨论的那样。当MLE应用于小数据集时，这一点尤为真实。我们实际上可以通过将MLE方法应用于一个估计公司实际是否能超过市场预期其收益估计的真实问题来看看为何如此。此示例受到参考文献中描述的硬币投掷示例的启发。^([5](ch06.html#ch06fn5))
- en: An MLE Model for Earnings Expectations
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用于收益预期的MLE模型
- en: Assume you have changed jobs and are now working at a mutual fund as an equity
    analyst. Last year, your fund was allocated equity shares in the initial public
    offering (IPO) of ZYX, a high-growth technology company. Even though ZYX has never
    turned a profit in its entire nascent life, its brand is already a household name
    due in large part to its aggressive marketing campaigns that were supported by
    massive amounts of venture capital. Clearly, private and public equity investors
    bought into its compelling growth story, as narrated by its charismatic CEO.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你已经换了工作，现在作为股票分析师在一家共同基金公司工作。去年，你的基金在高增长技术公司ZYX的首次公开募股(IPO)中分配了股权。尽管ZYX在其整个初创生涯中从未盈利过，但由于其侵略性营销活动得到大量风险投资的支持，其品牌已经成为家喻户晓的名字。显然，私募和公募股权投资者买入了其迷人的增长故事，由其富有魅力的首席执行官叙述。
- en: In all the last three quarters since its IPO, the negative earnings of ZYX beat
    market expectations of even bigger losses. In financial markets, less bad is good.
    The stock price of ZYX has continued its relentless climb upward and is currently
    trading at all-time highs, enriching everyone in the process. Your portfolio manager
    (PM) has asked you to estimate the probability that ZYX’s earnings will beat market
    expectations in the upcoming fourth quarter. Based on your probability estimate,
    your PM is going to increase or decrease the fund’s equity investment in ZYX before
    their earnings announcement, which is due shortly.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在其 IPO 以来的最后三个季度中，ZYX 的负盈利甚至超过了市场对更大亏损的预期。在金融市场中，较差即好。ZYX 的股价持续不断地上涨，并且目前正在创下历史新高，让每个人受益。您的投资组合经理（PM）已要求您估计
    ZYX 在即将到来的第四季度收益将超过市场预期的概率。根据您的概率估计，您的 PM 将在他们的收益公告前增加或减少基金对 ZYX 的权益投资。
- en: 'Having been schooled in conventional statistical methods, we decide to build
    a standard MLE model to compute the required probability. The earnings announcement
    event has only two outcomes that interest us: either the earnings beat market
    expectations, or they fall short of them. We don’t care about the outcome of earnings
    merely meeting market expectations. Like many other investors, your PM has decided
    that such an outcome is the equivalent of earnings falling short of market expectations.
    It is common knowledge that management of companies play a game with Wall Street
    analysts throughout the year, where they lower their earnings expectations so
    that it becomes easier to beat those expectations when the actual earnings are
    announced.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在接受传统统计方法教育后，我们决定建立一个标准的 MLE 模型来计算所需的概率。收益公告事件只有两个我们感兴趣的结果：要么收益超过市场预期，要么低于市场预期。我们不关心收益仅达到市场预期的结果。像许多其他投资者一样，您的投资组合经理（PM）已经决定这样的结果相当于收益低于市场预期。众所周知，公司管理层在一年中与华尔街分析师打一场游戏，他们降低他们的盈利预期，以便在实际盈利公告时更容易超过这些预期。
- en: 'Let’s design our quarterly earnings MLE model and specify the assumptions that
    underpin it:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们设计我们的季度收益最大似然估计（MLE）模型，并指定支撑它的假设：
- en: In a single event or trial, the model’s output variable y can assume only one
    of two possible outcomes, y = 1 or y = 0.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在单个事件或试验中，模型的输出变量 y 只能假定两种可能的结果之一，y = 1 或 y = 0。
- en: The two outcomes are mutually exclusive and collectively exhaustive.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这两个结果是互斥的且完全穷尽的。
- en: Assign y = 1 to the outcome that ZYX beats market expectations of its quarterly
    earnings.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 y = 1 分配给 ZYX 超过市场对其季度收益的预期的结果。
- en: Assign y = 0 to the outcome that ZYX does not beat or only meets market expectations
    of its quarterly earnings.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 y = 0 分配给 ZYX 没有超过或仅达到市场对其季度收益的预期的结果。
- en: We now have to select a statistical distribution for our likelihood function
    that best models the binary event of an earnings announcement. The Bernoulli distribution
    models a single event or trial that has binary outcomes. See [Figure 6-2](#shows_a_bernoulli_variable_with_outcome).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在必须为我们的似然函数选择一个最能模拟收益公告这一二元事件的统计分布。伯努利分布模拟了具有二元结果的单个事件或试验。参见 [图 6-2](#shows_a_bernoulli_variable_with_outcome)。
- en: '![Shows a Bernoulli variable with outcome x = 1 occurring with probability
    p and outcome x = 0 occurring with probability 1-p](assets/pmlf_0602.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![显示结果 x = 1 的伯努利变量的概率为 p，结果 x = 0 的概率为 1-p](assets/pmlf_0602.png)'
- en: Figure 6-2\. Bernoulli variable^([6](ch06.html#ch06fn6)) with outcome x = 1
    occurring with probability p and outcome x = 0 occurring with probability 1-p
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-2\. 伯努利变量^([6](ch06.html#ch06fn6))，其结果 x = 1 的发生概率为 p，结果 x = 0 的发生概率为 1-p
- en: Recall that in [Chapter 1](ch01.html#the_need_for_probabilistic_machine_lear),
    we used the binomial distribution to model the total number of interest rate increases
    by the Federal Reserve over several meetings or trials. The Bernoulli distribution
    is a special case of the binomial distribution since they both have the same probability
    distribution when used for a single trial.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，在 [第 1 章](ch01.html#the_need_for_probabilistic_machine_lear) 中，我们使用二项分布来模拟美联储在几次会议或试验中利率上涨的总次数。当用于单次试验时，伯努利分布是二项分布的特例，因为它们在用于单次试验时具有相同的概率分布。
- en: Assume that variable y follows a Bernoulli distribution with an unknown parameter
    p, which gives us the probability of an earnings beat (y = 1).
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设变量 y 服从一个未知参数 p 的伯努利分布，这给了我们收益超预期的概率（y = 1）。
- en: Since both probabilities must add up to 1, this implies that the probability
    of not beating earnings expectations (y = 0) is its complement, 1-p.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于两个概率必须加起来等于1，这意味着未能超过收益预期（y = 0）的概率是其补集，即1-p。
- en: Our objective is to find the MLE of the parameter p, the probability that ZYX
    beats the market expectations of its quarterly earnings based on ZYX’s short track
    record of setting market expectations and then beating them.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是找到参数p的MLE，即ZYX基于其短期设定市场预期并超过市场预期的季度收益的概率。
- en: A Bernoulli process of the variable y is a discrete time series of independent
    and identically distributed (i.i.d.) Bernoulli trials, denoted by y[i].
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 变量y的伯努利过程是独立同分布（i.i.d.）伯努利试验的离散时间序列，由y[i]表示。
- en: The i.i.d. assumption means that each earnings announcement is independent of
    all the previous ones and is drawn from the same Bernoulli distribution with constant
    parameter p.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: i.i.d.假设意味着每次收益公告都与之前的所有收益独立，并且都从具有常数参数p的相同伯努利分布中抽取。
- en: In its last three quarters, ZYX beat earnings expectations, so our training
    data for parameter p is D = (y[1] = 1, y[2] = 1, y[3] = 1).
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在其最后三个季度，ZYX超过了收益预期，因此我们对参数p的训练数据是D = (y[1] = 1, y[2] = 1, y[3] = 1)。
- en: 'Let’s call p′ the MLE for the parameter p of the Bernoulli variable y. It can
    be shown mathematically that p′ is the expected value or arithmetic mean of the
    sample of time series data D. It is the optimal parameter that when inserted in
    a Bernoulli likelihood function best fits the time series data D. This implies
    p′ trained on dataset D is:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们称p′为伯努利变量y参数p的MLE。可以数学地证明，p′是时间序列数据D样本的期望值或算术平均值。它是最优参数，当插入伯努利似然函数中时，最适合时间序列数据D。这意味着在数据集D上训练的p′是：
- en: p′(D) = (y[1]+y[2]+y[3]) / 3 = (1 + 1 + 1) / 3 = 3 / 3 = 1
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: p′(D) = (y[1]+y[2]+y[3]) / 3 = (1 + 1 + 1) / 3 = 3 / 3 = 1
- en: Therefore, the probability that ZYX will beat market expectations of its earnings
    in its fourth quarter is P(y[4] = 1 | p′) = p′ = 1 or 100%.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，ZYX在其第四季度超过市场对其收益预期的概率是P(y[4] = 1 | p′) = p′ = 1或100%。
- en: 'Since MLE models only allow aleatory uncertainty caused by random sampling
    of data, let’s compute the variance of y. The variance of a Bernoulli variable
    y with parameter p′ is given by:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 由于MLE模型仅允许由数据随机抽样引起的随机不确定性，让我们计算y的方差。具有参数p′的伯努利变量y的方差由以下给出：
- en: Aleatory uncertainty or variance (y | p′) = (p′) × (1 – p′) = 1 × (1 – 1) =
    1 × 0 = 0.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机不确定性或方差(y | p′) = (p′) × (1 – p′) = 1 × (1 – 1) = 1 × 0 = 0.
- en: Epistemic uncertainty = 0 since p′ is a point estimate that is an optimum.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于p′是一个最优点估计，认知不确定性= 0。
- en: Ontological uncertainty = 0 since p′ is considered a “true” constant and the
    Bernoulli distribution is assumed to be time invariant.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于p′被认为是一个“真实”的常数，而且假设伯努利分布是时间不变的，本体不确定性= 0。
- en: So our MLE model is assigning a 100% probability with a 0 sampling error that
    y[4] = 1\. In other words, our model is absolutely certain that ZYX is going to
    beat market expectations of its earnings estimate in the upcoming fourth quarter.
    Our model’s heroic prediction of ZYX’s earnings beating market expectations is
    based on only three data points of a fledgling, loss-making technology company.
    Moreover, our current MLE model will continue to predict an earnings beat for
    every quarterly earnings event for the rest of ZYX’s life. It’s not just death
    and taxes that are certain. We need to add our MLE model’s predictions to the
    list.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的MLE模型分配了一个100%的概率，没有抽样误差，y[4] = 1。换句话说，我们的模型绝对确定ZYX将在即将到来的第四季度打破市场对其收益预期的期望。我们模型对ZYX收益超过市场预期的英雄式预测基于一家新兴的亏损技术公司仅有三个数据点。此外，我们当前的MLE模型将继续预测ZYX在其余生命周期内每个季度的收益超出预期。不仅仅是死亡和税收是确定的。我们需要将我们的MLE模型的预测添加到列表中。
- en: Any financial analyst with even a modicum of common sense would not present
    this MLE model and its predictions to their portfolio manager. However, it is
    very common to have sparse datasets in finance and investing. For instance, we
    have financial data for only two occurrences of global pandemics. Early stage
    technology startup companies or strategy/special projects have little or no relevant
    data for making specific decisions. Since the Great Depression ended in 1933,
    the US economy has experienced only 13 recessions. Since 1942, the S&P 500 has
    had three consecutive years of negative total returns only once (2000–2003). These
    are some of the obvious examples. The list of sparse datasets in finance and investing
    is quite long indeed.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 任何具有常识的财务分析师都不会向其投资组合经理呈现这个 MLE 模型及其预测。然而，在金融和投资中拥有稀疏数据集是非常普遍的。例如，我们仅有两次全球大流行的财务数据。早期科技初创公司或战略/特别项目几乎没有相关数据可供作出具体决策。自大萧条于1933年结束以来，美国经济只经历了13次衰退。自1942年以来，标准普尔500指数仅有一次连续三年的负总回报（2000年至2003年）。这些是一些明显的例子。金融和投资中稀疏数据集的列表确实很长。
- en: Clearly, MLE models are dangerous when applied to sparse datasets common in
    finance and investing. They really don’t know their limitations and unabashedly
    flaunt their ignorance. Building complex financial ML systems based on MLE models
    will only lead to financial disasters sooner rather than later.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，在金融和投资中常见的稀疏数据集上应用 MLE 模型是危险的。它们真的不了解它们的局限性，并毫不掩饰地显示他们的无知。基于 MLE 模型构建复杂的金融机器学习系统只会更早或更晚导致财务灾难。
- en: A Probabilistic Model for Earnings Expectations
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于收益预期的概率模型
- en: Now let’s delete our useless MLE model and pause to reflect on the problem.
    With only three data points to work with, it would be foolhardy to be absolutely
    certain about any point estimate of the parameter p, the probability that ZYX’s
    fourth quarter earnings will beat market expectations. Why is that? There are
    so many possible things that could have gone wrong in the past quarter that only
    some company insiders might be aware of. Given the persistent asymmetry of information
    between the company management and its shareholders, this is always possible.
    This is a major source of our epistemic uncertainty about parameter p.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们删除无用的 MLE 模型，并停下来思考这个问题。仅有三个数据点可供参考，对于任何对参数 p 的点估计都绝对不应该抱有确定性，即 ZYX 第四季度收益超过市场预期的概率。为什么？在过去一个季度可能发生了很多可能的问题，只有一些公司内部人员可能意识到。考虑到公司管理层和股东之间信息的持久不对称性，这种情况总是可能发生。这是我们对参数
    p 的认识不确定性的一个主要来源。
- en: Most importantly, there are so many things—company specific, political, regulatory,
    legal, monetary, and economic—that can go wrong in the immediate future and change
    the market’s expectations before ZYX makes its earnings public. These are some
    of the sources of our ontological uncertainty. Of course, nobody knows what will
    happen in the future, but it is more likely that the future will reflect the recent
    past than not.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的是，眼前有太多因素——公司特定、政治、监管、法律、货币和经济——可能在 ZYX 公布其收益之前出现问题，并改变市场的预期。这些是我们本体不确定性的一些来源。当然，没有人知道未来会发生什么，但未来更有可能反映最近的过去。
- en: So based on our understanding of the three dimensions of uncertainty of the
    real world we live in and the information that we currently have, we can reasonably
    bet that it is very probable that ZYX will beat the market’s expectations of its
    fourth quarter earnings. However, it’s not a certainty. This implies that our
    model parameter p should be able to take any value between 0 and 1, with the ones
    closer to 1 being more probable. In other words, our estimate for p is better
    expressed by a probability distribution than as any particular point estimate.
    In particular, after seeing the dataset D, our estimate for p is best expressed
    as a positively skewed probability distribution.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，基于我们对我们生活的现实世界三个不确定性维度的理解以及我们目前掌握的信息，我们可以合理地打赌，ZYX 将超过市场对其第四季度收益的预期。然而，这并不是确定的。这意味着我们的模型参数
    p 应能够取 0 到 1 之间的任何值，越接近 1 越有可能。换句话说，我们对 p 的估计最好用概率分布而不是任何特定点估计来表达。特别是在看到数据集 D
    后，我们对 p 的估计最好表达为一个正偏态概率分布。
- en: Note that the MLE is the optimal value for p that best replicates the observed
    data. But there is no universal natural law that says that it is a certainty that
    the MLE is the value of p that produced the in-sample data. Other values of the
    parameter p could easily have generated the dataset D too. We are dealing with
    complex social systems with emotional beings that do suboptimal things all the
    time. Most importantly, we are not constrained by the problem to pick only one
    value for p.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，MLE 是最能复制观察数据的 p 值。但并没有普遍的自然法则表明 MLE 就是产生样本数据的 p 值的确证。参数 p 的其他值也很容易生成数据集
    D。我们处理情感体的复杂社会系统，这些体系的行为往往是次优的。更重要的是，我们不受约束地选择一个 p 值。
- en: 'Let’s actually quantify and visualize the statistical distribution for p more
    precisely by building a probabilistic model. Recall that a probabilistic model
    requires us to specify two probability distributions:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过构建一个概率模型来更加精确地量化和可视化参数 p 的统计分布。回想一下，一个概率模型要求我们指定两个概率分布：
- en: The first is a prior probability distribution P(p) that encapsulates our knowledge
    or hypothesis about model parameters before we observe any data. Let’s assume
    you have no prior knowledge about ZYX company or any idea of what the parameter
    p should be. This makes a uniform distribution, U(0, 1), that we learned in the
    Monty Hall problem a good choice for our prior distribution. This distribution
    assigns equal probability to all values of p between 0 and 1\. So P(p) ~U (0,
    1), where the tilde sign (~) is shorthand for “is statistically distributed as.”
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个是先验概率分布 P(p)，它概括了我们在观察任何数据之前对模型参数的知识或假设。假设您对ZYX公司没有任何先验知识或对参数 p 的任何概念。这使得一个均匀分布
    U(0, 1)，我们在蒙提霍尔问题中学到的，成为我们先验分布的一个良好选择。这个分布将所有介于0到1之间的 p 值赋予相等的概率。因此，P(p) ~U (0,
    1)，其中波浪号（~）是“统计分布为”的缩写。
- en: The second is a likelihood function P(D | p) that gives us the plausibility
    of observing our in-sample data D assuming any value for our parameter p between
    0 and 1\. We will continue to use the Bernoulli probability distribution and its
    related process in our probabilistic model. So the likelihood function of our
    probabilistic model is P(D | p) ~Bernoulli (p).
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个是似然函数 P(D | p)，它给出了观察我们样本数据 D 的可能性，假设参数 p 介于0到1之间的任何值。我们将继续使用伯努利概率分布及其相关过程作为我们概率模型中的似然函数。因此，我们概率模型的似然函数是
    P(D | p) ~Bernoulli (p)。
- en: 'Our objective is to estimate the posterior probability distribution of our
    model parameter p given the in-sample data D and our prior knowledge or hypothesis
    of p. This will give us the probability distribution for the outcome y = 1, the
    probability of an earnings beat. As always, we will use the inverse probability
    rule to compute the probability distribution of p given the data D. Our probabilistic
    model can be specified as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是估计我们的模型参数 p 在样本数据 D 和我们对 p 的先验知识或假设给定情况下的后验概率分布。这将为我们提供结果 y = 1 的概率分布，即盈利超预期的概率。我们将使用逆概率规则计算给定数据
    D 的 p 的概率分布。我们的概率模型可以规定如下：
- en: P( p | D) = P(D | p) ✕ P(p) / P(D) where
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P( p | D) = P(D | p) ✕ P(p) / P(D) 其中
- en: P(p) ~U (0, 1)
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P(p) ~U (0, 1)
- en: P(D | p) ~ Bernoulli (p)
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P(D | p) ~ Bernoulli (p)
- en: D = (y[1] = 1, y[2] = 1, y[3] = 1)
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: D = (y[1] = 1, y[2] = 1, y[3] = 1)
- en: This posterior distribution is simple enough to be solved analytically using
    basic calculus.^([7](ch06.html#ch06fn7)) However, this involves using integrals
    over probability density functions, which may not be accessible to many readers.
    Instead of doing that here, we will compute the posterior distribution using a
    simple numerical approach called grid approximation. This approach will convert
    our problem of integral calculus into a much simpler problem of descriptive statistics.
    This should help us to build our intuition for the underlying mechanism of our
    probabilistic model.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这个后验分布足够简单，可以用基本的微积分解析求解。^([7](ch06.html#ch06fn7)) 然而，这涉及到对概率密度函数的积分，这对许多读者来说可能不容易理解。因此，我们将使用一种称为网格逼近的简单数值方法来计算后验分布。这种方法将把我们的积分微积分问题转化为描述性统计问题，这有助于我们建立对概率模型基本机制的直觉理解。
- en: Since our prior distribution is discrete and uniformly distributed, we can split
    the interval between 0 and 1 into 9 equidistant points, 0.1 apart, as shown in
    [Figure 6-3](#there_are_n_number_of_grid_points_unifo).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的先验分布是离散且均匀分布的，我们可以将0到1之间的区间分成9个等距点，相隔0.1，如图[6-3](#there_are_n_number_of_grid_points_unifo)所示。
- en: '![There are n number of grid points uniformly distributed between a and b,
    and each has a probability of 1/n.](assets/pmlf_0603.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![在 a 和 b 之间均匀分布的 n 个网格点，每个点的概率均为 1/n。](assets/pmlf_0603.png)'
- en: Figure 6-3\. There are n number of grid points uniformly distributed between
    a and b, and each has a probability of 1/n^([8](ch06.html#ch06fn8))
  id: totrans-68
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-3。在 a 和 b 之间均匀分布的 n 个网格点，每个点的概率均为 1/n^([8](ch06.html#ch06fn8))
- en: So our grid points are {p[1] = 0.1, p[2] = 0.2, .., p[9] = 0.9}. Since the n
    grid points are uniformly distributed, they all have the same probability, namely
    P(p) = 1/n, where n is the number of grid points. In our approximation, we have
    n = 9 grid points.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的网格点是 {p[1] = 0.1, p[2] = 0.2, .., p[9] = 0.9}。由于 n 个网格点均匀分布，它们都具有相同的概率，即
    P(p) = 1/n，其中 n 是网格点的数量。在我们的近似中，我们有 n = 9 个网格点。
- en: The prior probability for every parameter p[1],...p[9] on our one-dimensional
    grid is P(p) = 1/9 = 0.111.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于我们一维网格上的每个参数 p[1],...p[9]，先验概率为 P(p) = 1/9 = 0.111。
- en: 'For every parameter p[i] we sample from the set of nine grid points to simulate
    an earnings event with a value of p[i], the Bernoulli likelihood function generates
    y = 1 with probability pi or y = 0 with probability 1-p[i]. The Bernoulli process
    for the last three quarters of ZYX’s earnings event is given by our training data
    D = (y[1] = 1, y[2] = 1, y[3] = 1). So the likelihood of the Bernoulli process
    is:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个参数 p[i]，我们从九个网格点集合中抽样，以模拟一个具有 p[i] 值的收益事件，伯努利似然函数生成 y = 1 的概率为 pi，y = 0
    的概率为 1-p[i]。ZYX 的收益事件最后三个季度的伯努利过程由我们的训练数据 D = (y[1] = 1, y[2] = 1, y[3] = 1) 给出。因此，伯努利过程的似然性为：
- en: P(D | p[i]) = p[i] × p[i] × p[i] = p[i]³
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P(D | p[i]) = p[i] × p[i] × p[i] = p[i]³
- en: 'For each parameter p[i], we use a grid point {p[1],...p[9]} to compute the
    unnormalized posterior distribution P*(p | D), using the inverse probability rule.
    To compute the normalized posterior P(p | D), we first add up the all the unnormalized
    posterior values and then divide each unnormalized posterior by the sum as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个参数 p[i]，我们使用一个网格点 {p[1],...p[9]} 计算未归一化后验分布 P*(p | D)，使用逆概率法则。为了计算归一化后验
    P(p | D)，我们首先将所有未归一化后验值相加，然后将每个未归一化后验值除以该总和，如下所示：
- en: <math alttext="upper P asterisk left-parenthesis p Subscript i Baseline vertical-bar
    upper D right-parenthesis proportional-to upper P left-parenthesis upper D vertical-bar
    p Subscript i Baseline right-parenthesis upper P left-parenthesis p Subscript
    i Baseline right-parenthesis equals p Subscript i Superscript 3 Baseline times
    0.111"><mrow><mi>P</mi> <mo>*</mo> <mrow><mo>(</mo> <msub><mi>p</mi> <mi>i</mi></msub>
    <mo>|</mo> <mi>D</mi> <mo>)</mo></mrow> <mo>∝</mo> <mi>P</mi> <mrow><mo>(</mo>
    <mi>D</mi> <mo>|</mo> <msub><mi>p</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mi>P</mi>
    <mrow><mo>(</mo> <msub><mi>p</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mo>=</mo>
    <msubsup><mi>p</mi> <mi>i</mi> <mn>3</mn></msubsup> <mo>×</mo> <mn>0</mn> <mo>.</mo>
    <mn>111</mn></mrow></math>
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <math alttext="upper P asterisk left-parenthesis p Subscript i Baseline vertical-bar
    upper D right-parenthesis proportional-to upper P left-parenthesis upper D vertical-bar
    p Subscript i Baseline right-parenthesis upper P left-parenthesis p Subscript
    i Baseline right-parenthesis equals p Subscript i Superscript 3 Baseline times
    0.111"><mrow><mi>P</mi> <mo>*</mo> <mrow><mo>(</mo> <msub><mi>p</mi> <mi>i</mi></msub>
    <mo>|</mo> <mi>D</mi> <mo>)</mo></mrow> <mo>∝</mo> <mi>P</mi> <mrow><mo>(</mo>
    <mi>D</mi> <mo>|</mo> <msub><mi>p</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mi>P</mi>
    <mrow><mo>(</mo> <msub><mi>p</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mo>=</mo>
    <msubsup><mi>p</mi> <mi>i</mi> <mn>3</mn></msubsup> <mo>×</mo> <mn>0</mn> <mo>.</mo>
    <mn>111</mn></mrow></math>
- en: P(p[i] | D) = <math alttext="upper P asterisk left-parenthesis p Subscript i
    Baseline vertical-bar upper D right-parenthesis slash sigma-summation Underscript
    i Endscripts upper P asterisk left-parenthesis p Subscript i Baseline vertical-bar
    upper D right-parenthesis"><mrow><mi>P</mi> <mo>*</mo> <mrow><mo>(</mo> <msub><mi>p</mi>
    <mi>i</mi></msub> <mo>|</mo> <mi>D</mi> <mo>)</mo></mrow> <mo>/</mo> <msub><mo>∑</mo>
    <mi>i</mi></msub> <mi>P</mi> <mo>*</mo> <mrow><mo>(</mo> <msub><mi>p</mi> <mi>i</mi></msub>
    <mo>|</mo> <mi>D</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P(p[i] | D) = <math alttext="upper P asterisk left-parenthesis p Subscript i
    Baseline vertical-bar upper D right-parenthesis slash sigma-summation Underscript
    i Endscripts upper P asterisk left-parenthesis p Subscript i Baseline vertical-bar
    upper D right-parenthesis"><mrow><mi>P</mi> <mo>*</mo> <mrow><mo>(</mo> <msub><mi>p</mi>
    <mi>i</mi></msub> <mo>|</mo> <mi>D</mi> <mo>)</mo></mrow> <mo>/</mo> <msub><mo>∑</mo>
    <mi>i</mi></msub> <mi>P</mi> <mo>*</mo> <mrow><mo>(</mo> <msub><mi>p</mi> <mi>i</mi></msub>
    <mo>|</mo> <mi>D</mi> <mo>)</mo></mrow></mrow></math>
- en: 'Let’s use Python code to develop a grid approximation of the solution:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 Python 代码开发解决方案的网格近似：
- en: '[PRE0]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![Image](assets/pmlf_06in01.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/pmlf_06in01.png)'
- en: '[PRE1]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![Image](assets/pmlf_06in02.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/pmlf_06in02.png)'
- en: This figure clearly shows that our probabilistic model has computed a probability
    distribution for the model parameter p before and after training the model on
    in-sample data D. This is a much more realistic solution, given that we always
    have incomplete information about any event.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 本图清楚地显示，我们的概率模型在训练模型之前和之后计算了模型参数 p 的概率分布。考虑到我们总是对任何事件都有不完整的信息，这是一个更加现实的解决方案。
- en: 'Our model has learned the parameter p from our prior knowledge and the data.
    This is only half the solution. We need to use our model to predict the probability
    that ZYX will beat the market’s expectations of its fourth quarter earnings estimates.
    In other words, we need to develop the predictive distributions of our model.
    Let’s continue coding that:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型从先验知识和数据中学习了参数 p。这只是解决方案的一半。我们需要使用我们的模型来预测 ZYX 将超越市场对其第四季度盈利预期的概率。换句话说，我们需要开发我们模型的预测分布。让我们继续编写代码：
- en: '[PRE2]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![Image](assets/pmlf_06in03.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/pmlf_06in03.png)'
- en: The expected value or posterior predictive mean is 76%, which is close to the
    theoretical value of 75%. Regardless, our probabilistic model is not 100% sure
    that ZYX will beat market expectations in the fourth quarter, even though it has
    successfully done so in the last three quarters. Our model predicts that it is
    about three times more likely to beat market expectations than not. This is a
    far more realistic probability distribution and something we can use to make our
    investment decisions.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 预期值或后验预测均值为76%，接近理论值75%。尽管如此，我们的概率模型并不百分之百确定ZYX在第四季度将超过市场预期，即使在过去三个季度它已成功这样做。我们的模型预测，超过市场预期的可能性是不超过市场预期的三倍。这是一个更为现实的概率分布，也是我们可以用来做投资决策的东西。
- en: Unfortunately, the numerical grid approximation technique we used to solve the
    earnings expectations problem does not scale if the model has more than a few
    parameters. So the most scalable and robust numerical methods that we are left
    with are random sampling methods for estimating approximate solutions for probabilistic
    inference problems.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，我们用来解决收益预期问题的数值网格逼近技术在模型具有多个参数时无法扩展。因此，我们剩下的最可扩展和最健壮的数值方法是用于估计概率推理问题近似解的随机抽样方法。
- en: Markov Chain Monte Carlo Simulations
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 马尔可夫链蒙特卡洛模拟
- en: 'Generally speaking, there are two types of random sampling methods: independent
    sampling, and dependent sampling. The standard Monte Carlo simulation (MCS) method
    that we learned in [Chapter 3](ch03.html#quantifying_output_uncertainty_with_mon)
    is an independent random sampling method. However, random sampling does not work
    well when samples are dependent or correlated with one another.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，有两种类型的随机抽样方法：独立抽样和依赖抽样。我们在[第三章](ch03.html#quantifying_output_uncertainty_with_mon)学到的标准蒙特卡洛模拟（MCS）方法是一种独立随机抽样方法。然而，当样本彼此依赖或相关时，随机抽样效果不佳。
- en: Furthermore, these independent sampling algorithms are inefficient when the
    target probability distribution they are trying to simulate has many parameters
    or dimensions. We generally encounter these two issues when simulating complex
    posterior probability distributions. So we need random sampling algorithms which
    work with samples that are dependent or correlated with one another.^([9](ch06.html#ch06fn9))
    Markov chains are a popular way of generating dependent random samples. The most
    important aspect of a Markov chain is that the next sample generated is only dependent
    on the previous sample and independent of everything else.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，当独立抽样算法试图模拟具有多个参数或维度的目标概率分布时，效率低下。当模拟复杂的后验概率分布时，我们通常会遇到这两个问题。因此，我们需要能够处理相互依赖或相关样本的随机抽样算法。^([9](ch06.html#ch06fn9))
    马尔可夫链是生成依赖随机样本的流行方法。马尔可夫链的最重要特点是下一个生成的样本仅依赖于前一个样本，与其他所有因素无关。
- en: Markov Chains
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 马尔可夫链
- en: 'A Markov chain is used to model a stochastic process consisting of a series
    of discrete and dependent states linked together in a chain-like structure. It
    is a sequential process that transitions probabilistically in discrete time from
    state to state in the chain. The most important aspect of a Markov state is that
    it is memoryless. For any state, its future state only depends on the transition
    probabilities of the current state and is independent of all past states and the
    path it took to get to its current state. It’s as if Markovian chains have encoded
    Master Oogway’s Zen saying from the movie *Kung Fu Panda*: “Yesterday is history,
    tomorrow is a mystery, but today is a gift. That is why it is called the present.”'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫链用于建模由一系列离散且相互依赖状态组成的随机过程，这些状态通过链式结构相互链接。它是一个顺序过程，在链中以离散时间概率转移从一个状态过渡到另一个状态。马尔可夫状态的最重要特点是它是无记忆的。对于任何状态，它的未来状态仅依赖于当前状态的转移概率，与所有过去状态和到达当前状态的路径无关。就像马尔可夫链编码了电影《功夫熊猫》中大师乌龟的禅宗名言：“昨日已成为历史，明日仍是个迷，今日则是馈赠，因此称为现在。”
- en: Equally important, this simplifying memoryless property makes the Markovian
    chain easy to understand and implement. A random walk process, whether arithmetic
    or geometric, is a specific type of Markov chain and is used extensively to model
    asset prices, returns, interest rates, and volatility. A graphic representation
    of a Markov chain depicting the three basic and discrete states of the financial
    markets and their hypothetical transition probabilities is shown in [Figure 6-4](#a_markov_chain_depicting_the_three_basi).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 同样重要的是，这种简化的无记忆特性使得马尔可夫链易于理解和实现。无论是算术还是几何的随机游走过程都是马尔可夫链的特定类型，并广泛用于模拟资产价格、回报率、利率和波动性。图 6-4中展示了一个马尔可夫链的图形表示，描绘了金融市场的三种基本离散状态及其假设的过渡概率。
- en: '![A Markov chain depicting the three basic states of the financial markets
    and their hypothetical transition probabilities](assets/pmlf_0604.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![一个马尔可夫链，描绘了金融市场的三个基本状态及其假设的过渡概率](assets/pmlf_0604.png)'
- en: Figure 6-4\. A Markov chain depicting the three basic states of the financial
    markets and their hypothetical transition probabilities^([10](ch06.html#ch06fn10))
  id: totrans-94
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-4\. 描绘了金融市场三种基本状态及其假设过渡概率的马尔可夫链^([10](ch06.html#ch06fn10))
- en: According to this state transition diagram, if the financial market is currently
    in a bear market state, there is an 80% probability it will remain in a bear market
    state. However, there is a 15% probability that the market will transition to
    a bull market state and a 5% probability it will transition to a stagnant market
    state.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这个状态转移图，如果金融市场当前处于熊市状态，则有80%的概率它将保持在熊市状态。然而，有15%的概率市场将转移到牛市状态，而5%的概率将转移到停滞市场状态。
- en: Say the market transitions from a bear market state to a stagnant market state
    and then to a bull market state over time. Once it is in the bull market state,
    it will have no dependence or memory of the stagnant market state or bear market
    state. Probabilities about its transition to its future state will be dependent
    only on its present bull market state. So, for example, there is a 90% probability
    that it will stay in a bull market state regardless of whether it came from a
    stagnant market state or a bear market state or some permutation of the two. In
    other words, the future state of any Markov chain is conditionally independent
    of all past states given the current state.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 假设市场随时间从熊市状态转移到停滞市场状态，然后再转移到牛市状态。一旦进入牛市状态，它将不会依赖或记忆停滞市场状态或熊市状态。关于其转移到未来状态的概率将仅依赖于其当前的牛市状态。因此，例如，无论它是从停滞市场状态还是熊市状态或两者的任何排列转移过来，有90%的概率它将保持在牛市状态。换句话说，任何马尔可夫链的未来状态在给定当前状态时是条件独立于所有过去状态的。
- en: Despite the random walks a stochastic process takes in the state space of a
    Markov chain, if it can go from one state to every other state in a finite number
    of moves, the Markov chain is said to be stationary ergodic. Based on this definition,
    the Markov chain of the hypothetical financial market process depicted in [Figure 6-3](#there_are_n_number_of_grid_points_unifo)
    is stationary ergodic because the market will eventually reach any state in the
    Markov chain given enough time. Such a hypothetical stationary ergodic financial
    market would imply that the ensemble average price returns of all investors is
    expected to equal the price returns of every single random trajectory taken by
    any single investor in the ensemble over a long enough time period.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管随机过程在马尔可夫链状态空间中进行随机游走，如果它可以在有限步数内从一个状态转移到任何其他状态，则该马尔可夫链被称为平稳遍历的。根据这个定义，在[图 6-3](#there_are_n_number_of_grid_points_unifo)中描绘的假想金融市场过程的马尔可夫链是平稳遍历的，因为市场将在足够的时间内最终达到马尔可夫链中的任何状态。这样一个假设的平稳遍历金融市场将意味着所有投资者的集成平均价格回报预期将等于集成中任何单个投资者采取的每条随机轨迹的价格回报，在足够长的时间段内。
- en: 'However, as was discussed earlier, real financial markets are neither stationary
    nor ergodic. For instance, as an investor, you could suffer heavy losses in an
    unrelenting bear market state, or make foolish investments in a bubblicious bull
    market state, or be forced to liquidate your investments to pay for expensive
    divorce lawyers in a stagnant market state, and never be in another market state
    again. You would then be banished to a special Markovian state called an absorbing
    state from which there is no escape. This special state absorbs the essence of
    the lyric from the Eagles’ song “Hotel California”: “You can check out any time
    you like, but you can never leave.” We will discuss the problem of ergodicity
    in finance and investing in [Chapter 8](ch08.html#making_probabilistic_decisions_with_gen).'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，正如前面讨论过的，真实的金融市场既不是稳态的，也不是遍历性的。例如，作为投资者，在持续的熊市状态下可能会遭受重大损失，或者在泡沫化的牛市状态下进行愚蠢的投资，或者被迫清算投资以支付昂贵的离婚律师费用，在停滞的市场状态下永远无法再进入另一个市场状态。然后，你将被放逐到一个特殊的马尔可夫状态，称为吸收状态，从中无法逃脱。这种特殊状态吸收了鹰乐队歌曲《加州旅馆》中的歌词精髓：“你可以随时退房，但你永远不能离开。”我们将在[第8章](ch08.html#making_probabilistic_decisions_with_gen)中讨论金融和投资中的遍历性问题。
- en: Metropolis Sampling
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Metropolis 抽样
- en: 'The Metropolis algorithm generates a Markov chain to simulate any discrete
    or continuous target probability distribution. The Metropolis algorithm iteratively
    generates dependent random samples based on three key elements:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Metropolis 算法生成一个马尔可夫链来模拟任何离散或连续的目标概率分布。Metropolis 算法基于三个关键元素迭代生成依赖随机样本：
- en: Proposal probability distribution
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 提议概率分布
- en: This is a probability distribution that helps explore the target probability
    distribution efficiently by proposing the next state in the Markov chain based
    on the current state. Different proposal distributions can be used depending on
    the problem.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个概率分布，通过提议马尔可夫链中的下一个状态，有效地探索目标概率分布。根据问题的不同，可以使用不同的提议分布。
- en: Proposal acceptance ratio
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 提议接受率
- en: This is a measure of the relative probability of the proposed move. In a probabilistic
    inference problem, the acceptance ratio is the ratio of the posterior probabilities
    of the target distribution evaluated at the proposed state to the current state
    in the Markov chain. Recall from the previous chapter that taking the ratio of
    the posterior probabilities at two different points gets rid of the analytically
    intractable marginal probability distribution.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这是提议移动的相对概率的度量。在概率推断问题中，接受率是目标分布的后验概率在马尔可夫链中提议状态和当前状态处评估的比率。回想一下上一章中，在两个不同点处取后验概率的比率消除了解析上难以处理的边际概率分布。
- en: Decision rules on the proposed state
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 提议状态的决策规则
- en: These are probabilistic decision rules that determine whether to accept or reject
    the proposed state in the chain. If the acceptance ratio is greater than or equal
    to 1, the proposed state is accepted and the Markov chain moves to the next state.
    If the acceptance ratio is less than 1, the algorithm generates a random number
    between 0 and 1\. If the random number is less than the acceptance ratio, the
    proposed state is accepted. Otherwise it is rejected.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是确定是否接受或拒绝链中提议状态的概率决策规则。如果接受率大于或等于1，则接受提议状态并使马尔可夫链移动到下一个状态。如果接受率小于1，则算法生成介于0和1之间的随机数。如果随机数小于接受率，则接受提议状态。否则拒绝。
- en: The Metropolis algorithm builds its Markov chain iteratively and stops when
    the required number of samples have been accepted. The accepted samples are then
    used to simulate the target probability distribution.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Metropolis 算法通过迭代构建其马尔可夫链，在接受所需数量的样本后停止。接受的样本然后用于模拟目标概率分布。
- en: As a proof-of-concept of MCMC simulation, we will use the Metropolis algorithm
    to simulate a Student’s t-distribution with six degrees of freedom. This distribution
    is widely used in finance and investing for modeling asset price return distributions
    with fat tails. The Student’s t-distribution is a family of probability distributions,
    with each specific distribution controlled by its degrees of freedom parameter.
    The lower that value, the fatter the tails of the distribution. We will apply
    this distribution and discuss it further in the next chapter.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 作为MCMC模拟的概念验证，我们将使用Metropolis算法模拟自由度为6的Student's t分布。这个分布在金融和投资领域广泛用于建模具有重尾分布的资产价格回报分布。Student's
    t分布是一个概率分布家族，每个特定分布由其自由度参数控制。该值越低，分布尾部越厚。我们将在下一章中应用该分布并进一步讨论。
- en: 'In the following Python code, we use the uniform distribution as the proposal
    distribution and the Student’s t-distribution with six degrees of freedom as our
    target distribution to simulate. It initializes the Markov chain arbitrarily at
    x = 0 and runs the Metropolis sampling algorithm 10,000 times. The resulting samples
    are stored in a list, which is plotted to visualize the sample path of the Markov
    chain. Finally, the code plots a histogram of the samples to show its convergence
    to the actual target distribution:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下Python代码中，我们使用均匀分布作为提议分布，并使用自由度为6的Student's t分布作为我们的目标分布进行模拟。它将马尔可夫链在x =
    0处任意初始化，并运行Metropolis抽样算法10000次。结果样本存储在一个列表中，用于可视化马尔可夫链的样本路径。最后，代码绘制直方图显示样本收敛到实际目标分布：
- en: '[PRE3]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![Image](assets/pmlf_06in04.png)![Image](assets/pmlf_06in05.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/pmlf_06in04.png)![图片](assets/pmlf_06in05.png)'
- en: In 1970, William Hastings generalized the Metropolis sampling algorithm so that
    asymmetric proposal distributions and more flexible acceptance criteria could
    be applied. The resulting Metropolis-Hastings MCMC algorithm can simulate any
    target probability distribution asymptotically, i.e., given enough samples, the
    simulation will converge to the target probability distribution. However, this
    algorithm can be inefficient and costly for high-dimensional, complex target distributions.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 1970年，William Hastings推广了Metropolis抽样算法，使得可以应用非对称提议分布和更灵活的接受标准。由此产生的Metropolis-Hastings
    MCMC算法可以渐近地模拟任何目标概率分布，即在足够的样本下，模拟将会收敛到目标概率分布。然而，对于高维复杂的目标分布，这种算法可能效率低下且成本高昂。
- en: The Metropolis-Hastings algorithm is dependent on the arbitrary initial starting
    value of the Markov chain. The initial samples gathered during this period, called
    the burn-in period, are generally discarded. The randomness of the walk-through
    state space can waste time due to the possibility of revisiting the same regions
    several times. Moreover, the algorithm can get stuck in narrow regions of multidimensional
    spaces.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: Metropolis-Hastings算法依赖于马尔可夫链的任意初始值。在此期间收集的初始样本称为burn-in期间，通常会被丢弃。由于可能多次访问相同区域，状态空间的随机遍历可能会浪费时间。此外，算法可能会陷入多维空间的狭窄区域。
- en: Modern dependent sampling algorithms have been developed to address the shortcomings
    of this general-purpose MCMC sampling algorithm. The Hamiltonian Monte Carlo (HMC)
    algorithm uses the geometry of any continuous target distribution to move efficiently
    in high-dimensional space. It is the default MCMC sampling algorithm in the PyMC
    library, and we don’t need any specialized knowledge to use it. In the next chapter,
    we will use these MCMC algorithms to simulate the posterior probability distributions
    of model parameters.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现代相关抽样算法已经发展出来，用以解决这种通用MCMC抽样算法的缺点。哈密尔顿蒙特卡罗（HMC）算法利用任意连续目标分布的几何结构在高维空间中高效移动。它是PyMC库中的默认MCMC抽样算法，我们使用它不需要任何专门的知识。在下一章中，我们将使用这些MCMC算法模拟模型参数的后验概率分布。
- en: Summary
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概要
- en: Traditional statistical MLE models on which most conventional ML systems are
    based are limited in their capabilities. They are designed to deal with only aleatory
    uncertainty and are unaware of their limitations. As we have demonstrated in this
    chapter, MLE-based models make silly predictions confidently. This makes them
    dangerous in our world of three-dimensional uncertainty. Poor predictive performance
    and disastrous risk management from such overconfident, simplistic, and hasty
    ML models are almost surely inevitable in the complex world of finance and investing.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数传统 ML 系统基于的传统统计 MLE 模型在能力上存在局限性。 它们仅设计用于处理偶然不确定性，并且对其局限性毫不知情。 正如我们在本章中展示的那样，基于
    MLE 的模型会自信地进行愚蠢的预测。 在我们这个三维不确定性世界中，这使得它们在金融和投资的复杂世界中具有危险性。 由于这些自信、简单和仓促的 ML 模型的预测性能差和灾难性的风险管理几乎是不可避免的。
- en: In designing probabilistic models, we acknowledge the fact that only death is
    certain—everything else, including taxes, has a probability distribution. Probabilistic
    models are designed to manage uncertainties generated from noisy sample data and
    inexact model parameters. These models enable us to go from a one-dimensional
    world of aleatory uncertainty to a two-dimensional world with aleatory and epistemic
    uncertainties. This makes them more appropriate for the world of finance and investing.
    However, this comes at the cost of higher computational complexities.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计概率模型时，我们承认只有死亡是肯定的事情 — — 其他事情，包括税收，都有可能分布。 概率模型旨在管理由嘈杂的样本数据和不准确的模型参数生成的不确定性。
    这些模型使我们能够从偶然不确定性的一维世界进入包含偶然和认识不确定性的二维世界。 这使得它们更适合金融和投资的世界。 然而，这是以更高的计算复杂性为代价的。
- en: To apply probabilistic machine learning to complex financial and investing problems,
    we have to use dependent random sampling because other numerical methods don’t
    work or don’t scale. MCMC simulation methods are transformative. They use dependent
    random sampling algorithms to simulate complex probability distributions that
    are difficult to sample from directly. We will apply MCMC methods in the next
    chapter, using a popular probabilistic ML Python library.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 要将概率机器学习应用于复杂的金融和投资问题，我们必须使用相关随机抽样，因为其他数值方法无法工作或无法扩展。 MCMC 模拟方法具有变革性。 它们使用相关随机抽样算法来模拟难以直接抽样的复杂概率分布。
    我们将在下一章中应用 MCMC 方法，使用流行的概率 ML Python 库。
- en: Ontological uncertainty emanates from complex social systems, which can be disruptive
    at times. Among other things, it involves rethinking and redesigning the probabilistic
    model from scratch and making it more appropriate for the new market environment.
    This is generally best managed by human beings with common sense, judgment, and
    experience. We are still very much relevant in the bold, new world of AI and have,
    indeed, the hardest job.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 本体论不确定性源自复杂的社会系统，有时可能会造成混乱。 除其他事项外，它还涉及从头重新思考和重新设计概率模型，并使其更适合新的市场环境。 这通常最好由具有常识、判断力和经验的人类来管理。
    在 AI 的大胆新世界中，我们仍然非常重要，实际上，这是最困难的工作。
- en: References
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: References
- en: Dürr, Oliver, and Beate Sick. *Probabilistic Deep Learning with Python, Keras,
    and TensorFlow Probability*. Manning Publications, 2020.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Dürr, Oliver, 和 Beate Sick. *Probabilistic Deep Learning with Python, Keras,
    and TensorFlow Probability*. Manning Publications，2020年。
- en: Guo, Chuan, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. “On Calibration
    of Modern Neural Networks.” Last revised August 3, 2017\. *https://arxiv.org/abs/1706.04599.*
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: Guo, Chuan, Geoff Pleiss, Yu Sun, 和 Kilian Q. Weinberger. “On Calibration of
    Modern Neural Networks.” 最后修订于 2017年8月3日。*https://arxiv.org/abs/1706.04599.*
- en: 'Lambert, Ben. *A Student’s Guide to Bayesian Statistics*. London, UK: Sage
    Publications, 2018.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: Lambert, Ben. *A Student’s Guide to Bayesian Statistics*. 伦敦，英国：Sage Publications，2018年。
- en: 'Mitchell, Melanie. *Artificial Intelligence: A Guide for Thinking Humans*.
    New York: Farrar, Straus and Giroux, 2020.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 'Mitchell, Melanie. *Artificial Intelligence: A Guide for Thinking Humans*.
    纽约：Farrar, Straus and Giroux，2020年。'
- en: 'Vigen, Tyler. *Spurious Correlations*. New York: Hachette Books, 2015.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Vigen, Tyler. *Spurious Correlations*. 纽约：Hachette Books，2015年。
- en: ^([1](ch06.html#ch06fn1-marker)) Oliver Dürr and Beate Sick, “Bayesian Learning,”
    in *Probabilistic Deep Learning with Python, Keras, and TensorFlow Probability*
    (Manning Publications, 2020), 197–228.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch06.html#ch06fn1-marker)) Oliver Dürr 和 Beate Sick，《贝叶斯统计学学生指南》（Manning
    Publications，2020年），197–228页。
- en: '^([2](ch06.html#ch06fn2-marker)) Tyler Vigen, *Spurious Correlations* (New
    York: Hachette Books, 2015).'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch06.html#ch06fn2-marker)) Tyler Vigen，《虚假相关性》（纽约：Hachette Books，2015年）。
- en: ^([3](ch06.html#ch06fn3-marker)) Adapted from an image on Wikimedia Commons.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch06.html#ch06fn3-marker)) 改编自维基共享资源上的一幅图像。
- en: '^([4](ch06.html#ch06fn4-marker)) Melanie Mitchell, “Knowledge, Abstraction,
    and Analogy in Artificial Intelligence,” in *Artificial Intelligence: A Guide
    for Thinking Humans* (New York: Farrar, Straus and Giroux, 2019), 247–65.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch06.html#ch06fn4-marker)) 梅兰妮·米切尔，“人工智能中的知识、抽象和类比”，收录于《*人工智能：智慧人类的指南*》（纽约：法拉尔、斯特劳斯和吉鲁出版社，2019年），247–65页。
- en: ^([5](ch06.html#ch06fn5-marker)) Dürr and Sick, “Bayesian Learning.”
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch06.html#ch06fn5-marker)) 杜尔和希克，“贝叶斯学习”。
- en: ^([6](ch06.html#ch06fn6-marker)) Adapted from an image on Wikimedia Commons.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch06.html#ch06fn6-marker)) 改编自维基共享资源上的一幅图像。
- en: ^([7](ch06.html#ch06fn7-marker)) Dürr and Sick, “Bayesian Learning.”
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch06.html#ch06fn7-marker)) 杜尔和希克，“贝叶斯学习”。
- en: ^([8](ch06.html#ch06fn8-marker)) Adapted from an image on Wikimedia Commons.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch06.html#ch06fn8-marker)) 改编自维基共享资源上的一幅图像。
- en: '^([9](ch06.html#ch06fn9-marker)) Ben Lambert, “Leaving Conjugates Behind: Markov
    Chain Monte Carlo,” in *A Student’s Guide to Bayesian Statistics* (London, UK:
    Sage Publications, 2018), 263–90.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch06.html#ch06fn9-marker)) 本·兰伯特，“告别共轭：马尔可夫链蒙特卡洛”，收录于《*贝叶斯统计学指南*》（伦敦，英国：Sage出版社，2018年），263–90页。
- en: ^([10](ch06.html#ch06fn10-marker)) Adapted from an image on Wikimedia Commons.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch06.html#ch06fn10-marker)) 改编自维基共享资源上的一幅图像。
