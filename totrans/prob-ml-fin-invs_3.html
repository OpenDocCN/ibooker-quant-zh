<html><head></head><body><section data-pdf-bookmark="Chapter 3. Quantifying Output Uncertainty with Monte Carlo Simulation" data-type="chapter" epub:type="chapter"><div class="chapter" id="quantifying_output_uncertainty_with_mon">&#13;
<h1><span class="label">Chapter 3. </span>Quantifying Output Uncertainty with Monte Carlo Simulation</h1>&#13;
<blockquote data-type="epigraph" epub:type="epigraph">&#13;
<p>I of dice possess the science and in numbers thus am skilled.</p>&#13;
<p>—King Rituparna of the Mahabharata (circa 900 BCE), on estimating the leaves on a tree from a randomly selected branch</p>&#13;
</blockquote>&#13;
<p>The importance of Monte Carlo simulation<a contenteditable="false" data-primary="Monte Carlo simulation (MCS)" data-secondary="importance of" data-type="indexterm" id="id722"/><a contenteditable="false" data-primary="MCS" data-see="Monte Carlo simulation" data-type="indexterm" id="id723"/> (MCS), also known as the Monte Carlo method, cannot be overstated. In finance and investing, MCS is used to value all types of assets, optimize diverse portfolios, estimate risks, and evaluate complex trading strategies. MCS is especially used to solve problems that don’t have an analytical solution.<sup><a data-type="noteref" href="ch03.html#ch03fn1" id="ch03fn1-marker">1</a></sup> Indeed, there are many types of financial derivatives—such as lookback options and Asian options—that cannot be valued using any other technique. While the mathematics underpinning MCS is not simple, applying the method is actually quite easy, especially once you understand the key statistical concepts on which it is based.</p>&#13;
<p>MCS also pervades machine learning algorithms in general and probabilistic machine learning in particular. As discussed in <a data-type="xref" href="ch01.html#the_need_for_probabilistic_machine_lear">Chapter 1</a> and demonstrated in the simulated solution to the Monte Hall problem in <a data-type="xref" href="ch02.html#analyzing_and_quantifying_uncertainty">Chapter 2</a>, MCS enables<a contenteditable="false" data-primary="uncertainty quantification and analysis" data-secondary="output uncertainty via forward propagation" data-tertiary="about" data-type="indexterm" id="id724"/><a contenteditable="false" data-primary="Monte Carlo simulation (MCS)" data-secondary="output uncertainty quantified via forward propagation" data-type="indexterm" id="id725"/><a contenteditable="false" data-primary="forward propagation quantifying output uncertainty" data-secondary="Monte Carlo simulation" data-type="indexterm" id="id726"/><a contenteditable="false" data-primary="output uncertainty quantified via forward propagation" data-secondary="PML models" data-tertiary="Monte Carlo simulation" data-type="indexterm" id="id727"/><a contenteditable="false" data-primary="uncertainty quantification and analysis" data-secondary="output uncertainty via forward propagation" data-tertiary="Monte Carlo simulation" data-type="indexterm" id="id728"/> you to quantify the uncertainty of a model’s outputs in a process called forward propagation. It takes the traditional scenario and sensitivity analysis used by financial analysts to a completely different level.</p>&#13;
<p>You might be wondering how<a contenteditable="false" data-primary="random sampling" data-secondary="leading to stable solution" data-seealso="Monte Carlo simulation" data-type="indexterm" id="id729"/> a method that uses random sampling can lead to a stable solution. Isn’t that a contradiction in terms? In a sense it is. However, when you understand a couple of statistical theorems, you will see that repetition of trials under certain circumstances tames randomness and makes it converge toward a stable solution. This is what we observed in the simulated solution to the Monty Hall problem, where the solution converged on the theoretical values after about 1000 trials. In this chapter, we use MCS to provide a refresher on key statistical concepts and show you how to apply this powerful tool to solve real-world problems in finance and investing. In particular, we apply MCS to a capital budgeting project, in this case a software development project, and estimate the uncertainty in its value and duration.</p>&#13;
<section data-pdf-bookmark="Monte Carlo Simulation: Proof of Concept" data-type="sect1"><div class="sect1" id="monte_carlo_simulation_proof_of_concep">&#13;
<h1>Monte Carlo Simulation: Proof of Concept</h1>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="monte_carlo_simulation_a_weapon_of_mas">&#13;
<h1>Monte Carlo Simulation: A Weapon of Mass Construction</h1>&#13;
<p>MCS was developed during the<a contenteditable="false" data-primary="Monte Carlo simulation (MCS)" data-secondary="history of" data-type="indexterm" id="id730"/><a contenteditable="false" data-primary="Ulam, Stanisław" data-type="indexterm" id="id731"/><a contenteditable="false" data-primary="Metropolis, Nicholas" data-type="indexterm" id="id732"/><a contenteditable="false" data-primary="von Neumann, John" data-type="indexterm" id="id733"/> Second World War by some of the best mathematicians and physicists working on the nuclear weapons program in the US. Stanisław Ulam, collaborating with Nicholas Metropolis and John von Neumann, invented the modern version of MCS and implemented it using the ENIAC, the first programmable, electronic, general-purpose digital computer. Given the secretive nature of the weapons program, Metropolis code named the method Monte Carlo after the famous casino in Monaco where Ulam’s uncle would gamble away borrowed money.<a contenteditable="false" data-primary="Buffon, Georges Louis Leclerc, comte de" data-type="indexterm" id="id734"/><a contenteditable="false" data-primary="Fermi, Enrico" data-type="indexterm" id="id735"/> Comte de Buffon had used a similar method in the 18th century, as did the physicist Enrico Fermi in the 1930s.</p>&#13;
</div></aside>&#13;
<p>Before we begin going down this path, how do we know that MCS actually works as described? <a contenteditable="false" data-primary="Monte Carlo simulation (MCS)" data-secondary="pi estimation" data-type="indexterm" id="id736"/><a contenteditable="false" data-primary="pi estimation via Monte Carlo simulation" data-type="indexterm" id="id737"/>Let’s do a simple proof-of-concept of MCS by computing the value of pi, a known constant. <a data-type="xref" href="#the_blue_circle_of_unit_length_in_a_red">Figure 3-1</a> shows how we set up the simulation to estimate pi.</p>&#13;
<figure><div class="figure" id="the_blue_circle_of_unit_length_in_a_red">&#13;
<img alt="The blue circle of unit length in a red square with sides of two unit lengths is simulated to estimate the value of pi using MCS." src="assets/pmlf_0301.png"/>&#13;
<h6><span class="label">Figure 3-1. </span>The blue circle of unit length in a red square with sides of two unit lengths is simulated to estimate the value of pi using MCS</h6>&#13;
</div></figure>&#13;
<p class="pagebreak-before">As the Python code shows, you simulate the random spraying of N points to fill up the entire square. Next we count M points in the circle of unit length R. The area of the circle is pi × R<sup>2</sup> = M. The length of the square is 2R, so its area is 2R × 2R = 4 × R<sup>2</sup> = N. This implies that the ratio of the area of the circle to the area of the square is pi/4 = M/N. So pi = 4 × M/N:</p>&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Import modules</code>&#13;
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>&#13;
<code class="kn">from</code> <code class="nn">numpy</code> <code class="kn">import</code> <code class="n">random</code> <code class="k">as</code> <code class="n">npr</code>&#13;
<code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="k">as</code> <code class="nn">plt</code>&#13;
&#13;
<code class="c1"># Number of iterations in the simulation</code>&#13;
<code class="n">n</code> <code class="o">=</code> <code class="mi">100000</code>&#13;
&#13;
<code class="c1"># Draw random points from a uniform distribution in the X-Y plane to fill </code>&#13;
<code class="c1">#the area of a square that has a side of 2 units</code>&#13;
<code class="n">x</code> <code class="o">=</code> <code class="n">npr</code><code class="o">.</code><code class="n">uniform</code><code class="p">(</code><code class="n">low</code><code class="o">=-</code><code class="mi">1</code><code class="p">,</code> <code class="n">high</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">size</code><code class="o">=</code><code class="n">n</code><code class="p">)</code>&#13;
<code class="n">y</code> <code class="o">=</code> <code class="n">npr</code><code class="o">.</code><code class="n">uniform</code><code class="p">(</code><code class="n">low</code><code class="o">=-</code><code class="mi">1</code><code class="p">,</code> <code class="n">high</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">size</code><code class="o">=</code><code class="n">n</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># Points with a distance less than or equal to one unit from the origin will </code>&#13;
<code class="c1"># be inside the area of the unit circle. </code>&#13;
<code class="c1"># Using Pythagoras's theorem c^2 = a^2 + b^2</code>&#13;
<code class="n">inside</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">sqrt</code><code class="p">(</code><code class="n">x</code><code class="o">**</code><code class="mi">2</code> <code class="o">+</code> <code class="n">y</code><code class="o">**</code><code class="mi">2</code><code class="p">)</code> <code class="o">&lt;=</code><code class="mi">1</code>&#13;
&#13;
<code class="c1"># We generate N random points within our square and count the number of points </code>&#13;
<code class="c1"># that fall within the circle. Summing the points inside the circle is equivalent </code>&#13;
<code class="c1"># to integrating over the area of the circle. </code>&#13;
&#13;
<code class="c1"># Note that the ratio of the area of the circle to the area of the square is </code>&#13;
<code class="c1"># pi*r^2/(2*r)^2 = pi/4. So if we can calculate the areas of the circle </code>&#13;
<code class="c1"># and the square, we can solve for pi</code>&#13;
<code class="n">pi</code> <code class="o">=</code> <code class="mf">4.0</code><code class="o">*</code><code class="nb">sum</code><code class="p">(</code><code class="n">inside</code><code class="p">)</code><code class="o">/</code><code class="n">n</code> &#13;
&#13;
<code class="c1"># Estimate percentage error using the theoretical value of Pi</code>&#13;
<code class="n">error</code> <code class="o">=</code> <code class="nb">abs</code><code class="p">((</code><code class="n">pi</code><code class="o">-</code><code class="n">np</code><code class="o">.</code><code class="n">pi</code><code class="p">)</code><code class="o">/</code><code class="n">np</code><code class="o">.</code><code class="n">pi</code><code class="p">)</code><code class="o">*</code><code class="mi">100</code>&#13;
&#13;
<code class="nb">print</code><code class="p">(</code><code class="s2">"After </code><code class="si">{0}</code><code class="s2"> simulations, our estimate of Pi is </code><code class="si">{1}</code><code class="s2"> with an error of </code><code class="si">{2}</code><code class="s2">%"</code>&#13;
<code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">n</code><code class="p">,</code> <code class="n">pi</code><code class="p">,</code> <code class="nb">round</code><code class="p">(</code><code class="n">error</code><code class="p">,</code><code class="mi">2</code><code class="p">)))</code>&#13;
&#13;
<code class="c1"># Points outside the circle are the negation of the boolean array inside</code>&#13;
<code class="n">outside</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">invert</code><code class="p">(</code><code class="n">inside</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># Plot the graph</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">x</code><code class="p">[</code><code class="n">inside</code><code class="p">],</code> <code class="n">y</code><code class="p">[</code><code class="n">inside</code><code class="p">],</code> <code class="s1">'b.'</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">x</code><code class="p">[</code><code class="n">outside</code><code class="p">],</code> <code class="n">y</code><code class="p">[</code><code class="n">outside</code><code class="p">],</code> <code class="s1">'r.'</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s1">'square'</code><code class="p">);</code></pre>&#13;
<p class="pagebreak-before">As in the Monty Hall simulation, you can see from the results of this simulation that the MCS approximation of pi is close to the theoretical value. Moreover, the difference between the estimate and the theoretical value gets closer to 0 as you increase the number of points N sprayed on the square. This makes the ratio of areas of the square and circle more accurate, giving you a better estimate of pi. Let’s now explore the key statistical concepts that enable MCS to harness randomness to solve complex problems with or without analytical solutions.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Key Statistical Concepts" data-type="sect1"><div class="sect1" id="key_statistical_concepts">&#13;
<h1>Key Statistical Concepts</h1>&#13;
<p>Here are some very important <a contenteditable="false" data-primary="statistical inference" data-secondary="key concepts behind Monte Carlo simulation" data-type="indexterm" id="ch03-key"/><a contenteditable="false" data-primary="Monte Carlo simulation (MCS)" data-secondary="statistical concepts of" data-type="indexterm" id="ch03-key2"/>statistical concepts that you need to understand so that you will have deeper insights into why MCS works and how to apply it to solve complex problems in finance and investing. These are also the concepts that provide the theoretical foundation of financial, statistical, and machine learning models in <span class="keep-together">general.</span></p>&#13;
<section data-pdf-bookmark="Mean and Variance" data-type="sect2"><div class="sect2" id="mean_and_variance">&#13;
<h2>Mean and Variance</h2>&#13;
<p><a data-type="xref" href="#formulas_for_arithmetic_mean_and_standa">Figure 3-2</a> should refresh<a contenteditable="false" data-primary="mean" data-type="indexterm" id="id738"/><a contenteditable="false" data-primary="arithmetic mean" data-type="indexterm" id="id739"/><a contenteditable="false" data-primary="central tendency measures" data-type="indexterm" id="id740"/> your memory of the basic descriptive statistical concepts you learned in high school.</p>&#13;
<figure><div class="figure" id="formulas_for_arithmetic_mean_and_standa">&#13;
<img alt="Formulas for arithmetic mean and standard deviation" src="assets/pmlf_0302.png"/>&#13;
<h6><span class="label">Figure 3-2. </span>Formulas for a sample’s mean and standard deviation<sup><a data-type="noteref" href="ch03.html#ch03fn2" id="ch03fn2-marker">2</a></sup></h6>&#13;
</div></figure>&#13;
<p class="pagebreak-before">The arithmetic mean is a measure of the central tendency of a sample of data points. It is simple to calculate: add up all the point values in a sample and divide the sum by the total number of points. Other measures of central tendencies of a dataset are the median and the mode. <a contenteditable="false" data-primary="median" data-type="indexterm" id="id741"/><a contenteditable="false" data-primary="mode" data-type="indexterm" id="id742"/>Recall that the median is the value in the dataset that divides it into an upper and lower half. <a contenteditable="false" data-primary="outliers" data-type="indexterm" id="id743"/><a contenteditable="false" data-primary="outliers" data-secondary="median unchanged by extremes of" data-type="indexterm" id="id744"/><a contenteditable="false" data-primary="outliers" data-secondary="mode unaffected by" data-type="indexterm" id="id745"/>While the arithmetic mean is sensitive to outliers, the median does not change regardless of how extreme the outliers are. The mode is the most frequent value observed in a data. It is also unaffected by outliers. Sometimes there may be many modes in a sample, and other times a mode may not even exist.</p>&#13;
<p>It is important to note that<a contenteditable="false" data-primary="mean" data-secondary="sum of all deviations from" data-type="indexterm" id="id746"/><a contenteditable="false" data-primary="arithmetic mean" data-secondary="sum of all deviations from" data-type="indexterm" id="id747"/> the sum of all deviations from the arithmetic mean of the values always equals zero. That is what makes the arithmetic mean a good measure of the central tendency of a sample. It is also why you have to square the deviations from the mean to make them positive, so they do not cancel one another out. <a contenteditable="false" data-primary="mean" data-secondary="average deviation from" data-type="indexterm" id="id748"/><a contenteditable="false" data-primary="arithmetic mean" data-secondary="average deviation from" data-type="indexterm" id="id749"/>The average deviation from the mean gives you a sense of dispersion, or spread of the data sample, from its arithmetic mean.</p>&#13;
<p>Note that the variance of a sample<a contenteditable="false" data-primary="variance" data-type="indexterm" id="id750"/><a contenteditable="false" data-primary="degrees of freedom in variance calculation" data-type="indexterm" id="id751"/> is calculated by adding the sum of the squared deviations and dividing them by one less than the total number of points (n). The reason you use n-1 instead of n is that you have lost a degree of freedom by calculating the mean; i.e. the mean and n-1 points will give you the entire dataset. <a contenteditable="false" data-primary="standard deviation" data-type="indexterm" id="id752"/><a contenteditable="false" data-primary="variance" data-secondary="standard deviation via" data-type="indexterm" id="id753"/>Standard deviation, which is in the units of the mean, is obtained by taking the square root of variance.</p>&#13;
<p>Volatility of asset price returns<a contenteditable="false" data-primary="volatility" data-type="indexterm" id="id754"/> is calculated using the standard deviation of sample returns. <a contenteditable="false" data-primary="geometric Brownian motion (GBM)" data-type="indexterm" id="id755"/>If the returns are compounded continuously in a financial model, such as is assumed in geometric Brownian motion (GBM), we use the natural logarithm of price returns to calculate volatility. This also has the added advantage of making analytical and numerical computations much easier, since the practice of multiplying numbers can be transformed into adding their logarithms. Moreover, when performing multiplications involving numerous values less than 1, the precision of the computation can be compromised due to the inherent numerical underflow limitations of computers.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Expected Value: Probability-Weighted Arithmetic Mean" data-type="sect2"><div class="sect2" id="expected_value_probability_weighted_ar">&#13;
<h2>Expected Value: Probability-Weighted Arithmetic Mean</h2>&#13;
<p>An important type of arithmetic mean<a contenteditable="false" data-primary="expected value" data-type="indexterm" id="id756"/><a contenteditable="false" data-primary="arithmetic mean" data-secondary="expected value" data-type="indexterm" id="id757"/><a contenteditable="false" data-primary="mean" data-secondary="expected value" data-type="indexterm" id="id758"/><a contenteditable="false" data-primary="probability-weighted average sum" data-type="indexterm" id="id759"/> is the expected value of a trade or investment. Expected value is defined as a probability-weighted arithmetic mean of future payoffs:</p>&#13;
<ul class="simplelist">&#13;
<li>E[S] = P(S<sub>1</sub>) x Payoff(S<sub>1</sub>) + ....+ P(S<sub>n</sub>) x Payoff(S<sub>n</sub>)</li></ul>&#13;
<p>In finance, you should use expected value to estimate the future returns of your trades and investments. Other measurements used for this purpose are incomplete or misleading. <a contenteditable="false" data-primary="reward-to-risk ratio" data-type="indexterm" id="id760"/>For instance, it is common to hear traders on financial news networks talk about the reward-to-risk ratio of their trades. That ratio is an incomplete metric to consider because it does not factor in the estimated probabilities of positive and negative payoffs. You can structure a trade to have any reward-to-risk ratio you want. It says nothing about how likely you think the payoffs are going to be. If the reward-to-risk ratio is the key metric you’re going to consider in an opportunity, don’t waste your time with investing. Just buy a lottery ticket. The reward-to-risk ratio can go over 100 million to 1.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Why Volatility Is a Nonsensical Measure of Risk" data-type="sect2"><div class="sect2" id="why_volatility_is_a_nonsensical_measure">&#13;
<h2>Why Volatility Is a Nonsensical Measure of Risk</h2>&#13;
<p>Suppose the price of stock A goes<a contenteditable="false" data-primary="volatility" data-secondary="nonsensical measure of risk" data-type="indexterm" id="id761"/><a contenteditable="false" data-primary="risk" data-secondary="volatility as nonsensical measure" data-type="indexterm" id="id762"/> up 5% in one month, 10% the next, and 20% in the third month. The monthly compounded return of A, the geometric mean of the returns, would be about 11.49%, with a monthly standard deviation, or volatility, of 7.64%. Note that we have computed the monthly volatility using the formula in <a data-type="xref" href="#formulas_for_arithmetic_mean_and_standa">Figure 3-2</a> and using 2 in the denominator, since this estimate is based on a sample of three months. Compare this to stock B, which declines –10% three months in a row. The monthly compounded return would be –10%, but the monthly volatility would be zero. Which stock would you like in your portfolio?</p>&#13;
<p>Volatility is a nonsensical measure of risk because it treats profits that don’t equal the arithmetic mean (a measure of expectation) as risky as losses that do the same. What is also absurd is that losses that meet expectations are not considered a risk. Clearly, a loss is a loss whether or not it equals the average loss of the sample of returns.</p>&#13;
<p>Volatility doesn’t consider the direction of the dispersion of returns and treats positive and negative deviations from the mean equally. So, volatility misestimates asymmetric risk. The volatility that investors talk about and don’t want is the semistandard deviation of losses. However, semistandard deviation is analytically intractable and doesn’t lend itself to elegant formulas in financial theories.</p>&#13;
<p>This implies that any risk or performance measure that is based on the volatility of returns is inherently flawed. <a contenteditable="false" data-primary="Sharpe ratio" data-type="indexterm" id="id763"/>The Sharpe ratio measures asset price returns in excess of a benchmark return and divides that by the volatility of asset price returns. It is a standard investment performance metric popular in academia and industry. <a contenteditable="false" data-primary="Buffet, Warren" data-type="indexterm" id="id764"/>However, many value investors, like Warren Buffet, hedge fund managers, and commodity trading advisors reject the Sharpe ratio as a flawed measure of performance. Worse still, volatility underestimates financial risk, which we discuss shortly.</p>&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p>If your investment’s positive returns are not meeting your expectations and its resultant volatility is keeping you up at night, you may rest assured, as help is nigh. Now you can lower the volatility of your investment returns by transferring those risky, positive return deviations to me for free!</p>&#13;
</div>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Skewness and Kurtosis" data-type="sect2"><div class="sect2" id="skewness_and_kurtosis">&#13;
<h2>Skewness and Kurtosis</h2>&#13;
<p>Skewness measures the asymmetry<a contenteditable="false" data-primary="skewness" data-type="indexterm" id="id765"/><a contenteditable="false" data-primary="normal (Gaussian) distribution" data-secondary="skewness of zero" data-type="indexterm" id="id766"/> of a distribution about its arithmetic mean. The skewness of a normal distribution is zero. Skewness is computed in a manner similar to that of variance, but instead of squaring the deviation from the mean, you raise it to the third power. This keeps the positive or negative sign of the deviations and so gives you the direction of average deviation from the mean. Skewness tells you where the expected value (mean) of the distribution is with respect to the median and the mode. See <a data-type="xref" href="#skewed_distributions_compared_to_a_symm">Figure 3-3</a>.</p>&#13;
<figure><div class="figure" id="skewed_distributions_compared_to_a_symm">&#13;
<img alt="Skewed distributions compared to a symmetrical distribution such as the normal distribution" src="assets/pmlf_0303.png"/>&#13;
<h6><span class="label">Figure 3-3. </span>Skewed distributions compared to a symmetrical distribution such as the normal distribution<sup><a data-type="noteref" href="ch03.html#ch03fn3" id="ch03fn3-marker">3</a></sup></h6>&#13;
</div></figure>&#13;
<p>As an investor or trader, you want your return distribution to be as positively skewed as possible. In a positively skewed distribution, the expected value is going to be greater than the median, and so it will be in the upper half of the distribution—positive returns are going to outweigh the negative returns on average. As discussed earlier, volatility is directionless and so will misestimate asymmetric risks of skewed distributions.</p>&#13;
<p>Kurtosis is a measure of how peaked<a contenteditable="false" data-primary="kurtosis" data-type="indexterm" id="id767"/> the distribution is about the arithmetic mean and how fat its tails are compared to that of a normal distribution. Like skewness, kurtosis is computed in a manner similar to that of variance, but instead of squaring the deviation from the mean, you raise it to the fourth power. <a contenteditable="false" data-primary="fat-tailed probability distributions" data-secondary="low probability events more likely" data-type="indexterm" id="id768"/>Fat-tailed distributions imply that low probability events are more likely than would be expected if the distribution were normal. <a contenteditable="false" data-primary="uniform distributions have no tails" data-type="indexterm" id="id769"/>A uniform distribution has no tails. In fact, <a contenteditable="false" data-primary="fat-tailed probability distributions" data-secondary="Cauchy distribution" data-type="indexterm" id="id770"/><a contenteditable="false" data-primary="probability distributions" data-secondary="fat-tailed Cauchy distribution" data-type="indexterm" id="id771"/><a contenteditable="false" data-primary="Cauchy distribution" data-type="indexterm" id="id772"/>a Cauchy (or Lorentzian) distribution looks deceptively similar to a normal distribution but has very fat tails because of its infinite mean and variance, as shown in <a data-type="xref" href="#compare_cauchysoliduslorentzian_distri">Figure 3-4</a>.</p>&#13;
<figure><div class="figure" id="compare_cauchysoliduslorentzian_distri">&#13;
<img alt="Compare Cauchy/Lorentzian distribution with the Normal distribution." src="assets/pmlf_0304.png"/>&#13;
<h6><span class="label">Figure 3-4. </span>Compare the tails of the Cauchy distribution with the normal distribution<sup><a data-type="noteref" href="ch03.html#ch03fn4" id="ch03fn4-marker">4</a></sup></h6>&#13;
</div></figure>&#13;
</div></section>&#13;
<section data-pdf-bookmark="The Gaussian or Normal Distribution" data-type="sect2"><div class="sect2" id="the_gaussian_or_normal_distribution">&#13;
<h2>The Gaussian or Normal Distribution</h2>&#13;
<p>Gaussian distributions are found<a contenteditable="false" data-primary="normal (Gaussian) distribution" data-type="indexterm" id="id773"/> everywhere in nature and are used in all the sciences. It is quite common to see data distributed like a bell curve, as shown in <a data-type="xref" href="#the_gaussian_or_normal_distributionsour">Figure 3-5</a>. That is why the Gaussian distribution is also called the normal <span class="keep-together">distribution.</span></p>&#13;
<figure><div class="figure" id="the_gaussian_or_normal_distributionsour">&#13;
<img alt="About 99.7% of the area of a Gaussian or normal distribution falls within 3 standard deviations from the mean" src="assets/pmlf_0305.png"/>&#13;
<h6><span class="label">Figure 3-5. </span>About 99.7% of the area of a Gaussian or normal distribution falls within 3 standard deviations from the mean<sup><a data-type="noteref" href="ch03.html#ch03fn5" id="ch03fn5-marker">5</a></sup></h6>&#13;
</div></figure>&#13;
<p>Unfortunately, financial data<a contenteditable="false" data-primary="normal (Gaussian) distribution" data-secondary="financial theories using" data-type="indexterm" id="id774"/> and academic research show that normal distributions are not so common in all financial markets. But that hasn’t stopped most academics and many practitioners from using them for their models. Why? Because Gaussian distributions are analytically tractable and lend themselves to elegant formulas that are solvable without using computers. If you know the mean and standard deviation of a Gaussian distribution, you know everything about the distribution. For instance, in <a data-type="xref" href="#skewed_distributions_compared_to_a_symm">Figure 3-3</a>, you can see that about 68% of the data are within one standard deviation of the mean, 95% are within two standard deviations of the mean, and almost all the data are within three standard deviations of the mean.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Why Volatility Underestimates Financial Risk" data-type="sect2"><div class="sect2" id="why_volatility_underestimates_financial">&#13;
<h2>Why Volatility Underestimates Financial Risk</h2>&#13;
<p>The S&amp;P 500 is a global market index<a contenteditable="false" data-primary="volatility" data-secondary="nonsensical measure of risk" data-tertiary="underestimating financial risk" data-type="indexterm" id="ch03-und"/><a contenteditable="false" data-primary="risk" data-secondary="volatility as nonsensical measure" data-tertiary="underestimating financial risk" data-type="indexterm" id="ch03-und2"/><a contenteditable="false" data-primary="S&amp;P 500" data-secondary="volatility underestimating risk" data-type="indexterm" id="ch03-und3"/> and is used by market participants worldwide as a benchmark for the equity market. The index represents an equity portfolio composed of 500 of some of the best companies in the world at any time. Financial instruments based on the S&amp;P 500 are the most liquid markets in the world and operate 24 hours a day for over 5 days of the week. <a contenteditable="false" data-primary="ETF (exchange-traded fund)" data-type="indexterm" id="id775"/>I can attest to that, as I trade the ETF (exchange-traded fund) as well as options and futures based on this index.</p>&#13;
<p>According to modern portfolio theory (MPT),<a contenteditable="false" data-primary="modern portfolio theory (MPT)" data-secondary="S&amp;P 500 index normally distributed" data-type="indexterm" id="id776"/><a contenteditable="false" data-primary="normal (Gaussian) distribution" data-secondary="S&amp;P 500 index" data-type="indexterm" id="id777"/> asset price returns of the S&amp;P 500 index should be approximately normally distributed. <a contenteditable="false" data-primary="time" data-secondary="stationary ergodic as time invariant" data-type="indexterm" id="id778"/><a contenteditable="false" data-primary="stationary ergodic as time invariant" data-type="indexterm" id="id779"/><a contenteditable="false" data-primary="S&amp;P 500" data-secondary="normally distributed" data-type="indexterm" id="id780"/><a contenteditable="false" data-primary="financial theory" data-secondary="S&amp;P 500 index normally distributed" data-type="indexterm" id="id781"/>It also assumes that the mean and variance of this distribution are stationary ergodic. What this means is that these two parameters are time invariant, and we can estimate them from a reasonably large sample taken from any time period.</p>&#13;
<p>In the following Python code, <a contenteditable="false" data-primary="S&amp;P 500" data-secondary="normally distributed" data-tertiary="testing" data-type="indexterm" id="ch03-tes"/><a contenteditable="false" data-primary="modern portfolio theory (MPT)" data-secondary="S&amp;P 500 index normally distributed" data-tertiary="testing" data-type="indexterm" id="ch03-tes2"/><a contenteditable="false" data-primary="normal (Gaussian) distribution" data-secondary="S&amp;P 500 index" data-tertiary="testing" data-type="indexterm" id="ch03-tes3"/><a contenteditable="false" data-primary="financial theory" data-secondary="S&amp;P 500 index normally distributed" data-tertiary="testing" data-type="indexterm" id="ch03-tes4"/><a contenteditable="false" data-primary="skewness" data-secondary="calculating for S&amp;P 500" data-type="indexterm" id="id782"/><a contenteditable="false" data-primary="kurtosis" data-secondary="calculating for S&amp;P 500" data-type="indexterm" id="id783"/>we test the fundamental tenet of MPT that asset price returns are normally distributed. We import 30 years of S&amp;P 500 price data and compute its daily returns, skewness, and kurtosis:</p>&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Import Python libraries</code>&#13;
<code class="kn">import</code> <code class="nn">pandas</code> <code class="k">as</code> <code class="nn">pd</code>&#13;
<code class="kn">from</code> <code class="nn">datetime</code> <code class="kn">import</code> <code class="n">datetime</code>&#13;
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>&#13;
&#13;
<code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="k">as</code> <code class="nn">plt</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">style</code><code class="o">.</code><code class="n">use</code><code class="p">(</code><code class="s1">'seaborn'</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># Install web scraper for Yahoo Finance</code>&#13;
<code class="err">!</code><code class="n">pip</code> <code class="n">install</code> <code class="n">yfinance</code>&#13;
<code class="kn">import</code> <code class="nn">yfinance</code> <code class="k">as</code> <code class="nn">yf</code>&#13;
&#13;
<code class="c1"># Import over 30 years of S&amp;P 500 ('SPY') price data into a dataframe </code>&#13;
<code class="c1"># called equity</code>&#13;
<code class="n">start</code> <code class="o">=</code> <code class="n">datetime</code><code class="p">(</code><code class="mi">1993</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>&#13;
<code class="n">end</code> <code class="o">=</code> <code class="n">datetime</code><code class="p">(</code><code class="mi">2022</code><code class="p">,</code> <code class="mi">10</code><code class="p">,</code> <code class="mi">15</code><code class="p">)</code>&#13;
<code class="n">equity</code> <code class="o">=</code> <code class="n">yf</code><code class="o">.</code><code class="n">Ticker</code><code class="p">(</code><code class="s1">'SPY'</code><code class="p">)</code><code class="o">.</code><code class="n">history</code><code class="p">(</code><code class="n">start</code><code class="o">=</code><code class="n">start</code><code class="p">,</code> <code class="n">end</code><code class="o">=</code><code class="n">end</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># Use SPY's closing prices to compute its daily returns. </code>&#13;
<code class="c1"># Remove NaNs from your dataframe.</code>&#13;
<code class="n">equity</code><code class="p">[</code><code class="s1">'Returns'</code><code class="p">]</code> <code class="o">=</code> <code class="n">equity</code><code class="p">[</code><code class="s1">'Close'</code><code class="p">]</code><code class="o">.</code><code class="n">pct_change</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code><code class="o">*</code><code class="mi">100</code>&#13;
<code class="n">equity</code> <code class="o">=</code> <code class="n">equity</code><code class="o">.</code><code class="n">dropna</code><code class="p">()</code>&#13;
&#13;
<code class="c1"># Visualize and summarize SPY's daily price returns. </code>&#13;
<code class="c1"># Compute its skewness and kurtosis.</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">hist</code><code class="p">(</code><code class="n">equity</code><code class="p">[</code><code class="s1">'Returns'</code><code class="p">]),</code> <code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="s1">'Distribution of S&amp;P 500 Daily Percentage </code><code class="w"/>&#13;
<code class="n">Returns</code> <code class="n">Over</code> <code class="n">the</code> <code class="n">Past</code> <code class="mi">30</code> <code class="n">Years</code><code class="s1">'), plt.xlabel('</code><code class="n">Daily</code> <code class="n">Percentage</code> <code class="n">Returns</code><code class="s1">'), </code><code class="w"/>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s1">'Frequency'</code><code class="p">),</code> <code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">();</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="s2">"Descriptive statistics of S&amp;P 500 percentage returns:</code><code class="se">\n</code><code class="si">{}</code><code class="s2">"</code>&#13;
<code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">equity</code><code class="p">[</code><code class="s1">'Returns'</code><code class="p">]</code><code class="o">.</code><code class="n">describe</code><code class="p">()</code><code class="o">.</code><code class="n">round</code><code class="p">(</code><code class="mi">2</code><code class="p">)))</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="s1">'The skewness of S&amp;P 500 returns is: </code><code class="si">{0:.2f}</code><code class="s1"> and the kurtosis is: </code><code class="si">{1:.2f}</code><code class="s1">.'</code>&#13;
<code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">equity</code><code class="p">[</code><code class="s1">'Returns'</code><code class="p">]</code><code class="o">.</code><code class="n">skew</code><code class="p">(),</code> <code class="n">equity</code><code class="p">[</code><code class="s1">'Returns'</code><code class="p">]</code><code class="o">.</code><code class="n">kurtosis</code><code class="p">()))</code></pre>&#13;
<figure class="informal"><div class="figure">&#13;
<img alt="Image" src="assets/pmlf_03in01.png"/>&#13;
<h6/>&#13;
</div></figure>&#13;
<p>Clearly, the daily return distribution doesn’t look anything close to normal. It has a negative skew of 0.07 and very fat tails with a kurtosis of 11.43. If S&amp;P 500 daily returns were normally distributed, what would it look like? Let’s simulate the world that theoretical finance claims we should be living in.</p>&#13;
<p>The time-invariant tenet of MPT implies that we can estimate its statistical moments using a sufficiently large sample from any time period. Thirty years’ worth of data certainly qualifies. We use the mean and standard deviation from the previously mentioned historical data as our estimates for those parameters:<a contenteditable="false" data-primary="NumPy random number generator" data-type="indexterm" id="id784"/><a contenteditable="false" data-primary="random number generator (NumPy)" data-type="indexterm" id="id785"/><a contenteditable="false" data-primary="mean" data-secondary="calculating for S&amp;P 500" data-type="indexterm" id="id786"/><a contenteditable="false" data-primary="arithmetic mean" data-secondary="calculating for S&amp;P 500" data-type="indexterm" id="id787"/><a contenteditable="false" data-primary="standard deviation" data-secondary="calculating for S&amp;P 500" data-type="indexterm" id="id788"/></p>&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Estimate the mean and standard deviation from SPY's 30 year historical data</code>&#13;
<code class="n">mean</code> <code class="o">=</code> <code class="n">equity</code><code class="p">[</code><code class="s1">'Returns'</code><code class="p">]</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code>&#13;
<code class="n">vol</code> <code class="o">=</code> <code class="n">equity</code><code class="p">[</code><code class="s1">'Returns'</code><code class="p">]</code><code class="o">.</code><code class="n">std</code><code class="p">()</code>&#13;
<code class="n">sample</code> <code class="o">=</code> <code class="n">equity</code><code class="p">[</code><code class="s1">'Returns'</code><code class="p">]</code><code class="o">.</code><code class="n">count</code><code class="p">()</code>&#13;
&#13;
<code class="c1"># Use NumPy's random number generator to sample from a normal distribution</code>&#13;
<code class="c1"># with the above estimates of its mean and standard deviation</code>&#13;
<code class="c1"># Create a new column called 'Simulated' and generate the same number of </code>&#13;
<code class="c1"># random samples from NumPy's normal distribution as the actual data sample</code>&#13;
<code class="c1"># you've imported above for SPY</code>&#13;
<code class="n">equity</code><code class="p">[</code><code class="s1">'Simulated'</code><code class="p">]</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">normal</code><code class="p">(</code><code class="n">mean</code><code class="p">,</code> <code class="n">vol</code><code class="p">,</code> <code class="n">sample</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># Visualize and summarize SPY's simulated daily price returns.</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">hist</code><code class="p">(</code><code class="n">equity</code><code class="p">[</code><code class="s1">'Simulated'</code><code class="p">]),</code> <code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="s1">'Distribution of S&amp;P 500 Simulated </code><code class="w"/>&#13;
<code class="n">Daily</code> <code class="n">Returns</code><code class="s1">'), plt.xlabel('</code><code class="n">Simulated</code> <code class="n">Daily</code> <code class="n">Percentage</code> <code class="n">Returns</code><code class="s1">'), </code><code class="w"/>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s1">'Frequency'</code><code class="p">),</code> <code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">();</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="s2">"Descriptive statistics of S&amp;P 500 stock's simulated percentage </code><code class="w"/>&#13;
<code class="n">returns</code><code class="p">:</code>\<code class="n">n</code><code class="p">{}</code><code class="s2">".</code><code class="w"/>&#13;
<code class="nb">format</code><code class="p">(</code><code class="n">equity</code><code class="p">[</code><code class="s1">'Simulated'</code><code class="p">]</code><code class="o">.</code><code class="n">describe</code><code class="p">()))</code>&#13;
&#13;
<code class="c1"># Compute the skewness and kurtosis of the simulated daily price returns.</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="s1">'The skewness of S&amp;P 500 simulated returns is: </code><code class="si">{0}</code><code class="s1"> </code><code class="w"/>&#13;
<code class="ow">and</code> <code class="n">the</code> <code class="n">kurtosis</code> <code class="ow">is</code><code class="p">:</code> &#13;
<code class="p">{</code><code class="mi">1</code><code class="p">}</code><code class="o">.</code><code class="s1">'.format(equity['</code><code class="n">Simulated</code><code class="s1">'].skew().round(2), equity['</code><code class="n">Simulated</code><code class="s1">']</code><code class="w"/>&#13;
<code class="o">.</code><code class="n">kurtosis</code><code class="p">()</code><code class="o">.</code><code class="n">round</code><code class="p">(</code><code class="mi">2</code><code class="p">)))</code></pre>&#13;
<figure class="informal"><div class="figure">&#13;
<img alt="Image" src="assets/pmlf_03in02.png"/>&#13;
<h6/>&#13;
</div></figure>&#13;
<p>Since we are sampling randomly from a normal distribution, the values for both skewness and kurtosis have minor sampling errors around zero. Regardless, the two distributions look nothing like each other. The daily returns of the S&amp;P 500 over the last 30 years are certainly not normally distributed.</p>&#13;
<p>Most financial time series are<a contenteditable="false" data-primary="time" data-secondary="financial time series as asymmetric and fat-tailed" data-type="indexterm" id="id789"/> asymmetric and fat-tailed. These are not nice-to-know financial and statistical trivia. Asset price return distributions with negatively skewed, fat tails have the potential to bankrupt investors, corporations, and entire economies if their modelers ignore them, since they would be underestimating the probabilities of extreme events. <a contenteditable="false" data-primary="models" data-secondary="wrong, even dangerous" data-tertiary="Great Financial Crisis reminder" data-type="indexterm" id="id790"/><a contenteditable="false" data-primary="Great Financial Crisis" data-type="indexterm" id="id791"/><a contenteditable="false" data-primary="financial theory" data-secondary="models wrong, even dangerous" data-tertiary="Great Financial Crisis reminder" data-type="indexterm" id="id792"/>The Great Financial Crisis is a recent reminder of the devastating consequences of building theoretical models using elegant mathematical equations that ignore the basic principles of the scientific method and the noisy, ugly, fat-tailed realities of real-world data.<a contenteditable="false" data-primary="" data-startref="ch03-und" data-type="indexterm" id="id793"/><a contenteditable="false" data-primary="" data-startref="ch03-und2" data-type="indexterm" id="id794"/><a contenteditable="false" data-primary="" data-startref="ch03-und3" data-type="indexterm" id="id795"/><a contenteditable="false" data-primary="" data-startref="ch03-tes" data-type="indexterm" id="id796"/><a contenteditable="false" data-primary="" data-startref="ch03-tes2" data-type="indexterm" id="id797"/><a contenteditable="false" data-primary="" data-startref="ch03-tes3" data-type="indexterm" id="id798"/><a contenteditable="false" data-primary="" data-startref="ch03-tes4" data-type="indexterm" id="id799"/></p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="The Law of Large Numbers" data-type="sect2"><div class="sect2" id="the_law_of_large_numbers">&#13;
<h2>The Law of Large Numbers</h2>&#13;
<p>This is one of the most important<a contenteditable="false" data-primary="law of large numbers (LLN)" data-type="indexterm" id="id800"/><a contenteditable="false" data-primary="mean" data-secondary="law of large numbers" data-type="indexterm" id="id801"/><a contenteditable="false" data-primary="arithmetic mean" data-secondary="law of large numbers" data-type="indexterm" id="id802"/> statistical theorems. The law of large numbers (LLN) says that if samples are independent and drawn from the same distribution, the sample mean will almost surely converge to the theoretical mean as the sample size grows larger. In <a data-type="xref" href="#the_sample_mean_of_dice_throws_approach">Figure 3-6</a>, the value of the sum of all the numbers that appear on each throw of a die divided by the total number of throws or trials approaches 3.5 as the number of trials increases.</p>&#13;
<figure><div class="figure" id="the_sample_mean_of_dice_throws_approach">&#13;
<img alt="The sample mean of dice throws approaches its theoretical mean as the sample size gets larger." src="assets/pmlf_0306.png"/>&#13;
<h6><span class="label">Figure 3-6. </span>The sample mean of dice throws approaches its theoretical mean as the sample size gets larger<sup><a data-type="noteref" href="ch03.html#ch03fn6" id="ch03fn6-marker">6</a></sup></h6>&#13;
</div></figure>&#13;
<p>Note that the theoretical average does not have to be a physical outcome. There is no 3.5 on any fair die. Also, notice how the outcomes of the first few trials vary widely about the mean. However, in the long run, they converge inexorably to the theoretical mean. Of course, we assume that the die is fair and that we don’t know the physics of the dice throws.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="The Central Limit Theorem" data-type="sect2"><div class="sect2" id="the_central_limit_theorem">&#13;
<h2>The Central Limit Theorem</h2>&#13;
<p>The central limit theorem (CLT) says<a contenteditable="false" data-primary="central limit theorem (CLT)" data-type="indexterm" id="ch03-clt"/><a contenteditable="false" data-primary="mean" data-secondary="central limit theorem" data-type="indexterm" id="ch03-clt2"/><a contenteditable="false" data-primary="arithmetic mean" data-secondary="central limit theorem" data-type="indexterm" id="ch03-clt3"/> that if you keep taking samples from an unknown population of any shape and calculate the mean of each of the samples of size n, the distribution of these sample means will be normally distributed, as shown in <a data-type="xref" href="#the_sampling_distribution_of_the_sample">Figure 3-7</a>.</p>&#13;
<figure><div class="figure" id="the_sampling_distribution_of_the_sample">&#13;
<img alt="The sampling distribution of the sample mean is normally distributed." src="assets/pmlf_0307.png"/>&#13;
<h6><span class="label">Figure 3-7. </span>The sampling distribution of the sample mean is normally distributed<sup><a data-type="noteref" href="ch03.html#ch03fn7" id="ch03fn7-marker">7</a></sup></h6>&#13;
</div></figure>&#13;
<p>This is one of the most amazing statistical phenomena. To appreciate the power of the CLT, consider a fair die that has a uniform distribution since each number on the die is equally likely at ⅙. <a data-type="xref" href="#the_clt_shows_us_how_uniform_distributi">Figure 3-8</a> shows what happens when you roll a fair die and add the numbers that show up on each throw and repeat the trials many times. Behold the magic of the CLT: horizontal lines are transformed into an approximate bell curve. If we increase the number of trials or tosses of the die, the curve will look like a bell curve.<a contenteditable="false" data-primary="" data-startref="ch03-key" data-type="indexterm" id="id803"/><a contenteditable="false" data-primary="" data-startref="ch03-key2" data-type="indexterm" id="id804"/></p>&#13;
<figure><div class="figure" id="the_clt_shows_us_how_uniform_distributi">&#13;
<img alt="The CLT shows us how uniform distributions of two dice are transformed into an approximate Gaussian distribution." src="assets/pmlf_0308.png"/>&#13;
<h6><span class="label">Figure 3-8. </span>The CLT shows us how the uniform distribution of a fair die is transformed into an approximate Gaussian distribution<sup><a data-type="noteref" href="ch03.html#ch03fn8" id="ch03fn8-marker">8</a></sup></h6>&#13;
</div></figure>&#13;
</div></section>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Theoretical Underpinnings of MCS" data-type="sect1"><div class="sect1" id="theoretical_underpinnings_of_mcs">&#13;
<h1>Theoretical Underpinnings of MCS</h1>&#13;
<p>MCS is based on the two most important<a contenteditable="false" data-primary="Monte Carlo simulation (MCS)" data-secondary="theoretical underpinnings of" data-type="indexterm" id="id805"/><a contenteditable="false" data-primary="law of large numbers (LLN)" data-type="indexterm" id="id806"/><a contenteditable="false" data-primary="mean" data-secondary="law of large numbers" data-type="indexterm" id="id807"/><a contenteditable="false" data-primary="arithmetic mean" data-secondary="law of large numbers" data-type="indexterm" id="id808"/> theorems in statistics already mentioned: the law of large numbers (LLN) and the central limit theorem (CLT).<sup><a data-type="noteref" href="ch03.html#ch03fn9" id="ch03fn9-marker">9</a></sup> Recall that the LLN ensures that as the number of trials increases, the sample mean almost surely converges to the theoretical or population mean. The CLT ensures that the sampling errors or fluctuations of the sample averages from the theoretical mean become normally distributed as sample sizes get larger.</p>&#13;
<p>One of the reasons MCS works<a contenteditable="false" data-primary="sampling error of Monte Carlo simulations" data-type="indexterm" id="id809"/><a contenteditable="false" data-primary="Monte Carlo simulation (MCS)" data-secondary="about" data-tertiary="sampling error" data-type="indexterm" id="id810"/> and is scalable to multidimensional problems is that the sampling error is independent of the dimension of the variable. This sampling error approaches zero asymptotically as the square root of the sample size and not the dimension of the variable. This is very important. It implies that a sampling error in an MCS is the same for a single variable as it is for a 100-dimensional variable.</p>&#13;
<p>However, the error decreases as the square root of the sample size n. So you have to increase the MCS iterations by a factor of 100 to increase the accuracy of its estimate by a single digit. But with computing power becoming cheaper by the day, this is not as big an issue now as it was in the last century.<a contenteditable="false" data-primary="" data-startref="ch03-clt" data-type="indexterm" id="id811"/><a contenteditable="false" data-primary="" data-startref="ch03-clt2" data-type="indexterm" id="id812"/><a contenteditable="false" data-primary="" data-startref="ch03-clt3" data-type="indexterm" id="id813"/></p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Valuing a Software Project" data-type="sect1"><div class="sect1" id="valuing_a_software_project">&#13;
<h1>Valuing a Software Project</h1>&#13;
<p>Let’s increase our understanding of MCS<a contenteditable="false" data-primary="Monte Carlo simulation (MCS)" data-secondary="valuing a software project" data-type="indexterm" id="ch03-vals"/><a contenteditable="false" data-primary="valuing a software project via MCS" data-type="indexterm" id="ch03-vals2"/><a contenteditable="false" data-primary="software project valuation via MCS" data-type="indexterm" id="ch03-vals3"/><a contenteditable="false" data-primary="discounted cash flow model (DCF)" data-secondary="valuing a software project via MCS" data-type="indexterm" id="ch03-vals4"/><a contenteditable="false" data-primary="free cash flows (FCF) in valuing a software project" data-type="indexterm" id="ch03-vals5"/><a contenteditable="false" data-primary="net present value (NPV)" data-type="indexterm" id="ch03-vals6"/><a contenteditable="false" data-primary="FCF (free cash flows) in valuing a software project" data-type="indexterm" id="ch03-vals7"/> by applying it to a real-life financial problem such as valuing a capital project. The discounted cash flow (DCF) model discussed in the previous chapter is used extensively in corporations worldwide for valuing capital projects and other investments like bonds and equities. A discounted cash flow (DCF) model forecasts expected free cash flows (FCF) over N periods, typically measured in years. FCF in a time period equals cash from operations minus capital expenditures. The model also needs an estimate of the rate of return (R) per period required by the firm’s investors. This rate is called the discount rate because it is used to discount each of the N period FCFs of the project to the present. The reason the FCFs are discounted is that we need to account for an investor’s opportunity cost of capital for undertaking the project instead of another investment of similar risk. The model is set up in four steps:</p>&#13;
<ol>&#13;
<li><p>Forecast the expected free cash flows (FCFs) of the project for each of the N <span class="keep-together">periods.</span></p></li>&#13;
<li><p>Estimate the appropriate opportunity cost or discount rate (R) per period.</p></li>&#13;
<li><p>Discount each period’s expected FCFs back to the present.</p></li>&#13;
<li><p>Add the discounted expected FCFs (previously described) to get the expected net present value (NPV):</p></li>&#13;
</ol>&#13;
<ul class="simplelist">&#13;
<li>Expected NPV of project = FCF<sub>0</sub> + FCF<sub>1</sub> / (1 + R) + FCF<sub>2</sub> / (1 + R)<sup>2</sup> + ...+ FCF<sub>n</sub> / (1 + R)<sup>N</sup></li></ul>&#13;
<p>The NPV decision rule says that you should accept any investment whose expected NPV is greater than zero. This is because an investment with a positive expected NPV gives investors a higher rate of return than an alternative investment of similar risk, which is their opportunity cost.</p>&#13;
<p class="pagebreak-before">To create our DCF model, we need to focus on the main drivers of costs and revenues for our software project. We also need to make sure that these variables are not strongly correlated with one another. Ideally, all FCFs of the model should be formulated using very few noncorrelated variables or risk factors.</p>&#13;
<p>As you know, software development is labor intensive, and so our main cost driver will be salaries and wages. Also, some developers will be working part time and some full time on the project. However, for developing the cost of labor, we only need the full-time equivalent (FTE) of the effort involved in producing the software, i.e., we estimate the effort as if all required developers were working full time. Scheduling is where we will figure out how much time to allot and when we will need each <span class="keep-together">developer:</span></p>&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Import key Python libraries and packages that we need to process and analyze </code>&#13;
<code class="c1"># our data</code>&#13;
<code class="kn">import</code> <code class="nn">pandas</code> <code class="k">as</code> <code class="nn">pd</code>&#13;
<code class="kn">from</code> <code class="nn">datetime</code> <code class="kn">import</code> <code class="n">datetime</code>&#13;
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>&#13;
<code class="kn">from</code> <code class="nn">numpy</code> <code class="kn">import</code> <code class="n">random</code> <code class="k">as</code> <code class="n">npr</code>&#13;
<code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="k">as</code> <code class="nn">plt</code> &#13;
<code class="n">plt</code><code class="o">.</code><code class="n">style</code><code class="o">.</code><code class="n">use</code><code class="p">(</code><code class="s1">'seaborn'</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># Specify model constants per full-time equivalent (fte)</code>&#13;
<code class="n">daily_rate</code> <code class="o">=</code> <code class="mi">400</code>&#13;
<code class="n">technology_charges</code> <code class="o">=</code> <code class="mi">500</code>&#13;
<code class="n">overhead_charges</code> <code class="o">=</code> <code class="mi">200</code>&#13;
&#13;
<code class="c1"># Specify other constants</code>&#13;
<code class="n">tax_rate</code> <code class="o">=</code> <code class="mf">0.15</code>&#13;
&#13;
<code class="c1"># Specify model risk factors that have little or no correlation among them.</code>&#13;
<code class="c1"># Number of trials/simulations</code>&#13;
<code class="n">n</code> <code class="o">=</code> <code class="mi">10000</code>&#13;
<code class="c1"># Number of full-time equivalent persons on the team</code>&#13;
<code class="n">fte</code> <code class="o">=</code> <code class="n">npr</code><code class="o">.</code><code class="n">uniform</code><code class="p">(</code><code class="n">low</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">high</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code> <code class="n">size</code><code class="o">=</code><code class="n">n</code><code class="p">)</code>&#13;
<code class="c1"># In person days and driven independently by the scope of the project</code>&#13;
<code class="n">effort</code> <code class="o">=</code> <code class="n">npr</code><code class="o">.</code><code class="n">uniform</code><code class="p">(</code><code class="n">low</code><code class="o">=</code><code class="mi">240</code><code class="p">,</code> <code class="n">high</code><code class="o">=</code><code class="mi">480</code><code class="p">,</code> <code class="n">size</code><code class="o">=</code><code class="n">n</code><code class="p">)</code>&#13;
<code class="c1"># Based on market research or expert judgment or both</code>&#13;
<code class="n">price</code> <code class="o">=</code> <code class="n">npr</code><code class="o">.</code><code class="n">uniform</code><code class="p">(</code><code class="n">low</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code> <code class="n">high</code><code class="o">=</code><code class="mi">200</code><code class="p">,</code> <code class="n">size</code><code class="o">=</code><code class="n">n</code><code class="p">)</code>&#13;
<code class="c1"># Independent of price in the price range considered</code>&#13;
<code class="n">units</code> <code class="o">=</code> <code class="n">npr</code><code class="o">.</code><code class="n">normal</code><code class="p">(</code><code class="n">loc</code><code class="o">=</code><code class="mi">1000</code><code class="p">,</code> <code class="n">scale</code><code class="o">=</code><code class="mi">500</code><code class="p">,</code> <code class="n">size</code><code class="o">=</code><code class="n">n</code><code class="p">)</code>&#13;
<code class="c1"># Discount rate for the project period based on risk of similar efforts </code>&#13;
<code class="n">discount_rate</code> <code class="o">=</code> <code class="n">npr</code><code class="o">.</code><code class="n">uniform</code><code class="p">(</code><code class="n">low</code><code class="o">=</code><code class="mf">0.06</code><code class="p">,</code> <code class="n">high</code><code class="o">=</code><code class="mf">0.10</code><code class="p">,</code> <code class="n">size</code><code class="o">=</code><code class="n">n</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># Specify how risk factors affect the project model</code>&#13;
<code class="n">labor_costs</code> <code class="o">=</code> <code class="n">effort</code> <code class="o">*</code> <code class="n">daily_rate</code>&#13;
<code class="n">technology_costs</code> <code class="o">=</code> <code class="n">fte</code> <code class="o">*</code> <code class="n">technology_charges</code>&#13;
<code class="n">overhead_costs</code> <code class="o">=</code> <code class="n">fte</code> <code class="o">*</code> <code class="n">overhead_charges</code>&#13;
<code class="n">revenues</code> <code class="o">=</code> <code class="n">price</code> <code class="o">*</code> <code class="n">units</code>&#13;
<code class="c1"># Duration determines the number of days the project will take to complete</code>&#13;
<code class="c1"># assuming no interruption. Different from the elapsed time of the project.</code>&#13;
<code class="n">duration</code> <code class="o">=</code> <code class="n">effort</code><code class="o">/</code><code class="n">fte</code>&#13;
&#13;
<code class="c1"># Specify target_value</code>&#13;
<code class="n">free_cash_flow</code> <code class="o">=</code> <code class="p">(</code><code class="n">revenues</code> <code class="o">-</code> <code class="n">labor_costs</code> <code class="o">-</code> <code class="n">technology_costs</code> <code class="o">-</code> <code class="n">overhead_costs</code><code class="p">)</code> &#13;
                 <code class="o">*</code> <code class="p">(</code><code class="mi">1</code> <code class="o">-</code> <code class="n">tax_rate</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># Simulate project NPV assuming initial FCF=0</code>&#13;
<code class="n">npv</code> <code class="o">=</code> <code class="n">free_cash_flow</code><code class="o">/</code><code class="p">(</code><code class="mi">1</code> <code class="o">+</code> <code class="n">discount_rate</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># Convert numpy array to pandas DataFrame for easier analysis</code>&#13;
<code class="n">NPV</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">npv</code><code class="p">,</code> <code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s1">'NPV'</code><code class="p">])</code>&#13;
<code class="c1"># Estimate project duration in days</code>&#13;
<code class="n">Duration</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">duration</code><code class="p">,</code> <code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s1">'Days'</code><code class="p">])</code>&#13;
&#13;
<code class="c1"># Plot histogram of NPV distribution</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">hist</code><code class="p">(</code><code class="n">NPV</code><code class="p">[</code><code class="s1">'NPV'</code><code class="p">],</code> <code class="n">bins</code><code class="o">=</code><code class="mi">50</code><code class="p">),</code> <code class="n">plt</code><code class="o">.</code><code class="n">title</code> <code class="p">(</code><code class="s1">'Distribution of Project NPV'</code><code class="p">),</code> &#13;
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s1">'Project NPV'</code><code class="p">),</code> &#13;
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s1">'Frequency'</code><code class="p">),</code> <code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">();</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="n">NPV</code><code class="o">.</code><code class="n">describe</code><code class="p">()</code><code class="o">.</code><code class="n">round</code><code class="p">())</code>&#13;
<code class="n">success_probability</code> <code class="o">=</code> <code class="nb">sum</code><code class="p">(</code><code class="n">NPV</code><code class="p">[</code><code class="s1">'NPV'</code><code class="p">]</code> <code class="o">&gt;</code> <code class="mi">0</code><code class="p">)</code><code class="o">/</code><code class="n">n</code> <code class="o">*</code><code class="mi">100</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="s1">'There is a </code><code class="si">{0}</code><code class="s1">% probability that the project will have a positive NPV.'</code>&#13;
<code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="nb">round</code><code class="p">(</code><code class="n">success_probability</code><code class="p">)))</code>&#13;
<code class="c1"># Plot histogram of project duration distribution</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">hist</code><code class="p">(</code><code class="n">Duration</code><code class="p">[</code><code class="s1">'Days'</code><code class="p">],</code> <code class="n">bins</code><code class="o">=</code><code class="mi">50</code><code class="p">),</code> &#13;
<code class="n">plt</code><code class="o">.</code><code class="n">title</code> <code class="p">(</code><code class="s1">'Distribution of Project Duration'</code><code class="p">),</code> <code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s1">'Days'</code><code class="p">),</code> &#13;
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s1">'Frequency'</code><code class="p">),</code> <code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">();</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="n">Duration</code><code class="o">.</code><code class="n">describe</code><code class="p">()</code><code class="o">.</code><code class="n">round</code><code class="p">())</code></pre>&#13;
<figure class="informal"><div class="figure">&#13;
<img alt="Image" src="assets/pmlf_03in03.png"/>&#13;
<h6/>&#13;
</div></figure>&#13;
<figure class="informal"><div class="figure">&#13;
<img alt="Image" src="assets/pmlf_03in04.png"/>&#13;
<h6/>&#13;
</div></figure>&#13;
<p>Note that we did not discount the FCF distributions at the risk-free rate. The risk-free rate is the interest rate on a government security such as the US 10-year note. This is a common mistake in NPV simulations, but is incorrect, since each simulation is estimating the expected value of the FCF. Each FCF needs to be discounted at the risk-adjusted discount rate to account for the total risk of the project.</p>&#13;
<p>The distribution of risk-adjusted NPVs in the code output needs to be interpreted with caution. Using the dispersion of NPVs to make decisions would double count project risk. Using dispersion of NPVs adjusted at the risk-free rate to account for total risk has no sound theoretical basis in corporate finance.<a contenteditable="false" data-primary="" data-startref="ch03-vals" data-type="indexterm" id="id814"/><a contenteditable="false" data-primary="" data-startref="ch03-vals2" data-type="indexterm" id="id815"/><a contenteditable="false" data-primary="" data-startref="ch03-vals3" data-type="indexterm" id="id816"/><a contenteditable="false" data-primary="" data-startref="ch03-vals4" data-type="indexterm" id="id817"/><a contenteditable="false" data-primary="" data-startref="ch03-vals5" data-type="indexterm" id="id818"/><a contenteditable="false" data-primary="" data-startref="ch03-vals6" data-type="indexterm" id="id819"/><a contenteditable="false" data-primary="" data-startref="ch03-vals7" data-type="indexterm" id="id820"/></p>&#13;
</div></section>&#13;
<section class="pagebreak-before" data-pdf-bookmark="Building a Sound MCS" data-type="sect1"><div class="sect1" id="building_a_sound_mcs">&#13;
<h1>Building a Sound MCS</h1>&#13;
<p>To harness the power of MCS<a contenteditable="false" data-primary="Monte Carlo simulation (MCS)" data-secondary="building a sound MCS" data-type="indexterm" id="id821"/> to solve complex financial and investment-related problems in the face of uncertainty, it is important that you follow a sound and replicable process. Here is a 10-step process for doing just that:</p>&#13;
<ol>&#13;
<li><p>Formulate how target/dependent variables of your model are affected by features/independent variables, also called risk factors in finance.</p></li>&#13;
<li><p>Specify the probability distribution of each risk factor. Some common ones include Gaussian, Student’s t-distribution, Cauchy, and binomial probability <span class="keep-together">distributions.</span></p></li>&#13;
<li><p>Specify initial values and how time is discretized, such as in seconds, minutes, days, weeks, or years.</p></li>&#13;
<li><p>Specify how each risk factor changes over time, if at all.</p></li>&#13;
<li><p>Specify how each risk factor is affected by other risk factors. This is important since correlation among risk factors can incorrectly amplify or dampen effects. This phenomenon is also called multicollinearity.</p></li>&#13;
<li><p>Let the computer draw a random value from the probability distribution of each independent risk factor.</p></li>&#13;
<li><p>Compute the value of each risk factor based on that random value.</p></li>&#13;
<li><p>Compute target/dependent variables based on the computed value of all risk <span class="keep-together">factors.</span></p></li>&#13;
<li><p>Iterate steps 6–8 as many times as necessary.</p></li>&#13;
<li><p>Record and analyze descriptive statistics of all iterations.</p></li>&#13;
</ol>&#13;
<p>The power of MCS is that it transforms a complex, intractable problem that involves integral calculus into a simple one of descriptive statistics with its sampling algorithms. However, there are many challenges to building a sound MCS. Here are the most important ones:<a contenteditable="false" data-primary="Monte Carlo simulation (MCS)" data-secondary="building a sound MCS" data-tertiary="challenges of" data-type="indexterm" id="id822"/></p>&#13;
<ul>&#13;
<li><p>Specifying how each independent variable changes over time. Serial correlation (also known as autocorrelation) is the correlation of a variable with an instance of itself in the past. This correlation is not constant and usually changes over time, especially in financial markets.</p></li>&#13;
<li><p>Specifying how each feature/independent variable is affected by other independent variables of the model. Correlations among independent variables/risk factors usually change over time.</p></li>&#13;
<li><p>Fitting a theoretical probability distribution to the actual outcomes. Probability distributions of variables usually change over time. </p></li>&#13;
<li><p>Convergence to the best estimate is nonlinear, making it slow and costly. It may not occur quickly enough to be of any practical value to trading or investing.</p></li>&#13;
</ul>&#13;
<p>These challenges can be met as follows:</p>&#13;
<ul>&#13;
<li><p>Rigorous data analysis, domain knowledge, and industry expertise. You need to balance rigorous financial modeling with time, cost, and the effectiveness of the models that you produce.</p></li>&#13;
<li><p>Treat all financial models as flawed and imperfect guides. Don’t let the mathematical jargon intimidate or lull you into a false sense of security. Remember the adage “All models are wrong, but some are useful.”</p></li>&#13;
<li><p>Managing risk is of paramount importance. Always size capital positions appropriately, have wide error margins, and fallback plans if models fail.</p></li>&#13;
<li><p>Clearly, there is no substitute for managerial experience and business judgment. Rely on your common sense, be skeptical, and ask difficult questions of a model’s assumptions, inputs, and outputs.</p></li>&#13;
</ul>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="summary-id00023">&#13;
<h1>Summary</h1>&#13;
<p>Fundamentally, MCS is a set of numerical techniques that uses random sampling of probability distributions for computing approximate estimates or for simulating uncertainties of outcomes of a model. The central idea is to harness the statistical properties of randomness to develop approximate solutions to complex deterministic models and analytically intractable problems. MCS transforms a complex, often intractable, multidimensional problem in integral calculus into a much easier problem of descriptive statistics that any practitioner can use.</p>&#13;
<p>MCS is especially useful when there is no analytically tractable solution to a problem you are trying to solve. It enables you to quantify the probability and impact of all possible outcomes given your assumptions. It should be used when the traditional analysis of best-, worst-, and base-case scenarios may be inadequate for your decision making and risk management. MCS gives you a better understanding of the risk of complex financial models. Monte Carlo methods are one of the most powerful numerical tools and are pivotal to probabilistic machine learning.</p>&#13;
<p>In this chapter, we have applied MCS using independent random sampling. This involves randomly selecting samples from a probability distribution, with each sample being independent of any previous samples. This approach is efficient for simulating simple target probability distributions when samples are not correlated.</p>&#13;
<p>However, when dealing with complex target distributions and correlated samples, we have to use more advanced correlated random sampling methods. These dependent random sampling Monte Carlos are crucial to probabilistic machine learning. In <a data-type="xref" href="ch06.html#the_dangers_of_conventional_ai_systems">Chapter 6</a>, we will examine Markov chain Monte Carlo (MCMC) methods, which are powerful techniques for sampling from complex distributions with dependencies. In <a data-type="xref" href="ch07.html#probabilistic_machine_learning_with_gen">Chapter 7</a>, we will apply these methods to financial modeling using the PMC library.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="References" data-type="sect1"><div class="sect1" id="references-id00014">&#13;
<h1>References</h1>&#13;
<p>Brandimarte, Paolo. <em>Handbook in Monte Carlo Simulation: Applications in Financial Engineering, Risk Management, and Economics</em>. Hoboken, NJ: John Wiley &amp; Sons, 2014.</p>&#13;
<p>Cemgil, A. Taylan. “A Tutorial Introduction to Monte Carlo Methods, Markov Chain Monte Carlo and Particle Filtering.” In <em>Academic Press Library in Signal Processing: Volume 1: Signal Processing Theory and Machine Learning</em>, edited by Paulo S. R. Diniz, Johan A. K. Suykens, Rama Chellappa, and Sergios Theodoridis, 1065–1114. Oxford, UK: Elsevier, 2014.</p>&#13;
</div></section>&#13;
<div data-type="footnotes"><p data-type="footnote" id="ch03fn1"><sup><a href="ch03.html#ch03fn1-marker">1</a></sup> Paolo Brandimarte, “Introduction to Monte Carlo Methods,” in <em>Handbook in Monte Carlo Simulation: Applications in Financial Engineering, Risk Management, and Economics</em> (Hoboken, NJ: John Wiley &amp; Sons, 2014).</p><p data-type="footnote" id="ch03fn2"><sup><a href="ch03.html#ch03fn2-marker">2</a></sup> Adapted from an image on Wikimedia Commons.</p><p data-type="footnote" id="ch03fn3"><sup><a href="ch03.html#ch03fn3-marker">3</a></sup> Adapted from an image on Wikimedia Commons.</p><p data-type="footnote" id="ch03fn4"><sup><a href="ch03.html#ch03fn4-marker">4</a></sup> Adapted from an image on Wikimedia Commons.</p><p data-type="footnote" id="ch03fn5"><sup><a href="ch03.html#ch03fn5-marker">5</a></sup> Adapted from an image on Wikimedia Commons.</p><p data-type="footnote" id="ch03fn6"><sup><a href="ch03.html#ch03fn6-marker">6</a></sup> Adapted from an image on Wikimedia Commons.</p><p data-type="footnote" id="ch03fn7"><sup><a href="ch03.html#ch03fn7-marker">7</a></sup> Adapted from an image on Wikimedia Commons.</p><p data-type="footnote" id="ch03fn8"><sup><a href="ch03.html#ch03fn8-marker">8</a></sup> Adapted from an image on Wikimedia Commons.</p><p data-type="footnote" id="ch03fn9"><sup><a href="ch03.html#ch03fn9-marker">9</a></sup> A. Taylan Cemgil, “A Tutorial Introduction to Monte Carlo Methods, Markov Chain Monte Carlo and Particle Filtering,” in <em>Academic Press Library in Signal Processing: Volume 1: Signal Processing Theory and Machine Learning</em>, ed. Paulo S. R. Diniz, Johan A. K. Suykens, Rama Chellappa, and Sergios Theodoridis (Oxford, UK: Elsevier, 2014), 1065–1114.</p></div></div></section></body></html>