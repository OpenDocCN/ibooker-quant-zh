- en: Chapter 3\. Artificial Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 3 章\. 人工神经网络
- en: There are many different types of models used in machine learning. However,
    one class of machine learning models that stands out is artificial neural networks
    (ANNs). Given that artificial neural networks are used across all machine learning
    types, this chapter will cover the basics of ANNs.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中使用许多不同类型的模型。然而，一种突出的机器学习模型类别是人工神经网络（ANNs）。鉴于人工神经网络应用于所有类型的机器学习，本章将介绍ANN的基础知识。
- en: ANNs are computing systems based on a collection of connected units or nodes
    called artificial neurons, which loosely model the neurons in a biological brain.
    Each connection, like the synapses in a biological brain, can transmit a signal
    from one artificial neuron to another. An artificial neuron that receives a signal
    can process it and then signal additional artificial neurons connected to it.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: ANN 是基于一组连接单位或节点（称为人工神经元）的计算系统，它们松散地模拟生物大脑中的神经元。每个连接就像生物大脑中的突触一样，可以将信号从一个人工神经元传输到另一个。接收信号的人工神经元可以处理它，然后将信号传递给连接到它的其他人工神经元。
- en: '*Deep learning* involves the study of complex ANN-related algorithms. The complexity
    is attributed to elaborate patterns of how information flows throughout the model.
    Deep learning has the ability to represent the world as a nested hierarchy of
    concepts, with each concept defined in relation to a simpler concept. Deep learning
    techniques are extensively used in reinforcement learning and natural language
    processing applications that we will look at in Chapters [9](ch09.xhtml#Chapter9)
    and [10](ch10.xhtml#Chapter10).'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '*深度学习* 包括复杂的与ANN相关的算法研究。其复杂性归因于信息如何在模型中的精细模式中流动。深度学习能够将世界表示为概念的嵌套层次结构，每个概念都与更简单的概念相关定义。我们将在第
    [9](ch09.xhtml#Chapter9) 和 [10](ch10.xhtml#Chapter10) 章中详细探讨深度学习技术在强化学习和自然语言处理应用中的使用。'
- en: 'ANNs: Architecture, Training, and Hyperparameters'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ANN：架构、训练和超参数
- en: ANNs contain multiple neurons arranged in layers. An ANN goes through a training
    phase by comparing the modeled output to the desired output, where it learns to
    recognize patterns in data. Let us go through the components of ANNs.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ANN 包含多个层中排列的神经元。ANN 通过将建模输出与期望输出进行比较，通过训练阶段来学习在数据中识别模式。让我们详细介绍ANN的组成部分。
- en: Architecture
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 架构
- en: An ANN architecture comprises neurons, layers, and weights.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ANN 架构包括神经元、层和权重。
- en: Neurons
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经元
- en: The building blocks for ANNs are neurons (also known as artificial neurons,
    nodes, or perceptrons). Neurons have one or more inputs and one output. It is
    possible to build a network of neurons to compute complex logical propositions.
    Activation functions in these neurons create complicated, nonlinear functional
    mappings between the inputs and the output.^([2](ch03.xhtml#idm45174933836904))
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ANN 的构建模块是神经元（也称为人工神经元、节点或感知器）。神经元具有一个或多个输入和一个输出。可以构建神经元网络来计算复杂的逻辑命题。这些神经元中的激活函数创建输入和输出之间复杂的非线性功能映射。^([2](ch03.xhtml#idm45174933836904))
- en: As shown in [Figure 3-1](#SingleNeuron), a neuron takes an input (*x[1]*, *x[2]*…*x[n]*),
    applies the learning parameters to generate a weighted sum (*z*), and then passes
    that sum to an activation function (*f*) that computes the output *f(z)*.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图 3-1](#SingleNeuron)所示，一个神经元接受输入 (*x[1]*, *x[2]*…*x[n]*)，应用学习参数生成加权和 (*z*)，然后将该和传递给计算输出
    *f(z)* 的激活函数 *f*。
- en: '![mlbf 0301](Images/mlbf_0301.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![mlbf 0301](Images/mlbf_0301.png)'
- en: Figure 3-1\. An artificial neuron
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-1\. 人工神经元
- en: Layers
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 层
- en: The output *f(z)* from a single neuron (as shown in [Figure 3-1](#SingleNeuron))
    will not be able to model complex tasks. So, in order to handle more complex structures,
    we have multiple layers of such neurons. As we keep stacking neurons horizontally
    and vertically, the class of functions we can get becomes increasing complex.
    [Figure 3-2](#Layers) shows an architecture of an ANN with an input layer, an
    output layer, and a hidden layer.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 单个神经元的输出 *f(z)*（如[图 3-1](#SingleNeuron)所示）无法对复杂任务进行建模。因此，为了处理更复杂的结构，我们使用多层这样的神经元。随着我们在水平和垂直方向上堆叠神经元，我们可以得到的函数类变得越来越复杂。[图 3-2](#Layers)展示了具有输入层、输出层和隐藏层的ANN架构。
- en: '![mlbf 0302](Images/mlbf_0302.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![mlbf 0302](Images/mlbf_0302.png)'
- en: Figure 3-2\. Neural network architecture
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-2\. 神经网络架构
- en: Input layer
  id: totrans-17
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 输入层
- en: The input layer takes input from the dataset and is the exposed part of the
    network. A neural network is often drawn with an input layer of one neuron per
    input value (or column) in the dataset. The neurons in the input layer simply
    pass the input value though to the next layer.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 输入层从数据集获取输入并且是网络的暴露部分。神经网络通常以数据集中每个输入值（或列）的一个神经元作为输入层来绘制。输入层中的神经元只是将输入值传递给下一层。
- en: Hidden layers
  id: totrans-19
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 隐藏层
- en: Layers after the input layer are called hidden layers because they are not directly
    exposed to the input. The simplest network structure is to have a single neuron
    in the hidden layer that directly outputs the value.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 输入层之后的层称为隐藏层，因为它们不直接暴露给输入。最简单的网络结构是在隐藏层中有一个单独的神经元直接输出该值。
- en: A multilayer ANN is capable of solving more complex machine learning–related
    tasks due to its hidden layer(s). Given increases in computing power and efficient
    libraries, neural networks with many layers can be constructed. ANNs with many
    hidden layers (more than three) are known as *deep neural networks*. Multiple
    hidden layers allow deep neural networks to learn features of the data in a so-called
    feature hierarchy, because simple features recombine from one layer to the next
    to form more complex features. ANNs with many layers pass input data (features)
    through more mathematical operations than do ANNs with few layers and are therefore
    more computationally intensive to train.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 多层ANN能够解决更复杂的与机器学习相关的任务，因为它具有隐藏层。随着计算能力的增加和高效的库的出现，可以构建具有许多层的神经网络。具有许多隐藏层（超过三层）的ANN被称为*深度神经网络*。多个隐藏层允许深度神经网络学习数据的特征，因为简单的特征在一层到下一层的重新组合形成更复杂的特征。具有许多层的ANN将输入数据（特征）通过更多的数学运算而不是具有较少层的ANN，并因此在训练过程中需要更多的计算量。
- en: Output layer
  id: totrans-22
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 输出层
- en: The final layer is called the output layer; it is responsible for outputting
    a value or vector of values that correspond to the format required to solve the
    problem.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 最终层称为输出层；它负责输出与解决问题所需格式相对应的值或值向量。
- en: Neuron weights
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经元权重
- en: A neuron weight represents the strength of the connection between units and
    measures the influence the input will have on the output. If the weight from neuron
    one to neuron two has greater magnitude, it means that neuron one has a greater
    influence over neuron two. Weights near zero mean changing this input will not
    change the output. Negative weights mean increasing this input will decrease the
    output.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元权重表示单元之间连接的强度，并测量输入对输出的影响。如果从神经元一到神经元二的权重具有较大的幅度，则意味着神经元一对神经元二的影响较大。接近零的权重意味着改变此输入不会改变输出。负权重意味着增加此输入会减少输出。
- en: Training
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练
- en: Training a neural network basically means calibrating all of the weights in
    the ANN. This optimization is performed using an iterative approach involving
    forward propagation and backpropagation steps.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 训练神经网络基本上意味着校准ANN中的所有权重。这种优化是使用涉及前向传播和反向传播步骤的迭代方法执行的。
- en: Forward propagation
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 前向传播
- en: Forward propagation is a process of feeding input values to the neural network
    and getting an output, which we call *predicted value*. When we feed the input
    values to the neural network’s first layer, it goes without any operations. The
    second layer takes values from the first layer and applies multiplication, addition,
    and activation operations before passing this value to the next layer. The same
    process repeats for any subsequent layers until an output value from the last
    layer is received.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 前向传播是将输入值馈送到神经网络并获得输出的过程，我们称之为*预测值*。当我们将输入值馈送到神经网络的第一层时，它会直接通过，不进行任何操作。第二层从第一层获取值，并在将该值传递到下一层之前应用乘法、加法和激活操作。同样的过程对任何后续层都重复，直到从最后一层接收到输出值。
- en: Backpropagation
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 反向传播
- en: After forward propagation, we get a predicted value from the ANN. Suppose the
    desired output of a network is *Y* and the predicted value of the network from
    forward propagation is *Y′*. The difference between the predicted output and the
    desired output (*Y*–*Y′* ) is converted into the loss (or cost) function *J(w)*,
    where *w* represents the weights in ANN.^([3](ch03.xhtml#idm45174933705544)) The
    goal is to optimize the loss function (i.e., make the loss as small as possible)
    over the training set.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 经过前向传播，我们从人工神经网络得到预测值。假设网络的期望输出是*Y*，前向传播的预测值是*Y′*。预测输出与期望输出的差异（*Y*–*Y′*）被转换为损失（或成本）函数*J(w)*，其中*w*表示人工神经网络中的权重。^([3](ch03.xhtml#idm45174933705544))
    目标是优化损失函数（即使损失尽可能小）在训练集上的表现。
- en: The optimization method used is *gradient descent*. The goal of the gradient
    descent method is to find the gradient of *J(w)* with respect to *w* at the current
    point and take a small step in the direction of the negative gradient until the
    minimum value is reached, as shown in [Figure 3-3](#GradDesc).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 使用的优化方法是*梯度下降*。梯度下降方法的目标是找到*J(w)*关于*w*的梯度，并沿着负梯度方向迈出一小步，直到达到最小值，如[图3-3](#GradDesc)所示。
- en: '![mlbf 0303](Images/mlbf_0303.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![mlbf 0303](Images/mlbf_0303.png)'
- en: Figure 3-3\. Gradient descent
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-3\. 梯度下降
- en: In an ANN, the function *J(w)* is essentially a composition of multiple layers,
    as explained in the preceding text. So, if layer one is represented as function
    *p()*, layer two as *q()*, and layer three as *r()*, then the overall function
    is *J(w)=r(q(p())).* *w* consists of all weights in all three layers. We want
    to find the gradient of *J(w)* with respect to each component of *w*.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在人工神经网络中，函数*J(w)*本质上是多个层的组合，如前文所述。因此，如果将第一层表示为函数*p()*，第二层表示为*q()*，第三层表示为*r()*，那么整体函数为*J(w)=r(q(p()))*。*w*包含所有三层中的所有权重。我们希望找到*J(w)*关于*w*的每个分量的梯度。
- en: Skipping the mathematical details, the above essentially implies that the gradient
    of a component *w* in the first layer would depend on the gradients in the second
    and third layers. Similarly, the gradients in the second layer will depend on
    the gradients in the third layer. Therefore, we start computing the derivatives
    in the reverse direction, starting with the last layer, and use backpropagation
    to compute gradients of the previous layer.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 略过数学细节，以上实质上意味着第一层中某个分量*w*的梯度将取决于第二层和第三层中的梯度。类似地，第二层中的梯度将取决于第三层中的梯度。因此，我们从最后一层开始反向计算导数，并使用反向传播计算前一层的梯度。
- en: Overall, in the process of backpropagation, the model error (difference between
    predicted and desired output) is propagated back through the network, one layer
    at a time, and the weights are updated according to the amount they contributed
    to the error.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 总体上，在反向传播过程中，模型误差（预测输出与期望输出之间的差异）逐层传播回网络，并根据它们对误差的贡献量更新权重。
- en: Almost all ANNs use gradient descent and backpropagation. Backpropagation is
    one of the cleanest and most efficient ways to find the gradient.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有人工神经网络都使用梯度下降和反向传播。反向传播是找到梯度的一种清晰高效的方法之一。
- en: Hyperparameters
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超参数
- en: '*Hyperparameters* are the variables that are set before the training process,
    and they cannot be learned during training. ANNs have a large number of hyperparameters,
    which makes them quite flexible. However, this flexibility makes the model tuning
    process difficult. Understanding the hyperparameters and the intuition behind
    them helps give an idea of what values are reasonable for each hyperparameter
    so we can restrict the search space. Let’s start with the number of hidden layers
    and nodes.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*超参数*是在训练过程之前设置的变量，无法在训练中学习。人工神经网络具有大量超参数，这使得它们非常灵活。然而，这种灵活性使得模型调整过程变得困难。理解超参数及其背后的直觉有助于确定每个超参数的合理值，从而限制搜索空间。让我们从隐藏层和节点的数量开始。'
- en: Number of hidden layers and nodes
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 隐藏层和节点数量
- en: More hidden layers or nodes per layer means more parameters in the ANN, allowing
    the model to fit more complex functions. To have a trained network that generalizes
    well, we need to pick an optimal number of hidden layers, as well as of the nodes
    in each hidden layer. Too few nodes and layers will lead to high errors for the
    system, as the predictive factors might be too complex for a small number of nodes
    to capture. Too many nodes and layers will overfit to the training data and not
    generalize well.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 更多的隐藏层或每层节点意味着ANN中有更多的参数，使模型能够拟合更复杂的函数。为了有一个泛化能力良好的训练好的网络，我们需要选择一个最佳的隐藏层数量，以及每个隐藏层中的节点数量。节点和层数量过少会导致系统错误率高，因为预测因素可能对于少数节点来说过于复杂，难以捕捉。节点和层数量过多则会导致对训练数据过拟合，泛化能力差。
- en: There is no hard-and-fast rule to decide the number of layers and nodes.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 没有硬性规定来决定层数和节点的数量。
- en: The number of hidden layers primarily depends on the complexity of the task.
    Very complex tasks, such as large image classification or speech recognition,
    typically require networks with dozens of layers and a huge amount of training
    data. For the majority of the problems, we can start with just one or two hidden
    layers and then gradually ramp up the number of hidden layers until we start overfitting
    the training set.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层数量主要取决于任务的复杂性。非常复杂的任务，如大规模图像分类或语音识别，通常需要具有数十层和大量训练数据的网络。对于大多数问题，我们可以从只有一个或两个隐藏层开始，然后逐渐增加隐藏层数量，直到开始过拟合训练集。
- en: The number of hidden nodes should have a relationship to the number of input
    and output nodes, the amount of training data available, and the complexity of
    the function being modeled. As a rule of thumb, the number of hidden nodes in
    each layer should be somewhere between the size of the input layer and the size
    of the output layer, ideally the mean. The number of hidden nodes shouldn’t exceed
    twice the number of input nodes in order to avoid overfitting.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏节点的数量应与输入和输出节点的数量、可用的训练数据量以及正在建模的函数复杂性有关。作为经验法则，每层隐藏节点的数量应该在输入层大小和输出层大小之间，理想情况下应接近平均值。每层隐藏节点的数量不应超过输入节点数量的两倍，以避免过拟合。
- en: Learning rate
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习率
- en: When we train ANNs, we use many iterations of forward propagation and backpropagation
    to optimize the weights. At each iteration we calculate the derivative of the
    loss function with respect to each weight and subtract it from that weight. The
    learning rate determines how quickly or slowly we want to update our weight (parameter)
    values. This learning rate should be high enough so that it converges in a reasonable
    amount of time. Yet it should be low enough so that it finds the minimum value
    of the loss function.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们训练ANN时，我们使用前向传播和反向传播的多次迭代来优化权重。在每次迭代中，我们计算损失函数对每个权重的导数，并从该权重中减去。学习率决定了我们希望更新权重值的速度。学习率应该足够高，以便在合理的时间内收敛。但它应该足够低，以便找到损失函数的最小值。
- en: Activation functions
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 激活函数
- en: Activation functions (as shown in [Figure 3-1](#SingleNeuron)) refer to the
    functions used over the weighted sum of inputs in ANNs to get the desired output.
    Activation functions allow the network to combine the inputs in more complex ways,
    and they provide a richer capability in the relationship they can model and the
    output they can produce. They decide which neurons will be activated—that is,
    what information is passed to further layers.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数（如在[图3-1](#SingleNeuron)中所示）指的是ANN中用于获取期望输出的加权输入的函数。激活函数允许网络以更复杂的方式组合输入，并在建模关系和生成输出方面提供更丰富的能力。它们决定哪些神经元将被激活，即传递给更深层的信息。
- en: 'Without activation functions, ANNs lose a bulk of their representation learning
    power. There are several activation functions. The most widely used are as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 没有激活函数，ANN将失去其表示学习能力的大部分功能。有几种激活函数。最广泛使用的如下：
- en: Linear (identity) function
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 线性（恒等）函数
- en: Represented by the equation of a straight line (i.e., <math alttext="f left-parenthesis
    x right-parenthesis equals m x plus c"><mrow><mi>f</mi> <mo>(</mo> <mi>x</mi>
    <mo>)</mo> <mo>=</mo> <mi>m</mi> <mi>x</mi> <mo>+</mo> <mi>c</mi></mrow></math>
    ), where activation is proportional to the input. If we have many layers, and
    all the layers are linear in nature, then the final activation function of the
    last layer is the same as the linear function of the first layer. The range of
    a linear function is *–inf* to *+inf*.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 由直线方程表示（即，<math alttext="f left-parenthesis x right-parenthesis equals m x plus
    c"><mrow><mi>f</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo> <mo>=</mo> <mi>m</mi> <mi>x</mi>
    <mo>+</mo> <mi>c</mi></mrow></math> ），其中激活与输入成正比。如果我们有多个层，并且所有层都是线性的，那么最后一层的激活函数与第一层的线性函数相同。线性函数的范围是
    *–inf* 到 *+inf*。
- en: Sigmoid function
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 函数
- en: Refers to a function that is projected as an S-shaped graph (as shown in [Figure 3-4](#ActFunc)).
    It is represented by the mathematical equation <math display="inline"><mrow><mi>f</mi>
    <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <mn>1</mn> <mo>/</mo>
    <mrow><mo>(</mo> <mn>1</mn> <mo>+</mo> <msup><mi>e</mi> <mrow><mo>–</mo><mi>x</mi></mrow></msup>
    <mo>)</mo></mrow></mrow></math> and ranges from 0 to 1\. A large positive input
    results in a large positive output; a large negative input results in a large
    negative output. It is also referred to as logistic activation function.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 指的是作为 S 形图像投射的函数（如图 3-4 所示）。其数学方程为 <math display="inline"><mrow><mi>f</mi> <mrow><mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <mn>1</mn> <mo>/</mo> <mrow><mo>(</mo>
    <mn>1</mn> <mo>+</mo> <msup><mi>e</mi> <mrow><mo>–</mo><mi>x</mi></mrow></msup>
    <mo>)</mo></mrow></mrow></math> ，范围从 0 到 1。大正输入导致大正输出；大负输入导致大负输出。它也称为 logistic
    激活函数。
- en: Tanh function
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 双曲正切函数
- en: Similar to sigmoid activation function with a mathematical equation <math><mrow><mi>T</mi>
    <mi>a</mi> <mi>n</mi> <mi>h</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo> <mo>=</mo> <mn>2</mn>
    <mi>S</mi> <mi>i</mi> <mi>g</mi> <mi>m</mi> <mi>o</mi> <mi>i</mi> <mi>d</mi> <mo>(</mo>
    <mn>2</mn> <mi>x</mi> <mo>)</mo> <mo>–</mo> <mn>1</mn></mrow></math> , where *Sigmoid*
    represents the `sigmoid` function discussed above. The output of this function
    ranges from –1 to 1, with an equal mass on both sides of the zero-axis, as shown
    in [Figure 3-4](#ActFunc).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 与上述 sigmoid 激活函数类似，具有数学方程 <math><mrow><mi>T</mi> <mi>a</mi> <mi>n</mi> <mi>h</mi>
    <mo>(</mo> <mi>x</mi> <mo>)</mo> <mo>=</mo> <mn>2</mn> <mi>S</mi> <mi>i</mi> <mi>g</mi>
    <mi>m</mi> <mi>o</mi> <mi>i</mi> <mi>d</mi> <mo>(</mo> <mn>2</mn> <mi>x</mi> <mo>)</mo>
    <mo>–</mo> <mn>1</mn></mrow></math> ，其中 *Sigmoid* 表示上述讨论的 `sigmoid` 函数。此函数的输出范围为
    –1 到 1，在零轴两侧具有相等的质量，如图 3-4 所示。
- en: ReLU function
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU 函数
- en: ReLU stands for the Rectified Linear Unit and is represented as <math alttext="f
    left-parenthesis x right-parenthesis equals m a x left-parenthesis x comma 0 right-parenthesis"><mrow><mi>f</mi>
    <mo>(</mo> <mi>x</mi> <mo>)</mo> <mo>=</mo> <mi>m</mi> <mi>a</mi> <mi>x</mi> <mo>(</mo>
    <mi>x</mi> <mo>,</mo> <mn>0</mn> <mo>)</mo></mrow></math> . So, if the input is
    a positive number, the function returns the number itself, and if the input is
    a negative number, then the function returns zero. It is the most commonly used
    function because of its simplicity.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU 代表修正线性单元，表示为 <math alttext="f left-parenthesis x right-parenthesis equals
    m a x left-parenthesis x comma 0 right-parenthesis"><mrow><mi>f</mi> <mo>(</mo>
    <mi>x</mi> <mo>)</mo> <mo>=</mo> <mi>m</mi> <mi>a</mi> <mi>x</mi> <mo>(</mo> <mi>x</mi>
    <mo>,</mo> <mn>0</mn> <mo>)</mo></mrow></math> 。因此，如果输入是正数，则函数返回该数本身，如果输入是负数，则函数返回零。由于其简单性，它是最常用的函数。
- en: '[Figure 3-4](#ActFunc) shows a summary of the activation functions discussed
    in this section.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-4](#ActFunc) 显示了本节讨论的激活函数的总结。'
- en: '![mlbf 0304](Images/mlbf_0304.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![mlbf 0304](Images/mlbf_0304.png)'
- en: Figure 3-4\. Activation functions
  id: totrans-61
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-4\. 激活函数
- en: There is no hard-and-fast rule for activation function selection. The decision
    completely relies on the properties of the problem and the relationships being
    modeled. We can try different activation functions and select the one that helps
    provide faster convergence and a more efficient training process. The choice of
    activation function in the output layer is strongly constrained by the type of
    problem that is modeled.^([4](ch03.xhtml#idm45174933620152))
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数的选择没有硬性规定。决策完全依赖于问题的性质和建模的关系。我们可以尝试不同的激活函数，并选择帮助提供更快收敛和更高效训练过程的激活函数。输出层的激活函数的选择在很大程度上受到建模问题类型的限制。^([4](ch03.xhtml#idm45174933620152))
- en: Cost functions
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 成本函数
- en: 'Cost functions (also known as loss functions) are a measure of the ANN performance,
    measuring how well the ANN fits empirical data. The two most common cost functions
    are:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 成本函数（也称为损失函数）是ANN性能的一种度量，用于衡量ANN对经验数据的拟合程度。最常见的两种成本函数是：
- en: Mean squared error (MSE)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 均方误差（MSE）
- en: This is the cost function used primarily for regression problems, where output
    is a continuous value. MSE is measured as the average of the squared difference
    between predictions and actual observation. MSE is described further in [Chapter 4](ch04.xhtml#Chapter4).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这是主要用于回归问题的损失函数，其中输出是连续值。MSE被定义为预测值与实际观察值之间差异的平方的平均值。在[第四章](ch04.xhtml#Chapter4)中进一步描述了MSE。
- en: Cross-entropy (or *log loss*)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵（或*log loss*）
- en: This cost function is used primarily for classification problems, where output
    is a probability value between zero and one. Cross-entropy loss increases as the
    predicted probability diverges from the actual label. A perfect model would have
    a cross-entropy of zero.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这个成本函数主要用于分类问题，其中输出是介于零和一之间的概率值。交叉熵损失随着预测概率与实际标签的偏差增大而增加。一个完美的模型的交叉熵为零。
- en: Optimizers
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优化器
- en: 'Optimizers update the weight parameters to minimize the loss function.^([5](ch03.xhtml#idm45174933603352))
    Cost function acts as a guide to the terrain, telling the optimizer if it is moving
    in the right direction to reach the global minimum. Some of the common optimizers
    are as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器更新权重参数以最小化损失函数。^([5](ch03.xhtml#idm45174933603352)) 成本函数作为地形的指南，告诉优化器是否朝着达到全局最小值的正确方向移动。以下是一些常见的优化器：
- en: Momentum
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 动量
- en: The *momentum optimizer* looks at previous gradients in addition to the current
    step. It will take larger steps if the previous updates and the current update
    move the weights in the same direction (gaining momentum). It will take smaller
    steps if the direction of the gradient is opposite. A clever way to visualize
    this is to think of a ball rolling down a valley—it will gain momentum as it approaches
    the valley bottom.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '*动量优化器*除了当前步骤外，还查看先前的梯度。如果先前的更新和当前更新将权重移动到相同方向（增加动量），则会采取较大步长。如果梯度方向相反，则采取较小步长。可以将这种情况巧妙地想象成一个球在山谷中滚动——它在接近山谷底部时会获得动量。'
- en: AdaGrad (Adaptive Gradient Algorithm)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: AdaGrad（自适应梯度算法）
- en: '*AdaGrad* adapts the learning rate to the parameters, performing smaller updates
    for parameters associated with frequently occurring features, and larger updates
    for parameters associated with infrequent features.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*AdaGrad*根据参数调整学习率，对于频繁出现的特征，执行较小的更新，并对于不频繁出现的特征执行较大的更新。'
- en: RMSProp
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: RMSProp
- en: '*RMSProp* stands for Root Mean Square Propagation. In RMSProp, the learning
    rate gets adjusted automatically, and it chooses a different learning rate for
    each parameter.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '*RMSProp*代表Root Mean Square Propagation。在RMSProp中，学习率会自动调整，并为每个参数选择不同的学习率。'
- en: Adam (Adaptive Moment Estimation)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Adam（自适应矩估计）
- en: '*Adam* combines the best properties of the AdaGrad and RMSProp algorithms to
    provide an optimization and is one of the most popular gradient descent optimization
    algorithms.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*Adam*结合了AdaGrad和RMSProp算法的最佳特性，提供了一种优化方式，是最流行的梯度下降优化算法之一。'
- en: Epoch
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Epoch
- en: One round of updating the network for the entire training dataset is called
    an *epoch*. A network may be trained for tens, hundreds, or many thousands of
    epochs depending on the data size and computational constraints.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 将整个训练数据集更新一轮称为*epoch*。根据数据大小和计算约束，网络可能会训练数十次、数百次，甚至数千次*epoch*。
- en: Batch size
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Batch size
- en: The batch size is the number of training examples in one forward/backward pass.
    A batch size of 32 means that 32 samples from the training dataset will be used
    to estimate the error gradient before the model weights are updated. The higher
    the batch size, the more memory space is needed.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Batch size是一次前向/反向传递中的训练样本数量。批量大小为32意味着在更新模型权重之前，将使用来自训练数据集的32个样本来估计误差梯度。批量大小越大，所需的内存空间就越多。
- en: Creating an Artificial Neural Network Model in Python
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Python中创建人工神经网络模型
- en: In [Chapter 2](ch02.xhtml#Chapter2) we discussed the steps for end-to-end model
    development in Python. In this section, we dig deeper into the steps involved
    in building an ANN-based model in Python.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第二章](ch02.xhtml#Chapter2)中，我们讨论了在Python中进行端到端模型开发的步骤。在本节中，我们深入探讨了在Python中构建基于ANN的模型所涉及的步骤。
- en: Our first step will be to look at Keras, the Python package specifically built
    for ANN and deep learning.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一步将是查看Keras，这是专为人工神经网络（ANN）和深度学习而构建的Python软件包。
- en: Installing Keras and Machine Learning Packages
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装Keras和机器学习包
- en: There are several Python libraries that allow building ANN and deep learning
    models easily and quickly without getting into the details of underlying algorithms.
    Keras is one of the most user-friendly packages that enables an efficient numerical
    computation related to ANNs. Using Keras, complex deep learning models can be
    defined and implemented in a few lines of code. We will primarily be using Keras
    packages for implementing deep learning models in several of the book’s case studies.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个Python库可以轻松快速地构建ANN和深度学习模型，而无需深入了解底层算法的细节。Keras是最用户友好的软件包之一，可以有效进行与ANN相关的数值计算。使用Keras，可以在几行代码中定义和实现复杂的深度学习模型。我们将主要使用Keras软件包来实现本书的几个案例研究中的深度学习模型。
- en: '[Keras](https://keras.io) is simply a wrapper around more complex numerical
    computation engines such as [TensorFlow](https://www.tensorflow.org) and [Theano](https://oreil.ly/-XFJP).
    In order to install Keras, TensorFlow or Theano needs to be installed first.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[Keras](https://keras.io)只是[TensorFlow](https://www.tensorflow.org)和[Theano](https://oreil.ly/-XFJP)等更复杂的数值计算引擎的一个包装器。要安装Keras，必须先安装TensorFlow或Theano。'
- en: This section describes the steps to define and compile an ANN-based model in
    Keras, with a focus on the following steps.^([6](ch03.xhtml#idm45174933565816))
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述了在Keras中定义和编译基于ANN的模型的步骤，重点放在以下步骤上。^([6](ch03.xhtml#idm45174933565816))
- en: Importing the packages
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 导入包
- en: 'Before you can start to build an ANN model, you need to import two modules
    from the Keras package: `Sequential` and `Dense`:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在您开始构建ANN模型之前，您需要从Keras软件包中导入两个模块：`Sequential`和`Dense`：
- en: '[PRE0]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Loading data
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 载入数据
- en: 'This example makes use of the `random` module of NumPy to quickly generate
    some data and labels to be used by ANN that we build in the next step. Specifically,
    an array with size *(1000,10)* is first constructed. Next, we create a labels
    array that consists of zeros and ones with a size *(1000,1)*:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例使用NumPy的`random`模块快速生成一些数据和标签，供我们在下一步中构建的ANN使用。具体来说，首先构建一个大小为*(1000,10)*的数组。接下来，我们创建一个包含零和一的标签数组，大小为*(1000,1)*：
- en: '[PRE1]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Model construction—defining the neural network architecture
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型构建-定义神经网络架构
- en: A quick way to get started is to use the Keras Sequential model, which is a
    linear stack of layers. We create a Sequential model and add layers one at a time
    until the network topology is finalized. The first thing to get right is to ensure
    the input layer has the right number of inputs. We can specify this when creating
    the first layer. We then select a dense or fully connected layer to indicate that
    we are dealing with an input layer by using the argument `input_dim`.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 快速入门的一种方式是使用Keras的Sequential模型，它是层的线性堆栈。我们创建一个Sequential模型，并一次添加一个层，直到网络拓扑结构最终确定。正确的第一步是确保输入层具有正确数量的输入。我们可以在创建第一层时指定这一点。然后，我们选择一个密集或全连接层，通过使用参数`input_dim`来指示我们正在处理一个输入层。
- en: We add a layer to the model with the `add()` function, and the number of nodes
    in each layer is specified. Finally, another dense layer is added as an output
    layer.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`add()`函数向模型添加一层，并指定每层中的节点数。最后，另一个密集层被添加为输出层。
- en: 'The architecture for the model shown in [Figure 3-5](#ANNArchitecture) is as
    follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图[3-5](#ANNArchitecture)中显示的模型架构如下：
- en: The model expects rows of data with 10 variables (`input_dim_=10` argument).
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模型期望具有10个变量的数据行（`input_dim_=10`参数）。
- en: The first hidden layer has 32 nodes and uses the `relu` activation function.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个隐藏层有32个节点，并使用`relu`激活函数。
- en: The second hidden layer has 32 nodes and uses the `relu` activation function.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个隐藏层有32个节点，并使用`relu`激活函数。
- en: The output layer has one node and uses the `sigmoid` activation function.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出层有一个节点，并使用`sigmoid`激活函数。
- en: '![mlbf 0305](Images/mlbf_0305.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![mlbf 0305](Images/mlbf_0305.png)'
- en: Figure 3-5\. An ANN architecture
  id: totrans-105
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-5\. ANN架构
- en: 'The Python code for the network in [Figure 3-5](#ANNArchitecture) is shown
    below:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图中[3-5](#ANNArchitecture)中的网络的Python代码如下所示：
- en: '[PRE2]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Compiling the model
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编译模型
- en: With the model constructed, it can be compiled with the help of the `compile()`
    function. Compiling the model leverages the efficient numerical libraries in the
    Theano or TensorFlow packages. When compiling, it is important to specify the
    additional properties required when training the network. Training a network means
    finding the best set of weights to make predictions for the problem at hand. So
    we must specify the loss function used to evaluate a set of weights, the optimizer
    used to search through different weights for the network, and any optional metrics
    we would like to collect and report during training.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 模型构建完毕后，可以通过`compile()`函数进行编译。编译模型利用Theano或TensorFlow软件包中的高效数值库。在编译时，重要的是指定训练网络时所需的附加属性。训练网络意味着找到一组最佳权重以对所面临的问题进行预测。因此，我们必须指定用于评估一组权重的损失函数，用于搜索网络不同权重的优化器，并在训练过程中收集和报告任何可选的度量标准。
- en: 'In the following example, we use `cross-entropy` loss function, which is defined
    in Keras as `binary_crossentropy`. We will also use the adam optimizer, which
    is the default option. Finally, because it is a classification problem, we will
    collect and report the classification accuracy as the metric.^([7](ch03.xhtml#idm45174933399704))
    The Python code follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例中，我们使用了`cross-entropy`损失函数，在Keras中被定义为`binary_crossentropy`。我们还将使用adam优化器，这是默认选项。最后，因为这是一个分类问题，我们将收集并报告分类准确性作为度量标准。^([7](ch03.xhtml#idm45174933399704))以下是Python代码：
- en: '[PRE3]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Fitting the model
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 适配模型
- en: With our model defined and compiled, it is time to execute it on data. We can
    train or fit our model on our loaded data by calling the `fit()` function on the
    model.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 定义并编译了我们的模型后，现在是在数据上执行它的时候了。我们可以通过在模型上调用`fit()`函数来训练或适配我们加载的数据。
- en: 'The training process will run for a fixed number of iterations (epochs) through
    the dataset, specified using the `nb_epoch` argument. We can also set the number
    of instances that are evaluated before a weight update in the network is performed.
    This is set using the `batch_size` argument. For this problem we will run a small
    number of epochs (10) and use a relatively small batch size of 32\. Again, these
    can be chosen experimentally through trial and error. The Python code follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程将在数据集上通过固定次数的迭代（epochs）运行，使用`nb_epoch`参数进行指定。我们还可以设置在网络执行权重更新之前评估的实例数。这是通过`batch_size`参数设置的。对于这个问题，我们将运行少量epochs（10），并使用相对较小的批量大小为32。同样，这些可以通过试验和错误选择。以下是Python代码：
- en: '[PRE4]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Evaluating the model
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估模型
- en: 'We have trained our neural network on the entire dataset and can evaluate the
    performance of the network on the same dataset. This will give us an idea of how
    well we have modeled the dataset (e.g., training accuracy) but will not provide
    insight on how well the algorithm will perform on new data. For this, we separate
    the data into training and test datasets. The model is evaluated on the training
    dataset using the `evaluation()` function. This will generate a prediction for
    each input and output pair and collect scores, including the average loss and
    any metrics configured, such as accuracy. The Python code follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在整个数据集上训练了我们的神经网络，并且可以评估网络在同一数据集上的性能。这将使我们了解我们对数据集建模的效果（例如，训练准确性），但不会提供有关算法在新数据上表现如何的洞察。为此，我们将数据分为训练和测试数据集。使用`evaluation()`函数在训练数据集上评估模型。这将为每个输入和输出对生成预测，并收集分数，包括平均损失和配置的任何指标，如准确性。以下是Python代码：
- en: '[PRE5]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Running an ANN Model Faster: GPU and Cloud Services'
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加速运行ANN模型：GPU和云服务
- en: For training ANNs (especially deep neural networks with many layers), a large
    amount of computation power is required. Available CPUs, or Central Processing
    Units, are responsible for processing and executing instructions on a local machine.
    Since CPUs are limited in the number of cores and take up the job sequentially,
    they cannot do rapid matrix computations for the large number of matrices required
    for training deep learning models. Hence, the training of deep learning models
    can be extremely slow on the CPUs.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练ANNs（特别是具有许多层的深度神经网络），需要大量的计算能力。可用的CPU，或称为中央处理单元，在本地机器上负责处理和执行指令。由于CPU在核心数量上受限，并且顺序地执行作业，它们无法快速执行训练深度学习模型所需的大量矩阵计算。因此，在CPU上训练深度学习模型可能非常慢。
- en: 'The following alternatives are useful for running ANNs that generally require
    a significant amount of time to run on a CPU:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 对于通常需要大量时间在CPU上运行的ANNs，以下备选方案非常有用：
- en: Running notebooks locally on a GPU.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本地GPU上运行笔记本。
- en: Running notebooks on Kaggle Kernels or Google Colaboratory.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Kaggle Kernels或Google Colaboratory上运行笔记本。
- en: Using Amazon Web Services.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用亚马逊网络服务。
- en: GPU
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GPU
- en: A GPU is composed of hundreds of cores that can handle thousands of threads
    simultaneously. Running ANNs and deep learning models can be accelerated by the
    use of GPUs.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: GPU由数百个核心组成，可以同时处理数千个线程。使用GPU可以加速运行ANN和深度学习模型。
- en: GPUs are particularly adept at processing complex matrix operations. The GPU
    cores are highly specialized, and they massively accelerate processes such as
    deep learning training by offloading the processing from CPUs to the cores in
    the GPU subsystem.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: GPU特别擅长处理复杂的矩阵运算。GPU核心高度专业化，并通过将处理从CPU转移到GPU子系统中的核心来大幅加速深度学习训练等过程。
- en: All the Python packages related to machine learning, including Tensorflow, Theano,
    and Keras, can be configured for the use of GPUs.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 所有与机器学习相关的Python包，包括Tensorflow、Theano和Keras，都可以配置为使用GPU。
- en: Cloud services such as Kaggle and Google Colab
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 云服务，如Kaggle和Google Colab
- en: 'If you have a GPU-enabled computer, you can run ANNs locally. If you do not,
    we recommend you use a service such as Kaggle Kernels, Google Colab, or AWS:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有启用GPU的计算机，可以在本地运行ANNs。如果没有，我们建议您使用Kaggle Kernels、Google Colab或AWS等服务：
- en: Kaggle
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle
- en: A popular data science website owned by Google that hosts Jupyter service and
    is also referred to as [Kaggle Kernels](https://www.kaggle.com). Kaggle Kernels
    are free to use and come with the most frequently used packages preinstalled.
    You can connect a kernel to any dataset hosted on Kaggle, or alternatively, you
    can just upload a new dataset on the fly.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 由Google拥有的一个流行的数据科学网站，托管Jupyter服务，也称为[Kaggle Kernels](https://www.kaggle.com)。Kaggle
    Kernels可免费使用，并预先安装了最常用的包。您可以将内核连接到托管在Kaggle上的任何数据集，或者您也可以随时上传新的数据集。
- en: Google Colaboratory
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Google Colaboratory
- en: A free Jupyter Notebook environment provided by Google where you can use free
    GPUs. The features of [Google Colaboratory](https://oreil.ly/keqHk) are the same
    as Kaggle.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 由Google提供的免费Jupyter Notebook环境，您可以使用免费的GPU。[Google Colaboratory](https://oreil.ly/keqHk)的功能与Kaggle相同。
- en: Amazon Web Services (AWS)
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊网络服务（AWS）
- en: '[AWS Deep Learning](https://oreil.ly/gU84O) provides an infrastructure to accelerate
    deep learning in the cloud, at any scale. You can quickly launch AWS server instances
    preinstalled with popular deep learning frameworks and interfaces to train sophisticated,
    custom AI models, experiment with new algorithms, or learn new skills and techniques.
    These web servers can run longer than Kaggle Kernels. So for big projects, it
    might be worth using an AWS instead of a kernel.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[AWS深度学习](https://oreil.ly/gU84O)提供了一个基础设施，可以在云中加速深度学习，无论规模大小。您可以快速启动预先安装了流行深度学习框架和接口的AWS服务器实例，用于训练复杂的自定义AI模型，尝试新算法，或学习新技能和技术。这些Web服务器可以比Kaggle
    Kernels运行更长时间。因此，对于大型项目，使用AWS而不是内核可能更值得。'
- en: Chapter Summary
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 章节总结
- en: ANNs comprise a family of algorithms used across all types of machine learning.
    These models are inspired by the biological neural networks containing neurons
    and layers of neurons that constitute animal brains. ANNs with many layers are
    referred to as deep neural networks. Several steps, including forward propagation
    and backpropagation, are required for training these ANNs. Python packages such
    as Keras make the training of these ANNs possible in a few lines of code. The
    training of these deep neural networks require more computational power, and CPUs
    alone might not be enough. Alternatives include using a GPU or cloud service such
    as Kaggle Kernels, Google Colaboratory, or Amazon Web Services for training deep
    neural networks.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ANNs是一类用于各种类型的机器学习的算法。这些模型受生物神经网络启发，包含构成动物大脑的神经元和神经元层。具有许多层的ANN称为深度神经网络。训练这些ANN需要几个步骤，包括前向传播和反向传播。诸如Keras之类的Python包可以让这些ANN的训练在几行代码内完成。训练这些深度神经网络需要更多的计算能力，仅靠CPU可能不够。备选方案包括使用GPU或云服务，如Kaggle
    Kernels、Google Colaboratory或Amazon Web Services来训练深度神经网络。
- en: Next Steps
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下一步
- en: As a next step, we will be going into the details of the machine learning concepts
    for supervised learning, followed by case studies using the concepts covered in
    this chapter.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 作为下一步，我们将深入探讨监督学习的机器学习概念的详细内容，然后进行使用本章涵盖的概念的案例研究。
- en: ^([1](ch03.xhtml#idm45174933851448-marker)) Readers are encouraged to refer
    to the book *Deep Learning* by Aaron Courville, Ian Goodfellow, and Yoshua Bengio
    (MIT Press) for more details on ANN and deep learning.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch03.xhtml#idm45174933851448-marker)) 读者被鼓励参考由Aaron Courville、Ian Goodfellow和Yoshua
    Bengio（MIT Press）合著的书籍*《深度学习》*，以获取有关ANN和深度学习的更多细节。
- en: ^([2](ch03.xhtml#idm45174933836904-marker)) Activation functions are described
    in detail later in this chapter.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch03.xhtml#idm45174933836904-marker)) 激活函数将在本章后面详细描述。
- en: ^([3](ch03.xhtml#idm45174933705544-marker)) There are many available loss functions
    discussed in the next section. The nature of our problem dictates our choice of
    loss function.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch03.xhtml#idm45174933705544-marker)) 下一节讨论了许多可用的损失函数。我们问题的性质决定了我们对损失函数的选择。
- en: ^([4](ch03.xhtml#idm45174933620152-marker)) Deriving a regression or classification
    output by changing the activation function of the output layer is described further
    in [Chapter 4](ch04.xhtml#Chapter4).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch03.xhtml#idm45174933620152-marker)) 通过改变输出层的激活函数来导出回归或分类输出在[第四章](ch04.xhtml#Chapter4)中进一步描述。
- en: ^([5](ch03.xhtml#idm45174933603352-marker)) Refer to [*https://oreil.ly/FSt-8*](https://oreil.ly/FSt-8)
    for more details on optimization.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch03.xhtml#idm45174933603352-marker)) 有关优化的更多细节，请参阅[*https://oreil.ly/FSt-8*](https://oreil.ly/FSt-8)。
- en: ^([6](ch03.xhtml#idm45174933565816-marker)) The steps and Python code related
    to implementing deep learning models using Keras, as demonstrated in this section,
    are used in several case studies in the subsequent chapters.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch03.xhtml#idm45174933565816-marker)) 使用Keras实现深度学习模型的步骤和Python代码，如本节所示，在后续章节中的几个案例研究中使用。
- en: ^([7](ch03.xhtml#idm45174933399704-marker)) A detailed discussion of the evaluation
    metrics for classification models is presented in [Chapter 4](ch04.xhtml#Chapter4).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch03.xhtml#idm45174933399704-marker)) 分类模型的评估指标的详细讨论在[第四章](ch04.xhtml#Chapter4)中呈现。
