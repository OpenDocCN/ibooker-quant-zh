<html><head></head><body><section data-pdf-bookmark="Chapter 6. The Dangers of Conventional AI Systems" data-type="chapter" epub:type="chapter"><div class="chapter" id="the_dangers_of_conventional_ai_systems">&#13;
<h1><span class="label">Chapter 6. </span>The Dangers of Conventional AI Systems</h1>&#13;
<blockquote data-type="epigraph" epub:type="epigraph">&#13;
<p>A man’s got to know his limitations.</p>&#13;
<p>—Detective “Dirty” Harry in the movie Magnum Force, as he watches an overconfident criminal mastermind’s car explode</p>&#13;
</blockquote>&#13;
<p>A model’s got to know its limitations.<a contenteditable="false" data-primary="models" data-secondary="limitations must be known" data-seealso="dangers of conventional AI" data-type="indexterm" id="id1110"/><a contenteditable="false" data-primary="AI" data-secondary="dangers of conventional AI" data-type="indexterm" id="id1111"/><a contenteditable="false" data-primary="dangers of conventional AI" data-secondary="about" data-type="indexterm" id="id1112"/><a contenteditable="false" data-primary="conventional AI dangers" data-see="dangers of conventional AI" data-type="indexterm" id="id1113"/> This is worth emphasizing because of the importance of this characteristic for models in finance and investing. The corollary is that an AI’s got to know its limitations. <a contenteditable="false" data-primary="dangers of conventional AI" data-secondary="common sense lacking" data-tertiary="causal relationship ignorance" data-type="indexterm" id="id1114"/><a contenteditable="false" data-primary="common sense lacking from AI" data-secondary="causal relationships not understood" data-type="indexterm" id="id1115"/><a contenteditable="false" data-primary="AI" data-secondary="dangers of conventional AI" data-tertiary="causal relationship ignorance" data-type="indexterm" id="id1116"/><a contenteditable="false" data-primary="AI" data-secondary="dangers of conventional AI" data-tertiary="common sense lacking" data-type="indexterm" id="id1117"/>The most serious limitation of all AI systems is that they lack common sense. This stems from their inability to understand causal relationships. AI systems only learn statistical relationships during training that are hard to generalize to new situations without comprehending causality.</p>&#13;
<p>In <a data-type="xref" href="ch01.html#the_need_for_probabilistic_machine_lear">Chapter 1</a>, we examined the three ways in which financial markets can humble you even when you apply our best models cautiously and thoughtfully. Markets will almost surely humiliate you when your models are based on flawed financial and statistical theories such as those discussed in the first half of the book. That’s actually not such a bad outcome, because a humiliating financial loss can often lead to personal insights and growth. A worse outcome is getting fired from your job or your career coming to an ignoble end. The worst outcome is personal financial ruin, where the wisdom gained from such an experience may not be timely enough to be useful.</p>&#13;
<p>When <a contenteditable="false" data-primary="AI" data-secondary="dangers of conventional AI" data-tertiary="maximum likelihood estimation use" data-type="indexterm" id="id1118"/><a contenteditable="false" data-primary="maximum likelihood estimation (MLE)" data-secondary="dangers of conventional AI" data-type="indexterm" id="id1119"/><a contenteditable="false" data-primary="maximum likelihood estimation (MLE)" data-secondary="flaws for finance and investing" data-type="indexterm" id="id1120"/><a contenteditable="false" data-primary="dangers of conventional AI" data-secondary="maximum likelihood estimation" data-tertiary="flaws of" data-type="indexterm" id="id1121"/>traditional ML models (such as deep learning networks and logistic regression) are trained, they generally use the maximum likelihood estimation (MLE) method to learn the model parameters from in-sample data. Consequently, these ML systems have three deep flaws that severely limit their use in finance and investing. First, the parameter estimates of their models are erroneous when used with small datasets, especially when they learn from noisy financial data. Second, these ML models are awful at extrapolating beyond the data ranges and classes on which they have been trained and tested. Third, the probability scores of MLE models have to be calibrated into valid probabilities by using a function such as a Sigmoid or Softmax <span class="keep-together">function.</span> However, these calibrations are not guaranteed to represent the underlying probabilities accurately leading to poor uncertainty quantifications.</p>&#13;
<p>What makes all these flaws egregious is that the conventional statistical models on which these ML systems are based make erroneous estimates and predictions with appallingly high confidence, making them very dangerous in an uncertain world. Just like in the movie <em>Magnum Force</em>, these overconfident AI models have the potential of blowing up investment accounts, companies, financial institutions, and economies if they are implemented without understanding their severe limitations.</p>&#13;
<p>In <a data-type="xref" href="ch04.html#the_dangers_of_conventional_statistical">Chapter 4</a>, we exposed the fallacious inferential reasoning of popular statistical methods such as NHST, p-values, and confidence intervals. In this chapter, we examine the severe limitations and flaws of the popular MLE method and why it fails in finance and investing. We do this by examining a case where we want to project whether a newly listed public company we have invested in will beat its quarterly earnings expectations, based on a short track record. By comparing the results of a traditional MLE model with that of a probabilistic model, we demonstrate why probabilistic models are better suited for finance and investing in general, especially when datasets are sparse.</p>&#13;
<p>As discussed earlier, most real-world<a contenteditable="false" data-primary="marginal probability distribution complexity" data-type="indexterm" id="id1122"/> probabilistic inference problems cannot be solved analytically because of the intractable complexity of the summations/integrals in the marginal probability distribution. Instead of using flawed probability calibration methods used by MLE models, we settle for approximate numerical solutions to probabilistic inference problems. Even though the earnings expectation problem can be solved analytically using basic calculus, we apply grid approximation to solve it to show how this simple, powerful technique works and makes probabilistic inference much easier to understand.</p>&#13;
<p>Markov chain Monte Carlo (MCMC) simulation<a contenteditable="false" data-primary="Markov chain Monte Carlo (MCMC) simulations" data-type="indexterm" id="id1123"/><a contenteditable="false" data-primary="probability distributions" data-secondary="Markov chain Monte Carlo simulation" data-type="indexterm" id="id1124"/><a contenteditable="false" data-primary="Monte Carlo simulation (MCS)" data-secondary="Markov chain Monte Carlo simulations" data-tertiary="about" data-type="indexterm" id="id1125"/> is a breakthrough numerical method that has transformed the usability of probabilistic inference by estimating analytically intractable, high dimensional posterior probability distributions. MCMC simulates complex probability distributions using dependent random sampling algorithms. We explore the fundamental concepts underlying this powerful, scalable simulation method. As a proof-of-concept of the MCMC method, we use the famous Metropolis sampling algorithm to simulate a Student’s t-distribution with fat tails.</p>&#13;
<section data-pdf-bookmark="AI Systems: A Dangerous Lack of Common Sense" data-type="sect1"><div class="sect1" id="ai_systems_a_dangerous_lack_of_common">&#13;
<h1>AI Systems: A Dangerous Lack of Common Sense</h1>&#13;
<p>Humans are endowed with a very<a contenteditable="false" data-primary="dangers of conventional AI" data-secondary="common sense lacking" data-type="indexterm" id="ch06-cs"/><a contenteditable="false" data-primary="AI" data-secondary="dangers of conventional AI" data-tertiary="common sense lacking" data-type="indexterm" id="ch06-cs2"/><a contenteditable="false" data-primary="common sense lacking from AI" data-type="indexterm" id="ch06-cs3"/><a contenteditable="false" data-primary="generalizing learnings inability in conventional AI" data-type="indexterm" id="ch06-cs4"/> important quality that no AI has been able to learn so far: a commonsensical ability to generalize our learnings reasonably well to unseen, out-of-sample related classes or ranges, even if we have not been specifically trained on them. Unlike AI systems, almost all humans can easily deduce, infer, and adjust their knowledge to new circumstances based on common sense. For instance, a deep neural network trained to recognize live elephants in the wilderness was unable to recognize a taxidermy elephant on display in a museum.<sup><a data-type="noteref" href="ch06.html#ch06fn1" id="ch06fn1-marker">1</a></sup> Even a toddler could do this task easily by just using their common sense. As others have pointed out, the AI system literally could not see the elephant in the room!</p>&#13;
<p>The primary reason for such common<a contenteditable="false" data-primary="dangers of conventional AI" data-secondary="common sense lacking" data-tertiary="causal relationship ignorance" data-type="indexterm" id="id1126"/><a contenteditable="false" data-primary="AI" data-secondary="dangers of conventional AI" data-tertiary="causal relationship ignorance" data-type="indexterm" id="id1127"/> failures is that AI models only compute correlations and don’t have the tools to comprehend causation. Furthermore, humans are able to abstract concepts from specific examples and think in terms of generalization of objects and causal relationships among them, while AI systems are just unable to do that. <a contenteditable="false" data-primary="dangers of conventional AI" data-secondary="common sense lacking" data-tertiary="spurious correlations" data-type="indexterm" id="id1128"/><a contenteditable="false" data-primary="AI" data-secondary="dangers of conventional AI" data-tertiary="spurious correlations" data-type="indexterm" id="id1129"/>This is a major problem when dealing with noisy, big datasets as they present abundant opportunities for correlating variables that have no plausible physical or causal relationship. With large datasets, spurious correlations among variables are the rule, not the exception.</p>&#13;
<p>For instance, <a data-type="xref" href="#spurious_correlations_are_the_rule_in_b">Figure 6-1</a> shows that between 1999 and 2009, there was a 99.8% correlation between US spending on science, space and technology, and suicides by hanging, strangulation, and suffocation.<sup><a data-type="noteref" href="ch06.html#ch06fn2" id="ch06fn2-marker">2</a></sup></p>&#13;
<figure><div class="figure" id="spurious_correlations_are_the_rule_in_b">&#13;
<img alt="Spurious correlations are the rule in big datasets." src="assets/pmlf_0601.png"/>&#13;
<h6><span class="label">Figure 6-1. </span>Spurious correlations are the rule in big datasets<sup><a data-type="noteref" href="ch06.html#ch06fn3" id="ch06fn3-marker">3</a></sup></h6>&#13;
</div></figure>&#13;
<p class="pagebreak-before">Clearly this relationship is nonsensical and underscores the adage that correlation does not imply causation. Humans would understand the absurdity of such spurious correlations quite easily, but not AI systems. This also makes AI systems easy to fool by humans who understand such weaknesses and can exploit them.</p>&#13;
<p>While artificial neural networks were inspired by the structure and function of the human brain, our understanding of how human neurons learn and work is still incomplete. As a result, artificial neural networks are not exact replicas of biological neurons, and there are still many unsolved mysteries surrounding the workings of the human brain. <a contenteditable="false" data-primary="deep neural networks" data-secondary="misleading marketing term" data-type="indexterm" id="id1130"/>The term “deep neural networks” is a misleading marketing term to describe artificial neural networks with more than two hidden layers between the input and output layers. There is nothing deep about a deep neural network that lacks the common sense of a toddler.<sup><a data-type="noteref" href="ch06.html#ch06fn4" id="ch06fn4-marker">4</a></sup><a contenteditable="false" data-primary="" data-startref="ch06-cs" data-type="indexterm" id="id1131"/><a contenteditable="false" data-primary="" data-startref="ch06-cs2" data-type="indexterm" id="id1132"/><a contenteditable="false" data-primary="" data-startref="ch06-cs3" data-type="indexterm" id="id1133"/><a contenteditable="false" data-primary="" data-startref="ch06-cs4" data-type="indexterm" id="id1134"/></p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Why MLE Models Fail in Finance" data-type="sect1"><div class="sect1" id="why_mle_models_fail_in_finance">&#13;
<h1>Why MLE Models Fail in Finance</h1>&#13;
<p>The MLE statistical method is used<a contenteditable="false" data-primary="MLE" data-see="maximum likelihood estimation" data-type="indexterm" id="id1135"/><a contenteditable="false" data-primary="maximum likelihood estimation (MLE)" data-secondary="dangers of conventional AI" data-tertiary="why MLE models fail in finance" data-type="indexterm" id="ch06-why"/><a contenteditable="false" data-primary="dangers of conventional AI" data-secondary="maximum likelihood estimation" data-type="indexterm" id="ch06-why2"/><a contenteditable="false" data-primary="AI" data-secondary="dangers of conventional AI" data-tertiary="maximum likelihood estimation use" data-type="indexterm" id="ch06-why3"/> by all conventional parametric ML systems, from simple linear models to complex deep learning neural networks. The MLE method is used to compute the optimal parameters that best fit the data of an assumed statistical distribution. The MLE algorithm is useful when the model is dealing with only aleatory uncertainty of large datasets that have time-invariant statistical distributions where optimization makes sense.</p>&#13;
<p>Much valuable information and assessment of uncertainty are lost when a statistical distribution is summarized by a point estimate, even if it is an optimal estimate. <a contenteditable="false" data-primary="epistemic uncertainty" data-secondary="point estimate cannot capture" data-type="indexterm" id="id1136"/><a contenteditable="false" data-primary="parameters of a model" data-secondary="point estimate not capturing epistemic uncertainty" data-type="indexterm" id="id1137"/><a contenteditable="false" data-primary="uncertainty quantification and analysis" data-secondary="MLE models failing in finance" data-type="indexterm" id="id1138"/>By definition and design, a point estimate cannot capture the epistemic uncertainty of model parameters because they are not probability distributions. This has serious consequences in finance and investing, where we are dealing with complex, dynamic social systems that are steeped in all three dimensions of uncertainty: aleatory, epistemic, and ontological. In <a data-type="xref" href="ch01.html#the_need_for_probabilistic_machine_lear">Chapter 1</a>, we discussed why it is dangerous and foolish to use point estimates in finance and investing given that we are continually dealing with erroneous measurements, incomplete information, and three-dimensional uncertainty. In other words, MLE-based traditional ML systems operate only along one dimension in the three-dimensional space of uncertainty as illustrated in <a data-type="xref" href="ch02.html#human_intelligence_supported_by_probabi">Figure 2-7</a>. What is even more alarming is that many of these ML systems are generally black boxes operating confidently at high speeds  with flawed probability <span class="keep-together">calibrations.</span></p>&#13;
<p class="pagebreak-before">Furthermore, MLE ignores prior<a contenteditable="false" data-primary="knowledge integration" data-secondary="MLE ignoring prior domain knowledge" data-type="indexterm" id="id1139"/> domain knowledge in the form of base rates or prior probabilities, which can lead to base-rate fallacies, as discussed in <a data-type="xref" href="ch04.html#the_dangers_of_conventional_statistical">Chapter 4</a>. This is especially true when MLE is applied to small datasets. Let’s actually see why this is indeed the case by applying the MLE method to a real-world problem of estimating the probability that a company will actually beat the market’s expectation of its earnings estimates based on a short track record. This example has been inspired by the coin tossing example illustrated in the book referred to in the references.<sup><a data-type="noteref" href="ch06.html#ch06fn5" id="ch06fn5-marker">5</a></sup></p>&#13;
<section data-pdf-bookmark="An MLE Model for Earnings Expectations" data-type="sect2"><div class="sect2" id="an_mle_model_for_earnings_expectations">&#13;
<h2>An MLE Model for Earnings Expectations</h2>&#13;
<p>Assume you have changed jobs<a contenteditable="false" data-primary="knowledge integration" data-secondary="MLE ignoring prior domain knowledge" data-tertiary="MLE model for earnings expectations" data-type="indexterm" id="ch06-mle"/><a contenteditable="false" data-primary="maximum likelihood estimation (MLE)" data-secondary="dangers of conventional AI" data-tertiary="MLE model for earnings expectations" data-type="indexterm" id="ch06-mle2"/><a contenteditable="false" data-primary="dangers of conventional AI" data-secondary="maximum likelihood estimation" data-tertiary="MLE model for earnings expectations" data-type="indexterm" id="ch06-mle3"/><a contenteditable="false" data-primary="AI" data-secondary="dangers of conventional AI" data-tertiary="MLE model for earnings expectations" data-type="indexterm" id="ch06-mle4"/><a contenteditable="false" data-primary="earnings expectations" data-secondary="MLE model" data-type="indexterm" id="id1140"/> and are now working at a mutual fund as an equity analyst. Last year, your fund was allocated equity shares in the initial public offering (IPO) of ZYX, a high-growth technology company. Even though ZYX has never turned a profit in its entire nascent life, its brand is already a household name due in large part to its aggressive marketing campaigns that were supported by massive amounts of venture capital. Clearly, private and public equity investors bought into its compelling growth story, as narrated by its charismatic CEO.</p>&#13;
<p>In all the last three quarters since its IPO, the negative earnings of ZYX beat market expectations of even bigger losses. In financial markets, less bad is good. The stock price of ZYX has continued its relentless climb upward and is currently trading at all-time highs, enriching everyone in the process. Your portfolio manager (PM) has asked you to estimate the probability that ZYX’s earnings will beat market expectations in the upcoming fourth quarter. Based on your probability estimate, your PM is going to increase or decrease the fund’s equity investment in ZYX before their earnings announcement, which is due shortly.</p>&#13;
<p>Having been schooled in conventional statistical methods, we decide to build a standard MLE model to compute the required probability. The earnings announcement event has only two outcomes that interest us: either the earnings beat market expectations, or they fall short of them. We don’t care about the outcome of earnings merely meeting market expectations. Like many other investors, your PM has decided that such an outcome is the equivalent of earnings falling short of market expectations. It is common knowledge that management of companies play a game with Wall Street analysts throughout the year, where they lower their earnings expectations so that it becomes easier to beat those expectations when the actual earnings are announced.</p>&#13;
<p class="pagebreak-before">Let’s design our quarterly earnings MLE model and specify the assumptions that underpin it:</p>&#13;
<ul>&#13;
<li><p>In a single event or trial, the model’s output variable y can assume only one of two possible outcomes, y = 1 or y = 0.</p></li>&#13;
<li><p>The two outcomes are mutually exclusive and collectively exhaustive.</p></li>&#13;
<li><p>Assign y = 1 to the outcome that ZYX beats market expectations of its quarterly earnings.</p></li>&#13;
<li><p>Assign y = 0 to the outcome that ZYX does not beat or only meets market expectations of its quarterly earnings.</p></li>&#13;
</ul>&#13;
<p>We now have to select a statistical distribution for our likelihood function that best models the binary event of an earnings announcement. <a contenteditable="false" data-primary="Bernoulli distribution" data-type="indexterm" id="id1141"/>The Bernoulli distribution models a single event or trial that has binary outcomes. See <a data-type="xref" href="#shows_a_bernoulli_variable_with_outcome">Figure 6-2</a>.</p>&#13;
<figure><div class="figure" id="shows_a_bernoulli_variable_with_outcome">&#13;
<img alt="Shows a Bernoulli variable with outcome x = 1 occurring with probability p and outcome x = 0 occurring with probability 1-p" src="assets/pmlf_0602.png"/>&#13;
<h6><span class="label">Figure 6-2. </span>Bernoulli variable<sup><a data-type="noteref" href="ch06.html#ch06fn6" id="ch06fn6-marker">6</a></sup> with outcome x = 1 occurring with probability p and outcome x = 0 occurring with probability 1-p</h6>&#13;
</div></figure>&#13;
<p class="pagebreak-before">Recall that in <a data-type="xref" href="ch01.html#the_need_for_probabilistic_machine_lear">Chapter 1</a>, we used<a contenteditable="false" data-primary="binomial distribution to model interest rates" data-secondary="Bernoulli distribution" data-type="indexterm" id="id1142"/><a contenteditable="false" data-primary="interest rates" data-secondary="binomial distribution to model" data-tertiary="Bernoulli distribution" data-type="indexterm" id="id1143"/><a contenteditable="false" data-primary="probability distributions" data-secondary="Bernoulli same as binomial for single trial" data-type="indexterm" id="id1144"/> the binomial distribution to model the total number of interest rate increases by the Federal Reserve over several meetings or trials. The Bernoulli distribution is a special case of the binomial distribution since they both have the same probability distribution when used for a single trial.</p>&#13;
<ul>&#13;
<li><p>Assume that variable y follows a Bernoulli distribution with an unknown parameter p, which gives us the probability of an earnings beat (y = 1).</p></li>&#13;
<li><p>Since both probabilities must add up to 1, this implies that the probability of not beating earnings expectations (y = 0) is its complement, 1-p.</p></li>&#13;
</ul>&#13;
<p>Our objective is to find the MLE of the parameter p, the probability that ZYX beats the market expectations of its quarterly earnings based on ZYX’s short track record of setting market expectations and then beating them.</p>&#13;
<p>A Bernoulli process of the variable y is a discrete time series of independent and identically distributed (i.i.d.) Bernoulli trials, denoted by y<sub>i</sub>.</p>&#13;
<ul>&#13;
<li><p>The i.i.d. assumption means that each earnings announcement is independent of all the previous ones and is drawn from the same Bernoulli distribution with constant parameter p.</p></li>&#13;
<li><p>In its last three quarters, ZYX beat earnings expectations, so our training data for parameter p is D = (y<sub>1</sub> = 1, y<sub>2</sub> = 1, y<sub>3</sub> = 1).</p></li>&#13;
</ul>&#13;
<p>Let’s call p′ the MLE for the parameter p of the Bernoulli variable y. It can be shown mathematically that p′ is the expected value or arithmetic mean of the sample of time series data D. It is the optimal parameter that when inserted in a Bernoulli likelihood function best fits the time series data D. This implies p′ trained on dataset D is:</p>&#13;
<ul>&#13;
<li><p>p′(D) = (y<sub>1</sub>+y<sub>2</sub>+y<sub>3</sub>) / 3 = (1 + 1 + 1) / 3 = 3 / 3 = 1</p></li>&#13;
<li><p>Therefore, the probability that ZYX will beat market expectations of its earnings in its fourth quarter is P(y<sub>4</sub> = 1 | p′) = p′ = 1 or 100%.</p></li>&#13;
</ul>&#13;
<p>Since MLE models only allow aleatory uncertainty caused by random sampling of data, let’s compute the variance of y. The variance of a Bernoulli variable y with parameter p′ is given by:</p>&#13;
<ul>&#13;
<li><p>Aleatory uncertainty or variance (y | p′) = (p′) × (1 – p′)  = 1 × (1 – 1) = 1 × 0 = 0.</p></li>&#13;
<li><p>Epistemic uncertainty = 0 since p′ is a point estimate that is an optimum.</p></li>&#13;
<li><p>Ontological uncertainty = 0 since p′ is considered a “true” constant and the <span class="keep-together">Bernoulli</span> distribution is assumed to be time invariant.</p></li>&#13;
</ul>&#13;
<p class="pagebreak-before">So our MLE model is assigning a 100% probability with a 0 sampling error that y<sub>4</sub> = 1. In other words, our model is absolutely certain that ZYX is going to beat market expectations of its earnings estimate in the upcoming fourth quarter. Our model’s heroic prediction of ZYX’s earnings beating market expectations is based on only three data points of a fledgling, loss-making technology company. Moreover, our current MLE model will continue to predict an earnings beat for every quarterly earnings event for the rest of ZYX’s life. It’s not just death and taxes that are certain. We need to add our MLE model’s predictions to the list.</p>&#13;
<p>Any financial analyst with even a modicum of common sense would not present this MLE model and its predictions to their portfolio manager. However, it is very common to have sparse datasets in finance and investing. For instance, we have financial data for only two occurrences of global pandemics. Early stage technology startup companies or strategy/special projects have little or no relevant data for making specific decisions. Since the Great Depression ended in 1933, the US economy has experienced only 13 recessions. Since 1942, the S&amp;P 500 has had three consecutive years of negative total returns only once (2000–2003). These are some of the obvious examples. The list of sparse datasets in finance and investing is quite long indeed.</p>&#13;
<p>Clearly, MLE models are dangerous when applied to sparse datasets common in finance and investing. They really don’t know their limitations and unabashedly flaunt their ignorance. Building complex financial ML systems based on MLE models will only  lead to financial disasters sooner rather than later.<a contenteditable="false" data-primary="" data-startref="ch06-mle" data-type="indexterm" id="id1145"/><a contenteditable="false" data-primary="" data-startref="ch06-mle2" data-type="indexterm" id="id1146"/><a contenteditable="false" data-primary="" data-startref="ch06-mle3" data-type="indexterm" id="id1147"/><a contenteditable="false" data-primary="" data-startref="ch06-mle4" data-type="indexterm" id="id1148"/></p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="A Probabilistic Model for Earnings Expectations" data-type="sect2"><div class="sect2" id="a_probabilistic_model_for_earnings_expe">&#13;
<h2>A Probabilistic Model for Earnings Expectations</h2>&#13;
<p>Now let’s delete our useless<a contenteditable="false" data-primary="earnings expectations" data-secondary="probabilistic model" data-type="indexterm" id="ch06-probee"/><a contenteditable="false" data-primary="probabilistic financial models" data-secondary="earnings expectations" data-type="indexterm" id="ch06-probee2"/><a contenteditable="false" data-primary="dangers of conventional AI" data-secondary="maximum likelihood estimation" data-tertiary="probabilistic model for earnings expectations" data-type="indexterm" id="ch06-probee3"/><a contenteditable="false" data-primary="maximum likelihood estimation (MLE)" data-secondary="dangers of conventional AI" data-tertiary="probabilistic model for earnings expectations" data-type="indexterm" id="ch06-probee4"/><a contenteditable="false" data-primary="AI" data-secondary="dangers of conventional AI" data-tertiary="probabilistic model for earnings expectations" data-type="indexterm" id="ch06-probee5"/> MLE model and pause to reflect on the problem. With only three data points to work with, it would be foolhardy to be absolutely certain about any point estimate of the parameter p, the probability that ZYX’s fourth quarter earnings will beat market expectations. Why is that? There are so many possible things that could have gone wrong in the past quarter that only some company insiders might be aware of. Given the persistent asymmetry of information between the company management and its shareholders, this is always possible. This is a major source of our epistemic uncertainty about parameter p.</p>&#13;
<p>Most importantly, there are so many things—company specific, political, regulatory, legal, monetary, and economic—that can go wrong in the immediate future and change the market’s expectations before ZYX makes its earnings public. These are some of the sources of our ontological uncertainty. Of course, nobody knows what will happen in the future, but it is more likely that the future will reflect the recent past than not.</p>&#13;
<p>So based on our understanding of the three dimensions of uncertainty of the real world we live in and the information that we currently have, we can reasonably bet that it is very probable that ZYX will beat the market’s expectations of its fourth <span class="keep-together">quarter earnings.</span> However, it’s not a certainty. This implies that our model parameter p should be able to take any value between 0 and 1, with the ones closer to 1 being more probable. In other words, our estimate for p is better expressed by a probability distribution than as any particular point estimate. In particular, after seeing the dataset D, our estimate for p is best expressed as a positively skewed probability <span class="keep-together">distribution.</span></p>&#13;
<p>Note that the MLE is the optimal value for p that best replicates the observed data. But there is no universal natural law that says that it is a certainty that the MLE is the value of p that produced the in-sample data. Other values of the parameter p could easily have generated the dataset D too. We are dealing with complex social systems with emotional beings that do suboptimal things all the time. Most importantly, we are not constrained by the problem to pick only one value for p.</p>&#13;
<p>Let’s actually quantify and visualize the statistical distribution for p more precisely by building a probabilistic model. Recall that a probabilistic model requires us to specify two probability distributions:</p>&#13;
<ul>&#13;
<li><p>The first is a prior probability distribution P(p) that encapsulates our knowledge or hypothesis about model parameters before we observe any data. Let’s assume you have no prior knowledge about ZYX company or any idea of what the parameter p should be. This makes a uniform distribution, U(0, 1), that we learned in the Monty Hall problem a good choice for our prior distribution. This distribution assigns equal probability to all values of p between 0 and 1. So P(p) ~U (0, 1), where the tilde sign (~) is shorthand for “is statistically distributed as.”</p></li>&#13;
<li><p>The second is a likelihood function P(D | p) that gives us the plausibility of observing our in-sample data D assuming any value for our parameter p between 0 and 1. We will continue to use the Bernoulli probability distribution and its related process in our probabilistic model. So the likelihood function of our probabilistic model is P(D | p) ~Bernoulli (p).</p></li>&#13;
</ul>&#13;
<p>Our objective is to estimate the posterior probability distribution of our model parameter p given the in-sample data D and our prior knowledge or hypothesis of p. This will give us the probability distribution for the outcome y = 1, the probability of an earnings beat. As always, we will use the inverse probability rule to compute the probability distribution of p given the data D. Our probabilistic model can be specified as follows:</p>&#13;
<ul class="simplelist">&#13;
<li><p>P( p | D) = P(D | p) ✕ P(p) / P(D) where</p></li>&#13;
<li><p>P(p) ~U (0, 1)</p></li>&#13;
<li><p>P(D | p) ~ Bernoulli (p)</p></li>&#13;
<li><p>D = (y<sub>1</sub> = 1, y<sub>2</sub> = 1, y<sub>3</sub> = 1)</p></li>&#13;
</ul>&#13;
<p>This posterior distribution is simple enough to be solved analytically using basic calculus.<sup><a data-type="noteref" href="ch06.html#ch06fn7" id="ch06fn7-marker">7</a></sup> However, this involves using integrals over probability density functions, which may not be accessible to many readers. Instead of doing that here, we will compute the posterior distribution using a simple numerical approach called grid approximation. This approach will convert our problem of integral calculus into a much simpler problem of descriptive statistics. This should help us to build our intuition for the underlying mechanism of our probabilistic model.</p>&#13;
<p>Since our prior distribution is discrete and uniformly distributed, we can split the interval between 0 and 1 into 9 equidistant points, 0.1 apart, as shown in <a data-type="xref" href="#there_are_n_number_of_grid_points_unifo">Figure 6-3</a>.</p>&#13;
<figure><div class="figure" id="there_are_n_number_of_grid_points_unifo">&#13;
<img alt="There are n number of grid points uniformly distributed between a and b, and each has a probability of 1/n." src="assets/pmlf_0603.png"/>&#13;
<h6><span class="label">Figure 6-3. </span>There are n number of grid points uniformly distributed between a and b, and each has a probability of 1/n<sup><a data-type="noteref" href="ch06.html#ch06fn8" id="ch06fn8-marker">8</a></sup></h6>&#13;
</div></figure>&#13;
<p>So our grid points are {p<sub>1</sub> = 0.1, p<sub>2</sub> = 0.2, .., p<sub>9</sub> = 0.9}. Since the n grid points are uniformly distributed, they all have the same probability, namely P(p) = 1/n, where n is the number of grid points. In our approximation, we have n = 9 grid points.</p>&#13;
<ul>&#13;
<li><p>The prior probability for every parameter p<sub>1</sub>,...p<sub>9</sub> on our one-dimensional grid is P(p) = 1/9 = 0.111.</p></li>&#13;
</ul>&#13;
<p>For every parameter p<sub>i</sub> we sample from the set of nine grid points to simulate an earnings event with a value of p<sub>i</sub>, the Bernoulli likelihood function generates y = 1 with probability pi or y = 0 with probability 1-p<sub>i</sub>. The Bernoulli process for the last three quarters of ZYX’s earnings event is given by our training data D = (y<sub>1</sub> = 1, y<sub>2</sub> = 1, y<sub>3</sub> = 1). So the likelihood of the Bernoulli process is:</p>&#13;
<ul class="simplelist">&#13;
<li><p>P(D | p<sub>i</sub>) = p<sub>i</sub> × p<sub>i</sub> × p<sub>i</sub> = p<sub>i</sub><sup>3</sup></p></li>&#13;
</ul>&#13;
<p>For each parameter p<sub>i</sub>, we use a grid point {p<sub>1</sub>,...p<sub>9</sub>} to compute the unnormalized posterior distribution P*(p | D), using the inverse probability rule. To compute the normalized posterior P(p | D), we first add up the all the unnormalized posterior values and then divide each unnormalized posterior by the sum as follows:</p>&#13;
<ul class="simplelist">&#13;
<li><p><math alttext="upper P asterisk left-parenthesis p Subscript i Baseline vertical-bar upper D right-parenthesis proportional-to upper P left-parenthesis upper D vertical-bar p Subscript i Baseline right-parenthesis upper P left-parenthesis p Subscript i Baseline right-parenthesis equals p Subscript i Superscript 3 Baseline times 0.111">&#13;
  <mrow>&#13;
    <mi>P</mi>&#13;
    <mo>*</mo>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <msub><mi>p</mi> <mi>i</mi> </msub>&#13;
      <mo>|</mo>&#13;
      <mi>D</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>∝</mo>&#13;
    <mi>P</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>D</mi>&#13;
      <mo>|</mo>&#13;
      <msub><mi>p</mi> <mi>i</mi> </msub>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mspace width="0.166667em"/>&#13;
    <mi>P</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <msub><mi>p</mi> <mi>i</mi> </msub>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <msubsup><mi>p</mi> <mi>i</mi> <mn>3</mn> </msubsup>&#13;
    <mo>×</mo>&#13;
    <mn>0</mn>&#13;
    <mo>.</mo>&#13;
    <mn>111</mn>&#13;
  </mrow>&#13;
</math></p></li>&#13;
<li><p>P(p<sub>i</sub> | D) = <math alttext="upper P asterisk left-parenthesis p Subscript i Baseline vertical-bar upper D right-parenthesis slash sigma-summation Underscript i Endscripts upper P asterisk left-parenthesis p Subscript i Baseline vertical-bar upper D right-parenthesis">&#13;
  <mrow>&#13;
    <mi>P</mi>&#13;
    <mo>*</mo>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <msub><mi>p</mi> <mi>i</mi> </msub>&#13;
      <mo>|</mo>&#13;
      <mi>D</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>/</mo>&#13;
    <msub><mo>∑</mo> <mi>i</mi> </msub>&#13;
    <mi>P</mi>&#13;
    <mo>*</mo>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <msub><mi>p</mi> <mi>i</mi> </msub>&#13;
      <mo>|</mo>&#13;
      <mi>D</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
  </mrow>&#13;
</math></p></li>&#13;
</ul>&#13;
<p>Let’s use Python code to develop a grid approximation of the solution:</p>&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Import the relevant Python libraries</code>&#13;
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>&#13;
<code class="kn">import</code> <code class="nn">pandas</code> <code class="k">as</code> <code class="nn">pd</code>&#13;
<code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="k">as</code> <code class="nn">plt</code>&#13;
&#13;
<code class="c1"># Create 9 grid points for the model parameter, from 0.1 to 0.9 spaced 0.1 apart</code>&#13;
<code class="n">p</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">arange</code><code class="p">(</code><code class="mf">0.1</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mf">0.1</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># Since all parameters are uniformly distributed and equally likely, the </code>&#13;
<code class="c1"># probability for each parameter = 1/n = 1/9</code>&#13;
<code class="n">prior</code> <code class="o">=</code> <code class="mi">1</code><code class="o">/</code><code class="nb">len</code><code class="p">(</code><code class="n">p</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># Create a pandas DataFrame with the relevant columns to store </code>&#13;
<code class="c1"># individual calculations</code>&#13;
<code class="n">earnings_beat</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">columns</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'parameter'</code><code class="p">,</code> <code class="s1">'prior'</code><code class="p">,</code> <code class="s1">'likelihood'</code><code class="p">,</code> &#13;
<code class="s1">'posterior*'</code><code class="p">,</code> <code class="s1">'posterior'</code><code class="p">])</code>&#13;
&#13;
<code class="c1"># Store each parameter value</code>&#13;
<code class="n">earnings_beat</code><code class="p">[</code><code class="s1">'parameter'</code><code class="p">]</code> <code class="o">=</code> <code class="n">p</code>&#13;
&#13;
<code class="c1"># Loop computes the unnormalized posterior probability distribution</code>&#13;
<code class="c1"># for each value of the parameter</code>&#13;
<code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code><code class="nb">len</code><code class="p">(</code><code class="n">p</code><code class="p">)):</code>&#13;
 <code class="n">earnings_beat</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="n">i</code><code class="p">,</code><code class="mi">1</code><code class="p">]</code> <code class="o">=</code> <code class="n">prior</code>&#13;
 <code class="c1"># Since our training data has three earnings beats in a row, </code>&#13;
 <code class="c1"># each having a probability of p</code>&#13;
 <code class="n">earnings_beat</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="n">i</code><code class="p">,</code><code class="mi">2</code><code class="p">]</code> <code class="o">=</code> <code class="n">p</code><code class="p">[</code><code class="n">i</code><code class="p">]</code><code class="o">**</code><code class="mi">3</code>&#13;
 <code class="c1"># Use the unnormalized inverse probability rule</code>&#13;
 <code class="n">earnings_beat</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="n">i</code><code class="p">,</code><code class="mi">3</code><code class="p">]</code> <code class="o">=</code> <code class="n">prior</code> <code class="o">*</code> <code class="p">(</code><code class="n">p</code><code class="p">[</code><code class="n">i</code><code class="p">]</code><code class="o">**</code><code class="mi">3</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># Normalize the probability distribution so that all values add up to 1</code>&#13;
<code class="n">earnings_beat</code><code class="p">[</code><code class="s1">'posterior'</code><code class="p">]</code> <code class="o">=</code> <code class="n">earnings_beat</code><code class="p">[</code><code class="s1">'posterior*'</code><code class="p">]</code>&#13;
                                <code class="o">/</code><code class="nb">sum</code><code class="p">(</code><code class="n">earnings_beat</code><code class="p">[</code><code class="s1">'posterior*'</code><code class="p">])</code>&#13;
&#13;
<code class="c1"># Display the data frame to show each calculation</code>&#13;
<code class="n">earnings_beat</code></pre>&#13;
<figure class="informal"><div class="figure">&#13;
<img alt="Image" src="assets/pmlf_06in01.png"/>&#13;
<h6/>&#13;
</div></figure>&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Plot the prior and posterior probability distribution for the model parameter</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">figure</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">16</code><code class="p">,</code><code class="mi">6</code><code class="p">)),</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplot</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code><code class="mi">2</code><code class="p">,</code><code class="mi">1</code><code class="p">),</code> <code class="n">plt</code><code class="o">.</code><code class="n">ylim</code><code class="p">([</code><code class="mi">0</code><code class="p">,</code><code class="mf">0.5</code><code class="p">])</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">stem</code><code class="p">(</code><code class="n">earnings_beat</code><code class="p">[</code><code class="s1">'parameter'</code><code class="p">],</code><code class="n">earnings_beat</code><code class="p">[</code><code class="s1">'prior'</code><code class="p">],</code> &#13;
<code class="n">use_line_collection</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s1">'Model parameter p'</code><code class="p">),</code> <code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s1">'Probability of parameter P(p)'</code><code class="p">),</code> &#13;
<code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="s1">'Prior distribution of our model parameter'</code><code class="p">)</code>&#13;
&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">subplot</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code><code class="mi">2</code><code class="p">,</code><code class="mi">2</code><code class="p">),</code> <code class="n">plt</code><code class="o">.</code><code class="n">ylim</code><code class="p">([</code><code class="mi">0</code><code class="p">,</code><code class="mf">0.5</code><code class="p">])</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">stem</code><code class="p">(</code><code class="n">earnings_beat</code><code class="p">[</code><code class="s1">'parameter'</code><code class="p">],</code><code class="n">earnings_beat</code><code class="p">[</code><code class="s1">'posterior'</code><code class="p">],</code> &#13;
<code class="n">use_line_collection</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s1">'Model parameter p'</code><code class="p">),</code> <code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s1">'Probability of parameter P(p)'</code><code class="p">),</code> &#13;
<code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="s1">'Posterior distribution of our model parameter'</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>&#13;
<figure class="informal"><div class="figure">&#13;
<img alt="Image" src="assets/pmlf_06in02.png"/>&#13;
<h6/>&#13;
</div></figure>&#13;
<p class="pagebreak-before">This figure clearly shows that our probabilistic model has computed a probability distribution for the model parameter p before and after training the model on in-sample data D. This is a much more realistic solution, given that we always have incomplete information about any event.</p>&#13;
<p>Our model has learned the parameter p from our prior knowledge and the data. This is only half the solution. We need to use our model to predict the probability that ZYX will beat the market’s expectations of its fourth quarter earnings estimates. In other words, we need to develop the predictive distributions of our model. Let’s continue coding that:</p>&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Since P(yi=1|pi) = pi, we compute the probability weighted average of  </code>&#13;
<code class="c1"># observing y=1 using our prior probabilities as the weights</code>&#13;
<code class="c1"># This probability weighted average gives us the prior predictive probability of </code>&#13;
<code class="c1"># observing y=1 before observing any data</code>&#13;
<code class="n">prior_predictive_1</code><code class="o">=</code><code class="nb">sum</code><code class="p">(</code><code class="n">earnings_beat</code><code class="p">[</code><code class="s1">'parameter'</code><code class="p">]</code><code class="o">*</code><code class="n">earnings_beat</code><code class="p">[</code><code class="s1">'prior'</code><code class="p">])</code>&#13;
&#13;
<code class="c1"># The prior predictive probability of observing outcome y=0 is the complement of</code>&#13;
<code class="c1"># P(y=1) calculated above</code>&#13;
<code class="n">prior_predictive_0</code> <code class="o">=</code> <code class="mi">1</code> <code class="o">-</code> <code class="n">prior_predictive_1</code>&#13;
&#13;
<code class="c1"># Since we have picked a uniform distribution for our parameter, our model </code>&#13;
<code class="c1"># predicts that both outcomes are equally likely prior to observing any data</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="n">prior_predictive_0</code><code class="p">,</code> <code class="n">prior_predictive_1</code><code class="p">)</code> &#13;
<code class="p">(</code><code class="mf">0.5</code><code class="p">,</code> <code class="mf">0.5</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># Since P(yi=1|pi) = pi, we compute the probability weighted average of </code>&#13;
<code class="c1"># observing y=1 but now we use the posterior probabilities as the weights</code>&#13;
<code class="c1"># This probability weighted average gives us the posterior predictive  </code>&#13;
<code class="c1"># probability of observing y=1 after observing in-sample data </code>&#13;
<code class="n">D</code><code class="o">=</code><code class="p">{</code><code class="n">y1</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">y2</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">y3</code><code class="o">=</code><code class="mi">1</code><code class="p">}</code>&#13;
<code class="n">posterior_predictive_1</code> <code class="o">=</code> &#13;
<code class="nb">sum</code><code class="p">(</code><code class="n">earnings_beat</code><code class="p">[</code><code class="s1">'parameter'</code><code class="p">]</code> <code class="o">*</code> <code class="n">earnings_beat</code><code class="p">[</code><code class="s1">'posterior'</code><code class="p">])</code>&#13;
&#13;
<code class="c1"># The posterior predictive probability of observing outcome y=0 is the  </code>&#13;
<code class="c1"># complement of P(y=1|D) calculated above</code>&#13;
<code class="n">posterior_predictive_0</code> <code class="o">=</code> <code class="mi">1</code><code class="o">-</code> <code class="n">posterior_predictive_1</code>&#13;
&#13;
<code class="c1"># After observing data D, our model predicts that observing y=1 is </code>&#13;
<code class="c1"># about 3 times more likely than observing y=0</code>&#13;
<code class="nb">round</code><code class="p">(</code><code class="n">posterior_predictive_0</code><code class="p">,</code><code class="mi">2</code><code class="p">),</code> <code class="nb">round</code><code class="p">(</code><code class="n">posterior_predictive_1</code><code class="p">,</code><code class="mi">2</code><code class="p">)</code> &#13;
<code class="p">(</code><code class="mf">0.24</code><code class="p">,</code> <code class="mf">0.76</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># Plot the prior and posterior predictive probability distribution </code>&#13;
<code class="c1"># for the event outcomes</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">figure</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">16</code><code class="p">,</code><code class="mi">6</code><code class="p">)),</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplot</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code><code class="mi">2</code><code class="p">,</code><code class="mi">1</code><code class="p">),</code> <code class="n">plt</code><code class="o">.</code><code class="n">ylim</code><code class="p">([</code><code class="mi">0</code><code class="p">,</code><code class="mi">1</code><code class="p">])</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">stem</code><code class="p">([</code><code class="mi">0</code><code class="p">,</code><code class="mi">1</code><code class="p">],[</code><code class="n">prior_predictive_0</code><code class="p">,</code> <code class="n">prior_predictive_1</code><code class="p">],</code> &#13;
&#13;
<code class="n">use_line_collection</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s1">'Binary outcome for variable y'</code><code class="p">),</code> <code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s1">'Probability P(y)'</code><code class="p">),</code> &#13;
<code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="s1">'Prior predictive distribution of an earnings beat'</code><code class="p">)</code>&#13;
&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">subplot</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code><code class="mi">2</code><code class="p">,</code><code class="mi">2</code><code class="p">),</code> <code class="n">plt</code><code class="o">.</code><code class="n">ylim</code><code class="p">([</code><code class="mi">0</code><code class="p">,</code><code class="mi">1</code><code class="p">])</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">stem</code><code class="p">([</code><code class="mi">0</code><code class="p">,</code><code class="mi">1</code><code class="p">],[</code><code class="n">posterior_predictive_0</code><code class="p">,</code> <code class="n">posterior_predictive_1</code><code class="p">],</code> &#13;
&#13;
<code class="n">use_line_collection</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s1">'Binary outcome for variable y'</code><code class="p">),</code> <code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s1">'Probability P(y)'</code><code class="p">),</code> &#13;
<code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="s1">'Posterior predictive distribution of an earnings beat'</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>&#13;
<figure class="informal"><div class="figure">&#13;
<img alt="Image" src="assets/pmlf_06in03.png"/>&#13;
<h6/>&#13;
</div></figure>&#13;
<p>The expected value or posterior predictive mean is 76%, which is close to the theoretical value of 75%. Regardless, our probabilistic model is not 100% sure that ZYX will beat market expectations in the fourth quarter, even though it has successfully done so in the last three quarters. Our model predicts that it is about three times more likely to beat market expectations than not. This is a far more realistic probability distribution and something we can use to make our investment decisions.</p>&#13;
<p>Unfortunately, the numerical grid approximation technique we used to solve the earnings expectations problem does not scale if the model has more than a few parameters. So the most scalable and robust numerical methods that we are left with are random sampling methods for estimating approximate solutions for probabilistic inference problems.<a contenteditable="false" data-primary="" data-startref="ch06-why" data-type="indexterm" id="id1149"/><a contenteditable="false" data-primary="" data-startref="ch06-why2" data-type="indexterm" id="id1150"/><a contenteditable="false" data-primary="" data-startref="ch06-why3" data-type="indexterm" id="id1151"/><a contenteditable="false" data-primary="" data-startref="ch06-probee" data-type="indexterm" id="id1152"/><a contenteditable="false" data-primary="" data-startref="ch06-probee2" data-type="indexterm" id="id1153"/><a contenteditable="false" data-primary="" data-startref="ch06-probee3" data-type="indexterm" id="id1154"/><a contenteditable="false" data-primary="" data-startref="ch06-probee4" data-type="indexterm" id="id1155"/><a contenteditable="false" data-primary="" data-startref="ch06-probee5" data-type="indexterm" id="id1156"/></p>&#13;
</div></section>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Markov Chain Monte Carlo Simulations" data-type="sect1"><div class="sect1" id="markov_chain_monte_carlo_simulations">&#13;
<h1>Markov Chain Monte Carlo Simulations</h1>&#13;
<p>Generally speaking, there are two<a contenteditable="false" data-primary="Monte Carlo simulation (MCS)" data-secondary="Markov chain Monte Carlo simulations" data-tertiary="about" data-type="indexterm" id="id1157"/><a contenteditable="false" data-primary="Markov chain Monte Carlo (MCMC) simulations" data-secondary="about" data-type="indexterm" id="id1158"/><a contenteditable="false" data-primary="random sampling" data-secondary="independent sampling" data-seealso="Monte Carlo simulation" data-type="indexterm" id="id1159"/><a contenteditable="false" data-primary="random sampling" data-secondary="dependent sampling" data-seealso="Markov chain Monte Carlo (MCMC) simulations" data-type="indexterm" id="id1160"/><a contenteditable="false" data-primary="Monte Carlo simulation (MCS)" data-secondary="about" data-tertiary="independent sampling" data-type="indexterm" id="id1161"/> types of random sampling methods: independent sampling, and dependent sampling. The standard Monte Carlo simulation (MCS) method that we learned in <a data-type="xref" href="ch03.html#quantifying_output_uncertainty_with_mon">Chapter 3</a> is an independent random sampling method. However, random sampling does not work well when samples are dependent or correlated with one another.</p>&#13;
<p>Furthermore, these independent sampling algorithms are inefficient when the target probability distribution they are trying to simulate has many parameters or dimensions. We generally encounter these two issues when simulating complex posterior probability distributions. <a contenteditable="false" data-primary="Markov chain Monte Carlo (MCMC) simulations" data-secondary="Markov chains" data-tertiary="dependent sampling" data-type="indexterm" id="id1162"/>So we need random sampling algorithms which work with samples that are dependent or correlated with one another.<sup><a data-type="noteref" href="ch06.html#ch06fn9" id="ch06fn9-marker">9</a></sup> Markov chains are a popular way of generating dependent random samples. The most important aspect of a Markov chain is that the next sample generated is only dependent on the previous sample and independent of everything else.</p>&#13;
<section data-pdf-bookmark="Markov Chains" data-type="sect2"><div class="sect2" id="markov_chains">&#13;
<h2>Markov Chains</h2>&#13;
<p>A Markov chain is used to model<a contenteditable="false" data-primary="Markov chain Monte Carlo (MCMC) simulations" data-secondary="Markov chains" data-type="indexterm" id="id1163"/><a contenteditable="false" data-primary="Monte Carlo simulation (MCS)" data-secondary="Markov chain Monte Carlo simulations" data-tertiary="Markov chains" data-type="indexterm" id="id1164"/><a contenteditable="false" data-primary="stochastic processes" data-secondary="modeled by Markov chains" data-type="indexterm" id="id1165"/><a contenteditable="false" data-primary="models" data-secondary="stochastic processes modeled by Markov chains" data-type="indexterm" id="id1166"/><a contenteditable="false" data-primary="time" data-secondary="stochastic processes modeled by Markov chains" data-type="indexterm" id="id1167"/><a contenteditable="false" data-primary="Markov chain Monte Carlo (MCMC) simulations" data-secondary="Markov chains" data-tertiary="memoryless" data-type="indexterm" id="id1168"/><a contenteditable="false" data-primary="knowledge integration" data-secondary="Markov chains memoryless" data-type="indexterm" id="id1169"/> a stochastic process consisting of a series of discrete and dependent states linked together in a chain-like structure. It is a sequential process that transitions probabilistically in discrete time from state to state in the chain. The most important aspect of a Markov state is that it is memoryless. For any state, its future state only depends on the transition probabilities of the current state and is independent of all past states and the path it took to get to its current state. It’s as if Markovian chains have encoded Master Oogway’s Zen saying from the movie <em>Kung Fu Panda</em>: “Yesterday is history, tomorrow is a mystery, but today is a gift. That is why it is called the present.”</p>&#13;
<p>Equally important, this simplifying memoryless property makes the Markovian chain easy to understand and implement. A random walk process, whether arithmetic or geometric, is a specific type of Markov chain and is used extensively to model asset prices, returns, interest rates, and volatility. <a contenteditable="false" data-primary="Markov chain Monte Carlo (MCMC) simulations" data-secondary="Markov chains" data-tertiary="bull, bear, stagnant financial markets" data-type="indexterm" id="id1170"/><a contenteditable="false" data-primary="market bull, bear, stagnant Markov chain" data-type="indexterm" id="id1171"/>A graphic representation of a Markov chain depicting the three basic and discrete states of the financial markets and their hypothetical transition probabilities is shown in <a data-type="xref" href="#a_markov_chain_depicting_the_three_basi">Figure 6-4</a>.</p>&#13;
<figure><div class="figure" id="a_markov_chain_depicting_the_three_basi">&#13;
<img alt="A Markov chain depicting the three basic states of the financial markets and their hypothetical transition probabilities" src="assets/pmlf_0604.png"/>&#13;
<h6><span class="label">Figure 6-4. </span>A Markov chain depicting the three basic states of the financial markets and their hypothetical transition probabilities<sup><a data-type="noteref" href="ch06.html#ch06fn10" id="ch06fn10-marker">10</a></sup></h6>&#13;
</div></figure>&#13;
<p>According to this state transition diagram, if the financial market is currently in a bear market state, there is an 80% probability it will remain in a bear market state. However, there is a 15% probability that the market will transition to a bull market state and a 5% probability it will transition to a stagnant market state.</p>&#13;
<p>Say the market transitions from a bear market state to a stagnant market state and then to a bull market state over time. Once it is in the bull market state, it will have no dependence or memory of the stagnant market state or bear market state. Probabilities about its transition to its future state will be dependent only on its present bull market state. So, for example, there is a 90% probability that it will stay in a bull market state regardless of whether it came from a stagnant market state or a bear market state or some permutation of the two. In other words, the future state of any Markov chain is conditionally independent of all past states given the current state.</p>&#13;
<p>Despite the random walks a <a contenteditable="false" data-primary="Markov chain Monte Carlo (MCMC) simulations" data-secondary="Markov chains" data-tertiary="conditional independence" data-type="indexterm" id="id1172"/><a contenteditable="false" data-primary="Markov chain Monte Carlo (MCMC) simulations" data-secondary="Markov chains" data-tertiary="stationary ergodic" data-type="indexterm" id="id1173"/><a contenteditable="false" data-primary="Monte Carlo simulation (MCS)" data-secondary="Markov chain Monte Carlo simulations" data-tertiary="Markov chains stationary ergodic" data-type="indexterm" id="id1174"/><a contenteditable="false" data-primary="stationary ergodic stochastic process" data-type="indexterm" id="id1175"/><a contenteditable="false" data-primary="stochastic processes" data-secondary="modeled by Markov chains" data-tertiary="stationary ergodic" data-type="indexterm" id="id1176"/><a contenteditable="false" data-primary="time" data-secondary="stationary ergodic as time invariant" data-tertiary="Markov chains as stationary ergodic" data-type="indexterm" id="id1177"/><a contenteditable="false" data-primary="financial theory" data-secondary="financial markets non-ergodic" data-type="indexterm" id="id1178"/>stochastic process takes in the state space of a Markov chain, if it can go from one state to every other state in a finite number of moves, the Markov chain is said to be stationary ergodic. Based on this definition, the Markov chain of the hypothetical financial market process depicted in <a data-type="xref" href="#there_are_n_number_of_grid_points_unifo">Figure 6-3</a> is stationary ergodic because the market will eventually reach any state in the Markov chain given enough time. Such a hypothetical stationary ergodic financial market would imply that the ensemble average price returns of all investors is expected to equal the price returns of every single random trajectory taken by any single investor in the ensemble over a long enough time period.</p>&#13;
<p>However, as was discussed earlier, real financial markets are neither stationary nor ergodic. For instance, as an investor, you could suffer heavy losses in an unrelenting bear market state, or make foolish investments in a bubblicious bull market state, or be forced to liquidate your investments to pay for expensive divorce lawyers in a stagnant market state, and never be in another market state again. You would then be banished to a special Markovian state called an absorbing state from which there is no escape. This special state absorbs the essence of the lyric from the Eagles’ song “Hotel California”: “You can check out any time you like, but you can never leave.” We will discuss the problem of ergodicity in finance and investing in <a data-type="xref" href="ch08.html#making_probabilistic_decisions_with_gen">Chapter 8</a>.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="the_metropolis_mcmc_algorithm_a_transf">&#13;
<h1>The Metropolis MCMC Algorithm: A Transformational Team Effort</h1>&#13;
<p>The idea of combining Monte Carlo<a contenteditable="false" data-primary="Metropolis, Nicholas" data-type="indexterm" id="id1179"/><a contenteditable="false" data-primary="Markov chain Monte Carlo (MCMC) simulations" data-secondary="combination origin" data-type="indexterm" id="id1180"/><a contenteditable="false" data-primary="Monte Carlo simulation (MCS)" data-secondary="Markov chain Monte Carlo simulations" data-tertiary="combination origin" data-type="indexterm" id="id1181"/><a contenteditable="false" data-primary="Rosenbluth, Arianna W." data-type="indexterm" id="id1182"/><a contenteditable="false" data-primary="Rosenbluth, Marshall" data-type="indexterm" id="id1183"/><a contenteditable="false" data-primary="Teller, Augusta H." data-type="indexterm" id="id1184"/><a contenteditable="false" data-primary="Teller, Edward" data-type="indexterm" id="id1185"/> methods with Markov chains to create a Markov chain Monte Carlo (MCMC) algorithm was first developed in the late 1940s by a team of brilliant physicists and mathematicians led by Nicholas Metropolis, for simulating the behavior of atoms in a lattice. The team included Arianna W. Rosenbluth, Marshall Rosenbluth, Augusta H. Teller, and Edward Teller, all of whom were instrumental in the development of the first MCMC algorithm. Arianna Rosenbluth wrote its first full implementation in machine language, the low-level computer language of 0’s and 1’s! The Metropolis algorithm was a groundbreaking MCMC algorithm and is ranked by many experts as one of the top 10 most important algorithms developed in the 20th century.</p>&#13;
<p>The Metropolis MCMC algorithm<a contenteditable="false" data-primary="Monte Carlo simulation (MCS)" data-secondary="Markov chain Monte Carlo simulations" data-tertiary="Metropolis MCMC algorithm" data-type="indexterm" id="ch06-met"/><a contenteditable="false" data-primary="Markov chain Monte Carlo (MCMC) simulations" data-secondary="Markov chains" data-tertiary="Metropolis MCMC algorithm" data-type="indexterm" id="ch06-met2"/><a contenteditable="false" data-primary="Metropolis MCMC algorithm" data-type="indexterm" id="ch06-met3"/> uses a symmetric normal proposal distribution to simulate any target distribution, and that is why it is also called the Random Walk Metropolis algorithm. The development of other MCMC algorithms and cheap computational resources made numerical approximations accessible to many scientists and practitioners in the 1990s, transforming the scope and usability of epistemic statistics and probabilistic inference.</p>&#13;
</div></aside>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Metropolis Sampling" data-type="sect2"><div class="sect2" id="metropolis_sampling">&#13;
<h2>Metropolis Sampling</h2>&#13;
<p>The Metropolis algorithm generates a Markov chain to simulate any discrete or continuous target probability distribution. The Metropolis algorithm iteratively generates dependent random samples based on three key elements:</p>&#13;
<dl>&#13;
<dt>Proposal probability distribution</dt>&#13;
<dd>This is a probability distribution that helps explore the target probability distribution efficiently by proposing the next state in the Markov chain based on the current state. Different proposal distributions can be used depending on the problem.</dd>&#13;
<dt>Proposal acceptance ratio</dt>&#13;
<dd>This is a measure of the relative probability of the proposed move. In a probabilistic inference problem, the acceptance ratio is the ratio of the posterior probabilities of the target distribution evaluated at the proposed state to the current state in the Markov chain. Recall from the previous chapter that taking the ratio of the posterior probabilities at two different points gets rid of the analytically intractable marginal probability distribution.</dd>&#13;
<dt>Decision rules on the proposed state</dt>&#13;
<dd>These are probabilistic decision rules that determine whether to accept or reject the proposed state in the chain. If the acceptance ratio is greater than or equal to 1, the proposed state is accepted and the Markov chain moves to the next state. If the acceptance ratio is less than 1, the algorithm generates a random number between 0 and 1. If the random number is less than the acceptance ratio, the proposed state is accepted. Otherwise it is rejected.</dd>&#13;
</dl>&#13;
<p>The Metropolis algorithm builds its Markov chain iteratively and stops when the required number of samples have been accepted. The accepted samples are then used to simulate the target probability distribution.</p>&#13;
<p>As a proof-of-concept of MCMC simulation,<a contenteditable="false" data-primary="Monte Carlo simulation (MCS)" data-secondary="Markov chain Monte Carlo simulations" data-tertiary="Student’s t-distribution simulation" data-type="indexterm" id="ch06-stud"/><a contenteditable="false" data-primary="Markov chain Monte Carlo (MCMC) simulations" data-secondary="Student’s t-distribution simulation" data-type="indexterm" id="ch06-stud2"/><a contenteditable="false" data-primary="Student’s t-distribution MCMC simulation" data-type="indexterm" id="ch06-stud3"/><a contenteditable="false" data-primary="t-distribution MCMC simulation" data-type="indexterm" id="ch06-stud4"/> we will use the Metropolis algorithm to simulate a Student’s t-distribution with six degrees of freedom. This distribution is widely used in finance and investing for modeling asset price return distributions with fat tails. The Student’s t-distribution is a family of probability distributions, with each specific distribution controlled by its degrees of freedom parameter. The lower that value, the fatter the tails of the distribution. We will apply this distribution and discuss it further in the next chapter.</p>&#13;
<p>In the following Python code, we use the uniform distribution as the proposal distribution and the Student’s t-distribution with six degrees of freedom as our target distribution to simulate. It initializes the Markov chain arbitrarily at x = 0 and runs the Metropolis sampling algorithm 10,000 times. The resulting samples are stored in a list, which is plotted to visualize the sample path of the Markov chain. Finally, the code plots a histogram of the samples to show its convergence to the actual target <span class="keep-together">distribution:</span></p>&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1">#Import Python libraries</code>&#13;
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>&#13;
<code class="kn">import</code> <code class="nn">scipy.stats</code> <code class="k">as</code> <code class="nn">stats</code>&#13;
<code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="k">as</code> <code class="nn">plt</code>&#13;
&#13;
<code class="c1"># Define the target distribution - Student's t-distribution </code>&#13;
<code class="c1"># with 6 degrees of freedom.</code>&#13;
<code class="c1"># Use location=0 and scale=1 parameters which are the default </code>&#13;
<code class="c1"># values of the Student's t-distribution</code>&#13;
<code class="c1"># x is any continuous variable</code>&#13;
<code class="k">def</code> <code class="nf">target</code><code class="p">(</code><code class="n">x</code><code class="p">):</code>&#13;
   <code class="k">return</code> <code class="n">stats</code><code class="o">.</code><code class="n">t</code><code class="o">.</code><code class="n">pdf</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">df</code><code class="o">=</code><code class="mi">6</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># Define the proposal distribution (uniform distribution)</code>&#13;
<code class="k">def</code> <code class="nf">proposal</code><code class="p">(</code><code class="n">x</code><code class="p">):</code>&#13;
   <code class="c1"># Returns random sample between x-0.5 and x+0.5 of the current value</code>&#13;
   <code class="k">return</code> <code class="n">stats</code><code class="o">.</code><code class="n">uniform</code><code class="o">.</code><code class="n">rvs</code><code class="p">(</code><code class="n">loc</code><code class="o">=</code><code class="n">x</code><code class="o">-</code><code class="mf">0.5</code><code class="p">,</code> <code class="n">scale</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># Set the initial state arbitrarily at 0 and set the number of </code>&#13;
<code class="c1"># iterations to 10,000</code>&#13;
<code class="n">x0</code> <code class="o">=</code> <code class="mi">0</code>&#13;
<code class="n">n_iter</code> <code class="o">=</code> <code class="mi">10000</code>&#13;
&#13;
<code class="c1"># Initialize the Markov chain and the samples list</code>&#13;
<code class="n">x</code> <code class="o">=</code> <code class="n">x0</code>&#13;
<code class="n">samples</code> <code class="o">=</code> <code class="p">[</code><code class="n">x</code><code class="p">]</code>&#13;
&#13;
<code class="c1"># Run the Metropolis algorithm to generate new samples and store them in </code>&#13;
<code class="c1"># the 'samples' list</code>&#13;
<code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">n_iter</code><code class="p">):</code>&#13;
   <code class="c1"># Generate a proposed state from the proposal distribution</code>&#13;
   <code class="n">x_proposed</code> <code class="o">=</code> <code class="n">proposal</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>&#13;
  &#13;
   <code class="c1"># Calculate the acceptance ratio</code>&#13;
   <code class="n">acceptance_ratio</code> <code class="o">=</code> <code class="n">target</code><code class="p">(</code><code class="n">x_proposed</code><code class="p">)</code> <code class="o">/</code> <code class="n">target</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>&#13;
  &#13;
   <code class="c1"># Accept or reject the proposed state</code>&#13;
   <code class="k">if</code> <code class="n">acceptance_ratio</code> <code class="o">&gt;=</code> <code class="mi">1</code><code class="p">:</code>&#13;
       <code class="c1"># Accept new sample</code>&#13;
       <code class="n">x</code> <code class="o">=</code> <code class="n">x_proposed</code>&#13;
   <code class="k">else</code><code class="p">:</code>&#13;
       <code class="n">u</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">rand</code><code class="p">()</code>&#13;
       <code class="c1"># Reject new sample</code>&#13;
       <code class="k">if</code> <code class="n">u</code> <code class="o">&lt;</code> <code class="n">acceptance_ratio</code><code class="p">:</code>&#13;
           <code class="n">x</code> <code class="o">=</code> <code class="n">x_proposed</code>&#13;
  &#13;
   <code class="c1"># Add the current state to the list of samples</code>&#13;
   <code class="n">samples</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># Plot the sample path of the Markov chain</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">samples</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s1">'Sample Number'</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s1">'Sample Value'</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="s1">'Sample Path of the Markov Chain'</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code>&#13;
&#13;
<code class="c1"># Plot the histogram of the samples and compare it with the target distribution</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">hist</code><code class="p">(</code><code class="n">samples</code><code class="p">,</code> <code class="n">bins</code><code class="o">=</code><code class="mi">50</code><code class="p">,</code> <code class="n">density</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">alpha</code><code class="o">=</code><code class="mf">0.5</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s1">'MCMC Samples'</code><code class="p">)</code>&#13;
<code class="n">x_range</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">linspace</code><code class="p">(</code><code class="o">-</code><code class="mi">5</code><code class="p">,</code> <code class="mi">5</code><code class="p">,</code> <code class="mi">1000</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">x_range</code><code class="p">,</code> <code class="n">target</code><code class="p">(</code><code class="n">x_range</code><code class="p">),</code> <code class="s1">'r-'</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s1">'Target Distribution'</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s1">'Sample Value'</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s1">'Probability Density'</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="s1">'MCMC Simulation of Students-T Distribution'</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">legend</code><code class="p">()</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>&#13;
<figure class="informal"><div class="figure">&#13;
<img alt="Image" src="assets/pmlf_06in04.png"/>&#13;
<h6/>&#13;
</div></figure>&#13;
<figure class="informal"><div class="figure">&#13;
<img alt="Image" src="assets/pmlf_06in05.png"/>&#13;
<h6/>&#13;
</div></figure>&#13;
<p>In 1970, William Hastings generalized<a contenteditable="false" data-primary="Hastings, William" data-type="indexterm" id="id1186"/><a contenteditable="false" data-primary="Monte Carlo simulation (MCS)" data-secondary="Markov chain Monte Carlo simulations" data-tertiary="Metropolis-Hastings MCMC algorithm" data-type="indexterm" id="id1187"/><a contenteditable="false" data-primary="Markov chain Monte Carlo (MCMC) simulations" data-secondary="Markov chains" data-tertiary="Metropolis-Hastings MCMC algorithm" data-type="indexterm" id="id1188"/><a contenteditable="false" data-primary="Metropolis-Hastings MCMC algorithm" data-type="indexterm" id="id1189"/> the Metropolis sampling algorithm so that asymmetric proposal distributions and more flexible acceptance criteria could be applied. The resulting Metropolis-Hastings MCMC algorithm can simulate any target probability distribution asymptotically, i.e., given enough samples, the simulation will converge to the target probability distribution. However, this algorithm can be inefficient and costly for high-dimensional, complex target distributions.</p>&#13;
<p>The Metropolis-Hastings algorithm is dependent on the arbitrary initial starting value of the Markov chain. The initial samples gathered during this period, called the burn-in period, are generally discarded. The randomness of the walk-through state space can waste time due to the possibility of revisiting the same regions several times. Moreover, the algorithm can get stuck in narrow regions of multidimensional spaces.</p>&#13;
<p>Modern dependent sampling algorithms have been developed to address the shortcomings of this general-purpose MCMC sampling algorithm. The Hamiltonian Monte Carlo (HMC) algorithm uses the geometry of any continuous target distribution to move efficiently in high-dimensional space. It is the default MCMC sampling algorithm in the PyMC library, and we don’t need any specialized knowledge to use it. In the next chapter, we will use these MCMC algorithms to simulate the posterior probability distributions of model parameters.<a contenteditable="false" data-primary="" data-startref="ch06-met" data-type="indexterm" id="id1190"/><a contenteditable="false" data-primary="" data-startref="ch06-met2" data-type="indexterm" id="id1191"/><a contenteditable="false" data-primary="" data-startref="ch06-met3" data-type="indexterm" id="id1192"/><a contenteditable="false" data-primary="" data-startref="ch06-stud" data-type="indexterm" id="id1193"/><a contenteditable="false" data-primary="" data-startref="ch06-stud2" data-type="indexterm" id="id1194"/><a contenteditable="false" data-primary="" data-startref="ch06-stud3" data-type="indexterm" id="id1195"/><a contenteditable="false" data-primary="" data-startref="ch06-stud4" data-type="indexterm" id="id1196"/></p>&#13;
</div></section>&#13;
</div></section>&#13;
<section class="pagebreak-before" data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="summary-id00025">&#13;
<h1>Summary</h1>&#13;
<p>Traditional statistical MLE models on which most conventional ML systems are based are limited in their capabilities. They are designed to deal with only aleatory uncertainty and are unaware of their limitations. As we have demonstrated in this chapter, MLE-based models make silly predictions confidently. This makes them dangerous in our world of three-dimensional uncertainty. Poor predictive performance and disastrous risk management from such overconfident, simplistic, and hasty ML models are almost surely inevitable in the complex world of finance and investing.</p>&#13;
<p>In designing probabilistic models, we acknowledge the fact that only death is certain—everything else, including taxes, has a probability distribution. Probabilistic models are designed to manage uncertainties generated from noisy sample data and inexact model parameters. These models enable us to go from a one-dimensional world of aleatory uncertainty to a two-dimensional world with aleatory and epistemic uncertainties. This makes them more appropriate for the world of finance and investing. However, this comes at the cost of higher computational complexities.</p>&#13;
<p>To apply probabilistic machine learning to complex financial and investing problems, we have to use dependent random sampling because other numerical methods don’t work or don’t scale. MCMC simulation methods are transformative. They use dependent random sampling algorithms to simulate complex probability distributions that are difficult to sample from directly. We will apply MCMC methods in the next chapter, using a popular probabilistic ML Python library.</p>&#13;
<p>Ontological uncertainty emanates from complex social systems, which can be disruptive at times. Among other things, it involves rethinking and redesigning the probabilistic model from scratch and making it more appropriate for the new market environment. This is generally best managed by human beings with common sense, judgment, and experience. We are still very much relevant in the bold, new world of AI and have, indeed, the hardest job.</p>&#13;
</div></section>&#13;
<section class="pagebreak-before" data-pdf-bookmark="References" data-type="sect1"><div class="sect1" id="references-id00016">&#13;
<h1>References</h1>&#13;
<p>Dürr, Oliver, and Beate Sick. <em>Probabilistic Deep Learning with Python, Keras, and TensorFlow Probability</em>. Manning Publications, 2020.</p>&#13;
<p>Guo, Chuan, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. “On Calibration of Modern Neural Networks.” Last revised August 3, 2017. <em>https://arxiv.org/abs/1706.04599.</em> </p>&#13;
<p>Lambert, Ben. <em>A Student’s Guide to Bayesian Statistics</em>. London, UK: Sage Publications, 2018.</p>&#13;
<p>Mitchell, Melanie. <em>Artificial Intelligence: A Guide for Thinking Humans</em>. New York: Farrar, Straus and Giroux, 2020.</p>&#13;
<p>Vigen, Tyler. <em>Spurious Correlations</em>. New York: Hachette Books, 2015.</p>&#13;
</div></section>&#13;
<div data-type="footnotes"><p data-type="footnote" id="ch06fn1"><sup><a href="ch06.html#ch06fn1-marker">1</a></sup> Oliver Dürr and Beate Sick, “Bayesian Learning,” in <em>Probabilistic Deep Learning with Python, Keras, and TensorFlow Probability</em> (Manning Publications, 2020), 197–228.</p><p data-type="footnote" id="ch06fn2"><sup><a href="ch06.html#ch06fn2-marker">2</a></sup> Tyler Vigen, <em>Spurious Correlations</em> (New York: Hachette Books, 2015).</p><p data-type="footnote" id="ch06fn3"><sup><a href="ch06.html#ch06fn3-marker">3</a></sup> Adapted from an image on Wikimedia Commons.</p><p data-type="footnote" id="ch06fn4"><sup><a href="ch06.html#ch06fn4-marker">4</a></sup> Melanie Mitchell, “Knowledge, Abstraction, and Analogy in Artificial Intelligence,” in <em>Artificial Intelligence: A Guide for Thinking Humans</em> (New York: Farrar, Straus and Giroux, 2019), 247–65.</p><p data-type="footnote" id="ch06fn5"><sup><a href="ch06.html#ch06fn5-marker">5</a></sup> Dürr and Sick, “Bayesian Learning.”</p><p data-type="footnote" id="ch06fn6"><sup><a href="ch06.html#ch06fn6-marker">6</a></sup> Adapted from an image on Wikimedia Commons.</p><p data-type="footnote" id="ch06fn7"><sup><a href="ch06.html#ch06fn7-marker">7</a></sup> Dürr and Sick, “Bayesian Learning.”</p><p data-type="footnote" id="ch06fn8"><sup><a href="ch06.html#ch06fn8-marker">8</a></sup> Adapted from an image on Wikimedia Commons.</p><p data-type="footnote" id="ch06fn9"><sup><a href="ch06.html#ch06fn9-marker">9</a></sup> Ben Lambert, “Leaving Conjugates Behind: Markov Chain Monte Carlo,” in <em>A Student’s Guide to Bayesian Statistics</em> (London, UK: Sage Publications, 2018), 263–90.</p><p data-type="footnote" id="ch06fn10"><sup><a href="ch06.html#ch06fn10-marker">10</a></sup> Adapted from an image on Wikimedia Commons.</p></div></div></section></body></html>