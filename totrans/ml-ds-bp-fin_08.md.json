["```py\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot\n\nfrom pandas import read_csv, set_option\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.preprocessing import StandardScaler\n```", "```py\nfrom sklearn.model_selection import train_test_split, KFold,\\\n cross_val_score, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier,\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix,\\\n  accuracy_score\n```", "```py\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasClassifier\n```", "```py\nfrom pickle import dump\nfrom pickle import load\n```", "```py\n# shape\ndataset.shape\n```", "```py\n(284807, 31)\n```", "```py\n#peek at data\nset_option('display.width', 100)\ndataset.head(5)\n```", "```py\nclass_names = {0:'Not Fraud', 1:'Fraud'}\nprint(dataset.Class.value_counts().rename(index = class_names))\n```", "```py\nNot Fraud    284315\nFraud           492\nName: Class, dtype: int64\n```", "```py\nY= dataset[\"Class\"]\nX = dataset.loc[:, dataset.columns != 'Class']\nvalidation_size = 0.2\nseed = 7\nX_train, X_validation, Y_train, Y_validation =\\\ntrain_test_split(X, Y, test_size=validation_size, random_state=seed)\n```", "```py\n# test options for classification\nnum_folds = 10\nscoring = 'accuracy'\n```", "```py\n# spot-check basic Classification algorithms\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\n```", "```py\nresults = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=num_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, \\\n      scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n```", "```py\nLR: 0.998942 (0.000229)\nLDA: 0.999364 (0.000136)\nKNN: 0.998310 (0.000290)\nCART: 0.999175 (0.000193)\n```", "```py\n# compare algorithms\nfig = pyplot.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\nfig.set_size_inches(8,4)\npyplot.show()\n```", "```py\n# prepare model\nmodel = DecisionTreeClassifier()\nmodel.fit(X_train, Y_train)\n\n# estimate accuracy on validation set\npredictions = model.predict(X_validation)\nprint(accuracy_score(Y_validation, predictions))\nprint(classification_report(Y_validation, predictions))\n```", "```py\n0.9992275552122467\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     56862\n           1       0.77      0.79      0.78       100\n\n    accuracy                           1.00     56962\n   macro avg       0.89      0.89      0.89     56962\nweighted avg       1.00      1.00      1.00     56962\n```", "```py\ndf_cm = pd.DataFrame(confusion_matrix(Y_validation, predictions), \\\ncolumns=np.unique(Y_validation), index = np.unique(Y_validation))\ndf_cm.index.name = 'Actual'\ndf_cm.columns.name = 'Predicted'\nsns.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 16})\n```", "```py\nscoring = 'recall'\n```", "```py\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\n```", "```py\nresults = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=num_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, \\\n      scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n```", "```py\nLR: 0.595470 (0.089743)\nLDA: 0.758283 (0.045450)\nKNN: 0.023882 (0.019671)\nCART: 0.735192 (0.073650)\n```", "```py\n# prepare model\nmodel = LinearDiscriminantAnalysis()\nmodel.fit(X_train, Y_train)\n# estimate accuracy on validation set\n\npredictions = model.predict(X_validation)\nprint(accuracy_score(Y_validation, predictions))\n```", "```py\n0.9995435553526912\n```", "```py\ndf = pd.concat([X_train, Y_train], axis=1)\n# amount of fraud classes 492 rows.\nfraud_df = df.loc[df['Class'] == 1]\nnon_fraud_df = df.loc[df['Class'] == 0][:492]\n\nnormal_distributed_df = pd.concat([fraud_df, non_fraud_df])\n\n# Shuffle dataframe rows\ndf_new = normal_distributed_df.sample(frac=1, random_state=42)\n# split out validation dataset for the end\nY_train_new= df_new[\"Class\"]\nX_train_new = df_new.loc[:, dataset.columns != 'Class']\n```", "```py\nprint('Distribution of the Classes in the subsample dataset')\nprint(df_new['Class'].value_counts()/len(df_new))\nsns.countplot('Class', data=df_new)\npyplot.title('Equally Distributed Classes', fontsize=14)\npyplot.show()\n```", "```py\nDistribution of the Classes in the subsample dataset\n1    0.5\n0    0.5\nName: Class, dtype: float64\n```", "```py\n#setting the evaluation metric\nscoring='accuracy'\n# spot-check the algorithms\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC()))\n#Neural Network\nmodels.append(('NN', MLPClassifier()))\n# Ensemble Models\n# Boosting methods\nmodels.append(('AB', AdaBoostClassifier()))\nmodels.append(('GBM', GradientBoostingClassifier()))\n# Bagging methods\nmodels.append(('RF', RandomForestClassifier()))\nmodels.append(('ET', ExtraTreesClassifier()))\n```", "```py\n# Function to create model, required for KerasClassifier\ndef create_model(neurons=12, activation='relu', learn_rate = 0.01, momentum=0):\n    # create model\n    model = Sequential()\n    model.add(Dense(X_train.shape[1], input_dim=X_train.shape[1], \\\n      activation=activation))\n    model.add(Dense(32,activation=activation))\n    model.add(Dense(1, activation='sigmoid'))\n    # Compile model\n    optimizer = SGD(lr=learn_rate, momentum=momentum)\n    model.compile(loss='binary_crossentropy', optimizer='adam', \\\n    metrics=['accuracy'])\n    return model\nmodels.append(('DNN', KerasClassifier(build_fn=create_model,\\\nepochs=50, batch_size=10, verbose=0)))\n```", "```py\n# Grid Search: GradientBoosting Tuning\nn_estimators = [20,180,1000]\nmax_depth= [2, 3,5]\nparam_grid = dict(n_estimators=n_estimators, max_depth=max_depth)\nmodel = GradientBoostingClassifier()\nkfold = KFold(n_splits=num_folds, random_state=seed)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, \\\n  cv=kfold)\ngrid_result = grid.fit(X_train_new, Y_train_new)\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n```", "```py\nBest: 0.936992 using {'max_depth': 5, 'n_estimators': 1000}\n```", "```py\n# prepare model\nmodel = GradientBoostingClassifier(max_depth= 5, n_estimators = 1000)\nmodel.fit(X_train_new, Y_train_new)\n# estimate accuracy on Original validation set\npredictions = model.predict(X_validation)\nprint(accuracy_score(Y_validation, predictions))\n```", "```py\n0.9668199852533268\n```", "```py\n# load dataset\ndataset = pd.read_csv('LoansData.csv.gz', compression='gzip', \\\nlow_memory=True)\n```", "```py\ndataset.shape\n```", "```py\n(1646801, 150)\n```", "```py\ndataset['loan_status'].value_counts(dropna=False)\n```", "```py\nCurrent                                                788950\nFully Paid                                             646902\nCharged Off                                            168084\nLate (31-120 days)                                      23763\nIn Grace Period                                         10474\nLate (16-30 days)                                        5786\nDoes not meet the credit policy. Status:Fully Paid       1988\nDoes not meet the credit policy. Status:Charged Off       761\nDefault                                                    70\nNaN                                                        23\nName: loan_status, dtype: int64\n```", "```py\ndataset = dataset.loc[dataset['loan_status'].isin(['Fully Paid', 'Charged Off'])]\ndataset['loan_status'].value_counts(normalize=True, dropna=False)\n```", "```py\nFully Paid     0.793758\nCharged Off    0.206242\nName: loan_status, dtype: float64\n```", "```py\ndataset['charged_off'] = (dataset['loan_status'] == 'Charged Off').apply(np.uint8)\ndataset.drop('loan_status', axis=1, inplace=True)\n```", "```py\nmissing_fractions = dataset.isnull().mean().sort_values(ascending=False)\n\n#Drop the missing fraction\ndrop_list = sorted(list(missing_fractions[missing_fractions > 0.3].index))\ndataset.drop(labels=drop_list, axis=1, inplace=True)\ndataset.shape\n```", "```py\n(814986, 92)\n```", "```py\nkeep_list = ['charged_off','funded_amnt','addr_state', 'annual_inc', \\\n'application_type','dti', 'earliest_cr_line', 'emp_length',\\\n'emp_title', 'fico_range_high',\\\n'fico_range_low', 'grade', 'home_ownership', 'id', 'initial_list_status', \\\n'installment', 'int_rate', 'loan_amnt', 'loan_status',\\\n'mort_acc', 'open_acc', 'pub_rec', 'pub_rec_bankruptcies', \\\n'purpose', 'revol_bal', 'revol_util', \\\n'sub_grade', 'term', 'title', 'total_acc',\\\n'verification_status', 'zip_code','last_pymnt_amnt',\\\n'num_actv_rev_tl', 'mo_sin_rcnt_rev_tl_op',\\\n'mo_sin_old_rev_tl_op',\"bc_util\",\"bc_open_to_buy\",\\\n\"avg_cur_bal\",\"acc_open_past_24mths\" ]\n\ndrop_list = [col for col in dataset.columns if col not in keep_list]\ndataset.drop(labels=drop_list, axis=1, inplace=True)\ndataset.shape\n```", "```py\n(814986, 39)\n```", "```py\ncorrelation = dataset.corr()\ncorrelation_chargeOff = abs(correlation['charged_off'])\ndrop_list_corr = sorted(list(correlation_chargeOff\\\n  [correlation_chargeOff < 0.03].index))\nprint(drop_list_corr)\n```", "```py\n['pub_rec', 'pub_rec_bankruptcies', 'revol_bal', 'total_acc']\n```", "```py\ndataset.drop(labels=drop_list_corr, axis=1, inplace=True)\n```", "```py\ndataset[['id','emp_title','title','zip_code']].describe()\n```", "```py\ndataset.drop(['id','emp_title','title','zip_code'], axis=1, inplace=True)\n```", "```py\ndataset['term'] = dataset['term'].apply(lambda s: np.int8(s.split()[0]))\ndataset.groupby('term')['charged_off'].value_counts(normalize=True).loc[:,1]\n```", "```py\nterm\n36    0.165710\n60    0.333793\nName: charged_off, dtype: float64\n```", "```py\ndataset['emp_length'].replace(to_replace='10+ years', value='10 years',\\\n  inplace=True)\n\ndataset['emp_length'].replace('< 1 year', '0 years', inplace=True)\n\ndef emp_length_to_int(s):\n    if pd.isnull(s):\n        return s\n    else:\n        return np.int8(s.split()[0])\n\ndataset['emp_length'] = dataset['emp_length'].apply(emp_length_to_int)\ncharge_off_rates = dataset.groupby('emp_length')['charged_off'].value_counts\\\n  (normalize=True).loc[:,1]\nsns.barplot(x=charge_off_rates.index, y=charge_off_rates.values)\n```", "```py\ndataset.drop(['emp_length'], axis=1, inplace=True)\n```", "```py\ncharge_off_rates = dataset.groupby('sub_grade')['charged_off'].value_counts\\\n(normalize=True).loc[:,1]\nsns.barplot(x=charge_off_rates.index, y=charge_off_rates.values)\n```", "```py\ndataset[['annual_inc']].describe()\n```", "```py\ndataset['log_annual_inc'] = dataset['annual_inc'].apply(lambda x: np.log10(x+1))\ndataset.drop('annual_inc', axis=1, inplace=True)\n```", "```py\ndataset[['fico_range_low','fico_range_high']].corr()\n```", "```py\ndataset['fico_score'] = 0.5*dataset['fico_range_low'] +\\\n 0.5*dataset['fico_range_high']\n\ndataset.drop(['fico_range_high', 'fico_range_low'], axis=1, inplace=True)\n```", "```py\nfrom sklearn.preprocessing import LabelEncoder\n# Categorical boolean mask\ncategorical_feature_mask = dataset.dtypes==object\n# filter categorical columns using mask and turn it into a list\ncategorical_cols = dataset.columns[categorical_feature_mask].tolist()\n```", "```py\ncategorical_cols\n```", "```py\n['grade',\n 'sub_grade',\n 'home_ownership',\n 'verification_status',\n 'purpose',\n 'addr_state',\n 'initial_list_status',\n 'application_type']\n```", "```py\nloanstatus_0 = dataset[dataset[\"charged_off\"]==0]\nloanstatus_1 = dataset[dataset[\"charged_off\"]==1]\nsubset_of_loanstatus_0 = loanstatus_0.sample(n=5500)\nsubset_of_loanstatus_1 = loanstatus_1.sample(n=5500)\ndataset = pd.concat([subset_of_loanstatus_1, subset_of_loanstatus_0])\ndataset = dataset.sample(frac=1).reset_index(drop=True)\nprint(\"Current shape of dataset :\",dataset.shape)\n```", "```py\nY= dataset[\"charged_off\"]\nX = dataset.loc[:, dataset.columns != 'charged_off']\nvalidation_size = 0.2\nseed = 7\nX_train, X_validation, Y_train, Y_validation = \\\ntrain_test_split(X, Y, test_size=validation_size, random_state=seed)\n```", "```py\nnum_folds = 10\nscoring = 'roc_auc'\n```", "```py\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\n# Neural Network\nmodels.append(('NN', MLPClassifier()))\n# Ensemble Models\n# Boosting methods\nmodels.append(('AB', AdaBoostClassifier()))\nmodels.append(('GBM', GradientBoostingClassifier()))\n# Bagging methods\nmodels.append(('RF', RandomForestClassifier()))\nmodels.append(('ET', ExtraTreesClassifier()))\n```", "```py\n# Grid Search: GradientBoosting Tuning\nn_estimators = [20,180]\nmax_depth= [3,5]\nparam_grid = dict(n_estimators=n_estimators, max_depth=max_depth)\nmodel = GradientBoostingClassifier()\nkfold = KFold(n_splits=num_folds, random_state=seed)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, \\\n  cv=kfold)\ngrid_result = grid.fit(X_train, Y_train)\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n```", "```py\nBest: 0.952950 using {'max_depth': 5, 'n_estimators': 180}\n```", "```py\nmodel = GradientBoostingClassifier(max_depth= 5, n_estimators= 180)\nmodel.fit(X_train, Y_train)\n\n# estimate accuracy on validation set\npredictions = model.predict(X_validation)\nprint(accuracy_score(Y_validation, predictions))\n```", "```py\n0.889090909090909\n```", "```py\nprint(model.feature_importances_) #use inbuilt class feature_importances\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\n#plot graph of feature importances for better visualization\nfeat_importances.nlargest(10).plot(kind='barh')\npyplot.show()\n```", "```py\n# load dataset\ndataset = pd.read_csv('BitstampData.csv')\n```", "```py\ndataset.shape\n```", "```py\n(2841377, 8)\n```", "```py\n# peek at data\nset_option('display.width', 100)\ndataset.tail(2)\n```", "```py\ndataset[dataset.columns.values] = dataset[dataset.columns.values].ffill()\n```", "```py\ndataset=dataset.drop(columns=['Timestamp'])\n```", "```py\n# Create short simple moving average over the short window\ndataset['short_mavg'] = dataset['Close'].rolling(window=10, min_periods=1,\\\ncenter=False).mean()\n\n# Create long simple moving average over the long window\ndataset['long_mavg'] = dataset['Close'].rolling(window=60, min_periods=1,\\\ncenter=False).mean()\n\n# Create signals\ndataset['signal'] = np.where(dataset['short_mavg'] >\ndataset['long_mavg'], 1.0, 0.0)\n```", "```py\n#calculation of exponential moving average\ndef EMA(df, n):\n    EMA = pd.Series(df['Close'].ewm(span=n, min_periods=n).mean(), name='EMA_'\\\n     + str(n))\n    return EMA\ndataset['EMA10'] = EMA(dataset, 10)\ndataset['EMA30'] = EMA(dataset, 30)\ndataset['EMA200'] = EMA(dataset, 200)\ndataset.head()\n\n#calculation of rate of change\ndef ROC(df, n):\n    M = df.diff(n - 1)\n    N = df.shift(n - 1)\n    ROC = pd.Series(((M / N) * 100), name = 'ROC_' + str(n))\n    return ROC\ndataset['ROC10'] = ROC(dataset['Close'], 10)\ndataset['ROC30'] = ROC(dataset['Close'], 30)\n\n#calculation of price momentum\ndef MOM(df, n):\n    MOM = pd.Series(df.diff(n), name='Momentum_' + str(n))\n    return MOM\ndataset['MOM10'] = MOM(dataset['Close'], 10)\ndataset['MOM30'] = MOM(dataset['Close'], 30)\n\n#calculation of relative strength index\ndef RSI(series, period):\n delta = series.diff().dropna()\n u = delta * 0\n d = u.copy()\n u[delta > 0] = delta[delta > 0]\n d[delta < 0] = -delta[delta < 0]\n u[u.index[period-1]] = np.mean( u[:period] ) #first value is sum of avg gains\n u = u.drop(u.index[:(period-1)])\n d[d.index[period-1]] = np.mean( d[:period] ) #first value is sum of avg losses\n d = d.drop(d.index[:(period-1)])\n rs = u.ewm(com=period-1, adjust=False).mean() / \\\n d.ewm(com=period-1, adjust=False).mean()\n return 100 - 100 / (1 + rs)\ndataset['RSI10'] = RSI(dataset['Close'], 10)\ndataset['RSI30'] = RSI(dataset['Close'], 30)\ndataset['RSI200'] = RSI(dataset['Close'], 200)\n\n#calculation of stochastic osillator.\n\ndef STOK(close, low, high, n):\n STOK = ((close - low.rolling(n).min()) / (high.rolling(n).max() - \\\n low.rolling(n).min())) * 100\n return STOK\n\ndef STOD(close, low, high, n):\n STOK = ((close - low.rolling(n).min()) / (high.rolling(n).max() - \\\n low.rolling(n).min())) * 100\n STOD = STOK.rolling(3).mean()\n return STOD\n\ndataset['%K10'] = STOK(dataset['Close'], dataset['Low'], dataset['High'], 10)\ndataset['%D10'] = STOD(dataset['Close'], dataset['Low'], dataset['High'], 10)\ndataset['%K30'] = STOK(dataset['Close'], dataset['Low'], dataset['High'], 30)\ndataset['%D30'] = STOD(dataset['Close'], dataset['Low'], dataset['High'], 30)\ndataset['%K200'] = STOK(dataset['Close'], dataset['Low'], dataset['High'], 200)\ndataset['%D200'] = STOD(dataset['Close'], dataset['Low'], dataset['High'], 200)\n\n#calculation of moving average\ndef MA(df, n):\n    MA = pd.Series(df['Close'].rolling(n, min_periods=n).mean(), name='MA_'\\\n     + str(n))\n    return MA\ndataset['MA21'] = MA(dataset, 10)\ndataset['MA63'] = MA(dataset, 30)\ndataset['MA252'] = MA(dataset, 200)\n```", "```py\ndataset[['Weighted_Price']].plot(grid=True)\nplt.show()\n```", "```py\nfig = plt.figure()\nplot = dataset.groupby(['signal']).size().plot(kind='barh', color='red')\nplt.show()\n```", "```py\n# split out validation dataset for the end\nsubset_dataset= dataset.iloc[-100000:]\nY= subset_dataset[\"signal\"]\nX = subset_dataset.loc[:, dataset.columns != 'signal']\nvalidation_size = 0.2\nseed = 1\nX_train, X_validation, Y_train, Y_validation =\\\ntrain_test_split(X, Y, test_size=validation_size, random_state=1)\n```", "```py\n# test options for classification\nnum_folds = 10\nscoring = 'accuracy'\n```", "```py\nmodels = []\nmodels.append(('LR', LogisticRegression(n_jobs=-1)))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\n#Neural Network\nmodels.append(('NN', MLPClassifier()))\n# Ensemble Models\n# Boosting methods\nmodels.append(('AB', AdaBoostClassifier()))\nmodels.append(('GBM', GradientBoostingClassifier()))\n# Bagging methods\nmodels.append(('RF', RandomForestClassifier(n_jobs=-1)))\n```", "```py\nn_estimators = [20,80]\nmax_depth= [5,10]\ncriterion = [\"gini\",\"entropy\"]\nparam_grid = dict(n_estimators=n_estimators, max_depth=max_depth, \\\n  criterion = criterion )\nmodel = RandomForestClassifier(n_jobs=-1)\nkfold = KFold(n_splits=num_folds, random_state=seed)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, \\\n  scoring=scoring, cv=kfold)\ngrid_result = grid.fit(X_train, Y_train)\nprint(\"Best: %f using %s\" % (grid_result.best_score_,\\\n  grid_result.best_params_))\n```", "```py\nBest: 0.903438 using {'criterion': 'gini', 'max_depth': 10, 'n_estimators': 80}\n```", "```py\n# prepare model\nmodel = RandomForestClassifier(criterion='gini', n_estimators=80,max_depth=10)\n\n#model = LogisticRegression()\nmodel.fit(X_train, Y_train)\n\n# estimate accuracy on validation set\npredictions = model.predict(X_validation)\nprint(accuracy_score(Y_validation, predictions))\n```", "```py\n0.9075\n```", "```py\nImportance = pd.DataFrame({'Importance':model.feature_importances_*100},\\\n index=X.columns)\nImportance.sort_values('Importance', axis=0, ascending=True).plot(kind='barh', \\\ncolor='r' )\nplt.xlabel('Variable Importance')\n```", "```py\nbacktestdata = pd.DataFrame(index=X_validation.index)\nbacktestdata['signal_pred'] = predictions\nbacktestdata['signal_actual'] = Y_validation\nbacktestdata['Market Returns'] = X_validation['Close'].pct_change()\nbacktestdata['Actual Returns'] = backtestdata['Market Returns'] *\\\nbacktestdata['signal_actual'].shift(1)\nbacktestdata['Strategy Returns'] = backtestdata['Market Returns'] * \\\nbacktestdata['signal_pred'].shift(1)\nbacktestdata=backtestdata.reset_index()\nbacktestdata.head()\nbacktestdata[['Strategy Returns','Actual Returns']].cumsum().hist()\nbacktestdata[['Strategy Returns','Actual Returns']].cumsum().plot()\n```"]