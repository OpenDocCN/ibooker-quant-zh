- en: Chapter 2\. Developing a Machine Learning Model in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In terms of the platforms used for machine learning, there are many algorithms
    and programming languages. However, the Python ecosystem is one of the most dominant
    and fastest-growing programming languages for machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Given the popularity and high adoption rate of Python, we will use it as the
    main programming language throughout the book. This chapter provides an overview
    of a Python-based machine learning framework. First, we will review the details
    of Python-based packages used for machine learning, followed by the model development
    steps in the Python framework.
  prefs: []
  type: TYPE_NORMAL
- en: The steps of model development in Python presented in this chapter serve as
    the foundation for the case studies presented in the rest of the book. The Python
    framework can also be leveraged while developing any machine learning–based model
    in finance.
  prefs: []
  type: TYPE_NORMAL
- en: Why Python?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Some reasons for Python’s popularity are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: High-level syntax (compared to lower-level languages of C, Java, and C++). Applications
    can be developed by writing fewer lines of code, making Python attractive to beginners
    and advanced programmers alike.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Efficient development lifecycle.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Large collection of community-managed, open-source libraries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Strong portability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The simplicity of Python has attracted many developers to create new libraries
    for machine learning, leading to strong adoption of Python.
  prefs: []
  type: TYPE_NORMAL
- en: Python Packages for Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main Python packages used for machine learning are highlighted in [Figure 2-1](#Packages).
  prefs: []
  type: TYPE_NORMAL
- en: '![mlbf 0201](Images/mlbf_0201.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-1\. Python packages
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Here is a brief summary of each of these packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[NumPy](https://numpy.org)'
  prefs: []
  type: TYPE_NORMAL
- en: Provides support for large, multidimensional arrays as well as an extensive
    collection of mathematical functions.
  prefs: []
  type: TYPE_NORMAL
- en: '[Pandas](https://pandas.pydata.org)'
  prefs: []
  type: TYPE_NORMAL
- en: A library for data manipulation and analysis. Among other features, it offers
    data structures to handle tables and the tools to manipulate them.
  prefs: []
  type: TYPE_NORMAL
- en: '[Matplotlib](https://matplotlib.org)'
  prefs: []
  type: TYPE_NORMAL
- en: A plotting library that allows the creation of 2D charts and plots.
  prefs: []
  type: TYPE_NORMAL
- en: '[SciPy](https://www.scipy.org)'
  prefs: []
  type: TYPE_NORMAL
- en: The combination of NumPy, Pandas, and Matplotlib is generally referred to as
    SciPy. SciPy is an ecosystem of Python libraries for mathematics, science, and
    engineering.
  prefs: []
  type: TYPE_NORMAL
- en: '[Scikit-learn](https://scikit-learn.org) (or sklearn)'
  prefs: []
  type: TYPE_NORMAL
- en: A machine learning library offering a wide range of algorithms and utilities.
  prefs: []
  type: TYPE_NORMAL
- en: '[StatsModels](https://www.statsmodels.org)'
  prefs: []
  type: TYPE_NORMAL
- en: A Python module that provides classes and functions for the estimation of many
    different statistical models, as well as for conducting statistical tests and
    statistical data exploration.
  prefs: []
  type: TYPE_NORMAL
- en: '[TensorFlow](https://www.tensorflow.org) and [Theano](http://deeplearning.net/software/theano)'
  prefs: []
  type: TYPE_NORMAL
- en: Dataflow programming libraries that facilitate working with neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: '[Keras](https://keras.io)'
  prefs: []
  type: TYPE_NORMAL
- en: An artificial neural network library that can act as a simplified interface
    to TensorFlow/Theano packages.
  prefs: []
  type: TYPE_NORMAL
- en: '[Seaborn](https://seaborn.pydata.org)'
  prefs: []
  type: TYPE_NORMAL
- en: A data visualization library based on Matplotlib. It provides a high-level interface
    for drawing attractive and informative statistical graphics.
  prefs: []
  type: TYPE_NORMAL
- en: '[pip](https://pypi.org/project/pip) and [Conda](https://docs.conda.io/en/latest)'
  prefs: []
  type: TYPE_NORMAL
- en: These are Python package managers. pip is a package manager that facilitates
    installation, upgrade, and uninstallation of Python packages. Conda is a package
    manager that handles Python packages as well as library dependencies outside of
    the Python packages.
  prefs: []
  type: TYPE_NORMAL
- en: Python and Package Installation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are different ways of installing Python. However, it is strongly recommended
    that you install Python through [Anaconda](https://www.anaconda.com). Anaconda
    contains Python, SciPy, and Scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: 'After installing Anaconda, a Jupyter server can be started locally by opening
    the machine’s terminal and typing in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: All code samples in this book use Python 3 and are presented in Jupyter notebooks.
    Several Python packages, especially Scikit-learn and Keras, are extensively used
    in the case studies.
  prefs: []
  type: TYPE_NORMAL
- en: Steps for Model Development in Python Ecosystem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Working through machine learning problems from end to end is critically important.
    Applied machine learning will not come alive unless the steps from beginning to
    end are well defined.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 2-2](#StepsofML) provides an outline of the simple seven-step machine
    learning project template that can be used to jump-start any machine learning
    model in Python. The first few steps include exploratory data analysis and data
    preparation, which are typical data science–based steps aimed at extracting meaning
    and insights from data. These steps are followed by model evaluation, fine-tuning,
    and finalizing the model.'
  prefs: []
  type: TYPE_NORMAL
- en: '![mlbf 0202](Images/mlbf_0202.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-2\. Model development steps
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: All the case studies in this book follow the standard seven-step model development
    process. However, there are a few case studies in which some of the steps are
    skipped, renamed, or reordered based on the appropriateness and intuitiveness
    of the steps.
  prefs: []
  type: TYPE_NORMAL
- en: Model Development Blueprint
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following section covers the details of each model development step with
    supporting Python code.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Problem definition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first step in any project is defining the problem. Powerful algorithms can
    be used for solving the problem, but the results will be meaningless if the wrong
    problem is solved.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following framework should be used for defining the problem:'
  prefs: []
  type: TYPE_NORMAL
- en: Describe the problem informally and formally. List assumptions and similar problems.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: List the motivation for solving the problem, the benefits a solution provides,
    and how the solution will be used.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Describe how the problem would be solved using the domain knowledge.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 2\. Loading the data and packages
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The second step gives you everything needed to start working on the problem.
    This includes loading libraries, packages, and individual functions needed for
    the model development.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1\. Load libraries
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A sample code for loading libraries is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The details of the libraries and modules for specific functionalities are defined
    further in the individual case studies.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2\. Load data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The following items should be checked and removed before loading the data:'
  prefs: []
  type: TYPE_NORMAL
- en: Column headers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comments or special characters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Delimiter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are many ways of loading data. Some of the most common ways are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Load CSV files with Pandas`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '`Load file from URL`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '`Load file using pandas_datareader`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 3\. Exploratory data analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this step, we look at the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1\. Descriptive statistics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Understanding the dataset is one of the most important steps of model development.
    The steps to understanding data include:'
  prefs: []
  type: TYPE_NORMAL
- en: Viewing the raw data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reviewing the dimensions of the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reviewing the data types of attributes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summarizing the distribution, descriptive statistics, and relationship among
    the variables in the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'These steps are demonstrated below using sample Python code:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Viewing the data`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '`Output`'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Age | Sex | Job | Housing | SavingAccounts | CheckingAccount | CreditAmount
    | Duration | Purpose | Risk |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 67 | male | 2 | own | NaN | little | 1169 | 6 | radio/TV | good |'
  prefs: []
  type: TYPE_TB
- en: '`Reviewing the dimensions of the dataset`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '`Output`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The results show the dimension of the dataset and mean that the dataset has
    284,807 rows and 31 columns.
  prefs: []
  type: TYPE_NORMAL
- en: '`Reviewing the data types of the attributes in the data`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '`Summarizing the data using descriptive statistics`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '`Output`'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Age | Job | CreditAmount | Duration |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| count | 1000.000 | 1000.000 | 1000.000 | 1000.000 |'
  prefs: []
  type: TYPE_TB
- en: '| mean | 35.546 | 1.904 | 3271.258 | 20.903 |'
  prefs: []
  type: TYPE_TB
- en: '| std | 11.375 | 0.654 | 2822.737 | 12.059 |'
  prefs: []
  type: TYPE_TB
- en: '| min | 19.000 | 0.000 | 250.000 | 4.000 |'
  prefs: []
  type: TYPE_TB
- en: '| 25% | 27.000 | 2.000 | 1365.500 | 12.000 |'
  prefs: []
  type: TYPE_TB
- en: '| 50% | 33.000 | 2.000 | 2319.500 | 18.000 |'
  prefs: []
  type: TYPE_TB
- en: '| 75% | 42.000 | 2.000 | 3972.250 | 24.000 |'
  prefs: []
  type: TYPE_TB
- en: '| max | 75.000 | 3.000 | 18424.000 | 72.000 |'
  prefs: []
  type: TYPE_TB
- en: 3.2\. Data visualization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The fastest way to learn more about the data is to visualize it. Visualization
    involves independently understanding each attribute of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the plot types are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Univariate plots
  prefs: []
  type: TYPE_NORMAL
- en: Histograms and density plots
  prefs: []
  type: TYPE_NORMAL
- en: Multivariate plots
  prefs: []
  type: TYPE_NORMAL
- en: Correlation matrix plot and scatterplot
  prefs: []
  type: TYPE_NORMAL
- en: 'The Python code for univariate plot types is illustrated with examples below:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Univariate plot: histogram`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '`Univariate plot: density plot`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 2-3](#HistDesn) illustrates the output.'
  prefs: []
  type: TYPE_NORMAL
- en: '![mlbf 0203](Images/mlbf_0203.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-3\. Histogram (top) and density plot (bottom)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The Python code for multivariate plot types is illustrated with examples below:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Multivariate plot: correlation matrix plot`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '`Multivariate plot: scatterplot matrix`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 2-4](#CorrScatter) illustrates the output.'
  prefs: []
  type: TYPE_NORMAL
- en: '![mlbf 0204](Images/mlbf_0204.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-4\. Correlation (left) and scatterplot (right)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 4\. Data preparation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data preparation is a preprocessing step in which data from one or more sources
    is cleaned and transformed to improve its quality prior to its use.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1\. Data cleaning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In machine learning modeling, incorrect data can be costly. Data cleaning involves
    checking the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Validity
  prefs: []
  type: TYPE_NORMAL
- en: The data type, range, etc.
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy
  prefs: []
  type: TYPE_NORMAL
- en: The degree to which the data is close to the true values.
  prefs: []
  type: TYPE_NORMAL
- en: Completeness
  prefs: []
  type: TYPE_NORMAL
- en: The degree to which all required data is known.
  prefs: []
  type: TYPE_NORMAL
- en: Uniformity
  prefs: []
  type: TYPE_NORMAL
- en: The degree to which the data is specified using the same unit of measure.
  prefs: []
  type: TYPE_NORMAL
- en: 'The different options for performing data cleaning include:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Dropping “NA” values within data*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '*Filling “NA” with 0*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '*Filling NAs with the mean of the column*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 4.2\. Feature selection
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The data features used to train the machine learning models have a huge influence
    on the performance. Irrelevant or partially relevant features can negatively impact
    model performance. Feature selection^([1](ch02.xhtml#idm45174937229416)) is a
    process in which features in data that contribute most to the prediction variable
    or output are automatically selected.
  prefs: []
  type: TYPE_NORMAL
- en: 'The benefits of performing feature selection before modeling the data are:'
  prefs: []
  type: TYPE_NORMAL
- en: Reduces overfitting^([2](ch02.xhtml#idm45174937225672))
  prefs: []
  type: TYPE_NORMAL
- en: Less redundant data means fewer opportunities for the model to make decisions
    based on noise.
  prefs: []
  type: TYPE_NORMAL
- en: Improves performance
  prefs: []
  type: TYPE_NORMAL
- en: Less misleading data means improved modeling performance.
  prefs: []
  type: TYPE_NORMAL
- en: Reduces training time and memory footprint
  prefs: []
  type: TYPE_NORMAL
- en: Less data means faster training and lower memory footprint.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following sample feature is an example demonstrating when the best two
    features are selected using the [`SelectKBest` function](https://oreil.ly/JDo-F)
    under sklearn. The `SelectKBest` function scores the features using an underlying
    function and then removes all but the *k* highest scoring feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '`Output`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'When features are irrelevant, they should be dropped. Dropping the irrelevant
    features is illustrated in the following sample code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 4.3\. Data transformation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Many machine learning algorithms make assumptions about the data. It is a good
    practice to perform the data preparation in such a way that exposes the data in
    the best possible manner to the machine learning algorithms. This can be accomplished
    through data transformation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The different data transformation approaches are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Rescaling
  prefs: []
  type: TYPE_NORMAL
- en: 'When data comprises attributes with varying scales, many machine learning algorithms
    can benefit from *rescaling* all the attributes to the same scale. Attributes
    are often rescaled in the range between zero and one. This is useful for optimization
    algorithms used in the core of machine learning algorithms, and it also helps
    to speed up the calculations in an algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Standardization
  prefs: []
  type: TYPE_NORMAL
- en: '*Standardization* is a useful technique to transform attributes to a standard
    [normal distribution](https://oreil.ly/4a70f) with a mean of zero and a standard
    deviation of one. It is most suitable for techniques that assume the input variables
    represent a normal distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Normalization
  prefs: []
  type: TYPE_NORMAL
- en: '*Normalization* refers to rescaling each observation (row) to have a length
    of one (called a unit norm or a vector). This preprocessing method can be useful
    for sparse datasets of attributes of varying scales when using algorithms that
    weight input values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 5\. Evaluate models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once we estimate the performance of our algorithm, we can retrain the final
    algorithm on the entire training dataset and get it ready for operational use.
    The best way to do this is to evaluate the performance of the algorithm on a new
    dataset. Different machine learning techniques require different evaluation metrics.
    Other than model performance, several other factors such as simplicity, interpretability,
    and training time are considered when selecting a model. The details regarding
    these factors are covered in [Chapter 4](ch04.xhtml#Chapter4).
  prefs: []
  type: TYPE_NORMAL
- en: 5.1\. Training and test split
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The simplest method we can use to evaluate the performance of a machine learning
    algorithm is to use different training and testing datasets. We can take our original
    dataset and split it into two parts: train the algorithm on the first part, make
    predictions on the second part, and evaluate the predictions against the expected
    results. The size of the split can depend on the size and specifics of the dataset,
    although it is common to use 80% of the data for training and the remaining 20%
    for testing. The differences in the training and test datasets can result in meaningful
    differences in the estimate of accuracy. The data can easily be split into the
    training and test sets using the `train_test_split` function available in sklearn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 5.2\. Identify evaluation metrics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Choosing which metric to use to evaluate machine learning algorithms is very
    important. An important aspect of evaluation metrics is the capability to discriminate
    among model results. Different types of evaluation metrics used for different
    kinds of ML models are covered in detail across several chapters of this book.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3\. Compare models and algorithms
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Selecting a machine learning model or algorithm is both an art and a science.
    There is no one solution or approach that fits all. There are several factors
    over and above the model performance that can impact the decision to choose a
    machine learning algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s understand the process of model comparison with a simple example. We
    define two variables, *X* and *Y*, and try to build a model to predict *Y* using
    *X*. As a first step, the data is divided into training and test split as mentioned
    in the preceding section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We have no idea which algorithms will do well on this problem. Let’s design
    our test now. We will use two models—one linear regression and the second polynomial
    regression to fit *Y* against *X*. We will evaluate algorithms using the *Root
    Mean Squared Error (RMSE)* metric, which is one of the measures of the model performance.
    RMSE will give a gross idea of how wrong all predictions are (zero is perfect):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '`Output`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the RMSE of the polynomial regression is slightly better than
    that of the linear regression.^([3](ch02.xhtml#idm45174934446200)) With the former
    having the better fit, it is the preferred model in this step.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Model tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Finding the best combination of hyperparameters of a model can be treated as
    a search problem.^([4](ch02.xhtml#idm45174934250120)) This searching exercise
    is often known as *model tuning* and is one of the most important steps of model
    development. It is achieved by searching for the best parameters of the model
    by using techniques such as a *grid search*. In a grid search, you create a grid
    of all possible hyperparameter combinations and train the model using each one
    of them. Besides a grid search, there are several other techniques for model tuning,
    including randomized search, [Bayesian optimization](https://oreil.ly/ZGVPM),
    and hyperbrand.
  prefs: []
  type: TYPE_NORMAL
- en: In the case studies presented in this book, we focus primarily on grid search
    for model tuning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing on from the preceding example, with the polynomial as the best model:
    next, run a grid search for the model, refitting the polynomial regression with
    different degrees. We compare the RMSE results for all the models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '`Output`'
  prefs: []
  type: TYPE_NORMAL
- en: '![mlbf 02in01](Images/mlbf_02in01.png)'
  prefs: []
  type: TYPE_IMG
- en: The RMSE decreases when the degree increases, and the lowest RMSE is for the
    model with degree 10\. However, models with degrees lower than 10 performed very
    well, and the test set will be used to finalize the best model.
  prefs: []
  type: TYPE_NORMAL
- en: While the generic set of input parameters for each algorithm provides a starting
    point for analysis, it may not have the optimal configurations for the particular
    dataset and business problem.
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Finalize the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here, we perform the final steps for selecting the model. First, we run predictions
    on the test dataset with the trained model. Then we try to understand the model
    intuition and save it for further usage.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1\. Performance on the test set
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The model selected during the training steps is further evaluated on the test
    set. The test set allows us to compare different models in an unbiased way, by
    basing the comparisons in data that were not used in any part of the training.
    The test results for the model developed in the previous step are shown in the
    following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '`Output`'
  prefs: []
  type: TYPE_NORMAL
- en: '![mlbf 02in02](Images/mlbf_02in02.png)'
  prefs: []
  type: TYPE_IMG
- en: In the training set we saw that the RMSE decreases with an increase in the degree
    of polynomial model, and the polynomial of degree 10 had the lowest RMSE. However,
    as shown in the preceding output for the polynomial of degree 10, although the
    training set had the best results, the results in the test set are poor. For the
    polynomial of degree 8, the RMSE in the test set is relatively higher. The polynomial
    of degree 6 shows the best result in the test set (although the difference is
    small compared to other lower-degree polynomials in the test set) as well as good
    results in the training set. For these reasons, this is the preferred model.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the model performance, there are several other factors to consider
    when selecting a model, such as simplicity, interpretability, and training time.
    These factors will be covered in the upcoming chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2\. Model/variable intuition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This step involves considering a holistic view of the approach taken to solve
    the problem, including the model’s limitations as it relates to the desired outcome,
    the variables used, and the selected model parameters. Details on model and variable
    intuition regarding different types of machine learning models are presented in
    the subsequent chapters and case studies.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3\. Save/deploy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: After finding an accurate machine learning model, it must be saved and loaded
    in order to ensure its usage later.
  prefs: []
  type: TYPE_NORMAL
- en: '*Pickle* is one of the packages for saving and loading a trained model in Python.
    Using pickle operations, trained machine learning models can be saved in the *serialized*
    format to a file. Later, this serialized file can be loaded to *de-serialize*
    the model for its usage. The following sample code demonstrates how to save the
    model to a file and load it to make predictions on new data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In recent years, frameworks such as [AutoML](https://oreil.ly/ChjFb) have been
    built to automate the maximum number of steps in a machine learning model development
    process. Such frameworks allow the model developers to build ML models with high
    scale, efficiency, and productivity. Readers are encouraged to explore such frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given its popularity, rate of adoption, and flexibility, Python is often the
    preferred language for machine learning development. There are many available
    Python packages to perform numerous tasks, including data cleaning, visualization,
    and model development. Some of these key packages are Scikit-learn and Keras.
  prefs: []
  type: TYPE_NORMAL
- en: The seven steps of model development mentioned in this chapter can be leveraged
    while developing any machine learning–based model in finance.
  prefs: []
  type: TYPE_NORMAL
- en: Next Steps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the next chapter, we will cover the key algorithm for machine learning—the
    artificial neural network. The artificial neural network is another building block
    of machine learning in finance and is used across all types of machine learning
    and deep learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch02.xhtml#idm45174937229416-marker)) Feature selection is more relevant
    for supervised learning models and is described in detail in the individual case
    studies in Chapters [5](ch05.xhtml#Chapter5) and [6](ch06.xhtml#Chapter6).
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch02.xhtml#idm45174937225672-marker)) Overfitting is covered in detail
    in [Chapter 4](ch04.xhtml#Chapter4).
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch02.xhtml#idm45174934446200-marker)) It should be noted that the difference
    in RMSE is small in this case and may not replicate with a different split of
    the train/test data.
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch02.xhtml#idm45174934250120-marker)) Hyperparameters are the external
    characteristics of the model, can be considered the model’s settings, and are
    not estimated based on data-like model parameters.
  prefs: []
  type: TYPE_NORMAL
