["```py\nIn [1]: import pandas as pd\n        import matplotlib.pyplot as plt\n        import numpy as np\n        import seaborn as sns; sns.set()\n        pd.set_option('use_inf_as_na', True)\n        import warnings\n        warnings.filterwarnings('ignore')\n\nIn [2]: crash_data = pd.read_csv('crash_data.csv')\n\nIn [3]: crash_data.head()\nOut[3]: Unnamed: 0       RET      date TICKER    vwretx  BIDLO   ASKHI    PRC  \\\n        0    27882462  0.041833  20100104    BAC  0.017045  15.12  15.750  15.69\n\n        1    27882463  0.032505  20100105    BAC  0.003362  15.70  16.210  16.20\n\n        2    27882464  0.011728  20100106    BAC  0.001769  16.03  16.540  16.39\n\n        3    27882465  0.032947  20100107    BAC  0.002821  16.51  17.185  16.93\n\n        4    27882466 -0.008860  20100108    BAC  0.004161  16.63  17.100  16.78\n\n                   VOL\n\n        0  180845100.0\n\n        1  209521200.0\n\n        2  205257900.0\n\n        3  320868400.0\n\n        4  220104600.0\n\nIn [4]: crash_data.date = pd.to_datetime(crash_data.date, format='%Y%m%d') ![1](assets/1.png)\n        crash_data = crash_data.set_index('date') ![2](assets/2.png)\n```", "```py\nIn [5]: crash_dataw = crash_data.groupby('TICKER').resample('W').\\\n                      agg({'RET':'mean', 'vwretx':'mean', 'VOL':'mean',\n                           'BIDLO':'mean', 'ASKHI':'mean', 'PRC':'mean'}) ![1](assets/1.png)\n\nIn [6]: crash_dataw = crash_dataw.reset_index()\n        crash_dataw.dropna(inplace=True)\n        stocks = crash_dataw.TICKER.unique()\n\nIn [7]: plt.figure(figsize=(12, 8))\n        k = 1\n\n        for i in stocks[: 4]: ![2](assets/2.png)\n            plt.subplot(2, 2, k)\n            plt.hist(crash_dataw[crash_dataw.TICKER == i]['RET'])\n            plt.title('Histogram of '+i)\n            k+=1\n        plt.show()\n```", "```py\nIn [8]: import statsmodels.api as sm\n        residuals = []\n\n        for i in stocks:\n            Y = crash_dataw.loc[crash_dataw['TICKER'] == i]['RET'].values\n            X = crash_dataw.loc[crash_dataw['TICKER'] == i]['vwretx'].values\n            X = sm.add_constant(X)\n            ols = sm.OLS(Y[2:-2], X[2:-2] + X[1:-3] + X[0:-4] + \\\n                         X[3:-1] + X[4:]).fit() ![1](assets/1.png)\n            residuals.append(ols.resid)\n\nIn [9]: residuals = list(map(lambda x: np.log(1 + x), residuals)) ![2](assets/2.png)\n\nIn [10]: crash_data_sliced = pd.DataFrame([])\n         for i in stocks:\n             crash_data_sliced = crash_data_sliced.\\\n                                 append(crash_dataw.loc[crash_dataw.TICKER == i]\n                                        [2:-2]) ![3](assets/3.png)\n         crash_data_sliced.head()\nOut[10]: TICKER       date       RET    vwretx          VOL       BIDLO\n          ASKHI  \\\n         2   AAPL 2010-01-24 -0.009510 -0.009480  25930885.00  205.277505\n          212.888450\n         3   AAPL 2010-01-31 -0.005426 -0.003738  52020594.00  198.250202\n          207.338002\n         4   AAPL 2010-02-07  0.003722 -0.001463  26953208.40  192.304004\n          197.378002\n         5   AAPL 2010-02-14  0.005031  0.002970  19731018.60  194.513998\n          198.674002\n         6   AAPL 2010-02-21  0.001640  0.007700  16618997.25  201.102500\n          203.772500\n\n                   PRC\n\n         2  208.146752\n\n         3  201.650398\n\n         4  195.466002\n\n         5  196.895200\n\n         6  202.636995\n```", "```py\nIn [11]: from sklearn.covariance import EllipticEnvelope\n         envelope = EllipticEnvelope(contamination=0.02, support_fraction=1) ![1](assets/1.png)\n         ee_predictions = {}\n\n         for i, j in zip(range(len(stocks)), stocks):\n             envelope.fit(np.array(residuals[i]).reshape(-1, 1))\n             ee_predictions[j] = envelope.predict(np.array(residuals[i])\n                                                  .reshape(-1, 1)) ![2](assets/2.png)\n\nIn [12]: transform = []\n\n         for i in stocks:\n             for j in range(len(ee_predictions[i])):\n                 transform.append(np.where(ee_predictions[i][j] == 1, 0, -1)) ![3](assets/3.png)\n\nIn [13]: crash_data_sliced = crash_data_sliced.reset_index()\n         crash_data_sliced['residuals'] = np.concatenate(residuals) ![4](assets/4.png)\n         crash_data_sliced['neg_outliers'] = np.where((np.array(transform)) \\\n                                                        == -1, 1, 0) ![5](assets/5.png)\n         crash_data_sliced.loc[(crash_data_sliced.neg_outliers == 1) &\n                               (crash_data_sliced.residuals > 0),\n                               'neg_outliers'] = 0 ![6](assets/6.png)\n```", "```py\nIn [14]: plt.figure(figsize=(12, 8))\n         k = 1\n\n         for i in stocks[8:12]:\n             plt.subplot(2, 2, k)\n             crash_data_sliced['residuals'][crash_data_sliced.TICKER == i]\\\n             .hist(label='normal', bins=30, color='gray')\n             outliers = crash_data_sliced['residuals']\n             [(crash_data_sliced.TICKER == i) &\n              (crash_data_sliced.neg_outliers > 0)]\n             outliers.hist(color='black', label='anomaly')\n             plt.title(i)\n             plt.legend()\n             k += 1\n         plt.show()\n```", "```py\nIn [15]: crash_data_sliced = crash_data_sliced.set_index('date')\n         crash_data_sliced.index = pd.to_datetime(crash_data_sliced.index)\n\nIn [16]: std = crash_data.groupby('TICKER')['RET'].resample('W').std()\\\n               .reset_index()\n         crash_dataw['std'] = pd.DataFrame(std['RET']) ![1](assets/1.png)\n\nIn [17]: yearly_data = crash_data_sliced.groupby('TICKER')['residuals']\\\n                       .resample('Y').agg({'residuals':{'mean', 'std'}})\\\n                       .reset_index()\n         yearly_data.columns = ['TICKER', 'date', 'mean', 'std']\n         yearly_data.head()\nOut[17]:   TICKER       date      mean       std\n         0   AAPL 2010-12-31  0.000686  0.008291\n         1   AAPL 2011-12-31  0.000431  0.009088\n         2   AAPL 2012-12-31 -0.000079  0.008056\n         3   AAPL 2013-12-31 -0.001019  0.009096\n         4   AAPL 2014-12-31  0.000468  0.006174\n\nIn [18]: merge_crash = pd.merge(crash_data_sliced.reset_index(), yearly_data,\n                                how='outer', on=['TICKER', 'date']) ![2](assets/2.png)\n\nIn [19]: merge_crash[['annual_mean', 'annual_std']] = merge_crash\\\n                                                      .sort_values(by=['TICKER',\n                                                                       'date'])\\\n                                                      .iloc[:, -2:]\\\n                                                      .fillna(method='bfill') ![3](assets/3.png)\n         merge_crash['residuals'] = merge_crash.sort_values(by=['TICKER',\n                                                                'date'])\\\n                                                               ['residuals']\\\n                                                      .fillna(method='ffill') ![3](assets/3.png)\n         merge_crash = merge_crash.drop(merge_crash.iloc[: ,-4:-2], axis=1) ![4](assets/4.png)\n```", "```py\nIn [20]: crash_risk_out = []\n\n         for j in stocks:\n             for k in range(len(merge_crash[merge_crash.TICKER == j])):\n                 if merge_crash[merge_crash.TICKER == j]['residuals'].iloc[k] < \\\n                 merge_crash[merge_crash.TICKER == j]['annual_mean'].iloc[k] - \\\n                 3.09 * \\\n                 merge_crash[merge_crash.TICKER == j]['annual_std'].iloc[k]:\n                     crash_risk_out.append(1)\n                 else:\n                     crash_risk_out.append(0)\n\nIn [21]: merge_crash['crash_risk'] = crash_risk_out\n         merge_crash['crash_risk'].value_counts()\nOut[21]: 0    13476\n         1       44\n         Name: crash_risk, dtype: int64\n\nIn [22]: merge_crash = merge_crash.set_index('date')\n         merge_crash_annual = merge_crash.groupby('TICKER')\\\n                              .resample('1Y')['crash_risk'].sum().reset_index()\n```", "```py\nIn [23]: down = []\n\n         for j in range(len(merge_crash)):\n             if merge_crash['residuals'].iloc[j] < \\\n                merge_crash['annual_mean'].iloc[j]:\n                 down.append(1) ![1](assets/1.png)\n             else:\n                 down.append(0) ![2](assets/2.png)\n\nIn [24]: merge_crash = merge_crash.reset_index()\n         merge_crash['down'] = pd.DataFrame(down)\n         merge_crash['up'] = 1 - merge_crash['down']\n         down_residuals = merge_crash[merge_crash.down == 1]\\\n                          [['residuals', 'TICKER', 'date']] ![3](assets/3.png)\n         up_residuals = merge_crash[merge_crash.up == 1]\\\n                        [['residuals', 'TICKER', 'date']] ![4](assets/4.png)\n\nIn [25]: down_residuals['residuals_down_sq'] = down_residuals['residuals'] ** 2\n         down_residuals['residuals_down_cubic'] = down_residuals['residuals'] **3\n         up_residuals['residuals_up_sq'] = up_residuals['residuals'] ** 2\n         up_residuals['residuals_up_cubic'] = up_residuals['residuals'] ** 3\n         down_residuals['down_residuals'] = down_residuals['residuals']\n         up_residuals['up_residuals'] = up_residuals['residuals']\n         del down_residuals['residuals']\n         del up_residuals['residuals']\n\nIn [26]: merge_crash['residuals_sq'] = merge_crash['residuals'] ** 2\n         merge_crash['residuals_cubic'] = merge_crash['residuals'] ** 3\n```", "```py\nIn [27]: merge_crash_all = merge_crash.merge(down_residuals,\n                                             on=['TICKER', 'date'],\n                                             how='outer')\n         merge_crash_all = merge_crash_all.merge(up_residuals,\n                                                 on=['TICKER', 'date'],\n                                                 how='outer')\n\nIn [28]: cols = ['BIDLO', 'ASKHI', 'residuals',\n                 'annual_std', 'residuals_sq', 'residuals_cubic',\n                 'down', 'up', 'residuals_up_sq', 'residuals_down_sq',\n                 'neg_outliers']\n         merge_crash_all = merge_crash_all.set_index('date')\n         merge_grouped = merge_crash_all.groupby('TICKER')[cols]\\\n                         .resample('1Y').sum().reset_index() ![1](assets/1.png)\n         merge_grouped['neg_outliers'] = np.where(merge_grouped.neg_outliers >=\n                                                  1, 1, 0) ![2](assets/2.png)\n```", "```py\nIn [29]: merge_grouped = merge_grouped.set_index('date')\n         merge_all = merge_grouped.groupby('TICKER')\\\n                     .resample('1Y').agg({'down':['sum', 'count'],\n                                          'up':['sum', 'count']})\\\n                     .reset_index() ![1](assets/1.png)\n         merge_all.head()\n\nOut[29]:   TICKER       date down        up\n                              sum count sum count\n         0   AAPL 2010-12-31   27     1  23     1\n         1   AAPL 2011-12-31   26     1  27     1\n         2   AAPL 2012-12-31   28     1  26     1\n         3   AAPL 2013-12-31   24     1  29     1\n         4   AAPL 2014-12-31   22     1  31     1\n\nIn [30]: merge_grouped['down'] = merge_all['down']['sum'].values\n         merge_grouped['up'] = merge_all['up']['sum'].values\n         merge_grouped['count'] = merge_grouped['down'] + merge_grouped['up']\n```", "```py\nIn [31]: merge_grouped = merge_grouped.reset_index()\n\nIn [32]: merge_grouped['duvol'] = np.log(((merge_grouped['up'] - 1) *\n                                          merge_grouped['residuals_down_sq']) /\n                                         ((merge_grouped['down'] - 1) *\n                                          merge_grouped['residuals_up_sq'])) ![1](assets/1.png)\n\nIn [33]: merge_grouped['duvol'].mean()\nOut[33]: -0.023371498758114867\n\nIn [34]: merge_grouped['ncskew'] = - (((merge_grouped['count'] *\n                                        (merge_grouped['count'] - 1) **\n                                        (3 / 2)) *\n                                      merge_grouped['residuals_cubic']) /\n                                      (((merge_grouped['count'] - 1) *\n                                        (merge_grouped['count'] - 2)) *\n                                       merge_grouped['residuals_sq'] **\n                                       (3 / 2))) ![2](assets/2.png)\n\nIn [35]: merge_grouped['ncskew'].mean()\nOut[35]: -0.031025284134663118\n\nIn [36]: merge_grouped['crash_risk'] = merge_crash_annual['crash_risk']\n         merge_grouped['crash_risk'] = np.where(merge_grouped.crash_risk >=\n                                                1, 1, 0)\n\nIn [37]: merge_crash_all_grouped2 = merge_crash_all.groupby('TICKER')\\\n                                     [['VOL', 'PRC']]\\\n                                    .resample('1Y').mean().reset_index()\n         merge_grouped[['VOL', 'PRC']] = merge_crash_all_grouped2[['VOL', 'PRC']]\n\n         merge_grouped[['ncskew','duvol']].corr()\n```", "```py\nIn [38]: bs = pd.read_csv('bs_v.3.csv')\n         bs['Date'] = pd.to_datetime(bs.datadate, format='%Y%m%d')\n         bs['annual_date'] = bs['Date'].dt.year\n\nIn [39]: bs['RoA'] = bs['ni'] / bs['at']\n         bs['leverage'] = bs['lt'] / bs['at']\n\nIn [40]: merge_grouped['annual_date'] = merge_grouped['date'].dt.year\n         bs['TICKER'] = bs.tic\n         del bs['tic']\n```", "```py\nIn [41]: merge_ret_bs = pd.merge(bs, merge_grouped,\n                                 on=['TICKER', 'annual_date'])\n\nIn [42]: merge_ret_bs2 = merge_ret_bs.set_index('Date')\n         merge_ret_bs2 = merge_ret_bs2.groupby('TICKER').resample('Y').mean()\n         merge_ret_bs2.reset_index(inplace=True)\n\nIn [43]: merge_ret_bs2['vol_csho_diff'] = (merge_ret_bs2.groupby('TICKER')\n                                           ['VOL'].shift(-1) /\n                                           merge_ret_bs2.groupby('TICKER')\n                                           ['csho'].shift(-1))\n         merge_ret_bs2['dturn1'] = merge_ret_bs2['VOL'] / merge_ret_bs2['csho']\n         merge_ret_bs2['dturn'] = merge_ret_bs2['vol_csho_diff'] - \\\n                                  merge_ret_bs2['dturn1']\n\nIn [44]: merge_ret_bs2['p/e'] = merge_ret_bs2['PRC'] / merge_ret_bs2['ni']\n         merge_ret_bs2['turnover_rate'] = merge_ret_bs2['VOL'] / \\\n                                          merge_ret_bs2['csho']\n         merge_ret_bs2['equity_share'] = merge_ret_bs2['ceq'] / \\\n                                         (merge_ret_bs2['ceq'] +\n                                          merge_ret_bs2['dt'])\n         merge_ret_bs2['firm_size'] = np.log(merge_ret_bs2['at'])\n         merge_ret_bs2['cefd'] = (((merge_ret_bs2['at'] -\n                                  merge_ret_bs2['lt']) / merge_ret_bs2['csho']) -\n                                  merge_ret_bs2['PRC']) / (merge_ret_bs2['at'] -\n                                  merge_ret_bs2['lt']) / merge_ret_bs2['csho']\n\nIn [45]: merge_ret_bs2 = merge_ret_bs2.set_index('Date')\n         merge_ret_bs2['buying_volume'] = merge_ret_bs2['VOL'] * \\\n                                          (merge_ret_bs2['PRC'] -\n                                           merge_ret_bs2['BIDLO']) / \\\n                                          (merge_ret_bs2['ASKHI'] -\n                                           merge_ret_bs2['BIDLO'])\n         merge_ret_bs2['selling_volume'] = merge_ret_bs2['VOL'] * \\\n                                           (merge_ret_bs2['ASKHI'] -\n                                            merge_ret_bs2['PRC']) / \\\n                                           (merge_ret_bs2['ASKHI'] -\n                                            merge_ret_bs2['BIDLO'])\n         buying_volume = merge_ret_bs2.groupby('TICKER')['buying_volume'] \\\n                         .resample('Y').sum().reset_index()\n         selling_volume = merge_ret_bs2.groupby('TICKER')['selling_volume'] \\\n                         .resample('Y').sum().reset_index()\n         del buying_volume['TICKER']\n         del buying_volume['Date']\n\nIn [46]: buy_sel_vol = pd.concat([buying_volume,selling_volume], axis=1)\n         buy_sel_vol['bsi'] = (buy_sel_vol.buying_volume -\n                               buy_sel_vol.selling_volume) / \\\n                              (buy_sel_vol.buying_volume +\n                               buy_sel_vol.selling_volume)\n\nIn [47]: merge_ret_bs2 = merge_ret_bs2.reset_index()\n         merge_ret_bs2 = pd.merge(buy_sel_vol ,merge_ret_bs2,\n                                  on=['TICKER', 'Date'])\n```", "```py\nIn [48]: from sklearn.preprocessing import StandardScaler\n         from sklearn.decomposition import PCA\n\nIn [49]: firm_sentiment = merge_ret_bs2[['p/e', 'turnover_rate',\n                                         'equity_share', 'cefd',\n                                         'leverage', 'bsi']]\n         firm_sentiment = firm_sentiment.apply(lambda x: x.fillna(x.mean()),\n                                               axis=0) ![1](assets/1.png)\n\nIn [50]: firm_sentiment_std = StandardScaler().fit_transform(firm_sentiment)\n         pca = PCA(n_components=6)\n         pca_market_sentiment = pca.fit_transform(firm_sentiment_std)\n         print('Explained Variance Ratios per Component are:\\n {}'\\\n               .format(pca.explained_variance_ratio_))\n         Explained Variance Ratios per Component are:\n          [0.35828322 0.2752777  0.15343653 0.12206041 0.06681776 0.02412438]\n\nIn [51]: loadings_1 = pd.DataFrame(pca.components_.T *\n                                   np.sqrt(pca.explained_variance_),\n                                   columns=['PC1', 'PC2', 'PC3',\n                                            'PC4', 'PC5', 'PC6'],\n                                   index=firm_sentiment.columns) ![2](assets/2.png)\n         loadings_1\nOut[51]: PC1       PC2       PC3       PC4       PC5       PC6\n         p/e           -0.250786  0.326182  0.911665  0.056323  0.000583\n          0.021730\n         turnover_rate -0.101554  0.854432 -0.197381  0.201749  0.428911\n          -0.008421\n         equity_share  -0.913620 -0.162406 -0.133783  0.224513 -0.031672\n          0.271443\n         cefd           0.639570 -0.118671  0.038422  0.754467 -0.100176\n          0.014146\n         leverage       0.917298  0.098311  0.068633 -0.264369  0.089224\n          0.265335\n         bsi            0.006731  0.878526 -0.173740 -0.044127 -0.446735\n          0.022520\n\nIn [52]: df_loading1 = pd.DataFrame(loadings_1.mean(axis=1)) ![3](assets/3.png)\n         df_loading1\nOut[52]:                       0\n         p/e            0.177616\n         turnover_rate  0.196289\n         equity_share  -0.124254\n         cefd           0.204626\n         leverage       0.195739\n         bsi            0.040529\n\nIn [53]: firm_sentiment = pd.DataFrame(np.dot(pca_market_sentiment,\n                                              np.array(df_loading1)))\n         merge_ret_bs2['firm_sent'] = firm_sentiment\n```", "```py\nIn [54]: merge_ret_bs2['log_size'] = np.log(merge_ret_bs2['at'])\n\nIn [55]: merge_ret_bs2.set_index(['TICKER', 'Date'], inplace=True)\n\nIn [56]: X = (merge_ret_bs2[['log_size', 'rect', 'ppegt', 'dturn',\n                         'ncskew', 'residuals', 'RoA', 'annual_std',\n                         'firm_sent']]).shift(1)\n         X['neg_outliers'] = merge_ret_bs2['neg_outliers']\n```", "```py\nIn [57]: from pyeconometrics.panel_discrete_models import FixedEffectPanelModel\n         from sklearn.model_selection import train_test_split\n         from sklearn.metrics import accuracy_score\n\nIn [58]: FE_ML = FixedEffectPanelModel()\n         FE_ML.fit(X, 'neg_outliers')\n         FE_ML.summary()\n    ======================================================================\n    ==========\n    Dep. Variable:                  neg_outliers   Pseudo R-squ.:      0.09611\n    Model:             Panel Fixed Effects Logit   Log-Likelihood:     -83.035\n    Method:                                  MLE   LL-Null:            -91.864\n    No. Observations:                        193   LLR p-value:          0.061\n    Df Model:                                  9\n\n    Converged:                                  True\n\n    ======================================================================\n                        coef  std err        t    P>|t| [95.0% Conf. Int.]\n    ----------------------------------------------------------------------\n    _cons                     -2.5897    1.085   -2.387    0.008    -4.716\n    -0.464\n    log_size                   0.1908    0.089    2.155    0.016     0.017\n    0.364\n    rect                      -0.0000    0.000   -4.508    0.000    -0.000\n    -0.000\n    ppegt                     -0.0000    0.000   -0.650    0.258    -0.000\n    0.000\n    dturn                      0.0003    0.000    8.848    0.000     0.000\n    0.000\n    ncskew                    -0.2156    0.089   -2.420    0.008    -0.390\n    -0.041\n    residuals                 -0.3843    1.711   -0.225    0.411    -3.737\n    2.968\n    RoA                        1.4897    1.061    1.404    0.080    -0.590\n    3.569\n    annual_std                 1.9252    0.547    3.517    0.000     0.852\n    2.998\n    firm_sent                  0.6847    0.151    4.541    0.000     0.389\n    0.980\n    ----------------------------------------------------------------------\n```", "```py\nIn [59]: del X['neg_outliers']\n         X['crash_risk'] = merge_ret_bs2['crash_risk']\n\nIn [60]: FE_crash = FixedEffectPanelModel()\n         FE_crash.fit(X, 'crash_risk')\n         FE_crash.summary()\n    ======================================================================\n    Dep. Variable:                   crash_risk   Pseudo R-squ.:   0.05324\n    Model:            Panel Fixed Effects Logit   Log-Likelihood:  -55.640\n    Method:                                 MLE   LL-Null:         -58.769\n    No. Observations:                       193   LLR p-value:       0.793\n    Df Model:                                 9\n\n    Converged:                                  True\n\n    ======================================================================\n                            coef  std err        t    P>|t| [95.0% Conf. Int.]\n\n    ----------------------------------------------------------------------\n    _cons                     -3.1859    1.154   -2.762    0.003    -5.447\n    -0.925\n    log_size                   0.2012    0.094    2.134    0.016     0.016\n    0.386\n    rect                      -0.0000    0.000   -1.861    0.031    -0.000\n    0.000\n    ppegt                     -0.0000    0.000   -0.638    0.262    -0.000\n    0.000\n    dturn                      0.0001    0.000    2.882    0.002     0.000\n    0.000\n    ncskew                     0.3840    0.114    3.367    0.000     0.160\n    0.608\n    residuals                  3.3976    2.062    1.648    0.050    -0.644\n    7.439\n    RoA                        2.5096    1.258    1.994    0.023     0.043\n    4.976\n    annual_std                 2.4094    0.657    3.668    0.000     1.122\n    3.697\n    firm_sent                 -0.0041    0.164   -0.025    0.490    -0.326\n    0.318\n    ----------------------------------------------------------------------\n```"]