- en: Chapter 8\. Modeling Operational Risk
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '...It’s not necessarily the biggest missteps that deliver the biggest blows;
    share prices can plummet as a result of even the smallest events.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Dunnett, Levy, and Simoes (2005)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Thus far, we have talked about three main financial risks: market, credit,
    and liquidity risks. Now it is time to discuss operational risk, which is more
    ambiguous than the other types of financial risks. This ambiguity arises from
    the huge variety of risk sources by which financial institutions may face huge
    losses.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Operational risk is the risk of direct or indirect loss resulting from inadequate
    or failed interval processes, people, and systems or from external events (BIS
    2019). Please note that loss can be direct and/or indirect. Some direct losses
    would be:'
  prefs: []
  type: TYPE_NORMAL
- en: Legal liability arising from judicial process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Write-downs due to theft or reduction in assets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compliance emanating from tax, license, fines, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Business interruption
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indirect cost is related to the opportunity cost in the way that a decision
    made by an institution may trigger a host of events resulting in a loss at an
    uncertain time in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Normally, financial institutions allocate a certain amount of money to cover
    the loss emanating from operational risk, which is known as *unexpected loss*.
    However, allocating an appropriate amount of funds to cover unexpected loss is
    not as easy as it sounds. It is necessary to determine the right amount of unexpected
    loss; otherwise, either more funds are devoted to it, which makes it idle and
    creates opportunity cost, or less than the required funds are allocated, resulting
    in a liquidity problem.
  prefs: []
  type: TYPE_NORMAL
- en: As we briefly touched on earlier, operational risk can take on several forms.
    Among them, we’ll restrict our focus to the fraud risk, which is considered to
    be the most pervasive and disruptive type of operational risk.
  prefs: []
  type: TYPE_NORMAL
- en: Fraud may generally be characterized as an intentional act, misstatement, or
    omission designed to deceive others, resulting in the victim suffering a loss
    or the perpetrator achieving a gain (OCC 2019). A fraud can be an internal one
    if losses occurred from inside a financial institution or an external one if it
    is committed by a third party.
  prefs: []
  type: TYPE_NORMAL
- en: 'What makes fraud a primary concern of financial institutions? What increases
    the likelihood of committing fraudulent activities? To address these questions,
    we can refer to three important factors:'
  prefs: []
  type: TYPE_NORMAL
- en: Globalization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lack of proper risk management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Economic pressure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Globalization led financial institutions to expand their operations across the
    world, and this came with a complexity that gave rise to a higher probability
    of corruption, bribery, and any kind of illegal act as financial institutions
    started operating in environments where they have no prior knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: Lack of proper risk management has been and is the most obvious reasons for
    fraud. Misleading reporting and rogue, unauthorized trading plants the seeds of
    fraudulent acts. A very well known example is the Barings case, in which Nick
    Leeson, a young trader at Barings, ran a speculative trading and subsequent cover-up
    operation using accounting tricks that cost Barings Bank a fortune, totaling $1.3
    billion. Thus, when there is a lack of well-defined risk management policies along
    with a well-established culture of risk, employees may tend to commit fraud.
  prefs: []
  type: TYPE_NORMAL
- en: Another motivation for fraud would be an employee’s worsening financial situation.
    Particularly during an economic downturn, employees might be tempted into fraudulent
    activities. In addition, financial institutions themselves might embrace illegal
    operations (such as accounting tricks) to find a way out of the downturn.
  prefs: []
  type: TYPE_NORMAL
- en: Fraud does not only cause a considerable amount of loss, but it also poses a
    threat to a company’s reputation, which may in turn disrupt the long-term sustainability
    of the company. Take the case of Enron, a good example of accounting fraud, which
    broke out in 2001\. Enron was established in 1985 and became one of the biggest
    companies in the United States and the world. Let me briefly tell you the story
    of this big collapse.
  prefs: []
  type: TYPE_NORMAL
- en: Due to the pressure that Enron faced in the energy market, executives were motivated
    to rely on dubious accounting practices, resulting in inflated profits from writing
    huge unrealized future gains. Thanks to whistleblower Sherron Watkins, who was
    the former vice president of corporate development, one of the biggest fraud cases
    in the history of modern finance came to light. This event also stresses the importance
    of preventing fraudulent activities, which otherwise might lead to huge damages
    to an individual’s or company’s reputation or financial collapse.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we aim to introduce an ML-based model to detect fraud or would-be
    fraud operations. This is and should be a constantly growing field to stay ahead
    of the perpetrators. Datasets related to fraud may come in two forms: *labeled*
    or *unlabeled data*. To take both into account, we first apply a supervised learning
    algorithm and then use an unsupervised learning algorithm pretending like we do
    not have labels, even though the dataset we’ll be using does include labels.'
  prefs: []
  type: TYPE_NORMAL
- en: The dataset we’ll use for our fraud analysis is known as the *Credit Card Transaction
    Fraud Detection Dataset* created by Brandon Harris. Credit card fraud is not a
    rare issue, and the goal is to detect the likelihood of fraud and inform the bank
    so that the bank can investigate the situation with due diligence. This is the
    way a bank protects itself from incurring huge losses. According to the Nilsen
    Report (2020), payment card fraud losses hit a record-high level of $32.04 billion,
    amounting to 6.8¢ for every $100 of total volume.
  prefs: []
  type: TYPE_NORMAL
- en: This dataset is a good example of a mix of attributes of variable types as we
    have continuous, discrete, and nominal data. You can find the data on [Kaggle](https://oreil.ly/fxxFg).
    An explanation of the data is provided in [Table 8-1](#att_exp).
  prefs: []
  type: TYPE_NORMAL
- en: Table 8-1\. Attributes and explanations
  prefs: []
  type: TYPE_NORMAL
- en: '| Attribute | Explanation |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `trans_date_trans_time` | Date the transaction |'
  prefs: []
  type: TYPE_TB
- en: '| `cc_num` | Credit card number of the customer |'
  prefs: []
  type: TYPE_TB
- en: '| `merchant` | Merchant by whom the trade occurred |'
  prefs: []
  type: TYPE_TB
- en: '| `amt` | Amount of transaction |'
  prefs: []
  type: TYPE_TB
- en: '| `first` | First name of customer |'
  prefs: []
  type: TYPE_TB
- en: '| `last` | Last name of customer |'
  prefs: []
  type: TYPE_TB
- en: '| `gender` | Gender of the customer |'
  prefs: []
  type: TYPE_TB
- en: '| `street, city, state` | Address of the customer |'
  prefs: []
  type: TYPE_TB
- en: '| `zip` | Zip code of the transaction |'
  prefs: []
  type: TYPE_TB
- en: '| `lat` | Latitude of the customer |'
  prefs: []
  type: TYPE_TB
- en: '| `long` | Longitude of the customer |'
  prefs: []
  type: TYPE_TB
- en: '| `city_pop` | Population of the city |'
  prefs: []
  type: TYPE_TB
- en: '| `job` | Type of the customer’s profession |'
  prefs: []
  type: TYPE_TB
- en: '| `dob` | Date of birth of the customer |'
  prefs: []
  type: TYPE_TB
- en: '| `trans_num` | Unique transaction number for each transaction |'
  prefs: []
  type: TYPE_TB
- en: '| `unix_time` | Time of the transaction in Unix |'
  prefs: []
  type: TYPE_TB
- en: '| `merch_lat` | Merchant latitude |'
  prefs: []
  type: TYPE_TB
- en: '| `merch_long` | Merchant longitude |'
  prefs: []
  type: TYPE_TB
- en: '| `is_fraud` | Whether the transaction is fraudulent or not |'
  prefs: []
  type: TYPE_TB
- en: Getting Familiar with Fraud Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you probably noticed, ML algorithms work better if the number of observations
    among different classes are more or less equal to each other—that is, it works
    best with balanced data. We do not have balanced data in the fraud case, so this
    is called a *class imbalance*. In [Chapter 6](ch06.html#chapter_6), we learned
    how to handle class imbalance problems, and we’ll use this skill again in this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start off. To begin with, it makes sense to go through the data types
    of the variables in the Credit Card Transaction Fraud Detection Dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'It turns out we have all types of data: object, integer, and float. However,
    the majority of the variables are of the object type, so additional analysis is
    required to turn these categorical variables into numerical ones.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The dependent variable is of considerable importance in such an analysis, as
    it often has imbalance characteristics that require due attention. This is shown
    in the following snippet (and resultant [Figure 8-1](#pie_chart_fraud)), which
    indicates a highly disproportionate number of observations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![pie_chart_fraud](assets/mlfr_0801.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-1\. Pie chart for dependent variable
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As we can see, the number of observations for the nonfraud case is 1,289,169,
    while there are only 7,506 for the fraud case, so we know that the data is highly
    imbalanced, as expected.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we can use a rather different tool to detect the number of missing
    observations. This tool is known as `missingno`, and it also provides us with
    a visualization module for missing values (as can be seen in [Figure 8-2](#missingno_fraud)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_modeling_operational_risk_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Importing `missingno`
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_modeling_operational_risk_CO1-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a bar plot for missing values
  prefs: []
  type: TYPE_NORMAL
- en: '![missingno_fraud](assets/mlfr_0802.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-2\. Missing observations
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Figure 8-2](#missingno_fraud) indicates the number of nonmissing observations
    per variable at the top, and on the left-hand side we can see the percentage of
    nonmissing values. This analysis shows that the data has no missing values.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next step, first we convert the date variable, `trans_date_trans_time`,
    into a proper format, and then we break time down into days and hours, assuming
    that fraudulent activities surge during particular time periods. It makes sense
    to analyze the effect of fraud on the different categories of a variable. To do
    that, we’ll employ a bar plot. It becomes clearer that the number of fraud cases
    may change given the category of some variables. But it stays the same in gender
    variables, meaning that gender has no impact on fraudulent activities. Another
    striking and evident observation is that the fraud cases change wildly per day
    and hour. This can be visually confirmed in the resulting [Figure 8-3](#bar_fraud):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_modeling_operational_risk_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Sorting `fraud_data` based on fraudulent activities in an ascending order
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the analysis and our previous knowledge about the fraud analysis,
    we can decide on the number of variables to be used in our modeling. The categorical
    variables sort out so that we can create dummy variables using `pd.get_dummies`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![bar_fraud](assets/mlfr_0803.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-3\. Bar plots per variable
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Subsequent to categorical variable analysis, it’s worth discussing the interactions
    between the numerical variables, namely, `amount`, `population`, and `hour`. A
    correlation analysis provides us with a strong tool for figuring out the interaction(s)
    among these variables, and the resulting heatmap ([Figure 8-4](#heatmap_fraud))
    suggests that the correlations are very low:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![heatmap_fraud](assets/mlfr_0804.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-4\. Heatmap
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Supervised Learning Modeling for Fraud Examination
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have determined the peculiar characteristics of the variables using interactions,
    missing values, and creating dummy variables. Now we are ready to move on and
    run ML models for fraud analysis. The models we are about to run are:'
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision tree
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random forest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XGBoost
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can imagine, it’s key to have balanced data before doing our modeling.
    Even though there are numerous ways to get balanced data, we’ll choose the undersampling
    method because of its performance. Undersampling is a technique that matches the
    majority classes to minority classes, as shown in [Figure 8-5](#imbalance).
  prefs: []
  type: TYPE_NORMAL
- en: '![imbalance](assets/mlfr_0805.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-5\. Undersampling
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Alternatively, the number of observations from the majority class is removed
    until we get the same number of observations as the minority class. We’ll apply
    undersampling in the following code block, where the independent and dependent
    variables are named `X_under` and `y_under`, respectively. In what follows, train-test
    split is used to obtain the train and test splits in a random fashion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_modeling_operational_risk_CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Sampling `fraud_count`
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_modeling_operational_risk_CO3-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Concatenating the data including fraudulent cases with data including no fraudulent
    cases
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_modeling_operational_risk_CO3-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Creating independent variables by dropping `is_fraud`
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_modeling_operational_risk_CO3-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Creating dependent variables by `is_fraud`
  prefs: []
  type: TYPE_NORMAL
- en: 'After using the undersampling method, let’s now run some of the classification
    models we described earlier and observe the performance of these models in detecting
    the fraud:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: First, let’s look at the confusion matrix. The confusion matrix suggests that
    the number of observations in false positives and false negatives are 310 and
    486, respectively. We’ll be using the confusion matrix in the cost-based method.
  prefs: []
  type: TYPE_NORMAL
- en: The *F1 score* is the metric that is used to measure the performance of these
    models. It presents a weighted average of recall and precision, making it an ideal
    measure for a case such as this one.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second model is decision tree, which works well in modeling fraud. After
    tuning hyperparameters, it turns out that F1 score is much higher, indicating
    that decision tree does a relatively good job. As expected, the number of false
    positive and false negative observations are much fewer compared to logistic regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'According to common belief, the random forest model, as an ensemble model,
    outperforms decision tree. However, this is true only if decision tree suffers
    from predictive instability in such a way that predictions of different samples
    vary wildly, and this is not the case here. As you can observe from the following
    result, random forest does not perform better than decision tree, even if it has
    an F1 score of 87:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The final model we’ll look at is XGBoost, which generates similar results to
    the decision tree, as it outputs an F1 score of 97:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Given all the applications, here is the summary result:'
  prefs: []
  type: TYPE_NORMAL
- en: Table 8-2\. The result of modeling fraud with undersampling
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | F1 score |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Logistic regression | 0.79 |'
  prefs: []
  type: TYPE_TB
- en: '| Decision tree | 0.96 |'
  prefs: []
  type: TYPE_TB
- en: '| Random forest | 0.87 |'
  prefs: []
  type: TYPE_TB
- en: '| XGBoost | 0.97 |'
  prefs: []
  type: TYPE_TB
- en: Cost-Based Fraud Examination
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Undersampling gives us a convenient tool for dealing with imbalanced data. It
    comes with costs, however, and the biggest cost is its discarding of important
    observations. Even though different sampling procedures can be applied to sensitive
    analyses such as health care, fraud, and so on, it should be noted that performance
    metrics fail to consider the extent to which different misclassifications have
    varying economic impact. Hence, if a method proposes different misclassification
    costs, it is referred to as a *cost-sensitive classifier*. Let’s consider the
    fraud case, which is a classic example of cost-sensitive analysis. In this type
    of analysis, it is evident that a false positive is less costly than a false negative.
    To be more precise, a false positive means blocking an already legitimate transaction.
    The cost of this type of classification tends to be administrative and opportunity
    cost–related, such as the time and energy spent on detection and the lost potential
    gain a financial institution can make from the transaction.
  prefs: []
  type: TYPE_NORMAL
- en: However, failing to detect a fraud (i.e., having a false negative) means a lot
    for a company, as it might imply various internal weaknesses as well as poorly
    designed operational procedures. Having failed to detect a real fraud, a company
    can incur large financial costs—including the transaction amount—not to mention
    costs stemming from any damage to its reputation. The former type of cost puts
    the burden on the company’s shoulder, but the latter can be neither quantified
    nor ignored.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the need to assign varying costs for different misclassifications
    leads us to a more pronounced, realistic solution. For the sake of simplicity,
    let’s assume the cost of false negative and true positive to be the transaction
    amount and 2, respectively. [Table 8-3](#cost_sen_mat) summarizes the results.
    Another approach for evaluating cost sensitivity would be to assume a constant
    false negative, as in other cases. However, this approach is considered unrealistic.
  prefs: []
  type: TYPE_NORMAL
- en: Table 8-3\. Cost-sensitive matrix
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | F1 score |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| True Positive = 2 | False Negative = Transaction Amount |'
  prefs: []
  type: TYPE_TB
- en: '| False Positive = 2 | True Negative = 0 |'
  prefs: []
  type: TYPE_TB
- en: 'Consequently, the total cost that an institution might face with varying false
    negative costs takes the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="Cost equals sigma-summation Underscript i equals 1 Overscript
    upper N Endscripts y Subscript i Baseline left-parenthesis c Subscript i Baseline
    upper C Subscript upper T upper P Sub Subscript i Subscript Baseline plus left-parenthesis
    1 minus c Subscript i Baseline right-parenthesis upper C Subscript upper F upper
    N Sub Subscript i Subscript Baseline right-parenthesis plus left-parenthesis 1
    minus y Subscript i Baseline right-parenthesis c Subscript i Baseline upper C
    Subscript upper F upper P Sub Subscript i Subscript Baseline" display="block"><mrow><mtext>Cost</mtext>
    <mo>=</mo> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>N</mi></munderover> <msub><mi>y</mi> <mi>i</mi></msub> <mrow><mo>(</mo> <msub><mi>c</mi>
    <mi>i</mi></msub> <msub><mi>C</mi> <mrow><mi>T</mi><msub><mi>P</mi> <mi>i</mi></msub></mrow></msub>
    <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>c</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow> <msub><mi>C</mi> <mrow><mi>F</mi><msub><mi>N</mi> <mi>i</mi></msub></mrow></msub>
    <mo>)</mo></mrow> <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>y</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow> <msub><mi>c</mi> <mi>i</mi></msub> <msub><mi>C</mi>
    <mrow><mi>F</mi><msub><mi>P</mi> <mi>i</mi></msub></mrow></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where <math alttext="c Subscript i"><msub><mi>c</mi> <mi>i</mi></msub></math>
    is the predicted label, <math alttext="y Subscript i"><msub><mi>y</mi> <mi>i</mi></msub></math>
    is the actual label, *N* is the number of observations, and <math alttext="upper
    C Subscript upper T upper P Sub Subscript i"><msub><mi>C</mi> <mrow><mi>T</mi><msub><mi>P</mi>
    <mi>i</mi></msub></mrow></msub></math> and <math alttext="upper C Subscript upper
    F upper P Sub Subscript i"><msub><mi>C</mi> <mrow><mi>F</mi><msub><mi>P</mi> <mi>i</mi></msub></mrow></msub></math>
    correspond to administrative cost, which is 2 in our case. <math alttext="upper
    C Subscript upper F upper N Sub Subscript i"><msub><mi>C</mi> <mrow><mi>F</mi><msub><mi>N</mi>
    <mi>i</mi></msub></mrow></msub></math> represents transaction amount.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, with this information in hand, let’s revisit the ML models considering
    the cost-sensitive approach and calculate the changing cost of these models. However,
    before we start, it is worth noting that cost-sensitive models are not fast-processing
    ones, so as we have a large number of observations, it would be wise to sample
    from them to model the data in a timely manner. A class-dependent cost measure
    is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_modeling_operational_risk_CO4-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Sampling from `fraud_df` data
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_modeling_operational_risk_CO4-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Computing the cost matrix
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_modeling_operational_risk_CO4-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Computing the total cost per models employed
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the total cost enables us to have different approaches in assessing
    model performance. The model with a high F1 score is expected to have low total
    cost, and this is what we have in [Table 8-4](#total_cost). Logistic regression
    has the highest total cost, and XGBoost has the lowest.
  prefs: []
  type: TYPE_NORMAL
- en: Table 8-4\. Total cost
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Total cost |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Logistic Regression | 5995 |'
  prefs: []
  type: TYPE_TB
- en: '| Decision Tree | 5351 |'
  prefs: []
  type: TYPE_TB
- en: '| Random Forest | 5413 |'
  prefs: []
  type: TYPE_TB
- en: '| XGBoost | 5371 |'
  prefs: []
  type: TYPE_TB
- en: Saving Score
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are different metrics that can be used in cost improvement, and saving
    score is absolutely one of them. To be able to define saving, let us give the
    formula of cost.
  prefs: []
  type: TYPE_NORMAL
- en: 'Bahnsen, Aouada, and Ottersten (2014) clearly explain the saving score formula
    in the following manner:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="Cost left-parenthesis f left-parenthesis upper S right-parenthesis
    right-parenthesis equals sigma-summation Underscript i equals 1 Overscript upper
    N Endscripts left-parenthesis y Subscript i Baseline left-parenthesis c Subscript
    i Baseline upper C Subscript upper T upper P Sub Subscript i Subscript Baseline
    plus left-parenthesis 1 minus c Subscript i Baseline right-parenthesis upper C
    Subscript upper F upper N Sub Subscript i Subscript Baseline right-parenthesis
    plus left-parenthesis 1 minus y Subscript i Baseline right-parenthesis left-parenthesis
    c Subscript i Baseline upper C Subscript upper F upper P Sub Subscript i Subscript
    Baseline plus left-parenthesis 1 minus c Subscript i Baseline right-parenthesis
    upper C Subscript upper T upper N Sub Subscript i Subscript Baseline right-parenthesis
    right-parenthesis" display="block"><mrow><mtext>Cost(f(S))</mtext> <mo>=</mo>
    <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>N</mi></munderover>
    <mfenced close=")" open="(" separators=""><msub><mi>y</mi> <mi>i</mi></msub> <mrow><mo>(</mo><msub><mi>c</mi>
    <mi>i</mi></msub> <msub><mi>C</mi> <mrow><mi>T</mi><msub><mi>P</mi> <mi>i</mi></msub></mrow></msub>
    <mo>+</mo><mrow><mo>(</mo><mn>1</mn><mo>-</mo><msub><mi>c</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow><msub><mi>C</mi> <mrow><mi>F</mi><msub><mi>N</mi> <mi>i</mi></msub></mrow></msub>
    <mo>)</mo></mrow> <mo>+</mo> <mrow><mo>(</mo><mn>1</mn><mo>-</mo><msub><mi>y</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow> <mrow><mo>(</mo><msub><mi>c</mi> <mi>i</mi></msub>
    <msub><mi>C</mi> <mrow><mi>F</mi><msub><mi>P</mi> <mi>i</mi></msub></mrow></msub>
    <mo>+</mo><mrow><mo>(</mo><mn>1</mn><mo>-</mo><msub><mi>c</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow><msub><mi>C</mi> <mrow><mi>T</mi><msub><mi>N</mi> <mi>i</mi></msub></mrow></msub>
    <mo>)</mo></mrow></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'where *TP*, *FN*, *FP*, and *TN* are true positive, false negative, false positive,
    and true negative, respectively. <math alttext="c Subscript i"><msub><mi>c</mi>
    <mi>i</mi></msub></math> is the predicted label for each observation *i* on training
    set *S*. <math alttext="y Subscript i"><msub><mi>y</mi> <mi>i</mi></msub></math>
    is the class label and takes the value of either 1 or 0—that is, <math alttext="y
    element-of 0 comma 1"><mrow><mi>y</mi> <mo>∈</mo> <mrow><mn>0</mn> <mo>,</mo>
    <mn>1</mn></mrow></mrow></math> . Our saving formula is then:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="Saving left-parenthesis f left-parenthesis upper S right-parenthesis
    right-parenthesis equals StartFraction Cost left-parenthesis f left-parenthesis
    upper S right-parenthesis right-parenthesis minus upper C o s t Subscript l Baseline
    left-parenthesis upper S right-parenthesis Over upper C o s t Subscript l Baseline
    left-parenthesis upper S right-parenthesis EndFraction" display="block"><mrow><mtext>Saving(f(S))</mtext>
    <mo>=</mo> <mfrac><mrow><mtext>Cost(f(S))</mtext><mo>-</mo><mi>C</mi><mi>o</mi><mi>s</mi><msub><mi>t</mi>
    <mi>l</mi></msub> <mrow><mo>(</mo><mi>S</mi><mo>)</mo></mrow></mrow> <mrow><mi>C</mi><mi>o</mi><mi>s</mi><msub><mi>t</mi>
    <mi>l</mi></msub> <mrow><mo>(</mo><mi>S</mi><mo>)</mo></mrow></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where <math alttext="upper C o s t Subscript l Baseline equals m i n upper C
    o s t left-parenthesis f 0 left-parenthesis upper S right-parenthesis right-parenthesis
    comma upper C o s t left-parenthesis f 1 left-parenthesis upper S right-parenthesis
    right-parenthesis"><mrow><mi>C</mi> <mi>o</mi> <mi>s</mi> <msub><mi>t</mi> <mi>l</mi></msub>
    <mo>=</mo> <mi>m</mi> <mi>i</mi> <mi>n</mi> <mrow><mi>C</mi> <mi>o</mi> <mi>s</mi>
    <mi>t</mi> <mrow><mo>(</mo> <msub><mi>f</mi> <mn>0</mn></msub> <mrow><mo>(</mo>
    <mi>S</mi> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>,</mo> <mi>C</mi> <mi>o</mi>
    <mi>s</mi> <mi>t</mi> <mrow><mo>(</mo> <msub><mi>f</mi> <mn>1</mn></msub> <mrow><mo>(</mo>
    <mi>S</mi> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></mrow></math> where <math
    alttext="f 0"><msub><mi>f</mi> <mn>0</mn></msub></math> predicts class 0, <math
    alttext="c 0"><msub><mi>c</mi> <mn>0</mn></msub></math> , and <math alttext="f
    1"><msub><mi>f</mi> <mn>1</mn></msub></math> predicts observations in class 1,
    <math alttext="c 1"><msub><mi>c</mi> <mn>1</mn></msub></math> .
  prefs: []
  type: TYPE_NORMAL
- en: 'In code, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_modeling_operational_risk_CO5-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the saving score
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_modeling_operational_risk_CO5-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the F1 score
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Please note that, if you are using `sklearn` version 0.23 or higher, you need
    to downgrade it to 0.22 to use `costcla` library. This adjustment is required
    due to the `sklearn.external.six` package inside the `costcla` library.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 8-5](#saving_score) shows that decision tree has the highest saving
    score among the three models, and interestingly, logistic regression produces
    a negative saving score, implying that the number of false negative and false
    positive predictions is quite large, which inflates the denominator of the saving
    score formula.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 8-5\. Saving scores
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Saving score | F1 score |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Logistic regression | -0.5602 | 0.0000 |'
  prefs: []
  type: TYPE_TB
- en: '| Decision tree | 0.6557 | 0.7383 |'
  prefs: []
  type: TYPE_TB
- en: '| Random forest | 0.4789 | 0.7068 |'
  prefs: []
  type: TYPE_TB
- en: Cost-Sensitive Modeling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Thus far, we have discussed the concepts of saving score and cost sensitivity,
    and now we are ready to run cost-sensitive logistic regression, decision tree,
    and random forest. The question that we are trying to address here is what happens
    if fraud is modeled by considering varying costs of misclassification? How does
    it affect the saving score?
  prefs: []
  type: TYPE_NORMAL
- en: To undertake this investigation, we’ll use the `costcla` library. This library
    was specifically created to employ the cost-sensitive classifiers in which varying
    costs of misclassification are considered. Because, as discussed earlier, traditional
    fraud models assume that all correctly classified and misclassified examples carry
    the same cost, which is not correct due to the varying costs of misclassification
    in fraud (Bahnsen 2021).
  prefs: []
  type: TYPE_NORMAL
- en: 'Having applied the cost-sensitive models, the saving score is used to compare
    the models in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_modeling_operational_risk_CO6-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Training the cost-sensitive models by iteration
  prefs: []
  type: TYPE_NORMAL
- en: 'According to [Table 8-6](#saving_score_model), the best and the worst saving
    scores are obtained in random forest and logistic regression, respectively. This
    confirms two important facts: first, it implies that random forest has a low number
    of inaccurate observations, and second, that those inaccurate observations are
    less costly. To be precise, modeling fraud with random forest generates a very
    low number of false negatives, which is the denominator of the saving score formula.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 8-6\. Saving scores of cost-sensitive models
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Saving score | F1 score |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Logistic regression | -0.5906 | 0.0000 |'
  prefs: []
  type: TYPE_TB
- en: '| Decision tree | 0.8414 | 0.3281 |'
  prefs: []
  type: TYPE_TB
- en: '| Random forest | 0.8913 | 0.4012 |'
  prefs: []
  type: TYPE_TB
- en: Bayesian Minimum Risk
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Bayesian decision can also be used to model fraud taking into account the cost
    sensitivity. The Bayesian minimum risk method rests on a decision process using
    different costs (or loss) and probabilities. Mathematically, if the transaction
    is predicted to be fraud, the overall risk is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper R left-parenthesis c Subscript f Baseline vertical-bar
    upper S right-parenthesis equals upper L left-parenthesis c Subscript f Baseline
    vertical-bar y Subscript f Baseline right-parenthesis upper P left-parenthesis
    c Subscript f Baseline vertical-bar upper S right-parenthesis plus upper L left-parenthesis
    c Subscript f Baseline vertical-bar y Subscript l Baseline right-parenthesis upper
    P left-parenthesis c Subscript l Baseline vertical-bar upper S right-parenthesis"
    display="block"><mrow><mi>R</mi> <mrow><mo>(</mo> <msub><mi>c</mi> <mi>f</mi></msub>
    <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>L</mi> <mrow><mo>(</mo>
    <msub><mi>c</mi> <mi>f</mi></msub> <mo>|</mo> <msub><mi>y</mi> <mi>f</mi></msub>
    <mo>)</mo></mrow> <mi>P</mi> <mrow><mo>(</mo> <msub><mi>c</mi> <mi>f</mi></msub>
    <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow> <mo>+</mo> <mi>L</mi> <mrow><mo>(</mo>
    <msub><mi>c</mi> <mi>f</mi></msub> <mo>|</mo> <msub><mi>y</mi> <mi>l</mi></msub>
    <mo>)</mo></mrow> <mi>P</mi> <mrow><mo>(</mo> <msub><mi>c</mi> <mi>l</mi></msub>
    <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, if a transaction is predicted to be legitimate, then the
    overall risk turns out to be:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper R left-parenthesis c Subscript l Baseline vertical-bar
    upper S right-parenthesis equals upper L left-parenthesis c Subscript l Baseline
    vertical-bar y Subscript l Baseline right-parenthesis upper P left-parenthesis
    c Subscript l Baseline vertical-bar upper S right-parenthesis plus upper L left-parenthesis
    c Subscript l Baseline vertical-bar y Subscript f Baseline right-parenthesis upper
    P left-parenthesis c Subscript f Baseline vertical-bar upper S right-parenthesis"
    display="block"><mrow><mi>R</mi> <mrow><mo>(</mo> <msub><mi>c</mi> <mi>l</mi></msub>
    <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>L</mi> <mrow><mo>(</mo>
    <msub><mi>c</mi> <mi>l</mi></msub> <mo>|</mo> <msub><mi>y</mi> <mi>l</mi></msub>
    <mo>)</mo></mrow> <mi>P</mi> <mrow><mo>(</mo> <msub><mi>c</mi> <mi>l</mi></msub>
    <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow> <mo>+</mo> <mi>L</mi> <mrow><mo>(</mo>
    <msub><mi>c</mi> <mi>l</mi></msub> <mo>|</mo> <msub><mi>y</mi> <mi>f</mi></msub>
    <mo>)</mo></mrow> <mi>P</mi> <mrow><mo>(</mo> <msub><mi>c</mi> <mi>f</mi></msub>
    <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where <math alttext="y Subscript f"><msub><mi>y</mi> <mi>f</mi></msub></math>
    and <math alttext="y Subscript l"><msub><mi>y</mi> <mi>l</mi></msub></math> are
    the actual classes for fraudulent and legitimate cases, respectively. <math alttext="upper
    L left-parenthesis c Subscript f Baseline vertical-bar y Subscript f Baseline
    right-parenthesis"><mrow><mi>L</mi> <mo>(</mo> <msub><mi>c</mi> <mi>f</mi></msub>
    <mo>|</mo> <msub><mi>y</mi> <mi>f</mi></msub> <mo>)</mo></mrow></math> represents
    the cost when fraud is detected and the real class is fraud. Similarly, <math
    alttext="upper L left-parenthesis c Subscript l Baseline vertical-bar y Subscript
    l Baseline right-parenthesis"><mrow><mi>L</mi> <mo>(</mo> <msub><mi>c</mi> <mi>l</mi></msub>
    <mo>|</mo> <msub><mi>y</mi> <mi>l</mi></msub> <mo>)</mo></mrow></math> denotes
    the cost when the transaction is predicted to be legitimate and the real class
    is legitimate. Conversely, <math alttext="upper L left-parenthesis c Subscript
    f Baseline vertical-bar y Subscript l Baseline right-parenthesis"><mrow><mi>L</mi>
    <mo>(</mo> <msub><mi>c</mi> <mi>f</mi></msub> <mo>|</mo> <msub><mi>y</mi> <mi>l</mi></msub>
    <mo>)</mo></mrow></math> and <math alttext="upper L left-parenthesis c Subscript
    l Baseline vertical-bar y Subscript f Baseline right-parenthesis"><mrow><mi>L</mi>
    <mo>(</mo> <msub><mi>c</mi> <mi>l</mi></msub> <mo>|</mo> <msub><mi>y</mi> <mi>f</mi></msub>
    <mo>)</mo></mrow></math> calculate the cost of the off-diagonal elements in [Table 8-3](#cost_sen_mat).
    The former calculates the cost when the transaction is predicted to be a fraud
    but the actual class is not, and the latter shows the cost when the transaction
    is legitimate but the actual class is fraud. <math alttext="upper P left-parenthesis
    c Subscript l Baseline vertical-bar upper S right-parenthesis"><mrow><mi>P</mi>
    <mo>(</mo> <msub><mi>c</mi> <mi>l</mi></msub> <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow></math>
    indicates the predicted probability of having a legitimate transaction given *S*
    and <math alttext="upper P left-parenthesis c Subscript f Baseline vertical-bar
    upper S right-parenthesis"><mrow><mi>P</mi> <mo>(</mo> <msub><mi>c</mi> <mi>f</mi></msub>
    <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow></math> and the predicted probability of
    having a fraudulent transaction given *S*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, the Bayesian minimum risk formula can be interpreted as:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper R left-parenthesis c Subscript f Baseline vertical-bar
    upper S right-parenthesis equals upper C Subscript a d m i n Baseline upper P
    left-parenthesis c Subscript f Baseline vertical-bar upper S right-parenthesis
    plus upper C Subscript a d m i n Baseline upper P left-parenthesis c Subscript
    l Baseline vertical-bar upper S right-parenthesis" display="block"><mrow><mi>R</mi>
    <mrow><mo>(</mo> <msub><mi>c</mi> <mi>f</mi></msub> <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow>
    <mo>=</mo> <msub><mi>C</mi> <mrow><mi>a</mi><mi>d</mi><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub>
    <mi>P</mi> <mrow><mo>(</mo> <msub><mi>c</mi> <mi>f</mi></msub> <mo>|</mo> <mi>S</mi>
    <mo>)</mo></mrow> <mo>+</mo> <msub><mi>C</mi> <mrow><mi>a</mi><mi>d</mi><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub>
    <mi>P</mi> <mrow><mo>(</mo> <msub><mi>c</mi> <mi>l</mi></msub> <mo>|</mo> <mi>S</mi>
    <mo>)</mo></mrow></mrow></math><math alttext="upper R left-parenthesis c Subscript
    l Baseline vertical-bar upper S right-parenthesis equals 0 plus upper C Subscript
    a m t Baseline upper P left-parenthesis c Subscript l Baseline vertical-bar upper
    S right-parenthesis" display="block"><mrow><mi>R</mi> <mrow><mo>(</mo> <msub><mi>c</mi>
    <mi>l</mi></msub> <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow> <mo>=</mo> <mn>0</mn>
    <mo>+</mo> <msub><mi>C</mi> <mrow><mi>a</mi><mi>m</mi><mi>t</mi></mrow></msub>
    <mi>P</mi> <mrow><mo>(</mo> <msub><mi>c</mi> <mi>l</mi></msub> <mo>|</mo> <mi>S</mi>
    <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'with *admin* is administrative cost and *amt* is the transaction amount. With
    that being said, the transaction is labeled as fraud if:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper R left-parenthesis c Subscript f Baseline vertical-bar
    upper S right-parenthesis greater-than-or-equal-to upper R left-parenthesis c
    Subscript l Baseline vertical-bar upper S right-parenthesis" display="block"><mrow><mi>R</mi>
    <mrow><mo>(</mo> <msub><mi>c</mi> <mi>f</mi></msub> <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow>
    <mo>≥</mo> <mi>R</mi> <mrow><mo>(</mo> <msub><mi>c</mi> <mi>l</mi></msub> <mo>|</mo>
    <mi>S</mi> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper C Subscript a d m i n Baseline upper P left-parenthesis
    c Subscript f Baseline vertical-bar upper S right-parenthesis plus upper C Subscript
    a d m i n Baseline upper P left-parenthesis c Subscript l Baseline vertical-bar
    upper S right-parenthesis greater-than-or-equal-to upper C Subscript a m t Baseline
    upper P left-parenthesis c Subscript l Baseline vertical-bar upper S right-parenthesis"
    display="block"><mrow><msub><mi>C</mi> <mrow><mi>a</mi><mi>d</mi><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub>
    <mi>P</mi> <mrow><mo>(</mo> <msub><mi>c</mi> <mi>f</mi></msub> <mo>|</mo> <mi>S</mi>
    <mo>)</mo></mrow> <mo>+</mo> <msub><mi>C</mi> <mrow><mi>a</mi><mi>d</mi><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub>
    <mi>P</mi> <mrow><mo>(</mo> <msub><mi>c</mi> <mi>l</mi></msub> <mo>|</mo> <mi>S</mi>
    <mo>)</mo></mrow> <mo>≥</mo> <msub><mi>C</mi> <mrow><mi>a</mi><mi>m</mi><mi>t</mi></mrow></msub>
    <mi>P</mi> <mrow><mo>(</mo> <msub><mi>c</mi> <mi>l</mi></msub> <mo>|</mo> <mi>S</mi>
    <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, it is time to apply the Bayesian Minimum Risk model in Python. Again,
    three models are employed and compared using F1 score: F1 score results can be
    found in [Table 8-7](#f1_score_bmr), and it turns out decision tree has the highest
    F1 score and logistic regression has the lowest one. So, the order of saving scores
    is other way around, indicating the effectiveness of the cost-sensitive approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_modeling_operational_risk_CO7-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Calling the Bayesian Minimum Risk Classifier library
  prefs: []
  type: TYPE_NORMAL
- en: Table 8-7\. F1 score based on BMR
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Saving score | F1 score |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Logistic regression | 0.8064 | 0.1709 |'
  prefs: []
  type: TYPE_TB
- en: '| Decision tree | 0.7343 | 0.6381 |'
  prefs: []
  type: TYPE_TB
- en: '| Random forest | 0.9624 | 0.4367 |'
  prefs: []
  type: TYPE_TB
- en: 'To create a plot of this data, we do the following (resulting in [Figure 8-6](#score_barplot_fraud)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_modeling_operational_risk_CO8-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Drawing the F1 score with a line plot
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_modeling_operational_risk_CO8-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Drawing the bar plot based on the models used
  prefs: []
  type: TYPE_NORMAL
- en: '![f1_saving](assets/mlfr_0806.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-6\. F1 and saving scores
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Figure 8-6](#score_barplot_fraud) shows the F1 and saving scores across the
    models we have employed so far. Accordingly, the cost-sensitive and Bayesian minimum
    risk model outperform the base models, as expected.'
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised Learning Modeling for Fraud Examination
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unsupervised learning models are also used to detect fraudulent activities in
    a way that extracts the hidden characteristics of the data. The most prominent
    advantage of this method over the supervised model is that there is no need to
    apply a sampling procedure to fix the imbalanced-data problem. Unsupervised models,
    by their nature, do not require any prior knowledge about the data. To see how
    unsupervised learning models perform on this type of data, we will explore the
    self-organizing map (SOM) and autoencoder models.
  prefs: []
  type: TYPE_NORMAL
- en: Self-Organizing Map
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SOM is an unsupervised method to obtain a low-dimensional space from a high-dimensional
    space. This is a method that was introduced by Finnish scholar Teuvo Kohonen in
    1980s and it became widespread. SOM is a type of artificial NN, and therefore
    it rests on competitive learning in the sense that output neurons compete to be
    activated. The activated neuron is referred to as the *winning neuron*, and each
    neuron has neighboring weights, so it is the spatial locations of the nodes in
    the output space that are indicative of the inherent statistical features in the
    input space (Haykin 1999).
  prefs: []
  type: TYPE_NORMAL
- en: 'The most distinctive features of SOM methods are as follows (Asan and Ercan
    2012):'
  prefs: []
  type: TYPE_NORMAL
- en: No assumptions regarding the distribution of variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dependent structure among variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dealing with nonlinear structure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coping with noisy and missing data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s walk through the important steps of the SOM technique. As you might have
    guessed, the first step is to identify the winning node, or the activated neuron.
    The winning node is identified by distance metrics—that is, Manhattan, Chebyshev,
    and Euclidean distances. Of these distance metrics, Euclidean distance is the
    most commonly used because it works well under the gradient descent process. Thus,
    given the following Euclidean formula, we can find the distance between sample
    and weight:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="parallel-to left-parenthesis x Subscript t Baseline minus w Subscript
    i Baseline left-parenthesis t right-parenthesis right-parenthesis parallel-to
    equals StartRoot sigma-summation Underscript i Endscripts equals 1 Superscript
    n Baseline left-parenthesis x Subscript t j Baseline minus w Subscript t j i Baseline
    right-parenthesis squared EndRoot comma i equals 1 comma 2 comma period period
    period comma n" display="block"><mrow><mfenced close="∥" open="∥" separators=""><mo>(</mo>
    <msub><mi>x</mi> <mi>t</mi></msub> <mo>-</mo> <msub><mi>w</mi> <mi>i</mi></msub>
    <mrow><mo>(</mo> <mi>t</mi> <mo>)</mo></mrow> <mo>)</mo></mfenced> <mo>=</mo>
    <msqrt><mrow><msub><mo>∑</mo> <mi>i</mi></msub> <mo>=</mo> <msup><mn>1</mn> <mi>n</mi></msup>
    <msup><mrow><mo>(</mo><msub><mi>x</mi> <mrow><mi>t</mi><mi>j</mi></mrow></msub>
    <mo>-</mo><msub><mi>w</mi> <mrow><mi>t</mi><mi>j</mi><mi>i</mi></mrow></msub>
    <mo>)</mo></mrow> <mn>2</mn></msup></mrow></msqrt> <mo>,</mo> <mi>i</mi> <mo>=</mo>
    <mn>1</mn> <mo>,</mo> <mn>2</mn> <mo>,</mo> <mo>.</mo> <mo>.</mo> <mo>.</mo> <mo>,</mo>
    <mi>n</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where *x* is sample, *w* is weight, and the winning node, *k(t)*, is shown in
    [Equation 8-1](#winning_node).
  prefs: []
  type: TYPE_NORMAL
- en: Equation 8-1\. Identifying the winning node
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math alttext="k left-parenthesis t right-parenthesis equals arg min parallel-to
    x left-parenthesis t right-parenthesis minus w right-parenthesis i left-parenthesis
    t right-parenthesis parallel-to" display="block"><mrow><mi>k</mi> <mrow><mo>(</mo>
    <mi>t</mi> <mo>)</mo></mrow> <mo>=</mo> <mtext>arg</mtext> <mtext>min</mtext>
    <mfenced close="∥" open="∥" separators=""><mi>x</mi> <mo>(</mo> <mi>t</mi> <mo>)</mo>
    <mo>-</mo> <mi>w</mi> <mo>)</mo> <mi>i</mi> <mo>(</mo> <mi>t</mi> <mo>)</mo></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'The other important step is to update the weight. Given the learning rate and
    neighborhood size, the following update is applied:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="w Subscript i Baseline left-parenthesis t plus 1 right-parenthesis
    equals w Subscript i Baseline left-parenthesis t right-parenthesis plus lamda
    left-bracket x left-parenthesis t right-parenthesis minus w Subscript i Baseline
    left-parenthesis t right-parenthesis right-bracket" display="block"><mrow><msub><mi>w</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo> <mi>t</mi> <mo>+</mo> <mn>1</mn> <mo>)</mo></mrow>
    <mo>=</mo> <msub><mi>w</mi> <mi>i</mi></msub> <mrow><mo>(</mo> <mi>t</mi> <mo>)</mo></mrow>
    <mo>+</mo> <mi>λ</mi> <mrow><mo>[</mo> <mi>x</mi> <mrow><mo>(</mo> <mi>t</mi>
    <mo>)</mo></mrow> <mo>-</mo> <msub><mi>w</mi> <mi>i</mi></msub> <mrow><mo>(</mo>
    <mi>t</mi> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where <math alttext="w Subscript i Baseline left-parenthesis t right-parenthesis"><mrow><msub><mi>w</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo> <mi>t</mi> <mo>)</mo></mrow></mrow></math>
    is the weight of the winning neuron *i* at <math alttext="t Superscript t h"><msup><mi>t</mi>
    <mrow><mi>t</mi><mi>h</mi></mrow></msup></math> iteration, and <math alttext="lamda"><mi>λ</mi></math>
    is the learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: 'Richardson, Risien, and Shillington (2003) state that the rate of adaptation
    of the weights decays as it moves away from the winning node. This is defined
    by neighborhood function, <math alttext="h Subscript k i Baseline left-parenthesis
    t right-parenthesis"><mrow><msub><mi>h</mi> <mrow><mi>k</mi><mi>i</mi></mrow></msub>
    <mrow><mo>(</mo> <mi>t</mi> <mo>)</mo></mrow></mrow></math> , where *i* is index
    of the neighbor. Of the neighborhood functions, the most famous one is the Gaussian
    function with the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="h Subscript k i Baseline left-parenthesis t right-parenthesis
    equals e x p left-parenthesis minus StartFraction d Subscript k i Superscript
    2 Baseline Over 2 sigma squared left-parenthesis t right-parenthesis EndFraction
    right-parenthesis" display="block"><mrow><msub><mi>h</mi> <mrow><mi>k</mi><mi>i</mi></mrow></msub>
    <mrow><mo>(</mo> <mi>t</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>e</mi> <mi>x</mi>
    <mi>p</mi> <mrow><mo>(</mo> <mo>-</mo> <mfrac><msubsup><mi>d</mi> <mrow><mi>k</mi><mi>i</mi></mrow>
    <mn>2</mn></msubsup> <mrow><mn>2</mn><msup><mi>σ</mi> <mn>2</mn></msup> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></mrow></mfrac>
    <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where <math alttext="d Subscript k i Superscript 2"><msubsup><mi>d</mi> <mrow><mi>k</mi><mi>i</mi></mrow>
    <mn>2</mn></msubsup></math> denotes the distance between the winning neuron and
    the related neuron, and <math alttext="sigma squared left-parenthesis t right-parenthesis"><mrow><msup><mi>σ</mi>
    <mn>2</mn></msup> <mrow><mo>(</mo> <mi>t</mi> <mo>)</mo></mrow></mrow></math>
    denotes the radius at iteration *t*.
  prefs: []
  type: TYPE_NORMAL
- en: Considering all this, the updating process becomes what’s shown in [Equation
    8-2](#weightupdate2).
  prefs: []
  type: TYPE_NORMAL
- en: Equation 8-2\. Updating the weight
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math alttext="w Subscript i Baseline left-parenthesis t plus 1 right-parenthesis
    equals w Subscript i Baseline left-parenthesis t right-parenthesis plus lamda
    h Subscript k i Baseline left-parenthesis t right-parenthesis left-bracket x left-parenthesis
    t right-parenthesis minus w Subscript i Baseline left-parenthesis t right-parenthesis
    right-bracket" display="block"><mrow><msub><mi>w</mi> <mi>i</mi></msub> <mrow><mo>(</mo>
    <mi>t</mi> <mo>+</mo> <mn>1</mn> <mo>)</mo></mrow> <mo>=</mo> <msub><mi>w</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo> <mi>t</mi> <mo>)</mo></mrow> <mo>+</mo> <mi>λ</mi>
    <msub><mi>h</mi> <mrow><mi>k</mi><mi>i</mi></mrow></msub> <mrow><mo>(</mo> <mi>t</mi>
    <mo>)</mo></mrow> <mrow><mo>[</mo> <mi>x</mi> <mrow><mo>(</mo> <mi>t</mi> <mo>)</mo></mrow>
    <mo>-</mo> <msub><mi>w</mi> <mi>i</mi></msub> <mrow><mo>(</mo> <mi>t</mi> <mo>)</mo></mrow>
    <mo>]</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'That’s all there is to it, but I’m aware that the process is a bit tedious.
    So let us summarize the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize the weights: assigning random values to weights is the most common
    approach.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the winning neuron using [Equation 8-1](#winning_node).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the weights as given in [Equation 8-2](#weightupdate2).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Adjust the parameters based on the results of [Equation 8-2](#weightupdate2)
    by setting *t* to *t* + 1.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We already know that there are two classes in the fraud data that we use, so
    the dimensions for our self organizing map should have a two-by-one structure.
    You can find the application in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_modeling_operational_risk_CO9-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Configuring the SOP
  prefs: []
  type: TYPE_NORMAL
- en: 'Having checked the classification report, it becomes obvious that the F1 score
    is somewhat similar to what we found with the other methods. This confirms that
    the SOM is a useful model in detecting fraud when we don’t have labeled data.
    In the following code, we generate [Figure 8-7](#som_pred), which shows the actual
    and predicted classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![som_prediction](assets/mlfr_0807.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-7\. SOM prediction
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Autoencoders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An *autoencoder* is an unsupervised deep learning model trained to transform
    inputs into outputs via a hidden layer. However, the network structure of autoencoder
    is different from other structures in the sense that autoencoder consists of two
    parts: an *encoder* and a *decoder*.'
  prefs: []
  type: TYPE_NORMAL
- en: The encoder serves as a feature extraction function, and the decoder works as
    a reconstruction function. To illustrate, let *x* be an input and *h* be a hidden
    layer. Then, the encoder function is *h* = <math alttext="f left-parenthesis x
    right-parenthesis"><mrow><mi>f</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></math>
    , and the decoder function reconstructs by *r* = <math alttext="g left-parenthesis
    h right-parenthesis"><mrow><mi>g</mi> <mo>(</mo> <mi>h</mi> <mo>)</mo></mrow></math>
    . If an autoencoder learns by simple copying, i.e., <math alttext="g left-parenthesis
    f left-parenthesis x right-parenthesis right-parenthesis"><mrow><mi>g</mi> <mo>(</mo>
    <mi>f</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo> <mo>)</mo></mrow></math> = *x*, it
    is not an ideal situation in that the autoencoder seeks feature extraction. This
    amounts to copying only the relevant aspects of the input (Goodfellow et al. 2016).
  prefs: []
  type: TYPE_NORMAL
- en: 'Consequently, autoencoder has a network structure such that it compresses knowledge
    in a way to have a lower-dimensional representation of the original input. Given
    the encoder and decoder functions, there are different types of autoencoders.
    Of them, we’ll discuss the three most commonly used autoencoders to keep ourselves
    on track:'
  prefs: []
  type: TYPE_NORMAL
- en: Undercomplete autoencoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sparse autoencoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Denoising autoencoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Undercomplete autoencoders
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is the most basic type of autoencoder, as the hidden layer, *h*, has a
    smaller dimension than training data, *x*. So the number of neurons is less than
    that of the training data. The aim of this autoencoder is to capture the latent
    attribute of the data by minimizing the loss function—that is, <math alttext="double-struck
    upper L left-parenthesis x comma g left-parenthesis f left-parenthesis x right-parenthesis
    right-parenthesis right-parenthesis"><mrow><mi>𝕃</mi> <mo>(</mo> <mi>x</mi> <mo>,</mo>
    <mi>g</mi> <mo>(</mo> <mi>f</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo> <mo>)</mo> <mo>)</mo></mrow></math>
    , where <math alttext="double-struck upper L"><mi>𝕃</mi></math> is the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders famously face a trade-off in ML known as the bias-variance trade-off,
    in which autoencoders aim to reconstruct the input well while having low-dimensional
    representations. To remedy this issue, we’ll introduce sparse and denoising autoencoders.
  prefs: []
  type: TYPE_NORMAL
- en: Sparse autoencoder
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Sparse autoencoders suggest a solution to this trade-off by imposing sparsity
    on the reconstruction error. There are two ways to enforce regularization in sparce
    autoencoders. The first way is to apply <math alttext="upper L 1"><msub><mi>L</mi>
    <mn>1</mn></msub></math> regularization. In this case, the autoencoders optimization
    becomes (Banks, Koenigstein, and Giryes 2020):'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign arg min Subscript g comma f Baseline double-struck
    upper L left-parenthesis x comma g left-parenthesis f left-parenthesis x right-parenthesis
    right-parenthesis right-parenthesis plus lamda left-parenthesis h right-parenthesis
    dollar-sign"><mrow><mtext>arg</mtext> <msub><mtext>min</mtext> <mrow><mi>g</mi><mo>,</mo><mi>f</mi></mrow></msub>
    <mi>𝕃</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>g</mi> <mrow><mo>(</mo>
    <mi>f</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>)</mo></mrow>
    <mo>+</mo> <mi>λ</mi> <mrow><mo>(</mo> <mi>h</mi> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where <math alttext="g left-parenthesis f left-parenthesis x right-parenthesis
    right-parenthesis"><mrow><mi>g</mi> <mo>(</mo> <mi>f</mi> <mo>(</mo> <mi>x</mi>
    <mo>)</mo> <mo>)</mo></mrow></math> is the decoder, and *h* is the encoder outputs.
    [Figure 8-8](#sparse_auto) illustrates the sparse autoencoder.
  prefs: []
  type: TYPE_NORMAL
- en: '![sparse_auto](assets/mlfr_0808.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-8\. Sparse autoencoder model stucture
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The second way to regularize the sparse autoencoders is with Kullback-Leibler
    (KL) divergence, which tells us the similarity of the two probability distributions
    simply by measuring the distance between them. KL divergence can be put mathematically
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign double-struck upper L left-parenthesis x comma ModifyingAbove
    x With caret right-parenthesis plus sigma-summation Underscript j Endscripts upper
    K upper L left-parenthesis rho parallel-to ModifyingAbove rho With caret parallel-to
    right-parenthesis dollar-sign"><mrow><mi>𝕃</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo>
    <mover accent="true"><mi>x</mi> <mo>^</mo></mover> <mo>)</mo></mrow> <mo>+</mo>
    <msub><mo>∑</mo> <mi>j</mi></msub> <mi>K</mi> <mi>L</mi> <mrow><mo>(</mo> <mi>ρ</mi>
    <mfenced close="∥" open="∥" separators=""><mover accent="true"><mi>ρ</mi> <mo>^</mo></mover></mfenced>
    <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where <math alttext="rho"><mi>ρ</mi></math> and <math alttext="ModifyingAbove
    rho With caret"><mover accent="true"><mi>ρ</mi> <mo>^</mo></mover></math> are
    ideal and observed distributions, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Denoising autoencoders
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The idea behind denoising autoencoders is that instead of using a penalty term,
    <math alttext="lamda"><mi>λ</mi></math> , add noise to the input data and learn
    from this changed construction—that is, reconstruction. Thus, instead of minimizing
    <math alttext="double-struck upper L left-parenthesis x comma g left-parenthesis
    f left-parenthesis x right-parenthesis right-parenthesis right-parenthesis"><mrow><mi>𝕃</mi>
    <mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>g</mi> <mo>(</mo> <mi>f</mi> <mo>(</mo> <mi>x</mi>
    <mo>)</mo> <mo>)</mo> <mo>)</mo></mrow></math> , denoising autoencoders offer
    to minimize the following loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="double-struck upper L left-parenthesis x comma g left-parenthesis
    f left-parenthesis ModifyingAbove x With caret right-parenthesis right-parenthesis
    right-parenthesis" display="block"><mrow><mi>𝕃</mi> <mo>(</mo> <mi>x</mi> <mo>,</mo>
    <mi>g</mi> <mrow><mo>(</mo> <mi>f</mi> <mrow><mo>(</mo> <mover accent="true"><mi>x</mi>
    <mo>^</mo></mover> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>)</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where <math alttext="ModifyingAbove x With caret"><mover accent="true"><mi>x</mi>
    <mo>^</mo></mover></math> is the corrupted input obtained by adding noise by,
    for instance, Gaussian noise. [Figure 8-9](#corrupt_noise) illustrates this process.
  prefs: []
  type: TYPE_NORMAL
- en: '![corrupt](assets/mlfr_0809.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-9\. Denoising autoencoder model structure
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In the following code, we’ll use an autoencoder model with Keras. Before moving
    forward, it is scaled using Standard Scaler, and then, using a batch size of 200
    and an epoch number of 100, we are able to get a satisfactory prediction result.
    We’ll then create a reconstruction error table from the autoencoder model to compare
    with the true class, and it turns out that the means and standard deviations of
    these models are close to each other:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_modeling_operational_risk_CO10-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Identifying 64 and 32 hidden layers in the encoder and decoder parts, respectively
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_modeling_operational_risk_CO10-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Identifying 32 and 64 hidden layers in the encoder and decoder parts, respectively
  prefs: []
  type: TYPE_NORMAL
- en: 'After configuring the autoencoder model, the next step is to fit and predict.
    After doing the prediction, we check the quality of the model using summary statistics,
    as they are a reliable way to see whether reconstruction works well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_modeling_operational_risk_CO11-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a table named `error_df` to compare the results obtained from the model
    with the real data
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we create our plot ([Figure 8-10](#autoencoder_fraud)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![autoencoder_fraud](assets/mlfr_0810.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-10\. Autoencoder performance
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Figure 8-10](#autoencoder_fraud) shows the results of our autoencoder modeling
    using a line plot, and we can see that the test loss result is more volatile than
    that of train but, on average, the mean loss is similar.'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fraud is a hot topic in finance for several reasons. Strict regulation, reputation
    loss, and costs arising from fraud are the primary reasons to fight it. Until
    recently, fraud has been a big problem for financial institutions, as modeling
    fraud had not produced satisfactory results and, because of this, financial institutions
    had to employ more resources to handle this phenomenon. Thanks to recent advancements
    in ML, we now have various tools at our disposal for combatting fraud, and this
    chapter was dedicated to introducing these models and comparing their results.
    These models ranged from parametric approaches such as logistic regression to
    deep learning models such as autoencoders.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll look at a rather different financial risk model known
    as stock price crash risk, which will enable us to gain insight about the well-being
    of corporate governance. This is an important tool for financial risk management
    because risk management is ultimately rooted in corporate management. It would
    be naive to expect low risk in a company with bad corporate governance.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Articles cited in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Asan, Umut, and Secil Ercan. 2012\. “An Introduction to Self-Organizing Maps.”
    In *Computational Intelligence Systems in Industrial Engineering*, edited by Cengiz
    Kahraman. 295-315\. Paris: Atlantis Press'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bahnsen, Alejandro Correa, Djamia Aouada, and Björn Ottersten. 2014\. “Example-Dependent
    Cost-Sensitive Logistic Regression for Credit Scoring.” In *The 13th International
    Conference on Machine Learning and Applications*, pp. 263-269\. IEEE.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bank, Dor, Noam Koenigstein, and Raja Giryes. 2020\. “Autoencoders.” arXiv preprint
    arXiv:2003.05991.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dunnett, Robert S., Cindy B. Levy, and Antonio P. Simoes. 2005\. “The Hidden
    Costs of Operational Risk.” McKinsey St Company.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Richardson, Anthony J., C. Risien, and Frank Alan Shillington. 2003\. “Using
    Self-Organizing Maps to Identify Patterns in Satellite Imagery.” Progress in Oceanography
    59 (2-3): 223-239.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Books and online resources cited in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Bahnsen, Alejandro Correa. 2021\. “Introduction to Example-Dependent Cost-Sensitive
    Classification.” [*https://oreil.ly/5eCsJ*](https://oreil.ly/5eCsJ).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016\. *Deep Learning*.
    Cambridge: MIT press.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nilsen. 2020\. “Card Fraud Losses Reach $28.65 Billion.” Nilsen Report. [*https://oreil.ly/kSls7*](https://oreil.ly/kSls7).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Office of the Comptroller of the Currency. 2019\. “Operational Risk: Fraud
    Risk Management Principles.” CC Bulletin. [*https://oreil.ly/GaQez*](https://oreil.ly/GaQez).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Simon, Haykin. 1999\. *Neural Networks: A Comprehensive Foundation*, second
    edition. Englewood Cliffs, New Jersey: Prentice-Hall.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
