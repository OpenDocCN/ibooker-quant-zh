<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 3. Artificial Neural Networks"><div class="chapter" id="Chapter3">
<h1><span class="label">Chapter 3. </span>Artificial Neural Networks</h1>


<p><a data-type="indexterm" data-primary="artificial neural networks (ANNs)" id="ix_Chapter3-asciidoc0"/>There are many different types of models used in machine learning. However, one class of machine learning models that stands out is artificial neural networks (ANNs). Given that artificial neural networks are used across all machine learning types, this chapter will cover the basics of ANNs.</p>

<p>ANNs are computing systems based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it.</p>

<p><a data-type="indexterm" data-primary="deep learning" data-secondary="ANNs and" id="idm45174933856248"/><em>Deep learning</em> involves the study of complex ANN-related algorithms. The complexity is attributed to elaborate patterns of how information flows throughout the model. Deep learning has the ability to represent the world as a nested hierarchy of concepts, with each concept defined in relation to a simpler concept. Deep learning techniques are extensively used in reinforcement learning and natural language processing applications that we will look at in Chapters <a href="ch09.xhtml#Chapter9">9</a> and <a href="ch10.xhtml#Chapter10">10</a>.</p>
<aside data-type="sidebar" epub:type="sidebar" class="pagebreak-before less_space"><div class="sidebar" id="idm45174933852888">
<h5/>
<p>We will review detailed terminology and processes used in the field of ANNs<sup><a data-type="noteref" id="idm45174933851448-marker" href="ch03.xhtml#idm45174933851448">1</a></sup> and cover the following topics:</p>

<ul>
<li>
<p>Architecture of ANNs: Neurons and layers</p>
</li>
<li>
<p>Training an ANN: Forward propagation, backpropagation and gradient descent</p>
</li>
<li>
<p>Hyperparameters of ANNs: Number of layers and nodes, activation function, loss function, learning rate, etc.</p>
</li>
<li>
<p>Defining and training a deep neural network–based model in Python</p>
</li>
<li>
<p>Improving the training speed of ANNs and deep learning models</p>
</li>
</ul>
</div></aside>






<section data-type="sect1" data-pdf-bookmark="ANNs: Architecture, Training, and Hyperparameters"><div class="sect1" id="idm45174933845000">
<h1>ANNs: Architecture, Training, and Hyperparameters</h1>

<p>ANNs contain multiple neurons arranged in layers. An ANN goes through a training phase by comparing the modeled output to the desired output, where it learns to recognize patterns in data. Let us go through the components of ANNs.</p>








<section data-type="sect2" data-pdf-bookmark="Architecture"><div class="sect2" id="idm45174933843096">
<h2>Architecture</h2>

<p><a data-type="indexterm" data-primary="artificial neural networks (ANNs)" data-secondary="architecture" id="ix_Chapter3-asciidoc1"/>An ANN architecture comprises neurons, layers, and weights.</p>










<section data-type="sect3" data-pdf-bookmark="Neurons"><div class="sect3" id="idm45174933840184">
<h3>Neurons</h3>

<p><a data-type="indexterm" data-primary="artificial neural networks (ANNs)" data-secondary="neurons" id="idm45174933839016"/><a data-type="indexterm" data-primary="neurons" id="idm45174933838072"/>The building blocks for ANNs are neurons (also known as artificial neurons, nodes, or perceptrons). Neurons have one or more inputs and one output. It is possible to build a network of neurons to compute complex logical propositions. Activation functions in these neurons create complicated, nonlinear functional mappings between the inputs and the output.<sup><a data-type="noteref" id="idm45174933836904-marker" href="ch03.xhtml#idm45174933836904">2</a></sup></p>

<p>As shown in <a data-type="xref" href="#SingleNeuron">Figure 3-1</a>, a neuron takes an input (<em>x<sub>1</sub></em>, <em>x<sub>2</sub></em>…<em>x<sub>n</sub></em>), applies the learning parameters to generate a weighted sum (<em>z</em>), and then passes that sum to an activation function (<em>f</em>) that computes the output <em>f(z)</em>.</p>

<figure><div id="SingleNeuron" class="figure">
<img src="Images/mlbf_0301.png" alt="mlbf 0301" width="747" height="668"/>
<h6><span class="label">Figure 3-1. </span>An artificial neuron</h6>
</div></figure>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Layers"><div class="sect3" id="idm45174933744776">
<h3>Layers</h3>

<p><a data-type="indexterm" data-primary="artificial neural networks (ANNs)" data-secondary="layers" id="idm45174933743304"/><a data-type="indexterm" data-primary="layers, ANN" id="idm45174933742312"/>The output <em>f(z)</em> from a single neuron (as shown in <a data-type="xref" href="#SingleNeuron">Figure 3-1</a>) will not be able to model complex tasks. So, in order to handle more complex structures, we have multiple layers of such neurons. As we keep stacking neurons horizontally and vertically, the class of functions we can get becomes increasing complex. <a data-type="xref" href="#Layers">Figure 3-2</a> shows an architecture of an ANN with an input layer, an output layer, and a hidden layer.</p>

<figure><div id="Layers" class="figure">
<img src="Images/mlbf_0302.png" alt="mlbf 0302" width="743" height="507"/>
<h6><span class="label">Figure 3-2. </span>Neural network architecture</h6>
</div></figure>












<section data-type="sect4" data-pdf-bookmark="Input layer"><div class="sect4" id="idm45174933736968">
<h4>Input layer</h4>

<p><a data-type="indexterm" data-primary="input layer, ANN" id="idm45174933735768"/>The input layer takes input from the dataset and is the exposed part of the network. A neural network is often drawn with an input layer of one neuron per input value (or column) in the dataset. The neurons in the input layer simply pass the input value though to the next layer.</p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="Hidden layers"><div class="sect4" id="idm45174933734392">
<h4>Hidden layers</h4>

<p><a data-type="indexterm" data-primary="hidden layers" id="idm45174933733224"/>Layers after the input layer are called hidden layers because they are not directly exposed to the input. The simplest network structure is to have a single neuron in the hidden layer that directly outputs the value.</p>

<p>A multilayer ANN is capable of solving more complex machine learning–related tasks due to its hidden layer(s). Given increases in computing power and efficient libraries, neural networks with many layers can be constructed. <a data-type="indexterm" data-primary="deep neural networks" data-secondary="defined" id="idm45174933731528"/>ANNs with many hidden layers (more than three) are known as <em>deep neural networks</em>. Multiple hidden layers allow deep neural networks to learn features of the data in a so-called feature hierarchy, because simple features recombine from one layer to the next to form more complex features. ANNs with many layers pass input data (features) through more mathematical operations than do ANNs with few layers and are therefore more computationally intensive to train.</p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="Output layer"><div class="sect4" id="idm45174933729384">
<h4>Output layer</h4>

<p><a data-type="indexterm" data-primary="output layer, ANN" id="idm45174933728184"/>The final layer is called the output layer; it is responsible for outputting a value or vector of values that correspond to the format required to solve the 
<span class="keep-together">problem</span>.</p>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="Neuron weights"><div class="sect3" id="idm45174933726264">
<h3>Neuron weights</h3>

<p><a data-type="indexterm" data-primary="neuron weights" id="idm45174933725144"/><a data-type="indexterm" data-primary="weights, neuron" id="idm45174933724440"/>A neuron weight represents the strength of the connection between units and 
<span class="keep-together">measures</span> the influence the input will have on the output. If the weight from neuron one to neuron two has greater magnitude, it means that neuron one has a greater influence over neuron two. Weights near zero mean changing this input will not change the output. Negative weights mean increasing this input will decrease the 
<span class="keep-together">output</span>.<a data-type="indexterm" data-startref="ix_Chapter3-asciidoc1" id="idm45174933721960"/></p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Training"><div class="sect2" id="idm45174933720968">
<h2>Training</h2>

<p><a data-type="indexterm" data-primary="artificial neural networks (ANNs)" data-secondary="training" id="ix_Chapter3-asciidoc2"/><a data-type="indexterm" data-primary="training, for ANNs" id="ix_Chapter3-asciidoc3"/>Training a neural network basically means calibrating all of the weights in the ANN.
This optimization is performed using an iterative approach involving forward propagation and backpropagation steps.</p>










<section data-type="sect3" data-pdf-bookmark="Forward propagation"><div class="sect3" id="idm45174933717112">
<h3>Forward propagation</h3>

<p><a data-type="indexterm" data-primary="forward propagation" id="idm45174933715944"/><a data-type="indexterm" data-primary="predicted value" id="idm45174933715240"/><a data-type="indexterm" data-primary="training, for ANNs" data-secondary="forward propagation" id="idm45174933714568"/>Forward propagation is a process of feeding input values to the neural network and getting an output, which we call <em>predicted value</em>. When we feed the input values to the neural network’s first layer, it goes without any operations. The second layer takes values from the first layer and applies multiplication, addition, and activation operations before passing this value to the next layer. The same process repeats for any subsequent layers until an output value from the last layer is received.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Backpropagation"><div class="sect3" id="idm45174933712344">
<h3>Backpropagation</h3>

<p><a data-type="indexterm" data-primary="backpropagation" id="ix_Chapter3-asciidoc4"/><a data-type="indexterm" data-primary="training, for ANNs" data-secondary="backpropagation" id="ix_Chapter3-asciidoc5"/>After forward propagation, we get a predicted value from the ANN. Suppose the desired output of a network is <em>Y</em> and the predicted value of the network from forward propagation is <em>Y′</em>. The difference between the predicted output and the desired output (<em>Y</em>–<em>Y′</em> ) is converted into the loss (or cost) function <em>J(w)</em>, where <em>w</em> represents the weights in ANN.<sup><a data-type="noteref" id="idm45174933705544-marker" href="ch03.xhtml#idm45174933705544">3</a></sup> The goal is to optimize the loss function (i.e., make the loss as small as possible) over the training set.</p>

<p>The optimization method used is <em>gradient descent</em>. The goal of the gradient descent method is to find the gradient of  <em>J(w)</em>  with respect to <em>w</em> at the current point and <a data-type="indexterm" data-primary="gradient descent" id="idm45174933702744"/>take a small step in the direction of the negative gradient until the minimum value is reached, as shown in <a data-type="xref" href="#GradDesc">Figure 3-3</a>.</p>

<figure><div id="GradDesc" class="figure">
<img src="Images/mlbf_0303.png" alt="mlbf 0303" width="941" height="520"/>
<h6><span class="label">Figure 3-3. </span>Gradient descent</h6>
</div></figure>

<p>In an ANN, the function  <em>J(w)</em>  is essentially a composition of multiple layers, as explained in the preceding text. So, if layer one is represented as function  <em>p()</em>, layer two as <em>q()</em>, and layer three as <em>r()</em>, then the overall function is <em>J(w)=r(q(p())).</em> <em>w</em> consists of all weights in all three layers. We want to find the gradient of <em>J(w)</em> with respect to each component of <em>w</em>.</p>

<p>Skipping the mathematical details, the above essentially implies that the gradient of a component <em>w</em> in the first layer would depend on the gradients in the second and third layers. Similarly, the gradients in the second layer will depend on the gradients in the third layer. Therefore, we start computing the derivatives in the reverse direction, starting with the last layer, and use backpropagation to compute gradients of the previous layer.</p>

<p>Overall, in the process of backpropagation, the model error (difference between predicted and desired output)
is propagated back through the network, one layer at a time, and the weights are
updated according to the amount they contributed to the error.</p>

<p>Almost all ANNs use gradient descent and backpropagation. Backpropagation is one of the cleanest and most efficient ways to find the gradient<a data-type="indexterm" data-startref="ix_Chapter3-asciidoc5" id="idm45174933692328"/><a data-type="indexterm" data-startref="ix_Chapter3-asciidoc4" id="idm45174933691624"/>.<a data-type="indexterm" data-startref="ix_Chapter3-asciidoc3" id="idm45174933690824"/><a data-type="indexterm" data-startref="ix_Chapter3-asciidoc2" id="idm45174933690120"/></p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Hyperparameters"><div class="sect2" id="idm45174933689192">
<h2>Hyperparameters</h2>

<p><a data-type="indexterm" data-primary="artificial neural networks (ANNs)" data-secondary="hyperparameters" id="ix_Chapter3-asciidoc6"/><a data-type="indexterm" data-primary="hyperparameters" data-secondary="ANNs and" id="ix_Chapter3-asciidoc7"/><em>Hyperparameters</em> are the variables that are set before the training process, and they cannot be learned during training. ANNs have a large number of hyperparameters, which makes them quite flexible. However, this flexibility makes the model tuning process difficult. Understanding the hyperparameters and the intuition behind them helps give an idea of what values are reasonable for each hyperparameter so we can restrict the search space. Let’s start with the number of hidden layers and nodes.</p>










<section data-type="sect3" data-pdf-bookmark="Number of hidden layers and nodes"><div class="sect3" id="idm45174933684344">
<h3>Number of hidden layers and nodes</h3>

<p><a data-type="indexterm" data-primary="hidden layers" id="idm45174933682936"/><a data-type="indexterm" data-primary="hyperparameters" data-secondary="number of hidden layers and nodes" id="idm45174933682232"/>More hidden layers or nodes per layer means more parameters in the ANN, allowing the model to fit more complex functions. To have a trained network that generalizes well, we need to pick an optimal number of hidden layers, as well as of the nodes in each hidden layer. Too few nodes and layers will lead to high errors for the system, as the predictive factors might be too complex for a small number of nodes to capture. Too many nodes and layers will overfit to the training data and not generalize well.</p>

<p>There is no hard-and-fast rule to decide the number of layers and nodes.</p>

<p>The number of hidden layers primarily depends on the complexity of the task. Very complex tasks, such as large image classification or speech recognition, typically require networks with dozens of layers and a huge amount of training data. For the majority of the problems, we can start with just one or two hidden layers and then gradually ramp up the number of hidden layers until we start overfitting the training set.</p>

<p>The number of hidden nodes should have a relationship to the number of input and output nodes, the amount of training data available, and the complexity of the function being modeled. As a rule of thumb, the number of hidden nodes in each layer should be somewhere between the size of the input layer and the size of the output layer, ideally the mean. The number of hidden nodes shouldn’t exceed twice the number of input nodes in order to avoid overfitting.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Learning rate"><div class="sect3" id="idm45174933678312">
<h3>Learning rate</h3>

<p><a data-type="indexterm" data-primary="hyperparameters" data-secondary="learning rate" id="idm45174933677224"/><a data-type="indexterm" data-primary="learning rate" id="idm45174933676248"/>When we train ANNs, we use many iterations of forward propagation and backpropagation to optimize the weights. At each iteration we calculate the derivative of the loss function with respect to each weight and subtract it from that weight. The learning rate determines how quickly or slowly we want to update our weight (parameter) values. This learning rate should be high enough so that it converges in a reasonable amount of time. Yet it should be low enough so that it finds the minimum value of the loss function.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Activation functions"><div class="sect3" id="idm45174933674664">
<h3>Activation functions</h3>

<p><a data-type="indexterm" data-primary="hyperparameters" data-secondary="activation functions" id="idm45174933673256"/>Activation functions (as shown in <a data-type="xref" href="#SingleNeuron">Figure 3-1</a>) refer to the functions used over the weighted sum of inputs in ANNs to get the desired output. Activation functions allow the network to combine the inputs in more complex ways, and they provide a richer capability in the relationship they can model and the output they can produce. They decide which neurons will be activated—that is, what information is passed to further layers.</p>

<p>Without activation functions, ANNs lose a bulk of their representation learning power. There are several activation functions. The most widely used are as follows:</p>
<dl>
<dt>Linear (identity) function</dt>
<dd>
<p><a data-type="indexterm" data-primary="linear (identity) function" id="idm45174933668824"/>Represented by the equation of a straight line (i.e., <math alttext="f left-parenthesis x right-parenthesis equals m x plus c">
  <mrow>
    <mi>f</mi>
    <mo>(</mo>
    <mi>x</mi>
    <mo>)</mo>
    <mo>=</mo>
    <mi>m</mi>
    <mi>x</mi>
    <mo>+</mo>
    <mi>c</mi>
  </mrow>
</math>), where activation is proportional to the input. If we have many layers, and all the layers are linear in nature, then the final activation function of the last layer is the same as the linear function of the first layer. The range of a linear function is <em>–inf</em> to <em>+inf</em>.</p>
</dd>
<dt>Sigmoid function</dt>
<dd>
<p><a data-type="indexterm" data-primary="sigmoid function" id="idm45174933660296"/>Refers to a function that is projected as an S-shaped graph (as shown in <a data-type="xref" href="#ActFunc">Figure 3-4</a>). It is represented by the mathematical equation <math display="inline">
  <mrow>
    <mi>f</mi>
    <mrow>
      <mo>(</mo>
      <mi>x</mi>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <mn>1</mn>
    <mo>/</mo>
    <mrow>
      <mo>(</mo>
      <mn>1</mn>
      <mo>+</mo>
      <msup><mi>e</mi> <mrow><mo>–</mo><mi>x</mi></mrow> </msup>
      <mo>)</mo>
    </mrow>
  </mrow>
</math>
and ranges from 0 to 1. A large positive input results in a large positive output; a large negative input results in a large negative output. It is also referred to as logistic activation function.</p>
</dd>
<dt>Tanh function</dt>
<dd>
<p><a data-type="indexterm" data-primary="tanh function" id="idm45174933648376"/>Similar to sigmoid activation function with a mathematical equation <math>
  <mrow>
    <mi>T</mi>
    <mi>a</mi>
    <mi>n</mi>
    <mi>h</mi>
    <mo>(</mo>
    <mi>x</mi>
    <mo>)</mo>
    <mo>=</mo>
    <mn>2</mn>
    <mi>S</mi>
    <mi>i</mi>
    <mi>g</mi>
    <mi>m</mi>
    <mi>o</mi>
    <mi>i</mi>
    <mi>d</mi>
    <mo>(</mo>
    <mn>2</mn>
    <mi>x</mi>
    <mo>)</mo>
    <mo>–</mo>
    <mn>1</mn>
  </mrow>
</math>, where <em>Sigmoid</em> represents the <code>sigmoid</code> function discussed above. The output of this function ranges from –1 to 1, with an equal mass on both sides of the zero-axis, as shown in <a data-type="xref" href="#ActFunc">Figure 3-4</a>.</p>
</dd>
<dt>ReLU function</dt>
<dd>
<p><a data-type="indexterm" data-primary="rectified linear unit (ReLU) function" id="idm45174933633592"/><a data-type="indexterm" data-primary="ReLU (rectified linear unit) function" id="idm45174933632824"/>ReLU stands for the Rectified Linear Unit and is represented as <math alttext="f left-parenthesis x right-parenthesis equals m a x left-parenthesis x comma 0 right-parenthesis">
  <mrow>
    <mi>f</mi>
    <mo>(</mo>
    <mi>x</mi>
    <mo>)</mo>
    <mo>=</mo>
    <mi>m</mi>
    <mi>a</mi>
    <mi>x</mi>
    <mo>(</mo>
    <mi>x</mi>
    <mo>,</mo>
    <mn>0</mn>
    <mo>)</mo>
  </mrow>
</math>. So, if the input is a positive number, the function returns the number itself, and if the input is a negative number, then the function returns zero. It is the most commonly used function because of its simplicity.</p>
</dd>
</dl>

<p class="pagebreak-before"><a data-type="xref" href="#ActFunc">Figure 3-4</a> shows a summary of the activation functions discussed in this section.</p>

<figure><div id="ActFunc" class="figure">
<img src="Images/mlbf_0304.png" alt="mlbf 0304" width="1438" height="993"/>
<h6><span class="label">Figure 3-4. </span>Activation functions</h6>
</div></figure>

<p>There is no hard-and-fast rule for activation function selection. The decision completely relies on the properties of the problem and the relationships being modeled. We can try different activation functions and select the one that helps provide faster convergence and a more efficient training process. The choice of activation function in the output layer is strongly constrained by the type of problem that is modeled.<sup><a data-type="noteref" id="idm45174933620152-marker" href="ch03.xhtml#idm45174933620152">4</a></sup></p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Cost functions"><div class="sect3" id="idm45174933618488">
<h3>Cost functions</h3>

<p><a data-type="indexterm" data-primary="cost functions" id="idm45174933617320"/><a data-type="indexterm" data-primary="hyperparameters" data-secondary="cost functions" id="idm45174933616616"/><a data-type="indexterm" data-primary="loss (cost) functions" id="idm45174933615672"/>Cost functions (also known as loss functions) are a measure of the ANN performance, measuring how well the ANN fits empirical data. The two most common cost functions are:</p>
<dl>
<dt>Mean squared error (MSE)</dt>
<dd>
<p><a data-type="indexterm" data-primary="mean squared error (MSE)" id="idm45174933613368"/><a data-type="indexterm" data-primary="MSE (mean squared error)" id="idm45174933612696"/>This is the cost function used primarily for regression problems, where output is a continuous value. MSE is measured as the average of the squared difference between predictions and actual observation. MSE is described further in 
<span class="keep-together"><a data-type="xref" href="ch04.xhtml#Chapter4">Chapter 4</a></span>.</p>
</dd>
<dt>Cross-entropy (or <em>log loss</em>)</dt>
<dd>
<p><a data-type="indexterm" data-primary="cross-entropy (log loss)" id="idm45174933608792"/><a data-type="indexterm" data-primary="log loss (cross-entropy)" id="idm45174933608072"/>This cost function is used primarily for classification problems, where output is a probability value between zero and one. Cross-entropy loss increases as the predicted probability diverges from the actual label. A perfect model would have a cross-entropy of zero.</p>
</dd>
</dl>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Optimizers"><div class="sect3" id="idm45174933606456">
<h3>Optimizers</h3>

<p>Optimizers <a data-type="indexterm" data-primary="hyperparameters" data-secondary="optimizers" id="idm45174933605160"/><a data-type="indexterm" data-primary="optimizers" id="idm45174933604152"/>update the weight parameters to minimize the loss function.<sup><a data-type="noteref" id="idm45174933603352-marker" href="ch03.xhtml#idm45174933603352">5</a></sup> Cost function acts as a guide to the terrain, telling the optimizer if it is moving in the right direction to reach the global minimum. Some of the common optimizers are as 
<span class="keep-together">follows</span>:</p>
<dl>
<dt>Momentum</dt>
<dd>
<p><a data-type="indexterm" data-primary="momentum optimizers" id="idm45174933599016"/>The <em>momentum optimizer</em> looks at previous gradients in addition to the current step. It will take larger steps if the previous updates and the current update move the weights in the same direction (gaining momentum). It will take smaller steps if the direction of the gradient is opposite. A clever way to visualize this is to think of a ball rolling down a valley—it will gain momentum as it approaches the valley bottom.</p>
</dd>
<dt>AdaGrad (Adaptive Gradient Algorithm)</dt>
<dd>
<p><em><a data-type="indexterm" data-primary="AdaGrad (adaptive gradient algorithm)" id="idm45174933596040"/><a data-type="indexterm" data-primary="adaptive gradient algorithm (AdaGrad)" id="idm45174933595272"/>AdaGrad</em> adapts the learning rate to the parameters, performing smaller updates for parameters associated with frequently occurring features, and larger updates for parameters associated with infrequent features.</p>
</dd>
<dt>RMSProp</dt>
<dd>
<p><a data-type="indexterm" data-primary="RMSProp (root mean square propagation)" id="idm45174933592952"/><a data-type="indexterm" data-primary="root mean square propagation (RMSProp)" id="idm45174933592232"/><em>RMSProp</em> stands for Root Mean Square Propagation. In RMSProp, the learning rate gets adjusted automatically, and it chooses a different learning rate for each parameter.</p>
</dd>
<dt>Adam (Adaptive Moment Estimation)</dt>
<dd>
<p><a data-type="indexterm" data-primary="Adam (adaptive moment estimation)" id="idm45174933589832"/><em>Adam</em> combines the best properties of the AdaGrad and RMSProp algorithms to provide an optimization and is one of the most popular gradient descent optimization algorithms.</p>
</dd>
</dl>
</div></section>













<section data-type="sect3" class="pagebreak-before less_space" data-pdf-bookmark="Epoch"><div class="sect3" id="idm45174933588120">
<h3>Epoch</h3>

<p><a data-type="indexterm" data-primary="epoch" id="idm45174933586680"/><a data-type="indexterm" data-primary="hyperparameters" data-secondary="epoch" id="idm45174933585976"/>One round of updating the network for the entire training dataset is called an <em>epoch</em>. A network may be trained for tens, hundreds, or many thousands of epochs depending on the data size and computational constraints.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Batch size"><div class="sect3" id="idm45174933584120">
<h3>Batch size</h3>

<p><a data-type="indexterm" data-primary="batch size" id="idm45174933582920"/><a data-type="indexterm" data-primary="hyperparameters" data-secondary="batch size" id="idm45174933582216"/>The batch size is the number of training examples in one forward/backward pass. A batch size of 32 means that 32 samples from the training dataset will be used to estimate the error gradient before the model weights are updated. The higher the batch size, the more memory space is needed.<a data-type="indexterm" data-startref="ix_Chapter3-asciidoc7" id="idm45174933580840"/><a data-type="indexterm" data-startref="ix_Chapter3-asciidoc6" id="idm45174933580168"/></p>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Creating an Artificial Neural Network Model in Python"><div class="sect1" id="idm45174933579240">
<h1>Creating an Artificial Neural Network Model in Python</h1>

<p><a data-type="indexterm" data-primary="artificial neural networks (ANNs)" data-secondary="creating in Python" id="ix_Chapter3-asciidoc8"/><a data-type="indexterm" data-primary="Python (generally)" data-secondary="creating an artificial neural network model in" id="ix_Chapter3-asciidoc9"/><a data-type="indexterm" data-primary="Python, creating an ANN model in" id="ix_Chapter3-asciidoc10"/>In <a data-type="xref" href="ch02.xhtml#Chapter2">Chapter 2</a> we discussed the steps for end-to-end model development in Python. In this section, we dig deeper into the steps involved in building an ANN-based model in Python.</p>

<p>Our first step will be to look at Keras, the Python package specifically built for ANN and deep learning.</p>








<section data-type="sect2" data-pdf-bookmark="Installing Keras and Machine Learning Packages"><div class="sect2" id="idm45174933572648">
<h2>Installing Keras and Machine Learning Packages</h2>

<p><a data-type="indexterm" data-primary="Keras" id="idm45174933571080"/><a data-type="indexterm" data-primary="Python, creating an ANN model in" data-secondary="installing Keras and machine learning packages" id="idm45174933570376"/>There are several Python libraries that allow building ANN and deep learning models easily and quickly without getting into the details of underlying algorithms. Keras is one of the most user-friendly packages that enables an efficient numerical computation related to ANNs. Using Keras, complex deep learning models can be defined and implemented in a few lines of code. We will primarily be using Keras packages for implementing deep learning models in several of the book’s case studies.</p>

<p><a href="https://keras.io">Keras</a> is simply a wrapper around more complex numerical computation engines such as <a href="https://www.tensorflow.org">TensorFlow</a> and <a href="https://oreil.ly/-XFJP">Theano</a>. In order to install Keras, TensorFlow or Theano needs to be installed first.</p>

<p>This section describes the steps to define and compile an ANN-based model in Keras, with a focus on the following steps.<sup><a data-type="noteref" id="idm45174933565816-marker" href="ch03.xhtml#idm45174933565816">6</a></sup></p>










<section data-type="sect3" data-pdf-bookmark="Importing the packages"><div class="sect3" id="idm45174933564936">
<h3>Importing the packages</h3>

<p>Before you can start to build an ANN model, you need to import two modules from the Keras package: <code>Sequential</code> and <code>Dense</code>:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">from</code> <code class="nn">Keras.models</code> <code class="k">import</code> <code class="n">Sequential</code>
<code class="kn">from</code> <code class="nn">Keras.layers</code> <code class="k">import</code> <code class="n">Dense</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code></pre>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Loading data"><div class="sect3" id="idm45174933545000">
<h3>Loading data</h3>

<p>This example makes use of the <code>random</code> module of NumPy to quickly generate some data and labels to be used by ANN that we build in the next step. Specifically, an array with size <em>(1000,10)</em> is first constructed. Next, we create a labels array that consists of zeros and ones with a size <em>(1000,1)</em>:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">data</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">random</code><code class="p">((</code><code class="mi">1000</code><code class="p">,</code><code class="mi">10</code><code class="p">))</code>
<code class="n">Y</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">randint</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code><code class="n">size</code><code class="o">=</code> <code class="p">(</code><code class="mi">1000</code><code class="p">,</code><code class="mi">1</code><code class="p">))</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">Sequential</code><code class="p">()</code></pre>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Model construction—defining the neural network architecture"><div class="sect3" id="idm45174933543384">
<h3>Model construction—defining the neural network architecture</h3>

<p><a data-type="indexterm" data-primary="Python, creating an ANN model in" data-secondary="model construction" id="idm45174933480712"/><a data-type="indexterm" data-primary="Sequential model (Keras)" id="idm45174933479800"/>A quick way to get started is to use the Keras Sequential model, which is a linear stack of layers. We create a Sequential model and add layers one at a time until the network topology is finalized. The first thing to get right is to ensure the input layer has the right number of inputs. We can specify this when creating the first layer. We then select a dense or fully connected layer to indicate that we are dealing with an input layer by using the argument <code>input_dim</code>.</p>

<p>We add a layer to the model with the <code>add()</code> function, and the number of nodes in each layer is specified. Finally, another dense layer is added as an output layer.</p>

<p>The architecture for the model shown in <a data-type="xref" href="#ANNArchitecture">Figure 3-5</a> is as follows:</p>

<ul>
<li>
<p>The model expects rows of data with 10 variables (<code>input_dim_=10</code> argument).</p>
</li>
<li>
<p>The first hidden layer has 32 nodes and uses the <code>relu</code> activation function.</p>
</li>
<li>
<p>The second hidden layer has 32 nodes and uses the <code>relu</code> activation function.</p>
</li>
<li>
<p>The output layer has one node and uses the <code>sigmoid</code> activation function.</p>
</li>
</ul>

<figure><div id="ANNArchitecture" class="figure">
<img src="Images/mlbf_0305.png" alt="mlbf 0305" width="945" height="777"/>
<h6><span class="label">Figure 3-5. </span>An ANN architecture</h6>
</div></figure>

<p>The Python code for the network in <a data-type="xref" href="#ANNArchitecture">Figure 3-5</a> is shown below:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">model</code> <code class="o">=</code> <code class="n">Sequential</code><code class="p">()</code>
<code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dense</code><code class="p">(</code><code class="mi">32</code><code class="p">,</code> <code class="n">input_dim</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code> <code class="s">'relu'</code> <code class="p">))</code>
<code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dense</code><code class="p">(</code><code class="mi">32</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code> <code class="s">'relu'</code> <code class="p">))</code>
<code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dense</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code> <code class="s">'sigmoid'</code><code class="p">))</code></pre>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Compiling the model"><div class="sect3" id="idm45174933465240">
<h3>Compiling the model</h3>

<p><a data-type="indexterm" data-primary="Python, creating an ANN model in" data-secondary="compiling the model" id="idm45174933403144"/>With the model constructed, it can be compiled with the help of the <code>compile()</code>
function. Compiling the model leverages the efficient numerical libraries in the Theano or TensorFlow packages. When compiling, it is important to specify the additional
properties required when training the network. Training a network means finding the
best set of weights to make predictions for the problem at hand. So we must specify the loss function used to evaluate a set of weights, the optimizer used
to search through different weights for the network, and any optional metrics we would like to collect and report during training.</p>

<p>In the following example, we use <code>cross-entropy</code> loss function, which is defined in Keras as <code>binary_crossentropy</code>. We will also use the adam optimizer, which is the default option. Finally, because it is a classification problem, we will collect and report the classification accuracy as the metric.<sup><a data-type="noteref" id="idm45174933399704-marker" href="ch03.xhtml#idm45174933399704">7</a></sup> The Python code follows:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">model</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">loss</code><code class="o">=</code> <code class="s">'binary_crossentropy'</code> <code class="p">,</code> <code class="n">optimizer</code><code class="o">=</code> <code class="s">'adam'</code> <code class="p">,</code> \
  <code class="n">metrics</code><code class="o">=</code><code class="p">[</code> <code class="s">'accuracy'</code> <code class="p">])</code></pre>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Fitting the model"><div class="sect3" id="idm45174933328904">
<h3>Fitting the model</h3>

<p><a data-type="indexterm" data-primary="Python, creating an ANN model in" data-secondary="fitting the model" id="idm45174933327096"/>With our model defined and compiled, it is time to execute it on data. We can train or fit our model on our loaded data by calling the <code>fit()</code> function on the model.</p>

<p>The training process will run for a fixed number of iterations (epochs) through the dataset, specified using the <code>nb_epoch</code> argument. We can also set the number of
instances that are evaluated before a weight update in the network is performed. This is set using the <code>batch_size</code> argument. For this problem we will run a small
number of epochs (10) and use a relatively small batch size of 32. Again, these can be chosen experimentally through trial and error. The Python code follows:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">data</code><code class="p">,</code> <code class="n">Y</code><code class="p">,</code> <code class="n">nb_epoch</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">32</code><code class="p">)</code></pre>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Evaluating the model"><div class="sect3" id="idm45174933377112">
<h3>Evaluating the model</h3>

<p><a data-type="indexterm" data-primary="Python, creating an ANN model in" data-secondary="evaluating the model" id="idm45174933376104"/>We have trained our neural network on the entire dataset and can evaluate the performance of the network on the same dataset. This will give us an idea of how well we have modeled the dataset (e.g., training accuracy) but will not provide insight on how well the algorithm will perform on new data. For this, we separate the data into training and test datasets. The model is evaluated on the training dataset using the <code>evaluation()</code> function. This will generate a prediction for each input and output pair and collect scores, including the average loss and any metrics configured, such as accuracy. The Python code follows:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">scores</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">evaluate</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">Y_test</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="s">"%s: %.2f%%"</code> <code class="o">%</code> <code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">metrics_names</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code> <code class="n">scores</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code><code class="o">*</code><code class="mi">100</code><code class="p">))</code></pre>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Running an ANN Model Faster: GPU and Cloud Services"><div class="sect2" id="idm45174933572152">
<h2>Running an ANN Model Faster: GPU and Cloud Services</h2>

<p><a data-type="indexterm" data-primary="artificial neural networks (ANNs)" data-secondary="alternatives to running on CPU" id="idm45174933254984"/>For training ANNs (especially deep neural networks with many layers), a large amount of computation power is required. Available CPUs, or Central Processing Units, are responsible for processing and executing instructions on a local machine. Since CPUs are limited in the number of cores and take up the job sequentially, they cannot do rapid matrix computations for the large number of matrices required for training deep learning models. Hence, the training of deep learning models can be extremely slow on the CPUs.</p>

<p>The following alternatives are useful for running ANNs that generally require a significant amount of time to run on a CPU:</p>

<ul>
<li>
<p>Running notebooks locally on a GPU.</p>
</li>
<li>
<p>Running notebooks on Kaggle Kernels or Google Colaboratory.</p>
</li>
<li>
<p>Using Amazon Web Services.</p>
</li>
</ul>










<section data-type="sect3" data-pdf-bookmark="GPU"><div class="sect3" id="idm45174933249752">
<h3>GPU</h3>

<p><a data-type="indexterm" data-primary="artificial neural networks (ANNs)" data-secondary="running on GPU" id="idm45174933248552"/>A GPU is composed of hundreds of cores that can handle thousands of threads simultaneously. Running ANNs and deep learning models can be accelerated by the use of GPUs.</p>

<p>GPUs are particularly adept at processing complex matrix operations. The GPU cores are highly specialized, and they massively accelerate processes such as deep learning training by offloading the processing from CPUs to the cores in the GPU subsystem.</p>

<p>All the Python packages related to machine learning, including Tensorflow, Theano, and Keras, can be configured for the use of GPUs.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Cloud services such as Kaggle and Google Colab"><div class="sect3" id="idm45174933245816">
<h3>Cloud services such as Kaggle and Google Colab</h3>

<p><a data-type="indexterm" data-primary="artificial neural networks (ANNs)" data-secondary="running on cloud services" id="idm45174933244584"/>If you have a GPU-enabled computer, you can run ANNs locally. If you do not, we recommend you use a service such as Kaggle Kernels, Google Colab, or AWS:</p>
<dl>
<dt>Kaggle</dt>
<dd>
<p><a data-type="indexterm" data-primary="Kaggle, running ANNs on" id="idm45174933241704"/>A popular data science website owned by Google that hosts Jupyter service and is also referred to as <a href="https://www.kaggle.com">Kaggle Kernels</a>. Kaggle Kernels are free to use and come with the most frequently used packages preinstalled. You can connect a kernel to any dataset hosted on Kaggle, or alternatively, you can just upload a new dataset on the fly.</p>
</dd>
<dt>Google Colaboratory</dt>
<dd>
<p><a data-type="indexterm" data-primary="Google Colaboratory" id="idm45174933238680"/>A free Jupyter Notebook environment provided by Google where you can use free GPUs. The features of <a href="https://oreil.ly/keqHk">Google Colaboratory</a> are the same as Kaggle.</p>
</dd>
<dt>Amazon Web Services (AWS)</dt>
<dd>
<p><a data-type="indexterm" data-primary="Amazon Web Services (AWS)" id="idm45174933235928"/><a href="https://oreil.ly/gU84O">AWS Deep Learning</a> provides an infrastructure to accelerate deep learning in the cloud, at any scale. You can quickly launch AWS server instances preinstalled with popular deep learning frameworks and interfaces to train sophisticated, custom AI models, experiment with new algorithms, or learn new skills and techniques. These web servers can run longer than Kaggle Kernels. So for big projects, it might be worth using an AWS instead of a kernel.<a data-type="indexterm" data-startref="ix_Chapter3-asciidoc10" id="idm45174933234056"/><a data-type="indexterm" data-startref="ix_Chapter3-asciidoc9" id="idm45174933233352"/><a data-type="indexterm" data-startref="ix_Chapter3-asciidoc8" id="idm45174933232680"/></p>
</dd>
</dl>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" class="pagebreak-before less_space" data-pdf-bookmark="Chapter Summary"><div class="sect1" id="idm45174933231624">
<h1>Chapter Summary</h1>

<p>ANNs comprise a family of algorithms used across all types of machine learning. These models are inspired by the biological neural networks containing neurons and layers of neurons that constitute animal brains. ANNs with many layers are referred to as deep neural networks. Several steps, including forward propagation and backpropagation, are required for training these ANNs. Python packages such as Keras make the training of these ANNs possible in a few lines of code. The training of these deep neural networks require more computational power, and CPUs alone might not be enough. Alternatives include using a GPU or cloud service such as Kaggle Kernels, Google Colaboratory, or Amazon Web Services for training deep neural networks.<a data-type="indexterm" data-startref="ix_Chapter3-asciidoc0" id="idm45174933229336"/></p>








<section data-type="sect2" class="notoc" data-pdf-bookmark="Next Steps"><div class="sect2" id="idm45174933228504">
<h2>Next Steps</h2>

<p>As a next step, we will be going into the details of the machine learning concepts for supervised learning, followed by case studies using the concepts covered in this 
<span class="keep-together">chapter</span>.</p>
</div></section>





</div></section>







<div data-type="footnotes"><p data-type="footnote" id="idm45174933851448"><sup><a href="ch03.xhtml#idm45174933851448-marker">1</a></sup> Readers are encouraged to refer to the book <em>Deep Learning</em>  by Aaron Courville, Ian Goodfellow, and Yoshua Bengio (MIT Press) for more details on ANN and deep learning.</p><p data-type="footnote" id="idm45174933836904"><sup><a href="ch03.xhtml#idm45174933836904-marker">2</a></sup> Activation functions are described in detail later in this chapter.</p><p data-type="footnote" id="idm45174933705544"><sup><a href="ch03.xhtml#idm45174933705544-marker">3</a></sup> There are many available loss functions discussed in the next section. The nature of our problem dictates our choice of loss function.</p><p data-type="footnote" id="idm45174933620152"><sup><a href="ch03.xhtml#idm45174933620152-marker">4</a></sup> Deriving a regression or classification output by changing the activation function of the output layer is described further in <a data-type="xref" href="ch04.xhtml#Chapter4">Chapter 4</a>.</p><p data-type="footnote" id="idm45174933603352"><sup><a href="ch03.xhtml#idm45174933603352-marker">5</a></sup> Refer to <a href="https://oreil.ly/FSt-8"><em class="hyperlink">https://oreil.ly/FSt-8</em></a> for more details on optimization.</p><p data-type="footnote" id="idm45174933565816"><sup><a href="ch03.xhtml#idm45174933565816-marker">6</a></sup> The steps and Python code related to implementing deep learning models using Keras, as demonstrated in this section, are used in several case studies in the subsequent chapters.</p><p data-type="footnote" id="idm45174933399704"><sup><a href="ch03.xhtml#idm45174933399704-marker">7</a></sup> A detailed discussion of the evaluation metrics for classification models is presented in <a data-type="xref" href="ch04.xhtml#Chapter4">Chapter 4</a>.</p></div></div></section></div>



  </body></html>