["```py\nIn [1]: import pandas as pd\n\nIn [2]: credit = pd.read_csv('credit_data_risk.csv')\n\nIn [3]: credit.head()\nOut[3]: Unnamed: 0  Age     Sex  Job Housing Saving accounts Checking account  \\\n        0           0   67    male    2     own             NaN           little\n\n        1           1   22  female    2     own          little         moderate\n\n        2           2   49    male    1     own          little              NaN\n\n        3           3   45    male    2    free          little           little\n\n        4           4   53    male    2    free          little           little\n\n           Credit amount  Duration              Purpose  Risk\n\n        0           1169         6             radio/TV  good\n\n        1           5951        48             radio/TV   bad\n\n        2           2096        12            education  good\n\n        3           7882        42  furniture/equipment  good\n\n        4           4870        24                  car   bad\n\nIn [4]: del credit['Unnamed: 0'] ![1](assets/1.png)\n```", "```py\nIn [5]: credit.describe()\nOut[5]:                Age          Job  Credit amount     Duration\n        count  1000.000000  1000.000000    1000.000000  1000.000000\n        mean     35.546000     1.904000    3271.258000    20.903000\n        std      11.375469     0.653614    2822.736876    12.058814\n        min      19.000000     0.000000     250.000000     4.000000\n        25%      27.000000     2.000000    1365.500000    12.000000\n        50%      33.000000     2.000000    2319.500000    18.000000\n        75%      42.000000     2.000000    3972.250000    24.000000\n        max      75.000000     3.000000   18424.000000    72.000000\n```", "```py\nIn [6]: import matplotlib.pyplot as plt\n        import seaborn as sns; sns.set()\n        plt.rcParams[\"figure.figsize\"] = (10,6) ![1](assets/1.png)\n\nIn [7]: numerical_credit = credit.select_dtypes(exclude='O') ![2](assets/2.png)\n\nIn [8]: plt.figure(figsize=(10, 8))\n        k = 0\n        cols = numerical_credit.columns\n        for i, j in zip(range(len(cols)), cols):\n            k +=1\n            plt.subplot(2, 2, k)\n            plt.hist(numerical_credit.iloc[:, i])\n            plt.title(j)\n```", "```py\nIn [9]: from sklearn.preprocessing import StandardScaler\n        from sklearn.cluster import KMeans\n        import numpy as np\n\nIn [10]: scaler = StandardScaler()\n         scaled_credit = scaler.fit_transform(numerical_credit) ![1](assets/1.png)\n\nIn [11]: distance = []\n         for k in range(1, 10):\n             kmeans = KMeans(n_clusters=k) ![2](assets/2.png)\n             kmeans.fit(scaled_credit)\n             distance.append(kmeans.inertia_) ![3](assets/3.png)\n\nIn [12]: plt.plot(range(1, 10), distance, 'bx-')\n         plt.xlabel('k')\n         plt.ylabel('Inertia')\n         plt.title('The Elbow Method')\n         plt.show()\n```", "```py\nIn [13]: from sklearn.metrics import silhouette_score ![1](assets/1.png)\n         from yellowbrick.cluster import SilhouetteVisualizer ![2](assets/2.png)\n\nIn [14]: fig, ax = plt.subplots(4, 2, figsize=(25, 20))\n         for i in range(2, 10):\n             km = KMeans(n_clusters=i)\n             q, r = divmod(i, 2) ![3](assets/3.png)\n             visualizer = SilhouetteVisualizer(km, colors='yellowbrick',\n                                               ax=ax[q - 1][r]) ![4](assets/4.png)\n             visualizer.fit(scaled_credit)\n             ax[q - 1][r].set_title(\"For Cluster_\"+str(i))\n             ax[q - 1][r].set_xlabel(\"Silhouette Score\")\n```", "```py\nIn [15]: from yellowbrick.cluster import KElbowVisualizer ![1](assets/1.png)\n         model = KMeans()\n         visualizer = KElbowVisualizer(model, k=(2, 10),\n                                       metric='calinski_harabasz',\n                                       timings=False) ![2](assets/2.png)\n         visualizer.fit(scaled_credit)\n         visualizer.show()\nOut[]: <Figure size 576x396 with 0 Axes>\n```", "```py\nIn [16]: from gap_statistic.optimalK import OptimalK ![1](assets/1.png)\n\nIn [17]: optimalK = OptimalK(n_jobs=8, parallel_backend='joblib') ![2](assets/2.png)\n         n_clusters = optimalK(scaled_credit, cluster_array=np.arange(1, 10)) ![3](assets/3.png)\n\nIn [18]: gap_result = optimalK.gap_df ![4](assets/4.png)\n         gap_result.head()\nOut[18]:    n_clusters  gap_value         gap*  ref_dispersion_std        sk  \\\n         0         1.0   0.889755  5738.286952           54.033596  0.006408\n         1         2.0   0.968585  4599.736451          366.047394  0.056195\n         2         3.0   1.003974  3851.032471           65.026259  0.012381\n         3         4.0   1.044347  3555.819296          147.396138  0.031187\n         4         5.0   1.116450  3305.617917           27.894622  0.006559\n\n                    sk*      diff        diff*\n         0  6626.296782 -0.022635  6466.660374\n         1  5328.109873 -0.023008  5196.127130\n         2  4447.423150 -0.009186  4404.645656\n         3  4109.432481 -0.065543  4067.336067\n         4  3817.134689  0.141622  3729.880829\n\nIn [19]: plt.plot(gap_result.n_clusters, gap_result.gap_value)\n         min_ylim, max_ylim = plt.ylim()\n         plt.axhline(np.max(gap_result.gap_value), color='r',\n                     linestyle='dashed', linewidth=2)\n         plt.title('Gap Analysis')\n         plt.xlabel('Number of Cluster')\n         plt.ylabel('Gap Value')\n         plt.show()\n```", "```py\nIn [20]: kmeans = KMeans(n_clusters=2)\n         clusters = kmeans.fit_predict(scaled_credit)\n\nIn [21]: plt.figure(figsize=(10, 12))\n         plt.subplot(311)\n         plt.scatter(scaled_credit[:, 0], scaled_credit[:, 2],\n                     c=kmeans.labels_, cmap='viridis')\n         plt.scatter(kmeans.cluster_centers_[:, 0],\n                     kmeans.cluster_centers_[:, 2], s = 80,\n                     marker= 'x', color = 'k')\n         plt.title('Age vs Credit')\n         plt.subplot(312)\n         plt.scatter(scaled_credit[:, 0], scaled_credit[:, 2],\n                     c=kmeans.labels_, cmap='viridis')\n         plt.scatter(kmeans.cluster_centers_[:, 0],\n                     kmeans.cluster_centers_[:, 2], s = 80,\n                     marker= 'x', color = 'k')\n         plt.title('Credit vs Duration')\n         plt.subplot(313)\n         plt.scatter(scaled_credit[:, 2], scaled_credit[:, 3],\n                     c=kmeans.labels_, cmap='viridis')\n         plt.scatter(kmeans.cluster_centers_[:, 2],\n                     kmeans.cluster_centers_[:, 3], s = 120,\n                     marker= 'x', color = 'k')\n         plt.title('Age vs Duration')\n         plt.show()\n```", "```py\nIn [22]: clusters, counts = np.unique(kmeans.labels_, return_counts=True) ![1](assets/1.png)\n\nIn [23]: cluster_dict = {}\n         for i in range(len(clusters)):\n             cluster_dict[i] = scaled_credit[np.where(kmeans.labels_==i)] ![2](assets/2.png)\n\nIn [24]: credit['clusters'] = pd.DataFrame(kmeans.labels_) ![3](assets/3.png)\n\nIn [25]: df_scaled = pd.DataFrame(scaled_credit)\n         df_scaled['clusters'] = credit['clusters']\n\nIn [26]: df_scaled['Risk'] = credit['Risk']\n         df_scaled.columns = ['Age', 'Job', 'Credit amount',\n                              'Duration', 'Clusters', 'Risk']\n\nIn [27]: df_scaled[df_scaled.Clusters == 0]['Risk'].value_counts() ![4](assets/4.png)\nOut[27]: good    571\n         bad     193\n         Name: Risk, dtype: int64\n\nIn [28]: df_scaled[df_scaled.Clusters == 1]['Risk'].value_counts() ![5](assets/5.png)\nOut[28]: good    129\n         bad     107\n         Name: Risk, dtype: int64\n```", "```py\nIn [29]: df_scaled[df_scaled.Clusters == 0]['Risk'].value_counts()\\\n                                             .plot(kind='bar',\n                                             figsize=(10, 6),\n                                             title=\"Frequency of Risk Level\");\nIn [30]: df_scaled[df_scaled.Clusters == 1]['Risk'].value_counts()\\\n                                             .plot(kind='bar',\n                                             figsize=(10, 6),\n                                             title=\"Frequency of Risk Level\");\n```", "```py\nIn [31]: from sklearn.model_selection import train_test_split\n\nIn [32]: df_scaled['Risk'] = df_scaled['Risk'].replace({'good': 1, 'bad': 0}) ![1](assets/1.png)\n\nIn [33]: X = df_scaled.drop('Risk', axis=1)\n         y = df_scaled.loc[:, ['Risk', 'Clusters']]\n\nIn [34]: X_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                             test_size=0.2,\n                                                             random_state=42)\n\nIn [35]: first_cluster_train = X_train[X_train.Clusters == 0].iloc[:, :-1] ![2](assets/2.png)\n         second_cluster_train = X_train[X_train.Clusters == 1].iloc[:, :-1] ![3](assets/3.png)\n```", "```py\nIn [36]: import statsmodels.api as sm\n         from sklearn.linear_model import LogisticRegression\n         from sklearn.metrics import roc_auc_score, roc_curve\n         from imblearn.combine import SMOTEENN ![1](assets/1.png)\n         import warnings\n         warnings.filterwarnings('ignore')\n\nIn [37]: X_train1 = first_cluster_train\n         y_train1 = y_train[y_train.Clusters == 0]['Risk'] ![2](assets/2.png)\n         smote = SMOTEENN(random_state = 2) ![3](assets/3.png)\n         X_train1, y_train1 = smote.fit_resample(X_train1, y_train1.ravel()) ![4](assets/4.png)\n         logit = sm.Logit(y_train1, X_train1) ![5](assets/5.png)\n         logit_fit1 = logit.fit() ![6](assets/6.png)\n         print(logit_fit1.summary())\n         Optimization terminated successfully.\n         Current function value: 0.479511\n         Iterations 6\n                           Logit Regression Results\n==============================================================================\nDep. Variable:                      y   No. Observations:                  370\nModel:                          Logit   Df Residuals:                      366\nMethod:                           MLE   Df Model:                            3\nDate:                Wed, 01 Dec 2021   Pseudo R-squ.:                  0.2989\nTime:                        20:34:31   Log-Likelihood:                -177.42\nconverged:                       True   LL-Null:                       -253.08\nCovariance Type:            nonrobust   LLR p-value:                 1.372e-32\n================================================================================\n                    coef   std err          z      P>|z|      [0.025      0.975]\n--------------------------------------------------------------------------------\nAge               1.3677     0.164      8.348      0.000       1.047       1.689\nJob               0.4393     0.153      2.873      0.004       0.140       0.739\nCredit amount     1.3290     0.305      4.358      0.000       0.731       1.927\nDuration         -1.2709     0.246     -5.164      0.000      -1.753      -0.789\n================================================================================\n```", "```py\nIn [38]: first_cluster_test = X_test[X_test.Clusters == 0].iloc[:, :-1] ![1](assets/1.png)\n         second_cluster_test = X_test[X_test.Clusters == 1].iloc[:, :-1] ![2](assets/2.png)\n\nIn [39]: X_test1 = first_cluster_test\n         y_test1 = y_test[y_test.Clusters == 0]['Risk']\n         pred_prob1 = logit_fit1.predict(X_test1) ![3](assets/3.png)\n\nIn [40]: false_pos, true_pos, _ = roc_curve(y_test1.values,  pred_prob1) ![4](assets/4.png)\n         auc = roc_auc_score(y_test1, pred_prob1) ![5](assets/5.png)\n         plt.plot(false_pos,true_pos, label=\"AUC for cluster 1={:.4f} \"\n                  .format(auc))\n         plt.plot([0, 1], [0, 1], linestyle = '--', label='45 degree line')\n         plt.legend(loc='best')\n         plt.title('ROC-AUC Curve 1')\n         plt.show()\n```", "```py\nIn [41]: X_train2 = second_cluster_train\n         y_train2 = y_train[y_train.Clusters == 1]['Risk']\n         logit = sm.Logit(y_train2, X_train2)\n         logit_fit2 = logit.fit()\n         print(logit_fit2.summary())\n         Optimization terminated successfully.\n         Current function value: 0.688152\n         Iterations 4\n                           Logit Regression Results\n==============================================================================\nDep. Variable:                   Risk   No. Observations:                  199\nModel:                          Logit   Df Residuals:                      195\nMethod:                           MLE   Df Model:                            3\nDate:                Wed, 01 Dec 2021   Pseudo R-squ.:              -0.0008478\nTime:                        20:34:33   Log-Likelihood:                -136.94\nconverged:                       True   LL-Null:                       -136.83\nCovariance Type:            nonrobust   LLR p-value:                     1.000\n================================================================================\n                    coef   std err          z      P>|z|      [0.025      0.975]\n--------------------------------------------------------------------------------\nAge               0.0281     0.146      0.192      0.848      -0.259       0.315\nJob               0.1536     0.151      1.020      0.308      -0.142       0.449\nCredit amount    -0.1090     0.115     -0.945      0.345      -0.335       0.117\nDuration          0.1046     0.126      0.833      0.405      -0.142       0.351\n================================================================================\n\nIn [42]: X_test2 = second_cluster_test\n         y_test2 = y_test[y_test.Clusters == 1]['Risk']\n         pred_prob2 = logit_fit2.predict(X_test2)\n\nIn [43]: false_pos, true_pos, _ = roc_curve(y_test2.values,  pred_prob2)\n         auc = roc_auc_score(y_test2, pred_prob2)\n         plt.plot(false_pos,true_pos,label=\"AUC for cluster 2={:.4f} \"\n                  .format(auc))\n         plt.plot([0, 1], [0, 1], linestyle = '--', label='45 degree line')\n         plt.legend(loc='best')\n         plt.title('ROC-AUC Curve 2')\n         plt.show()\n```", "```py\nIn [44]: import pymc3 as pm ![1](assets/1.png)\n         import arviz as az ![2](assets/2.png)\n\nIn [45]: with pm.Model() as logistic_model1: ![3](assets/3.png)\n             beta_age = pm.Normal('coeff_age', mu=0, sd=10) ![4](assets/4.png)\n             beta_job = pm.Normal('coeff_job', mu=0, sd=10)\n             beta_credit = pm.Normal('coeff_credit_amount', mu=0, sd=10)\n             beta_dur = pm.Normal('coeff_duration', mu=0, sd=10)\n             p = pm.Deterministic('p', pm.math.sigmoid(beta_age *\n                                       X_train1['Age'] + beta_job *\n                                       X_train1['Job'] + beta_credit *\n                                       X_train1['Credit amount'] + beta_dur *\n                                       X_train1['Duration'])) ![5](assets/5.png)\n         with logistic_model1:\n             observed = pm.Bernoulli(\"risk\", p, observed=y_train1) ![6](assets/6.png)\n             map_estimate = pm.find_MAP() ![7](assets/7.png)\nOut[]: <IPython.core.display.HTML object>\n\nIn [46]: param_list = ['coeff_age', 'coeff_job',\n                       'coeff_credit_amount', 'coeff_duration']\n         params = {}\n         for i in param_list:\n             params[i] = [np.round(map_estimate[i], 6)] ![8](assets/8.png)\n\n         bayesian_params = pd.DataFrame.from_dict(params)\n         print('The result of Bayesian estimation:\\n {}'.format(bayesian_params))\n         The result of Bayesian estimation:\n             coeff_age  coeff_job  coeff_credit_amount  coeff_duration\n         0   1.367247   0.439128              1.32721       -1.269345\n```", "```py\nIn [47]: with pm.Model() as logistic_model2:\n             beta_age = pm.Normal('coeff_age', mu=0, sd=10)\n             beta_job = pm.Normal('coeff_job', mu=0, sd=10)\n             beta_credit = pm.Normal('coeff_credit_amount', mu=0, sd=10)\n             beta_dur = pm.Normal('coeff_duration', mu=0, sd=10)\n             p = pm.Deterministic('p', pm.math.sigmoid(beta_age *\n                                       second_cluster_train['Age'] +\n                                       beta_job * second_cluster_train['Job'] +\n                                       beta_credit *\n                                       second_cluster_train['Credit amount'] +\n                                       beta_dur *\n                                       second_cluster_train['Duration']))\n         with logistic_model2:\n             observed = pm.Bernoulli(\"risk\", p,\n                                     observed=y_train[y_train.Clusters == 1]\n                                     ['Risk'])\n             map_estimate = pm.find_MAP()\nOut[]: <IPython.core.display.HTML object>\n\nIn [48]: param_list = [ 'coeff_age', 'coeff_job',\n                       'coeff_credit_amount', 'coeff_duration']\n         params = {}\n         for i in param_list:\n             params[i] = [np.round(map_estimate[i], 6)]\n\n         bayesian_params = pd.DataFrame.from_dict(params)\n         print('The result of Bayesian estimation:\\n {}'.format(bayesian_params))\n         The result of Bayesian estimation:\n             coeff_age  coeff_job  coeff_credit_amount  coeff_duration\n         0   0.028069   0.153599            -0.109003        0.104581\n```", "```py\nIn [49]: import logging ![1](assets/1.png)\n         logger = logging.getLogger('pymc3') ![2](assets/2.png)\n         logger.setLevel(logging.ERROR) ![3](assets/3.png)\n\nIn [50]: with logistic_model1:\n             step = pm.Metropolis() ![4](assets/4.png)\n             trace = pm.sample(10000, step=step,progressbar = False) ![5](assets/5.png)\n         az.plot_trace(trace) ![6](assets/6.png)\n         plt.show()\nIn [51]: with logistic_model1:\n             display(az.summary(trace, round_to=6)[:4]) ![7](assets/7.png)\nOut[]:                          mean        sd    hdi_3%   hdi_97%  mcse_mean  \\\n       coeff_age            1.392284  0.164607  1.086472  1.691713   0.003111\n       coeff_job            0.448694  0.155060  0.138471  0.719332   0.002925\n       coeff_credit_amount  1.345549  0.308100  0.779578  1.928159   0.008017\n       coeff_duration      -1.290292  0.252505 -1.753565 -0.802707   0.006823\n\n                             mcse_sd     ess_bulk     ess_tail     r_hat\n       coeff_age            0.002200  2787.022099  3536.314548  1.000542\n       coeff_job            0.002090  2818.973167  3038.790307  1.001246\n       coeff_credit_amount  0.005670  1476.746667  2289.532062  1.001746\n       coeff_duration       0.004826  1369.393339  2135.308468  1.001022\n```", "```py\nIn [52]: with logistic_model2:\n             step = pm.Metropolis()\n             trace = pm.sample(10000, step=step,progressbar = False)\n         az.plot_trace(trace)\n         plt.show()\nIn [53]: with logistic_model2:\n             display(az.summary(trace, round_to=6)[:4])\nOut[]:                          mean        sd    hdi_3%   hdi_97%  mcse_mean  \\\n       coeff_age            0.029953  0.151466 -0.262319  0.309050   0.002855\n       coeff_job            0.158140  0.153030 -0.125043  0.435734   0.003513\n       coeff_credit_amount -0.108844  0.116542 -0.328353  0.105858   0.003511\n       coeff_duration       0.103149  0.128264 -0.142609  0.339575   0.003720\n\n                             mcse_sd     ess_bulk     ess_tail     r_hat\n       coeff_age            0.002019  2823.255277  3195.005913  1.000905\n       coeff_job            0.002485  1886.026245  2336.516309  1.000594\n       coeff_credit_amount  0.002483  1102.228318  1592.047959  1.002032\n       coeff_duration       0.002631  1188.042552  1900.179695  1.000988\n```", "```py\nIn [54]: from sklearn.svm import SVC\n         from sklearn.experimental import enable_halving_search_cv ![1](assets/1.png)\n         from sklearn.model_selection import HalvingRandomSearchCV ![2](assets/2.png)\n         import time\n\nIn [55]: param_svc = {'gamma': [1e-6, 1e-2],\n                      'C':[0.001,.09,1,5,10],\n                      'kernel':('linear','rbf')}\n\nIn [56]: svc = SVC(class_weight='balanced')\n         halve_SVC = HalvingRandomSearchCV(svc, param_svc,\n                                           scoring = 'roc_auc', n_jobs=-1) ![3](assets/3.png)\n         halve_SVC.fit(X_train1, y_train1)\n         print('Best hyperparameters for first cluster in SVC {} with {}'.\n               format(halve_SVC.best_score_, halve_SVC.best_params_))\n         Best hyperparameters for first cluster in SVC 0.8273860106443562 with\n         {'kernel': 'rbf', 'gamma': 0.01, 'C': 1}\n\nIn [57]: y_pred_SVC1 = halve_SVC.predict(X_test1) ![4](assets/4.png)\n         print('The ROC AUC score of SVC for first cluster is {:.4f}'.\n               format(roc_auc_score(y_test1, y_pred_SVC1)))\n         The ROC AUC score of SVC for first cluster is 0.5179\n```", "```py\nIn [58]: halve_SVC.fit(X_train2, y_train2)\n         print('Best hyperparameters for second cluster in SVC {} with {}'.\n               format(halve_SVC.best_score_, halve_SVC.best_params_))\n         Best hyperparameters for second cluster in SVC 0.5350758636788049 with\n         {'kernel': 'rbf', 'gamma': 0.01, 'C': 0.001}\n\nIn [59]: y_pred_SVC2 = halve_SVC.predict(X_test2)\n         print('The ROC AUC score of SVC for first cluster is {:.4f}'.\n               format(roc_auc_score(y_test2, y_pred_SVC2)))\n         The ROC AUC score of SVC for first cluster is 0.5000\n```", "```py\nIn [60]: from sklearn.ensemble import RandomForestClassifier\n\nIn [61]: rfc = RandomForestClassifier(random_state=42)\n\nIn [62]: param_rfc = {'n_estimators': [100, 300],\n             'criterion' :['gini', 'entropy'],\n             'max_features': ['auto', 'sqrt', 'log2'],\n             'max_depth' : [3, 4, 5, 6],\n             'min_samples_split':[5, 10]}\n\nIn [63]: halve_RF = HalvingRandomSearchCV(rfc, param_rfc,\n                                          scoring = 'roc_auc', n_jobs=-1)\n         halve_RF.fit(X_train1, y_train1)\n         print('Best hyperparameters for first cluster in RF {} with {}'.\n               format(halve_RF.best_score_, halve_RF.best_params_))\n         Best hyperparameters for first cluster in RF 0.8890871444218126 with\n         {'n_estimators': 300, 'min_samples_split': 10, 'max_features': 'sqrt',\n         'max_depth': 6, 'criterion': 'gini'}\n\nIn [64]: y_pred_RF1 = halve_RF.predict(X_test1)\n         print('The ROC AUC score of RF for first cluster is {:.4f}'.\n               format(roc_auc_score(y_test1, y_pred_RF1)))\n         The ROC AUC score of RF for first cluster is 0.5387\n```", "```py\nIn [65]: halve_RF.fit(X_train2, y_train2)\n         print('Best hyperparameters for second cluster in RF {} with {}'.\n               format(halve_RF.best_score_, halve_RF.best_params_))\n         Best hyperparameters for second cluster in RF 0.6565 with\n         {'n_estimators': 100, 'min_samples_split': 5, 'max_features': 'auto',\n         'max_depth': 5, 'criterion': 'entropy'}\n\nIn [66]: y_pred_RF2 = halve_RF.predict(X_test2)\n         print('The ROC AUC score of RF for first cluster is {:.4f}'.\n               format(roc_auc_score(y_test2, y_pred_RF2)))\n         The ROC AUC score of RF for first cluster is 0.5906\n```", "```py\nIn [67]: from sklearn.neural_network import MLPClassifier\n\nIn [68]: param_NN = {\"hidden_layer_sizes\": [(100, 50), (50, 50), (10, 100)],\n                     \"solver\": [\"lbfgs\", \"sgd\", \"adam\"],\n                     \"learning_rate_init\": [0.001, 0.05]}\n\nIn [69]: MLP = MLPClassifier(random_state=42)\n\nIn [70]: param_halve_NN = HalvingRandomSearchCV(MLP, param_NN,\n                                                scoring = 'roc_auc')\n         param_halve_NN.fit(X_train1, y_train1)\n         print('Best hyperparameters for first cluster in NN are {}'.\n               format(param_halve_NN.best_params_))\n         Best hyperparameters for first cluster in NN are {'solver': 'lbfgs',\n         'learning_rate_init': 0.05, 'hidden_layer_sizes': (100, 50)}\n\nIn [71]: y_pred_NN1 = param_halve_NN.predict(X_test1)\n         print('The ROC AUC score of NN for first cluster is {:.4f}'.\n               format(roc_auc_score(y_test1, y_pred_NN1)))\n         The ROC AUC score of NN for first cluster is 0.5263\n```", "```py\nIn [72]: param_halve_NN.fit(X_train2, y_train2)\n         print('Best hyperparameters for first cluster in NN are {}'.\n               format(param_halve_NN.best_params_))\n         Best hyperparameters for first cluster in NN are {'solver': 'lbfgs',\n         'learning_rate_init': 0.05, 'hidden_layer_sizes': (10, 100)}\n\nIn [73]: y_pred_NN2 = param_halve_NN.predict(X_test2)\n         print('The ROC AUC score of NN for first cluster is {:.4f}'.\n               format(roc_auc_score(y_test2, y_pred_NN2)))\n         The ROC AUC score of NN for first cluster is 0.6155\n```", "```py\nIn [74]: from tensorflow import keras\n         from tensorflow.keras.wrappers.scikit_learn import KerasClassifier ![1](assets/1.png)\n         from tensorflow.keras.layers import Dense, Dropout\n         from sklearn.model_selection import GridSearchCV\n         import tensorflow as tf\n         import logging ![2](assets/2.png)\n         tf.get_logger().setLevel(logging.ERROR) ![3](assets/3.png)\n\nIn [75]: def DL_risk(dropout_rate,verbose=0):\n             model = keras.Sequential()\n             model.add(Dense(128,kernel_initializer='normal',\n                 activation = 'relu', input_dim=4))\n             model.add(Dense(64, kernel_initializer='normal',\n                 activation = 'relu'))\n             model.add(Dense(8,kernel_initializer='normal',\n                 activation = 'relu'))\n             model.add(Dropout(dropout_rate))\n             model.add(Dense(1, activation=\"sigmoid\"))\n             model.compile(loss='binary_crossentropy', optimizer='rmsprop')\n             return model\n```", "```py\nIn [76]: parameters = {'batch_size':  [10, 50, 100],\n                   'epochs':  [50, 100, 150],\n                      'dropout_rate':[0.2, 0.4]}\n         model = KerasClassifier(build_fn = DL_risk) ![1](assets/1.png)\n         gs = GridSearchCV(estimator = model,\n                                param_grid = parameters,\n                                   scoring = 'roc_auc') ![2](assets/2.png)\n\nIn [77]: gs.fit(X_train1, y_train1, verbose=0)\n         print('Best hyperparameters for first cluster in DL are {}'.\n               format(gs.best_params_))\n         Best hyperparameters for first cluster in DL are {'batch_size': 10,\n         'dropout_rate': 0.2, 'epochs': 50}\n```", "```py\nIn [78]: model = KerasClassifier(build_fn = DL_risk,                    ![1](assets/1.png)\n                                 dropout_rate = gs.best_params_['dropout_rate'],\n                                 verbose = 0,\n                                 batch_size = gs.best_params_['batch_size'], ![2](assets/2.png)\n                                 epochs = gs.best_params_['epochs']) ![3](assets/3.png)\n         model.fit(X_train1, y_train1)\n         DL_predict1 = model.predict(X_test1)                           ![4](assets/4.png)\n         DL_ROC_AUC = roc_auc_score(y_test1, pd.DataFrame(DL_predict1.flatten()))\n         print('DL_ROC_AUC is {:.4f}'.format(DL_ROC_AUC))\n         DL_ROC_AUC is 0.5628\n```", "```py\nIn [79]: gs.fit(X_train2.values, y_train2.values, verbose=0)\n         print('Best parameters for second cluster in DL are {}'.\n               format(gs.best_params_))\n         Best parameters for second cluster in DL are {'batch_size': 10,\n         'dropout_rate': 0.2, 'epochs': 150}\n\nIn [80]: model = KerasClassifier(build_fn = DL_risk,\n                                 dropout_rate= gs.best_params_['dropout_rate'],\n                                 verbose = 0,\n                                 batch_size = gs.best_params_['batch_size'],\n                                 epochs = gs.best_params_['epochs'])\n         model.fit(X_train2, y_train2)\n         DL_predict2 =  model.predict(X_test2)\n         DL_ROC_AUC = roc_auc_score(y_test2, DL_predict2.flatten())\n         print('DL_ROC_AUC is {:.4f}'.format(DL_ROC_AUC))\n         DL_ROC_AUC is 0.5614\n```"]