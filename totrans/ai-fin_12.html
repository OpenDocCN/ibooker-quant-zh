<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 9. Reinforcement Learning"><div class="chapter" id="reinforcement_learning">
<h1><span class="label">Chapter 9. </span>Reinforcement Learning</h1>

<blockquote>
<p class="align_me_right">Like a human, our agents learn for themselves to achieve successful strategies that lead to the greatest long-term rewards. This paradigm of learning by trial-and-error, solely from rewards or punishments, is known as reinforcement learning.<sup><a data-type="noteref" id="idm45625285042408-marker" href="ch09.xhtml#idm45625285042408">1</a></sup></p>
<p data-type="attribution">DeepMind (2016)</p>
</blockquote>

<p><a data-type="indexterm" data-primary="statistical inefficiencies" data-secondary="RL" id="ix_stat_ineffic_RL_ch9"/><a data-type="indexterm" data-primary="RL (reinforcement learning)" id="ix_RL_ch9"/>The learning algorithms applied in  Chapters <a data-xrefstyle="select:labelnumber" data-type="xref" href="ch07.xhtml#dense_networks">7</a> and <a data-xrefstyle="select:labelnumber" data-type="xref" href="ch08.xhtml#recurrent_networks">8</a> fall into the category of <em>supervised learning</em>. These methods require that there is a data set available with features and labels that allows the algorithms to learn relationships between the features and labels to succeed at estimation or classification tasks. As the simple example in <a data-type="xref" href="ch01.xhtml#artificial_intelligence">Chapter 1</a> illustrates, <em>reinforcement learning</em> (RL) works differently. To begin with, there is no need for a comprehensive data set of features and labels to be given up front. The data is rather generated by the learning agent while interacting with the environment of interest. This chapter covers RL in some detail and introduces fundamental notions, as well as one of the most popular algorithms used in the field: <em>Q-learning</em> (QL). Neural networks are not replaced by RL algorithms; they generally play an important role in this context as well.</p>

<p><a data-type="xref" href="#rl_notions">“Fundamental Notions”</a> explains fundamental notions in RL, such as environments, states, and agents. <a data-type="xref" href="#rl_oai_gym">“OpenAI Gym”</a> introduces the OpenAI Gym suite of RL environments of which the <code>CartPole</code> environment is used as an example. In this environment, which <a data-type="xref" href="ch02.xhtml#superintelligence">Chapter 2</a> introduces and discusses briefly, agents must learn how to balance a pole on a cart by moving the cart to 
<span class="keep-together">the left</span> or to the right. <a data-type="xref" href="#rl_mc_agent">“Monte Carlo Agent”</a> shows how to solve the 
<span class="keep-together"><code>CartPole</code></span> problem by the use of dimensionality reduction and Monte Carlo simulation. Standard supervised learning algorithms such as DNNs are in general not suited to solve 
<span class="keep-together">problems</span> such as the <code>CartPole</code> one since they lack a notion of delayed reward. This problem is illustrated in <a data-type="xref" href="#rl_nn_agent">“Neural Network Agent”</a>. <a data-type="xref" href="#rl_dql_agent">“DQL Agent”</a> discusses a QL agent that explicitly takes into account delayed rewards and is able to solve the <code>CartPole</code> problem. The same agent is applied in <a data-type="xref" href="#rl_sf_gym">“Simple Finance Gym”</a> to a simple financial market environment. Although the agent does not perform too well in this setting, the example shows that QL agents can also learn to trade and to become what is often called a <em>trading bot</em>. To improve the learning of QL agents, <a data-type="xref" href="#rl_bf_gym">“Better Finance Gym”</a> presents an improved financial market environment that, among other benefits, allows the use of more than one type of feature to describe the state of the environment. Based on this improved environment, <a data-type="xref" href="#rl_fql_agent">“FQL Agent”</a> introduces and applies an improved financial QL agent that performs better as a trading bot.</p>






<section data-type="sect1" data-pdf-bookmark="Fundamental Notions"><div class="sect1" id="rl_notions">
<h1>Fundamental Notions</h1>

<p><a data-type="indexterm" data-primary="RL (reinforcement learning)" data-secondary="fundamental notions" id="ix_RL_fund_notions"/>This section gives a brief overview of the fundamental notions in RL. Among them are the following:</p>
<dl>
<dt>Environment</dt>
<dd>
<p><a data-type="indexterm" data-primary="environment, in RL" id="idm45625284482200"/>The <em>environment</em> defines the problem at hand. This can be a computer game to be played or a financial market to be traded in.</p>
</dd>
<dt>State</dt>
<dd>
<p><a data-type="indexterm" data-primary="state, in RL" id="idm45625285087160"/>A <em>state</em> subsumes all relevant parameters that describe the current state of the environment. In a computer game, this might be the whole screen with all its 
<span class="keep-together">pixels.</span> In a financial market, this might include current and historical price levels or financial indicators such as moving averages, macroeconomic variables, 
<span class="keep-together">and so on.</span></p>
</dd>
<dt>Agent</dt>
<dd>
<p><a data-type="indexterm" data-primary="agents" data-seealso="algorithmic trading" id="idm45625284884248"/>The term <em>agent</em> subsumes all elements of the RL algorithm that interacts with the environment and that learns from these interactions. In a gaming context, the agent might represent a player playing the game. In a financial context, the agent could represent a trader placing bets on rising or falling markets.</p>
</dd>
<dt>Action</dt>
<dd>
<p><a data-type="indexterm" data-primary="agent, in RL" id="idm45625284881240"/>An agent can choose one <em>action</em> from a (limited) set of allowed actions. In a computer game, movements to the left or right might be allowed actions, whereas in a financial market, going long or short could be admissible actions.</p>
</dd>
<dt>Step</dt>
<dd>
<p><a data-type="indexterm" data-primary="step, in RL" id="idm45625285061816"/>Given an action of an agent, the state of the environment is updated. One such update is generally called a <em>step</em>. The concept of a step is general enough to encompass both heterogeneous and homogeneous time intervals between two steps. While in computer games, real-time interaction with the game environment is simulated by rather short, homogeneous time intervals (“game clock”), a trading bot interacting with a financial market environment could take actions at longer, heterogeneous time intervals, for instance.</p>
</dd>
<dt>Reward</dt>
<dd>
<p><a data-type="indexterm" data-primary="reward, in RL" id="idm45625285058856"/>Depending on the action an agent chooses, a <em>reward</em> (or penalty) is awarded. For a computer game, points are a typical reward. In a financial context, profit (or loss) is a standard reward (or penalty).</p>
</dd>
<dt>Target</dt>
<dd>
<p><a data-type="indexterm" data-primary="target, in RL" id="idm45625284662600"/>The <em>target</em> specifies what the agent tries to maximize. In a computer game, this in general is the score reached by the agent. For a financial trading bot, this might be the accumulated trading profit.</p>
</dd>
<dt>Policy</dt>
<dd>
<p><a data-type="indexterm" data-primary="policy, in RL" id="idm45625284659960"/>The <em>policy</em> defines which action an agent takes given a certain state of the environment. Given a certain state of a computer game, represented by all the pixels that make up the current scene, the policy might specify that the agent chooses “move right” as the action. A trading bot that observes three price increases in a row might decide, according to its policy, to short the market.</p>
</dd>
<dt>Episode</dt>
<dd>
<p><a data-type="indexterm" data-primary="episode, in RL" id="idm45625285054856"/>An <em>episode</em> is a set of steps from the initial state of the environment until success is achieved or failure is observed. In a game, this is from the start of the game until a win or loss. In the financial world, for example, this is from the beginning of the year to the end of the year or to bankruptcy.</p>
</dd>
</dl>

<p><a data-type="indexterm" data-primary="Sutton, Richard" id="idm45625285053048"/>Sutton and Barto (2018) provide a detailed introduction to the RL field. The book discusses the preceding notions in detail and illustrates them on the basis of a multitude of concrete examples. The following sections again choose a practical, implementation-oriented approach to RL. The examples discussed illustrate all of the preceding notions on the basis of Python code.<a data-type="indexterm" data-primary="" data-startref="ix_RL_fund_notions" id="idm45625285266632"/></p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="OpenAI Gym"><div class="sect1" id="rl_oai_gym">
<h1>OpenAI Gym</h1>

<p><a data-type="indexterm" data-primary="OpenAI Gym environment" id="ix_openai_gym_env_ch9"/><a data-type="indexterm" data-primary="RL (reinforcement learning)" data-secondary="OpenAI Gym" id="ix_RL_openai_gym"/>In most of the success stories as presented in <a data-type="xref" href="ch02.xhtml#superintelligence">Chapter 2</a>, RL plays a dominant role. This has spurred widespread interest in RL as an algorithm. OpenAI is an organization that strives to facilitate research in AI in general and in RL in particular. OpenAI has developed and open sourced a suite of environments, called <a href="https://gym.openai.com"><code>OpenAI Gym</code></a>, that allows the training of RL agents via a standardized API.</p>

<p><a data-type="indexterm" data-primary="CartPole game" id="ix_cartpole_gen"/>Among the many environments, there is the <a href="https://oreil.ly/f6tAK"><code>CartPole</code></a> environment (or game) that simulates a classical RL problem. A pole is standing upright on a cart, and the goal is to learn a policy to balance the pole on the cart by moving the cart either to the right or to the left. The state of the environment is given by four parameters, describing the following physical measurements: cart position, cart velocity, pole angle, and pole velocity (at tip). <a data-type="xref" href="#figure_rl_cp">Figure 9-1</a> shows a visualization of the environment.</p>

<figure class="thumb"><div id="figure_rl_cp" class="figure">
<img src="Images/aiif_0901.png" alt="aiif 0901" width="2497" height="1491"/>
<h6><span class="label">Figure 9-1. </span>CartPole environment of OpenAI Gym</h6>
</div></figure>

<p><a data-type="indexterm" data-primary="observation space, CartPole environment" id="idm45625284916808"/>Consider the following Python code that instantiates an environment object for 
<span class="keep-together"><code>CartPole</code></span> and inspects the <em>observation space</em>. The observation space is a model for the state of the environment:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">1</code><code class="p">]</code><code class="p">:</code><code> </code><code class="kn">import</code><code> </code><code class="nn">os</code><code>
</code><code>        </code><code class="kn">import</code><code> </code><code class="nn">math</code><code>
</code><code>        </code><code class="kn">import</code><code> </code><code class="nn">random</code><code>
</code><code>        </code><code class="kn">import</code><code> </code><code class="nn">numpy</code><code> </code><code class="kn">as</code><code> </code><code class="nn">np</code><code>
</code><code>        </code><code class="kn">import</code><code> </code><code class="nn">pandas</code><code> </code><code class="kn">as</code><code> </code><code class="nn">pd</code><code>
</code><code>        </code><code class="kn">from</code><code> </code><code class="nn">pylab</code><code> </code><code class="kn">import</code><code> </code><code class="n">plt</code><code class="p">,</code><code> </code><code class="n">mpl</code><code>
</code><code>        </code><code class="n">plt</code><code class="o">.</code><code class="n">style</code><code class="o">.</code><code class="n">use</code><code class="p">(</code><code class="s1">'</code><code class="s1">seaborn</code><code class="s1">'</code><code class="p">)</code><code>
</code><code>        </code><code class="n">mpl</code><code class="o">.</code><code class="n">rcParams</code><code class="p">[</code><code class="s1">'</code><code class="s1">savefig.dpi</code><code class="s1">'</code><code class="p">]</code><code> </code><code class="o">=</code><code> </code><code class="mi">300</code><code>
</code><code>        </code><code class="n">mpl</code><code class="o">.</code><code class="n">rcParams</code><code class="p">[</code><code class="s1">'</code><code class="s1">font.family</code><code class="s1">'</code><code class="p">]</code><code> </code><code class="o">=</code><code> </code><code class="s1">'</code><code class="s1">serif</code><code class="s1">'</code><code>
</code><code>        </code><code class="n">np</code><code class="o">.</code><code class="n">set_printoptions</code><code class="p">(</code><code class="n">precision</code><code class="o">=</code><code class="mi">4</code><code class="p">,</code><code> </code><code class="n">suppress</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code><code>
</code><code>        </code><code class="n">os</code><code class="o">.</code><code class="n">environ</code><code class="p">[</code><code class="s1">'</code><code class="s1">PYTHONHASHSEED</code><code class="s1">'</code><code class="p">]</code><code> </code><code class="o">=</code><code> </code><code class="s1">'</code><code class="s1">0</code><code class="s1">'</code><code>
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">2</code><code class="p">]</code><code class="p">:</code><code> </code><code class="kn">import</code><code> </code><code class="nn">gym</code><code>
</code><code>
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">3</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">env</code><code> </code><code class="o">=</code><code> </code><code class="n">gym</code><code class="o">.</code><code class="n">make</code><code class="p">(</code><code class="s1">'</code><code class="s1">CartPole-v0</code><code class="s1">'</code><code class="p">)</code><code>  </code><a class="co" id="co_reinforcement_learning_CO1-1" href="#callout_reinforcement_learning_CO1-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>
</code><code>
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">4</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">env</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="mi">100</code><code class="p">)</code><code>  </code><a class="co" id="co_reinforcement_learning_CO1-2" href="#callout_reinforcement_learning_CO1-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>
</code><code>        </code><code class="n">env</code><code class="o">.</code><code class="n">action_space</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="mi">100</code><code class="p">)</code><code>  </code><a class="co" id="co_reinforcement_learning_CO1-3" href="#callout_reinforcement_learning_CO1-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>
</code><code class="n">Out</code><code class="p">[</code><code class="mi">4</code><code class="p">]</code><code class="p">:</code><code> </code><code class="p">[</code><code class="mi">100</code><code class="p">]</code><code>
</code><code>
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">5</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">env</code><code class="o">.</code><code class="n">observation_space</code><code>  </code><a class="co" id="co_reinforcement_learning_CO1-4" href="#callout_reinforcement_learning_CO1-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a><code>
</code><code class="n">Out</code><code class="p">[</code><code class="mi">5</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">Box</code><code class="p">(</code><code class="mi">4</code><code class="p">,</code><code class="p">)</code><code>
</code><code>
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">6</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">env</code><code class="o">.</code><code class="n">observation_space</code><code class="o">.</code><code class="n">low</code><code class="o">.</code><code class="n">astype</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">float16</code><code class="p">)</code><code>  </code><a class="co" id="co_reinforcement_learning_CO1-5" href="#callout_reinforcement_learning_CO1-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a><code>
</code><code class="n">Out</code><code class="p">[</code><code class="mi">6</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">array</code><code class="p">(</code><code class="p">[</code><code class="o">-</code><code class="mf">4.8</code><code>  </code><code class="p">,</code><code>   </code><code class="o">-</code><code class="n">inf</code><code class="p">,</code><code> </code><code class="o">-</code><code class="mf">0.419</code><code class="p">,</code><code>   </code><code class="o">-</code><code class="n">inf</code><code class="p">]</code><code class="p">,</code><code> </code><code class="n">dtype</code><code class="o">=</code><code class="n">float16</code><code class="p">)</code><code>
</code><code>
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">7</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">env</code><code class="o">.</code><code class="n">observation_space</code><code class="o">.</code><code class="n">high</code><code class="o">.</code><code class="n">astype</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">float16</code><code class="p">)</code><code>  </code><a class="co" id="co_reinforcement_learning_CO1-6" href="#callout_reinforcement_learning_CO1-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a><code>
</code><code class="n">Out</code><code class="p">[</code><code class="mi">7</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">array</code><code class="p">(</code><code class="p">[</code><code class="mf">4.8</code><code>  </code><code class="p">,</code><code>   </code><code class="n">inf</code><code class="p">,</code><code> </code><code class="mf">0.419</code><code class="p">,</code><code>   </code><code class="n">inf</code><code class="p">]</code><code class="p">,</code><code> </code><code class="n">dtype</code><code class="o">=</code><code class="n">float16</code><code class="p">)</code><code>
</code><code>
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">8</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">state</code><code> </code><code class="o">=</code><code> </code><code class="n">env</code><code class="o">.</code><code class="n">reset</code><code class="p">(</code><code class="p">)</code><code>  </code><a class="co" id="co_reinforcement_learning_CO1-7" href="#callout_reinforcement_learning_CO1-3"><img src="Images/3.png" alt="3" width="12" height="12"/></a><code>
</code><code>
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">9</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">state</code><code>  </code><a class="co" id="co_reinforcement_learning_CO1-8" href="#callout_reinforcement_learning_CO1-4"><img src="Images/4.png" alt="4" width="12" height="12"/></a><code>
</code><code class="n">Out</code><code class="p">[</code><code class="mi">9</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">array</code><code class="p">(</code><code class="p">[</code><code class="o">-</code><code class="mf">0.0163</code><code class="p">,</code><code>  </code><code class="mf">0.0238</code><code class="p">,</code><code> </code><code class="o">-</code><code class="mf">0.0392</code><code class="p">,</code><code> </code><code class="o">-</code><code class="mf">0.0148</code><code class="p">]</code><code class="p">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_reinforcement_learning_CO1-1" href="#co_reinforcement_learning_CO1-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>The environment object, with fixed seed values</p></dd>
<dt><a class="co" id="callout_reinforcement_learning_CO1-2" href="#co_reinforcement_learning_CO1-4"><img src="Images/2.png" alt="2" width="12" height="12"/></a></dt>
<dd><p>The observation space with minimal and maximal values</p></dd>
<dt><a class="co" id="callout_reinforcement_learning_CO1-3" href="#co_reinforcement_learning_CO1-7"><img src="Images/3.png" alt="3" width="12" height="12"/></a></dt>
<dd><p>Reset of the environment</p></dd>
<dt><a class="co" id="callout_reinforcement_learning_CO1-4" href="#co_reinforcement_learning_CO1-8"><img src="Images/4.png" alt="4" width="12" height="12"/></a></dt>
<dd><p>Initial state: cart position, cart velocity, pole angle, pole angular velocity</p></dd>
</dl>

<p><a data-type="indexterm" data-primary="action space, CartPole environment" id="idm45625285530840"/>In the following environment, the allowed actions are described by the <em>action space</em>. In this case there are two, and they are represented by <code>0</code> (push cart to the left) and <code>1</code> (push cart to the right):</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">10</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">env</code><code class="o">.</code><code class="n">action_space</code><code>  </code><a class="co" id="co_reinforcement_learning_CO2-1" href="#callout_reinforcement_learning_CO2-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>
</code><code class="n">Out</code><code class="p">[</code><code class="mi">10</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">Discrete</code><code class="p">(</code><code class="mi">2</code><code class="p">)</code><code>
</code><code>
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">11</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">env</code><code class="o">.</code><code class="n">action_space</code><code class="o">.</code><code class="n">n</code><code>  </code><a class="co" id="co_reinforcement_learning_CO2-2" href="#callout_reinforcement_learning_CO2-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>
</code><code class="n">Out</code><code class="p">[</code><code class="mi">11</code><code class="p">]</code><code class="p">:</code><code> </code><code class="mi">2</code><code>
</code><code>
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">12</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">env</code><code class="o">.</code><code class="n">action_space</code><code class="o">.</code><code class="n">sample</code><code class="p">(</code><code class="p">)</code><code>  </code><a class="co" id="co_reinforcement_learning_CO2-3" href="#callout_reinforcement_learning_CO2-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a><code>
</code><code class="n">Out</code><code class="p">[</code><code class="mi">12</code><code class="p">]</code><code class="p">:</code><code> </code><code class="mi">1</code><code>
</code><code>
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">13</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">env</code><code class="o">.</code><code class="n">action_space</code><code class="o">.</code><code class="n">sample</code><code class="p">(</code><code class="p">)</code><code>   </code><a class="co" id="co_reinforcement_learning_CO2-4" href="#callout_reinforcement_learning_CO2-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a><code>
</code><code class="n">Out</code><code class="p">[</code><code class="mi">13</code><code class="p">]</code><code class="p">:</code><code> </code><code class="mi">0</code><code>
</code><code>
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">14</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">a</code><code> </code><code class="o">=</code><code> </code><code class="n">env</code><code class="o">.</code><code class="n">action_space</code><code class="o">.</code><code class="n">sample</code><code class="p">(</code><code class="p">)</code><code>  </code><a class="co" id="co_reinforcement_learning_CO2-5" href="#callout_reinforcement_learning_CO2-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a><code>
</code><code>         </code><code class="n">a</code><code>  </code><a class="co" id="co_reinforcement_learning_CO2-6" href="#callout_reinforcement_learning_CO2-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a><code>
</code><code class="n">Out</code><code class="p">[</code><code class="mi">14</code><code class="p">]</code><code class="p">:</code><code> </code><code class="mi">1</code><code>
</code><code>
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">15</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">state</code><code class="p">,</code><code> </code><code class="n">reward</code><code class="p">,</code><code> </code><code class="n">done</code><code class="p">,</code><code> </code><code class="n">info</code><code> </code><code class="o">=</code><code> </code><code class="n">env</code><code class="o">.</code><code class="n">step</code><code class="p">(</code><code class="n">a</code><code class="p">)</code><code>  </code><a class="co" id="co_reinforcement_learning_CO2-7" href="#callout_reinforcement_learning_CO2-3"><img src="Images/3.png" alt="3" width="12" height="12"/></a><code>
</code><code>         </code><code class="n">state</code><code class="p">,</code><code> </code><code class="n">reward</code><code class="p">,</code><code> </code><code class="n">done</code><code class="p">,</code><code> </code><code class="n">info</code><code>  </code><a class="co" id="co_reinforcement_learning_CO2-8" href="#callout_reinforcement_learning_CO2-4"><img src="Images/4.png" alt="4" width="12" height="12"/></a><code>
</code><code class="n">Out</code><code class="p">[</code><code class="mi">15</code><code class="p">]</code><code class="p">:</code><code> </code><code class="p">(</code><code class="n">array</code><code class="p">(</code><code class="p">[</code><code class="o">-</code><code class="mf">0.0158</code><code class="p">,</code><code>  </code><code class="mf">0.2195</code><code class="p">,</code><code> </code><code class="o">-</code><code class="mf">0.0395</code><code class="p">,</code><code> </code><code class="o">-</code><code class="mf">0.3196</code><code class="p">]</code><code class="p">)</code><code class="p">,</code><code> </code><code class="mf">1.0</code><code class="p">,</code><code> </code><code class="bp">False</code><code class="p">,</code><code> </code><code class="p">{</code><code class="p">}</code><code class="p">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_reinforcement_learning_CO2-1" href="#co_reinforcement_learning_CO2-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>The action space</p></dd>
<dt><a class="co" id="callout_reinforcement_learning_CO2-2" href="#co_reinforcement_learning_CO2-3"><img src="Images/2.png" alt="2" width="12" height="12"/></a></dt>
<dd><p>Random actions sampled from the action space</p></dd>
<dt><a class="co" id="callout_reinforcement_learning_CO2-3" href="#co_reinforcement_learning_CO2-7"><img src="Images/3.png" alt="3" width="12" height="12"/></a></dt>
<dd><p>Step forward based on random action</p></dd>
<dt><a class="co" id="callout_reinforcement_learning_CO2-4" href="#co_reinforcement_learning_CO2-8"><img src="Images/4.png" alt="4" width="12" height="12"/></a></dt>
<dd><p>New state of the environment, reward, success/failure, additional information</p></dd>
</dl>

<p>As long as <code>done=False</code>, the agent is still in the game and can choose another action. Success is achieved when the agent reaches a total of 200 steps in a row or a total reward of 200 (reward of 1.0 per step). A failure is observed when the pole on the cart reaches a certain angle that would lead to the pole falling from the cart. In that case, <code>done=True</code> is returned.</p>

<p>A simple agent is one that follows a completely random policy: no matter what state is observed, the agent chooses a random action. This is what the following code implements. The number of steps the agent can go only depends in such a case on how lucky it is. No learning in the form of updating the policy is taking place:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">16</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">env</code><code class="o">.</code><code class="n">reset</code><code class="p">(</code><code class="p">)</code><code>
</code><code>         </code><code class="k">for</code><code> </code><code class="n">e</code><code> </code><code class="ow">in</code><code> </code><code class="nb">range</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code><code> </code><code class="mi">200</code><code class="p">)</code><code class="p">:</code><code>
</code><code>             </code><code class="n">a</code><code> </code><code class="o">=</code><code> </code><code class="n">env</code><code class="o">.</code><code class="n">action_space</code><code class="o">.</code><code class="n">sample</code><code class="p">(</code><code class="p">)</code><code>  </code><a class="co" id="co_reinforcement_learning_CO3-1" href="#callout_reinforcement_learning_CO3-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>
</code><code>             </code><code class="n">state</code><code class="p">,</code><code> </code><code class="n">reward</code><code class="p">,</code><code> </code><code class="n">done</code><code class="p">,</code><code> </code><code class="n">info</code><code> </code><code class="o">=</code><code> </code><code class="n">env</code><code class="o">.</code><code class="n">step</code><code class="p">(</code><code class="n">a</code><code class="p">)</code><code> </code><a class="co" id="co_reinforcement_learning_CO3-2" href="#callout_reinforcement_learning_CO3-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a><code>
</code><code>             </code><code class="k">print</code><code class="p">(</code><code class="n">f</code><code class="s1">'</code><code class="s1">step={e:2d} | state={state} | action={a} | reward={reward}</code><code class="s1">'</code><code class="p">)</code><code>
</code><code>             </code><code class="k">if</code><code> </code><code class="n">done</code><code> </code><code class="ow">and</code><code> </code><code class="p">(</code><code class="n">e</code><code> </code><code class="o">+</code><code> </code><code class="mi">1</code><code class="p">)</code><code> </code><code class="o">&lt;</code><code> </code><code class="mi">200</code><code class="p">:</code><code>  </code><a class="co" id="co_reinforcement_learning_CO3-3" href="#callout_reinforcement_learning_CO3-3"><img src="Images/3.png" alt="3" width="12" height="12"/></a><code>
</code><code>                 </code><code class="k">print</code><code class="p">(</code><code class="s1">'</code><code class="s1">*** FAILED ***</code><code class="s1">'</code><code class="p">)</code><code>  </code><a class="co" id="co_reinforcement_learning_CO3-4" href="#callout_reinforcement_learning_CO3-3"><img src="Images/3.png" alt="3" width="12" height="12"/></a><code>
</code><code>                 </code><code class="k">break</code><code>
</code><code>         </code><code class="n">step</code><code class="o">=</code><code> </code><code class="mi">1</code><code> </code><code class="o">|</code><code> </code><code class="n">state</code><code class="o">=</code><code class="p">[</code><code class="o">-</code><code class="mf">0.0423</code><code>  </code><code class="mf">0.1982</code><code>  </code><code class="mf">0.0256</code><code> </code><code class="o">-</code><code class="mf">0.2476</code><code class="p">]</code><code> </code><code class="o">|</code><code> </code><code class="n">action</code><code class="o">=</code><code class="mi">1</code><code> </code><code class="o">|</code><code> </code><code class="n">reward</code><code class="o">=</code><code class="mf">1.0</code><code>
</code><code>         </code><code class="n">step</code><code class="o">=</code><code> </code><code class="mi">2</code><code> </code><code class="o">|</code><code> </code><code class="n">state</code><code class="o">=</code><code class="p">[</code><code class="o">-</code><code class="mf">0.0383</code><code>  </code><code class="mf">0.0028</code><code>  </code><code class="mf">0.0206</code><code>  </code><code class="mf">0.0531</code><code class="p">]</code><code> </code><code class="o">|</code><code> </code><code class="n">action</code><code class="o">=</code><code class="mi">0</code><code> </code><code class="o">|</code><code> </code><code class="n">reward</code><code class="o">=</code><code class="mf">1.0</code><code>
</code><code>         </code><code class="n">step</code><code class="o">=</code><code> </code><code class="mi">3</code><code> </code><code class="o">|</code><code> </code><code class="n">state</code><code class="o">=</code><code class="p">[</code><code class="o">-</code><code class="mf">0.0383</code><code>  </code><code class="mf">0.1976</code><code>  </code><code class="mf">0.0217</code><code> </code><code class="o">-</code><code class="mf">0.2331</code><code class="p">]</code><code> </code><code class="o">|</code><code> </code><code class="n">action</code><code class="o">=</code><code class="mi">1</code><code> </code><code class="o">|</code><code> </code><code class="n">reward</code><code class="o">=</code><code class="mf">1.0</code><code>
</code><code>         </code><code class="n">step</code><code class="o">=</code><code> </code><code class="mi">4</code><code> </code><code class="o">|</code><code> </code><code class="n">state</code><code class="o">=</code><code class="p">[</code><code class="o">-</code><code class="mf">0.0343</code><code>  </code><code class="mf">0.0022</code><code>  </code><code class="mf">0.017</code><code>   </code><code class="mf">0.0664</code><code class="p">]</code><code> </code><code class="o">|</code><code> </code><code class="n">action</code><code class="o">=</code><code class="mi">0</code><code> </code><code class="o">|</code><code> </code><code class="n">reward</code><code class="o">=</code><code class="mf">1.0</code><code>
</code><code>         </code><code class="n">step</code><code class="o">=</code><code> </code><code class="mi">5</code><code> </code><code class="o">|</code><code> </code><code class="n">state</code><code class="o">=</code><code class="p">[</code><code class="o">-</code><code class="mf">0.0343</code><code>  </code><code class="mf">0.197</code><code>   </code><code class="mf">0.0184</code><code> </code><code class="o">-</code><code class="mf">0.2209</code><code class="p">]</code><code> </code><code class="o">|</code><code> </code><code class="n">action</code><code class="o">=</code><code class="mi">1</code><code> </code><code class="o">|</code><code> </code><code class="n">reward</code><code class="o">=</code><code class="mf">1.0</code><code>
</code><code>         </code><code class="n">step</code><code class="o">=</code><code> </code><code class="mi">6</code><code> </code><code class="o">|</code><code> </code><code class="n">state</code><code class="o">=</code><code class="p">[</code><code class="o">-</code><code class="mf">0.0304</code><code>  </code><code class="mf">0.0016</code><code>  </code><code class="mf">0.0139</code><code>  </code><code class="mf">0.0775</code><code class="p">]</code><code> </code><code class="o">|</code><code> </code><code class="n">action</code><code class="o">=</code><code class="mi">0</code><code> </code><code class="o">|</code><code> </code><code class="n">reward</code><code class="o">=</code><code class="mf">1.0</code><code>
</code><code>         </code><code class="n">step</code><code class="o">=</code><code> </code><code class="mi">7</code><code> </code><code class="o">|</code><code> </code><code class="n">state</code><code class="o">=</code><code class="p">[</code><code class="o">-</code><code class="mf">0.0303</code><code>  </code><code class="mf">0.1966</code><code>  </code><code class="mf">0.0155</code><code> </code><code class="o">-</code><code class="mf">0.2107</code><code class="p">]</code><code> </code><code class="o">|</code><code> </code><code class="n">action</code><code class="o">=</code><code class="mi">1</code><code> </code><code class="o">|</code><code> </code><code class="n">reward</code><code class="o">=</code><code class="mf">1.0</code><code>
</code><code>         </code><code class="n">step</code><code class="o">=</code><code> </code><code class="mi">8</code><code> </code><code class="o">|</code><code> </code><code class="n">state</code><code class="o">=</code><code class="p">[</code><code class="o">-</code><code class="mf">0.0264</code><code>  </code><code class="mf">0.0012</code><code>  </code><code class="mf">0.0113</code><code>  </code><code class="mf">0.0868</code><code class="p">]</code><code> </code><code class="o">|</code><code> </code><code class="n">action</code><code class="o">=</code><code class="mi">0</code><code> </code><code class="o">|</code><code> </code><code class="n">reward</code><code class="o">=</code><code class="mf">1.0</code><code>
</code><code>         </code><code class="n">step</code><code class="o">=</code><code> </code><code class="mi">9</code><code> </code><code class="o">|</code><code> </code><code class="n">state</code><code class="o">=</code><code class="p">[</code><code class="o">-</code><code class="mf">0.0264</code><code>  </code><code class="mf">0.1962</code><code>  </code><code class="mf">0.013</code><code>  </code><code class="o">-</code><code class="mf">0.2023</code><code class="p">]</code><code> </code><code class="o">|</code><code> </code><code class="n">action</code><code class="o">=</code><code class="mi">1</code><code> </code><code class="o">|</code><code> </code><code class="n">reward</code><code class="o">=</code><code class="mf">1.0</code><code>
</code><code>         </code><code class="n">step</code><code class="o">=</code><code class="mi">10</code><code> </code><code class="o">|</code><code> </code><code class="n">state</code><code class="o">=</code><code class="p">[</code><code class="o">-</code><code class="mf">0.0224</code><code>  </code><code class="mf">0.3911</code><code>  </code><code class="mf">0.009</code><code>  </code><code class="o">-</code><code class="mf">0.4908</code><code class="p">]</code><code> </code><code class="o">|</code><code> </code><code class="n">action</code><code class="o">=</code><code class="mi">1</code><code> </code><code class="o">|</code><code> </code><code class="n">reward</code><code class="o">=</code><code class="mf">1.0</code><code>
</code><code>         </code><code class="n">step</code><code class="o">=</code><code class="mi">11</code><code> </code><code class="o">|</code><code> </code><code class="n">state</code><code class="o">=</code><code class="p">[</code><code class="o">-</code><code class="mf">0.0146</code><code>  </code><code class="mf">0.5861</code><code> </code><code class="o">-</code><code class="mf">0.0009</code><code> </code><code class="o">-</code><code class="mf">0.7807</code><code class="p">]</code><code> </code><code class="o">|</code><code> </code><code class="n">action</code><code class="o">=</code><code class="mi">1</code><code> </code><code class="o">|</code><code> </code><code class="n">reward</code><code class="o">=</code><code class="mf">1.0</code><code>
</code><code>         </code><code class="n">step</code><code class="o">=</code><code class="mi">12</code><code> </code><code class="o">|</code><code> </code><code class="n">state</code><code class="o">=</code><code class="p">[</code><code class="o">-</code><code class="mf">0.0029</code><code>  </code><code class="mf">0.7812</code><code> </code><code class="o">-</code><code class="mf">0.0165</code><code> </code><code class="o">-</code><code class="mf">1.0736</code><code class="p">]</code><code> </code><code class="o">|</code><code> </code><code class="n">action</code><code class="o">=</code><code class="mi">1</code><code> </code><code class="o">|</code><code> </code><code class="n">reward</code><code class="o">=</code><code class="mf">1.0</code><code>
</code><code>         </code><code class="n">step</code><code class="o">=</code><code class="mi">13</code><code> </code><code class="o">|</code><code> </code><code class="n">state</code><code class="o">=</code><code class="p">[</code><code> </code><code class="mf">0.0127</code><code>  </code><code class="mf">0.9766</code><code> </code><code class="o">-</code><code class="mf">0.0379</code><code> </code><code class="o">-</code><code class="mf">1.3714</code><code class="p">]</code><code> </code><code class="o">|</code><code> </code><code class="n">action</code><code class="o">=</code><code class="mi">1</code><code> </code><code class="o">|</code><code> </code><code class="n">reward</code><code class="o">=</code><code class="mf">1.0</code><code>
</code><code>         </code><code class="n">step</code><code class="o">=</code><code class="mi">14</code><code> </code><code class="o">|</code><code> </code><code class="n">state</code><code class="o">=</code><code class="p">[</code><code> </code><code class="mf">0.0323</code><code>  </code><code class="mf">1.1722</code><code> </code><code class="o">-</code><code class="mf">0.0654</code><code> </code><code class="o">-</code><code class="mf">1.6758</code><code class="p">]</code><code> </code><code class="o">|</code><code> </code><code class="n">action</code><code class="o">=</code><code class="mi">1</code><code> </code><code class="o">|</code><code> </code><code class="n">reward</code><code class="o">=</code><code class="mf">1.0</code><code>
</code><code>         </code><code class="n">step</code><code class="o">=</code><code class="mi">15</code><code> </code><code class="o">|</code><code> </code><code class="n">state</code><code class="o">=</code><code class="p">[</code><code> </code><code class="mf">0.0557</code><code>  </code><code class="mf">0.9779</code><code> </code><code class="o">-</code><code class="mf">0.0989</code><code> </code><code class="o">-</code><code class="mf">1.4041</code><code class="p">]</code><code> </code><code class="o">|</code><code> </code><code class="n">action</code><code class="o">=</code><code class="mi">0</code><code> </code><code class="o">|</code><code> </code><code class="n">reward</code><code class="o">=</code><code class="mf">1.0</code><code>
</code><code>         </code><code class="n">step</code><code class="o">=</code><code class="mi">16</code><code> </code><code class="o">|</code><code> </code><code class="n">state</code><code class="o">=</code><code class="p">[</code><code> </code><code class="mf">0.0753</code><code>  </code><code class="mf">0.7841</code><code> </code><code class="o">-</code><code class="mf">0.127</code><code>  </code><code class="o">-</code><code class="mf">1.1439</code><code class="p">]</code><code> </code><code class="o">|</code><code> </code><code class="n">action</code><code class="o">=</code><code class="mi">0</code><code> </code><code class="o">|</code><code> </code><code class="n">reward</code><code class="o">=</code><code class="mf">1.0</code><code>
</code><code>         </code><code class="n">step</code><code class="o">=</code><code class="mi">17</code><code> </code><code class="o">|</code><code> </code><code class="n">state</code><code class="o">=</code><code class="p">[</code><code> </code><code class="mf">0.0909</code><code>  </code><code class="mf">0.5908</code><code> </code><code class="o">-</code><code class="mf">0.1498</code><code> </code><code class="o">-</code><code class="mf">0.8936</code><code class="p">]</code><code> </code><code class="o">|</code><code> </code><code class="n">action</code><code class="o">=</code><code class="mi">0</code><code> </code><code class="o">|</code><code> </code><code class="n">reward</code><code class="o">=</code><code class="mf">1.0</code><code>
</code><code>         </code><code class="n">step</code><code class="o">=</code><code class="mi">18</code><code> </code><code class="o">|</code><code> </code><code class="n">state</code><code class="o">=</code><code class="p">[</code><code> </code><code class="mf">0.1028</code><code>  </code><code class="mf">0.7876</code><code> </code><code class="o">-</code><code class="mf">0.1677</code><code> </code><code class="o">-</code><code class="mf">1.2294</code><code class="p">]</code><code> </code><code class="o">|</code><code> </code><code class="n">action</code><code class="o">=</code><code class="mi">1</code><code> </code><code class="o">|</code><code> </code><code class="n">reward</code><code class="o">=</code><code class="mf">1.0</code><code>
</code><code>         </code><code class="n">step</code><code class="o">=</code><code class="mi">19</code><code> </code><code class="o">|</code><code> </code><code class="n">state</code><code class="o">=</code><code class="p">[</code><code> </code><code class="mf">0.1185</code><code>  </code><code class="mf">0.9845</code><code> </code><code class="o">-</code><code class="mf">0.1923</code><code> </code><code class="o">-</code><code class="mf">1.5696</code><code class="p">]</code><code> </code><code class="o">|</code><code> </code><code class="n">action</code><code class="o">=</code><code class="mi">1</code><code> </code><code class="o">|</code><code> </code><code class="n">reward</code><code class="o">=</code><code class="mf">1.0</code><code>
</code><code>         </code><code class="n">step</code><code class="o">=</code><code class="mi">20</code><code> </code><code class="o">|</code><code> </code><code class="n">state</code><code class="o">=</code><code class="p">[</code><code> </code><code class="mf">0.1382</code><code>  </code><code class="mf">0.7921</code><code> </code><code class="o">-</code><code class="mf">0.2237</code><code> </code><code class="o">-</code><code class="mf">1.3425</code><code class="p">]</code><code> </code><code class="o">|</code><code> </code><code class="n">action</code><code class="o">=</code><code class="mi">0</code><code> </code><code class="o">|</code><code> </code><code class="n">reward</code><code class="o">=</code><code class="mf">1.0</code><code>
</code><code>         </code><code class="o">*</code><code class="o">*</code><code class="o">*</code><code> </code><code class="n">FAILED</code><code> </code><code class="o">*</code><code class="o">*</code><code class="o">*</code><code>
</code><code>
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">17</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">done</code><code>
</code><code class="n">Out</code><code class="p">[</code><code class="mi">17</code><code class="p">]</code><code class="p">:</code><code> </code><code class="bp">True</code></pre>
<div style="page-break-after: always;"/>
<dl class="calloutlist">
<dt><a class="co" id="callout_reinforcement_learning_CO3-1" href="#co_reinforcement_learning_CO3-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>Random action policy</p></dd>
<dt><a class="co" id="callout_reinforcement_learning_CO3-2" href="#co_reinforcement_learning_CO3-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a></dt>
<dd><p>Stepping forward one step</p></dd>
<dt><a class="co" id="callout_reinforcement_learning_CO3-3" href="#co_reinforcement_learning_CO3-3"><img src="Images/3.png" alt="3" width="12" height="12"/></a></dt>
<dd><p>Failure if less than 200 steps</p></dd>
</dl>
<div data-type="note" epub:type="note"><h1>Data Through Interaction</h1>
<p><a data-type="indexterm" data-primary="SL (supervised learning)" data-seealso="DNNs; RNNs" id="idm45625283530392"/>Whereas in supervised learning the training, validation, and test data sets are assumed to exist before the training begins, in RL the agent generates its data itself by interacting with the environment. In many contexts, such as in games, this is a huge simplification. Consider the game of chess: instead of loading thousands of historical human-played chess games into a computer, an RL agent can generate thousands or millions of games itself by playing against another chess engine or another version of itself, for instance.<a data-type="indexterm" data-primary="" data-startref="ix_cartpole_gen" id="idm45625283528776"/><a data-type="indexterm" data-primary="" data-startref="ix_RL_openai_gym" id="idm45625283568920"/><a data-type="indexterm" data-primary="" data-startref="ix_openai_gym_env_ch9" id="idm45625283567976"/></p>
</div>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Monte Carlo Agent"><div class="sect1" id="rl_mc_agent">
<h1>Monte Carlo Agent</h1>

<p><a data-type="indexterm" data-primary="RL (reinforcement learning)" data-secondary="Monte Carlo agent" id="ix_RL_montecarlo_agent"/><a data-type="indexterm" data-primary="CartPole game" data-secondary="Monte Carlo simulation" id="ix_cartpole_MonteCarlo"/><a data-type="indexterm" data-primary="Monte Carlo simulations" id="ix_montecarlo_cartpole"/><a data-type="indexterm" data-primary="agents" data-secondary="Monte Carlo simulation" id="ix_agent_MonteCarlo"/>The <code>CartPole</code> problem does not necessarily require a full-fledged RL approach nor some neural network to be solved. This section presents a simple solution to the problem based on Monte Carlo simulation. <a data-type="indexterm" data-primary="dimensionality reduction, CartPole environment" id="idm45625283608232"/>To this end, a specific policy is defined that makes use of <em>dimensionality reduction</em>. In that case, the four parameters defining a state of the environment are collapsed, via a linear combination, into a single real-valued parameter.<sup><a data-type="noteref" id="idm45625283606888-marker" href="ch09.xhtml#idm45625283606888">2</a></sup> The following Python code implements this idea:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">18</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="mi">100</code><code class="p">)</code><code>  </code><a class="co" id="co_reinforcement_learning_CO4-1" href="#callout_reinforcement_learning_CO4-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>
</code><code>
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">19</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">weights</code><code> </code><code class="o">=</code><code> </code><code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">random</code><code class="p">(</code><code class="mi">4</code><code class="p">)</code><code> </code><code class="o">*</code><code> </code><code class="mi">2</code><code> </code><code class="o">-</code><code> </code><code class="mi">1</code><code>  </code><a class="co" id="co_reinforcement_learning_CO4-2" href="#callout_reinforcement_learning_CO4-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>
</code><code>
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">20</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">weights</code><code>  </code><a class="co" id="co_reinforcement_learning_CO4-3" href="#callout_reinforcement_learning_CO4-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>
</code><code class="n">Out</code><code class="p">[</code><code class="mi">20</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">array</code><code class="p">(</code><code class="p">[</code><code> </code><code class="mf">0.0868</code><code class="p">,</code><code> </code><code class="o">-</code><code class="mf">0.4433</code><code class="p">,</code><code> </code><code class="o">-</code><code class="mf">0.151</code><code> </code><code class="p">,</code><code>  </code><code class="mf">0.6896</code><code class="p">]</code><code class="p">)</code><code>
</code><code>
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">21</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">state</code><code> </code><code class="o">=</code><code> </code><code class="n">env</code><code class="o">.</code><code class="n">reset</code><code class="p">(</code><code class="p">)</code><code>  </code><a class="co" id="co_reinforcement_learning_CO4-4" href="#callout_reinforcement_learning_CO4-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a><code>
</code><code>
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">22</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">state</code><code>  </code><a class="co" id="co_reinforcement_learning_CO4-5" href="#callout_reinforcement_learning_CO4-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a><code>
</code><code class="n">Out</code><code class="p">[</code><code class="mi">22</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">array</code><code class="p">(</code><code class="p">[</code><code class="o">-</code><code class="mf">0.0347</code><code class="p">,</code><code> </code><code class="o">-</code><code class="mf">0.0103</code><code class="p">,</code><code>  </code><code class="mf">0.047</code><code> </code><code class="p">,</code><code> </code><code class="o">-</code><code class="mf">0.0315</code><code class="p">]</code><code class="p">)</code><code>
</code><code>
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">23</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">s</code><code> </code><code class="o">=</code><code> </code><code class="n">np</code><code class="o">.</code><code class="n">dot</code><code class="p">(</code><code class="n">state</code><code class="p">,</code><code> </code><code class="n">weights</code><code class="p">)</code><code>  </code><a class="co" id="co_reinforcement_learning_CO4-6" href="#callout_reinforcement_learning_CO4-3"><img src="Images/3.png" alt="3" width="12" height="12"/></a><code>
</code><code>         </code><code class="n">s</code><code>  </code><a class="co" id="co_reinforcement_learning_CO4-7" href="#callout_reinforcement_learning_CO4-3"><img src="Images/3.png" alt="3" width="12" height="12"/></a><code>
</code><code class="n">Out</code><code class="p">[</code><code class="mi">23</code><code class="p">]</code><code class="p">:</code><code> </code><code class="o">-</code><code class="mf">0.02725361929630797</code></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_reinforcement_learning_CO4-1" href="#co_reinforcement_learning_CO4-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>Random weights for fixed seed value</p></dd>
<dt><a class="co" id="callout_reinforcement_learning_CO4-2" href="#co_reinforcement_learning_CO4-4"><img src="Images/2.png" alt="2" width="12" height="12"/></a></dt>
<dd><p>Initial state of the environment</p></dd>
<dt><a class="co" id="callout_reinforcement_learning_CO4-3" href="#co_reinforcement_learning_CO4-6"><img src="Images/3.png" alt="3" width="12" height="12"/></a></dt>
<dd><p>Dot product of state and weights</p></dd>
</dl>

<p>The policy is then defined based on the sign of the single state parameter <code>s</code>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">In</code> <code class="p">[</code><code class="mi">24</code><code class="p">]:</code> <code class="k">if</code> <code class="n">s</code> <code class="o">&lt;</code> <code class="mi">0</code><code class="p">:</code>
             <code class="n">a</code> <code class="o">=</code> <code class="mi">0</code>
         <code class="k">else</code><code class="p">:</code>
             <code class="n">a</code> <code class="o">=</code> <code class="mi">1</code>

<code class="n">In</code> <code class="p">[</code><code class="mi">25</code><code class="p">]:</code> <code class="n">a</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">25</code><code class="p">]:</code> <code class="mi">0</code></pre>

<p>This policy can then be used to play an episode of the <code>CartPole</code> game. Given the random nature of the weights applied, the results are in general not better than those of the random action policy of the previous section:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">In</code> <code class="p">[</code><code class="mi">26</code><code class="p">]:</code> <code class="k">def</code> <code class="nf">run_episode</code><code class="p">(</code><code class="n">env</code><code class="p">,</code> <code class="n">weights</code><code class="p">):</code>
             <code class="n">state</code> <code class="o">=</code> <code class="n">env</code><code class="o">.</code><code class="n">reset</code><code class="p">()</code>
             <code class="n">treward</code> <code class="o">=</code> <code class="mi">0</code>
             <code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">200</code><code class="p">):</code>
                 <code class="n">s</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">dot</code><code class="p">(</code><code class="n">state</code><code class="p">,</code> <code class="n">weights</code><code class="p">)</code>
                 <code class="n">a</code> <code class="o">=</code> <code class="mi">0</code> <code class="k">if</code> <code class="n">s</code> <code class="o">&lt;</code> <code class="mi">0</code> <code class="k">else</code> <code class="mi">1</code>
                 <code class="n">state</code><code class="p">,</code> <code class="n">reward</code><code class="p">,</code> <code class="n">done</code><code class="p">,</code> <code class="n">info</code> <code class="o">=</code> <code class="n">env</code><code class="o">.</code><code class="n">step</code><code class="p">(</code><code class="n">a</code><code class="p">)</code>
                 <code class="n">treward</code> <code class="o">+=</code> <code class="n">reward</code>
                 <code class="k">if</code> <code class="n">done</code><code class="p">:</code>
                     <code class="k">break</code>
             <code class="k">return</code> <code class="n">treward</code>

<code class="n">In</code> <code class="p">[</code><code class="mi">27</code><code class="p">]:</code> <code class="n">run_episode</code><code class="p">(</code><code class="n">env</code><code class="p">,</code> <code class="n">weights</code><code class="p">)</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">27</code><code class="p">]:</code> <code class="mf">41.0</code></pre>

<p>Therefore, Monte Carlo simulation is applied to test a large number of different weights.
The following code simulates a large number of weights, checks them for success or failure, and then chooses the weights that yield success:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">28</code><code class="p">]</code><code class="p">:</code><code> </code><code class="k">def</code><code> </code><code class="nf">set_seeds</code><code class="p">(</code><code class="n">seed</code><code class="o">=</code><code class="mi">100</code><code class="p">)</code><code class="p">:</code><code>
</code><code>             </code><code class="n">random</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="n">seed</code><code class="p">)</code><code>
</code><code>             </code><code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="n">seed</code><code class="p">)</code><code>
</code><code>             </code><code class="n">env</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="n">seed</code><code class="p">)</code><code>
</code><code>
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">29</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">set_seeds</code><code class="p">(</code><code class="p">)</code><code>
</code><code>         </code><code class="n">num_episodes</code><code> </code><code class="o">=</code><code> </code><code class="mi">1000</code><code>
</code><code>
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">30</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">besttreward</code><code> </code><code class="o">=</code><code> </code><code class="mi">0</code><code>
</code><code>         </code><code class="k">for</code><code> </code><code class="n">e</code><code> </code><code class="ow">in</code><code> </code><code class="nb">range</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code><code> </code><code class="n">num_episodes</code><code> </code><code class="o">+</code><code> </code><code class="mi">1</code><code class="p">)</code><code class="p">:</code><code>
</code><code>             </code><code class="n">weights</code><code> </code><code class="o">=</code><code> </code><code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">rand</code><code class="p">(</code><code class="mi">4</code><code class="p">)</code><code> </code><code class="o">*</code><code> </code><code class="mi">2</code><code> </code><code class="o">-</code><code> </code><code class="mi">1</code><code>  </code><a class="co" id="co_reinforcement_learning_CO5-1" href="#callout_reinforcement_learning_CO5-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>
</code><code>             </code><code class="n">treward</code><code> </code><code class="o">=</code><code> </code><code class="n">run_episode</code><code class="p">(</code><code class="n">env</code><code class="p">,</code><code> </code><code class="n">weights</code><code class="p">)</code><code>  </code><a class="co" id="co_reinforcement_learning_CO5-2" href="#callout_reinforcement_learning_CO5-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a><code>
</code><code>             </code><code class="k">if</code><code> </code><code class="n">treward</code><code> </code><code class="o">&gt;</code><code> </code><code class="n">besttreward</code><code class="p">:</code><code>  </code><a class="co" id="co_reinforcement_learning_CO5-3" href="#callout_reinforcement_learning_CO5-3"><img src="Images/3.png" alt="3" width="12" height="12"/></a><code>
</code><code>                 </code><code class="n">besttreward</code><code> </code><code class="o">=</code><code> </code><code class="n">treward</code><code>  </code><a class="co" id="co_reinforcement_learning_CO5-4" href="#callout_reinforcement_learning_CO5-4"><img src="Images/4.png" alt="4" width="12" height="12"/></a><code>
</code><code>                 </code><code class="n">bestweights</code><code> </code><code class="o">=</code><code> </code><code class="n">weights</code><code>  </code><a class="co" id="co_reinforcement_learning_CO5-5" href="#callout_reinforcement_learning_CO5-5"><img src="Images/5.png" alt="5" width="12" height="12"/></a><code>
</code><code>                 </code><code class="k">if</code><code> </code><code class="n">treward</code><code> </code><code class="o">==</code><code> </code><code class="mi">200</code><code class="p">:</code><code>
</code><code>                     </code><code class="k">print</code><code class="p">(</code><code class="n">f</code><code class="s1">'</code><code class="s1">SUCCESS | episode={e}</code><code class="s1">'</code><code class="p">)</code><code>
</code><code>                     </code><code class="k">break</code><code>
</code><code>                 </code><code class="k">print</code><code class="p">(</code><code class="n">f</code><code class="s1">'</code><code class="s1">UPDATE  | episode={e}</code><code class="s1">'</code><code class="p">)</code><code>
</code><code>         </code><code class="n">UPDATE</code><code>  </code><code class="o">|</code><code> </code><code class="n">episode</code><code class="o">=</code><code class="mi">1</code><code>
</code><code>         </code><code class="n">UPDATE</code><code>  </code><code class="o">|</code><code> </code><code class="n">episode</code><code class="o">=</code><code class="mi">2</code><code>
</code><code>         </code><code class="n">SUCCESS</code><code> </code><code class="o">|</code><code> </code><code class="n">episode</code><code class="o">=</code><code class="mi">13</code><code>
</code><code>
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">31</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">weights</code><code>
</code><code class="n">Out</code><code class="p">[</code><code class="mi">31</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">array</code><code class="p">(</code><code class="p">[</code><code class="o">-</code><code class="mf">0.4282</code><code class="p">,</code><code>  </code><code class="mf">0.7048</code><code class="p">,</code><code>  </code><code class="mf">0.95</code><code>  </code><code class="p">,</code><code>  </code><code class="mf">0.7697</code><code class="p">]</code><code class="p">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_reinforcement_learning_CO5-1" href="#co_reinforcement_learning_CO5-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>Random weights.</p></dd>
<dt><a class="co" id="callout_reinforcement_learning_CO5-2" href="#co_reinforcement_learning_CO5-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a></dt>
<dd><p>Total reward for these weights.</p></dd>
<dt><a class="co" id="callout_reinforcement_learning_CO5-3" href="#co_reinforcement_learning_CO5-3"><img src="Images/3.png" alt="3" width="12" height="12"/></a></dt>
<dd><p>Improvement observed?</p></dd>
<dt><a class="co" id="callout_reinforcement_learning_CO5-4" href="#co_reinforcement_learning_CO5-4"><img src="Images/4.png" alt="4" width="12" height="12"/></a></dt>
<dd><p>Replace best total reward.</p></dd>
<dt><a class="co" id="callout_reinforcement_learning_CO5-5" href="#co_reinforcement_learning_CO5-5"><img src="Images/5.png" alt="5" width="12" height="12"/></a></dt>
<dd><p>Replace best weights.</p></dd>
</dl>

<p>The <code>CartPole</code> problem is considered solved by an agent if the average total reward over 100 consecutive episodes is 195 or higher. As the following code demonstrates, this is indeed the case for the Monte Carlo agent:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">In</code> <code class="p">[</code><code class="mi">32</code><code class="p">]:</code> <code class="n">res</code> <code class="o">=</code> <code class="p">[]</code>
         <code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">100</code><code class="p">):</code>
             <code class="n">treward</code> <code class="o">=</code> <code class="n">run_episode</code><code class="p">(</code><code class="n">env</code><code class="p">,</code> <code class="n">weights</code><code class="p">)</code>
             <code class="n">res</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">treward</code><code class="p">)</code>
         <code class="n">res</code><code class="p">[:</code><code class="mi">10</code><code class="p">]</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">32</code><code class="p">]:</code> <code class="p">[</code><code class="mf">200.0</code><code class="p">,</code> <code class="mf">200.0</code><code class="p">,</code> <code class="mf">200.0</code><code class="p">,</code> <code class="mf">200.0</code><code class="p">,</code> <code class="mf">200.0</code><code class="p">,</code> <code class="mf">200.0</code><code class="p">,</code> <code class="mf">200.0</code><code class="p">,</code> <code class="mf">200.0</code><code class="p">,</code> <code class="mf">200.0</code><code class="p">,</code> <code class="mf">200.0</code><code class="p">]</code>

<code class="n">In</code> <code class="p">[</code><code class="mi">33</code><code class="p">]:</code> <code class="nb">sum</code><code class="p">(</code><code class="n">res</code><code class="p">)</code> <code class="o">/</code> <code class="nb">len</code><code class="p">(</code><code class="n">res</code><code class="p">)</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">33</code><code class="p">]:</code> <code class="mf">200.0</code></pre>

<p>This is, of course, a strong benchmark that other, more sophisticated approaches are up against.<a data-type="indexterm" data-primary="" data-startref="ix_agent_MonteCarlo" id="idm45625282673144"/><a data-type="indexterm" data-primary="" data-startref="ix_cartpole_MonteCarlo" id="idm45625282502792"/><a data-type="indexterm" data-primary="" data-startref="ix_montecarlo_cartpole" id="idm45625282501880"/><a data-type="indexterm" data-primary="" data-startref="ix_RL_montecarlo_agent" id="idm45625282500936"/></p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Neural Network Agent"><div class="sect1" id="rl_nn_agent">
<h1>Neural Network Agent</h1>

<p><a data-type="indexterm" data-primary="RL (reinforcement learning)" data-secondary="neural network agent" id="ix_RL_nn_agent_ch9"/><a data-type="indexterm" data-primary="neural networks" data-secondary="CartPole game" id="ix_nn_app_cartpole_ch9"/><a data-type="indexterm" data-primary="CartPole game" data-secondary="neural network approach" id="ix_cartpole_nn_app_ch9"/>The <code>CartPole</code> game can be cast into a classification setting as well. The state of an environment consists of four feature values. The correct action given the feature values is the label. By interacting with the environment, a neural network agent can collect a data set consisting of combinations of feature values and labels. Given this incrementally growing data set, a neural network can be trained to learn the correct action given a state of the environment. The neural network represents the policy in this case. The agent updates the policy based on new experiences.</p>
<div style="page-break-after: always;"/>

<p>First, some imports:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">In</code> <code class="p">[</code><code class="mi">34</code><code class="p">]:</code> <code class="kn">import</code> <code class="nn">tensorflow</code> <code class="kn">as</code> <code class="nn">tf</code>
         <code class="kn">from</code> <code class="nn">keras.layers</code> <code class="kn">import</code> <code class="n">Dense</code><code class="p">,</code> <code class="n">Dropout</code>
         <code class="kn">from</code> <code class="nn">keras.models</code> <code class="kn">import</code> <code class="n">Sequential</code>
         <code class="kn">from</code> <code class="nn">keras.optimizers</code> <code class="kn">import</code> <code class="n">Adam</code><code class="p">,</code> <code class="n">RMSprop</code>
         <code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">accuracy_score</code>
         <code class="n">Using</code> <code class="n">TensorFlow</code> <code class="n">backend</code><code class="o">.</code>

<code class="n">In</code> <code class="p">[</code><code class="mi">35</code><code class="p">]:</code> <code class="k">def</code> <code class="nf">set_seeds</code><code class="p">(</code><code class="n">seed</code><code class="o">=</code><code class="mi">100</code><code class="p">):</code>
             <code class="n">random</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="n">seed</code><code class="p">)</code>
             <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="n">seed</code><code class="p">)</code>
             <code class="n">tf</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">set_seed</code><code class="p">(</code><code class="n">seed</code><code class="p">)</code>
             <code class="n">env</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="n">seed</code><code class="p">)</code>
             <code class="n">env</code><code class="o">.</code><code class="n">action_space</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="mi">100</code><code class="p">)</code></pre>

<p><a data-type="indexterm" data-primary="NNAgent class" id="idm45625282490568"/>Second is the <code>NNAgent</code> class that combines the major elements of the agent: the neural network model for the policy, choosing an action given the policy, updating the policy (training the neural network), and the learning process itself over a number of 
<span class="keep-together">episodes.</span> <a data-type="indexterm" data-primary="exploration, in RL" id="idm45625282405384"/><a data-type="indexterm" data-primary="exploitation, in RL" id="idm45625282404648"/>The agent uses both <em>exploration</em> and <em>exploitation</em> to choose an action. Exploration refers to a random action, independent of the current policy. Exploitation refers to an action as derived from the current policy. The idea is that some degree of exploration ensures a richer experience and thereby improved learning for the agent:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">36</code><code class="p">]</code><code class="p">:</code><code> </code><code class="k">class</code><code> </code><code class="nc">NNAgent</code><code class="p">:</code><code>
</code><code>             </code><code class="k">def</code><code> </code><code class="nf-Magic">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">)</code><code class="p">:</code><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">max</code><code> </code><code class="o">=</code><code> </code><code class="mi">0</code><code>  </code><a class="co" id="co_reinforcement_learning_CO6-1" href="#callout_reinforcement_learning_CO6-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">scores</code><code> </code><code class="o">=</code><code> </code><code class="nb">list</code><code class="p">(</code><code class="p">)</code><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">memory</code><code> </code><code class="o">=</code><code> </code><code class="nb">list</code><code class="p">(</code><code class="p">)</code><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">model</code><code> </code><code class="o">=</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">_build_model</code><code class="p">(</code><code class="p">)</code><code>
</code><code>
</code><code>             </code><code class="k">def</code><code> </code><code class="nf">_build_model</code><code class="p">(</code><code class="bp">self</code><code class="p">)</code><code class="p">:</code><code>  </code><a class="co" id="co_reinforcement_learning_CO6-2" href="#callout_reinforcement_learning_CO6-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a><code>
</code><code>                 </code><code class="n">model</code><code> </code><code class="o">=</code><code> </code><code class="n">Sequential</code><code class="p">(</code><code class="p">)</code><code>
</code><code>                 </code><code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dense</code><code class="p">(</code><code class="mi">24</code><code class="p">,</code><code> </code><code class="n">input_dim</code><code class="o">=</code><code class="mi">4</code><code class="p">,</code><code>
</code><code>                                 </code><code class="n">activation</code><code class="o">=</code><code class="s1">'</code><code class="s1">relu</code><code class="s1">'</code><code class="p">)</code><code class="p">)</code><code>
</code><code>                 </code><code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dense</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code><code> </code><code class="n">activation</code><code class="o">=</code><code class="s1">'</code><code class="s1">sigmoid</code><code class="s1">'</code><code class="p">)</code><code class="p">)</code><code>
</code><code>                 </code><code class="n">model</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">loss</code><code class="o">=</code><code class="s1">'</code><code class="s1">binary_crossentropy</code><code class="s1">'</code><code class="p">,</code><code>
</code><code>                               </code><code class="n">optimizer</code><code class="o">=</code><code class="n">RMSprop</code><code class="p">(</code><code class="n">lr</code><code class="o">=</code><code class="mf">0.001</code><code class="p">)</code><code class="p">)</code><code>
</code><code>                 </code><code class="k">return</code><code> </code><code class="n">model</code><code>
</code><code>
</code><code>             </code><code class="k">def</code><code> </code><code class="nf">act</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code><code> </code><code class="n">state</code><code class="p">)</code><code class="p">:</code><code>  </code><a class="co" id="co_reinforcement_learning_CO6-3" href="#callout_reinforcement_learning_CO6-3"><img src="Images/3.png" alt="3" width="12" height="12"/></a><code>
</code><code>                 </code><code class="k">if</code><code> </code><code class="n">random</code><code class="o">.</code><code class="n">random</code><code class="p">(</code><code class="p">)</code><code> </code><code class="o">&lt;</code><code class="o">=</code><code> </code><code class="mf">0.5</code><code class="p">:</code><code>
</code><code>                     </code><code class="k">return</code><code> </code><code class="n">env</code><code class="o">.</code><code class="n">action_space</code><code class="o">.</code><code class="n">sample</code><code class="p">(</code><code class="p">)</code><code>
</code><code>                 </code><code class="n">action</code><code> </code><code class="o">=</code><code> </code><code class="n">np</code><code class="o">.</code><code class="n">where</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code>
</code><code>                     </code><code class="n">state</code><code class="p">,</code><code> </code><code class="n">batch_size</code><code class="o">=</code><code class="bp">None</code><code class="p">)</code><code class="p">[</code><code class="mi">0</code><code class="p">,</code><code> </code><code class="mi">0</code><code class="p">]</code><code> </code><code class="o">&gt;</code><code> </code><code class="mf">0.5</code><code class="p">,</code><code> </code><code class="mi">1</code><code class="p">,</code><code> </code><code class="mi">0</code><code class="p">)</code><code>
</code><code>                 </code><code class="k">return</code><code> </code><code class="n">action</code><code>
</code><code>
</code><code>             </code><code class="k">def</code><code> </code><code class="nf">train_model</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code><code> </code><code class="n">state</code><code class="p">,</code><code> </code><code class="n">action</code><code class="p">)</code><code class="p">:</code><code>  </code><a class="co" id="co_reinforcement_learning_CO6-4" href="#callout_reinforcement_learning_CO6-4"><img src="Images/4.png" alt="4" width="12" height="12"/></a><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">state</code><code class="p">,</code><code> </code><code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="p">[</code><code class="n">action</code><code class="p">,</code><code class="p">]</code><code class="p">)</code><code class="p">,</code><code>
</code><code>                                </code><code class="n">epochs</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code><code> </code><code class="n">verbose</code><code class="o">=</code><code class="bp">False</code><code class="p">)</code><code>
</code><code>
</code><code>             </code><code class="k">def</code><code> </code><code class="nf">learn</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code><code> </code><code class="n">episodes</code><code class="p">)</code><code class="p">:</code><code>  </code><a class="co" id="co_reinforcement_learning_CO6-5" href="#callout_reinforcement_learning_CO6-5"><img src="Images/5.png" alt="5" width="12" height="12"/></a><code>
</code><code>                 </code><code class="k">for</code><code> </code><code class="n">e</code><code> </code><code class="ow">in</code><code> </code><code class="nb">range</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code><code> </code><code class="n">episodes</code><code> </code><code class="o">+</code><code> </code><code class="mi">1</code><code class="p">)</code><code class="p">:</code><code>
</code><code>                     </code><code class="n">state</code><code> </code><code class="o">=</code><code> </code><code class="n">env</code><code class="o">.</code><code class="n">reset</code><code class="p">(</code><code class="p">)</code><code>
</code><code>                     </code><code class="k">for</code><code> </code><code class="n">_</code><code> </code><code class="ow">in</code><code> </code><code class="nb">range</code><code class="p">(</code><code class="mi">201</code><code class="p">)</code><code class="p">:</code><code>
</code><code>                         </code><code class="n">state</code><code> </code><code class="o">=</code><code> </code><code class="n">np</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">state</code><code class="p">,</code><code> </code><code class="p">[</code><code class="mi">1</code><code class="p">,</code><code> </code><code class="mi">4</code><code class="p">]</code><code class="p">)</code><code>
</code><code>                         </code><code class="n">action</code><code> </code><code class="o">=</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">act</code><code class="p">(</code><code class="n">state</code><code class="p">)</code><code>
</code><code>                         </code><code class="n">next_state</code><code class="p">,</code><code> </code><code class="n">reward</code><code class="p">,</code><code> </code><code class="n">done</code><code class="p">,</code><code> </code><code class="n">info</code><code> </code><code class="o">=</code><code> </code><code class="n">env</code><code class="o">.</code><code class="n">step</code><code class="p">(</code><code class="n">action</code><code class="p">)</code><code>
</code><code>                         </code><code class="k">if</code><code> </code><code class="n">done</code><code class="p">:</code><code>
</code><code>                             </code><code class="n">score</code><code> </code><code class="o">=</code><code> </code><code class="n">_</code><code> </code><code class="o">+</code><code> </code><code class="mi">1</code><code>
</code><code>                             </code><code class="bp">self</code><code class="o">.</code><code class="n">scores</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">score</code><code class="p">)</code><code>
</code><code>                             </code><code class="bp">self</code><code class="o">.</code><code class="n">max</code><code> </code><code class="o">=</code><code> </code><code class="nb">max</code><code class="p">(</code><code class="n">score</code><code class="p">,</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">max</code><code class="p">)</code><code>  </code><a class="co" id="co_reinforcement_learning_CO6-6" href="#callout_reinforcement_learning_CO6-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>
</code><code>                             </code><code class="k">print</code><code class="p">(</code><code class="s1">'</code><code class="s1">episode: {:4d}/{} | score: {:3d} | max: {:3d}</code><code class="s1">'</code><code>
</code><code>                                   </code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">e</code><code class="p">,</code><code> </code><code class="n">episodes</code><code class="p">,</code><code> </code><code class="n">score</code><code class="p">,</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">max</code><code class="p">)</code><code class="p">,</code><code> </code><code class="n">end</code><code class="o">=</code><code class="s1">'</code><code class="se">\r</code><code class="s1">'</code><code class="p">)</code><code>
</code><code>                             </code><code class="k">break</code><code>
</code><code>                         </code><code class="bp">self</code><code class="o">.</code><code class="n">memory</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="p">(</code><code class="n">state</code><code class="p">,</code><code> </code><code class="n">action</code><code class="p">)</code><code class="p">)</code><code>
</code><code>                         </code><code class="bp">self</code><code class="o">.</code><code class="n">train_model</code><code class="p">(</code><code class="n">state</code><code class="p">,</code><code> </code><code class="n">action</code><code class="p">)</code><code>  </code><a class="co" id="co_reinforcement_learning_CO6-7" href="#callout_reinforcement_learning_CO6-4"><img src="Images/4.png" alt="4" width="12" height="12"/></a><code>
</code><code>                         </code><code class="n">state</code><code> </code><code class="o">=</code><code> </code><code class="n">next_state</code></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_reinforcement_learning_CO6-1" href="#co_reinforcement_learning_CO6-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>The maximum total reward</p></dd>
<dt><a class="co" id="callout_reinforcement_learning_CO6-2" href="#co_reinforcement_learning_CO6-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a></dt>
<dd><p>The DNN classification model for the policy</p></dd>
<dt><a class="co" id="callout_reinforcement_learning_CO6-3" href="#co_reinforcement_learning_CO6-3"><img src="Images/3.png" alt="3" width="12" height="12"/></a></dt>
<dd><p>The method to choose an action (exploration and exploitation)</p></dd>
<dt><a class="co" id="callout_reinforcement_learning_CO6-4" href="#co_reinforcement_learning_CO6-4"><img src="Images/4.png" alt="4" width="12" height="12"/></a></dt>
<dd><p>The method to update the policy (train the neural network)</p></dd>
<dt><a class="co" id="callout_reinforcement_learning_CO6-5" href="#co_reinforcement_learning_CO6-5"><img src="Images/5.png" alt="5" width="12" height="12"/></a></dt>
<dd><p>The method to learn from interacting with the environment</p></dd>
</dl>

<p>The neural network agent does not solve the problem for the configuration shown. The maximum total reward of 200 is not achieved even once:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">37</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">set_seeds</code><code class="p">(</code><code class="mi">100</code><code class="p">)</code><code>
</code><code>         </code><code class="n">agent</code><code> </code><code class="o">=</code><code> </code><code class="n">NNAgent</code><code class="p">(</code><code class="p">)</code><code>
</code><code>
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">38</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">episodes</code><code> </code><code class="o">=</code><code> </code><code class="mi">500</code><code>
</code><code>
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">39</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">agent</code><code class="o">.</code><code class="n">learn</code><code class="p">(</code><code class="n">episodes</code><code class="p">)</code><code>
</code><code>         </code><code class="n">episode</code><code class="p">:</code><code>  </code><code class="mi">500</code><code class="o">/</code><code class="mi">500</code><code> </code><code class="o">|</code><code> </code><code class="n">score</code><code class="p">:</code><code>  </code><code class="mi">11</code><code> </code><code class="o">|</code><code> </code><code class="nb">max</code><code class="p">:</code><code>  </code><code class="mi">44</code><code>
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">40</code><code class="p">]</code><code class="p">:</code><code> </code><code class="nb">sum</code><code class="p">(</code><code class="n">agent</code><code class="o">.</code><code class="n">scores</code><code class="p">)</code><code> </code><code class="o">/</code><code> </code><code class="nb">len</code><code class="p">(</code><code class="n">agent</code><code class="o">.</code><code class="n">scores</code><code class="p">)</code><code>  </code><a class="co" id="co_reinforcement_learning_CO7-1" href="#callout_reinforcement_learning_CO7-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>
</code><code class="n">Out</code><code class="p">[</code><code class="mi">40</code><code class="p">]</code><code class="p">:</code><code> </code><code class="mf">13.682</code></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_reinforcement_learning_CO7-1" href="#co_reinforcement_learning_CO7-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>Average total reward over all episodes</p></dd>
</dl>

<p>Something seems to be missing with this approach. One major missing element is the idea of looking beyond the current state and action to be chosen. The approach implemented does not, by any means, take into account that success is only achieved when the agent survives 200 consecutive steps. Simply speaking, the agent avoids taking the wrong action but does not learn to win the game.</p>

<p>Analyzing the collected history of states (features) and actions (labels) reveals that the neural network reaches an accuracy of around 75%.</p>

<p>However, this does not translate into a winning policy as seen before:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">41</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">f</code><code> </code><code class="o">=</code><code> </code><code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="p">[</code><code class="n">m</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code> </code><code class="k">for</code><code> </code><code class="n">m</code><code> </code><code class="ow">in</code><code> </code><code class="n">agent</code><code class="o">.</code><code class="n">memory</code><code class="p">]</code><code class="p">)</code><code>  </code><a class="co" id="co_reinforcement_learning_CO8-1" href="#callout_reinforcement_learning_CO8-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>
</code><code>         </code><code class="n">f</code><code>  </code><a class="co" id="co_reinforcement_learning_CO8-2" href="#callout_reinforcement_learning_CO8-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>
</code><code class="n">Out</code><code class="p">[</code><code class="mi">41</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">array</code><code class="p">(</code><code class="p">[</code><code class="p">[</code><code class="o">-</code><code class="mf">0.0163</code><code class="p">,</code><code>  </code><code class="mf">0.0238</code><code class="p">,</code><code> </code><code class="o">-</code><code class="mf">0.0392</code><code class="p">,</code><code> </code><code class="o">-</code><code class="mf">0.0148</code><code class="p">]</code><code class="p">,</code><code>
</code><code>                </code><code class="p">[</code><code class="o">-</code><code class="mf">0.0158</code><code class="p">,</code><code>  </code><code class="mf">0.2195</code><code class="p">,</code><code> </code><code class="o">-</code><code class="mf">0.0395</code><code class="p">,</code><code> </code><code class="o">-</code><code class="mf">0.3196</code><code class="p">]</code><code class="p">,</code><code>
</code><code>                </code><code class="p">[</code><code class="o">-</code><code class="mf">0.0114</code><code class="p">,</code><code>  </code><code class="mf">0.0249</code><code class="p">,</code><code> </code><code class="o">-</code><code class="mf">0.0459</code><code class="p">,</code><code> </code><code class="o">-</code><code class="mf">0.0396</code><code class="p">]</code><code class="p">,</code><code>
</code><code>                </code><code class="o">.</code><code class="o">.</code><code class="o">.</code><code class="p">,</code><code>
</code><code>                </code><code class="p">[</code><code> </code><code class="mf">0.0603</code><code class="p">,</code><code>  </code><code class="mf">0.9682</code><code class="p">,</code><code> </code><code class="o">-</code><code class="mf">0.0852</code><code class="p">,</code><code> </code><code class="o">-</code><code class="mf">1.4595</code><code class="p">]</code><code class="p">,</code><code>
</code><code>                </code><code class="p">[</code><code> </code><code class="mf">0.0797</code><code class="p">,</code><code>  </code><code class="mf">1.1642</code><code class="p">,</code><code> </code><code class="o">-</code><code class="mf">0.1144</code><code class="p">,</code><code> </code><code class="o">-</code><code class="mf">1.7776</code><code class="p">]</code><code class="p">,</code><code>
</code><code>                </code><code class="p">[</code><code> </code><code class="mf">0.103</code><code> </code><code class="p">,</code><code>  </code><code class="mf">1.3604</code><code class="p">,</code><code> </code><code class="o">-</code><code class="mf">0.15</code><code>  </code><code class="p">,</code><code> </code><code class="o">-</code><code class="mf">2.1035</code><code class="p">]</code><code class="p">]</code><code class="p">)</code><code>
</code><code>
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">42</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">l</code><code> </code><code class="o">=</code><code> </code><code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="p">[</code><code class="n">m</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code><code> </code><code class="k">for</code><code> </code><code class="n">m</code><code> </code><code class="ow">in</code><code> </code><code class="n">agent</code><code class="o">.</code><code class="n">memory</code><code class="p">]</code><code class="p">)</code><code>  </code><a class="co" id="co_reinforcement_learning_CO8-3" href="#callout_reinforcement_learning_CO8-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a><code>
</code><code>         </code><code class="n">l</code><code>  </code><a class="co" id="co_reinforcement_learning_CO8-4" href="#callout_reinforcement_learning_CO8-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a><code>
</code><code class="n">Out</code><code class="p">[</code><code class="mi">42</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">array</code><code class="p">(</code><code class="p">[</code><code class="mi">1</code><code class="p">,</code><code> </code><code class="mi">0</code><code class="p">,</code><code> </code><code class="mi">1</code><code class="p">,</code><code> </code><code class="o">.</code><code class="o">.</code><code class="o">.</code><code class="p">,</code><code> </code><code class="mi">1</code><code class="p">,</code><code> </code><code class="mi">1</code><code class="p">,</code><code> </code><code class="mi">1</code><code class="p">]</code><code class="p">)</code><code>
</code><code>
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">43</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">accuracy_score</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">where</code><code class="p">(</code><code class="n">agent</code><code class="o">.</code><code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">f</code><code class="p">)</code><code> </code><code class="o">&gt;</code><code> </code><code class="mf">0.5</code><code class="p">,</code><code> </code><code class="mi">1</code><code class="p">,</code><code> </code><code class="mi">0</code><code class="p">)</code><code class="p">,</code><code> </code><code class="n">l</code><code class="p">)</code><code>
</code><code class="n">Out</code><code class="p">[</code><code class="mi">43</code><code class="p">]</code><code class="p">:</code><code> </code><code class="mf">0.7525626872733008</code></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_reinforcement_learning_CO8-1" href="#co_reinforcement_learning_CO8-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>Features (states) from all episodes</p></dd>
<dt><a class="co" id="callout_reinforcement_learning_CO8-2" href="#co_reinforcement_learning_CO8-3"><img src="Images/2.png" alt="2" width="12" height="12"/></a></dt>
<dd><p>Labels (actions) from all episodes<a data-type="indexterm" data-primary="" data-startref="ix_cartpole_nn_app_ch9" id="idm45625283360728"/><a data-type="indexterm" data-primary="" data-startref="ix_nn_app_cartpole_ch9" id="idm45625283359752"/><a data-type="indexterm" data-primary="" data-startref="ix_RL_nn_agent_ch9" id="idm45625283171320"/></p></dd>
</dl>
</div></section>













<section data-type="sect1" data-pdf-bookmark="DQL Agent"><div class="sect1" id="rl_dql_agent">
<h1>DQL Agent</h1>

<p><a data-type="indexterm" data-primary="RL (reinforcement learning)" data-secondary="DQLAgent" id="ix_RL_dql_agent_ch9"/><a data-type="indexterm" data-primary="QL (Q-learning)" data-secondary="DQLAgent" id="ix_ql_dql_agent_ch9"/><a data-type="indexterm" data-primary="DQLAgent, in RL" id="ix_dql_agent_ch9"/><a data-type="indexterm" data-primary="agents" data-secondary="DQLAgent" id="ix_agents_dql"/><a data-type="indexterm" data-primary="agents" data-secondary="QL agent" id="ix_agent_ql_ch9"/><a data-type="indexterm" data-primary="CartPole game" data-secondary="DQL agent" id="ix_cartpole_dqlagent"/>Q-learning (QL) is an algorithm that takes into account delayed rewards in addition to immediate rewards from an action. The algorithm is due to Watkins (1989) and Watkins and Dayan (1992) and is explained in detail in Sutton and Barto (2018, ch. 6). QL addresses the problem of looking beyond the immediate next reward as encountered with the neural network agent.</p>

<p>The algorithm works roughly as follows. <a data-type="indexterm" data-primary="action-value policy, in QL" id="idm45625281690616"/>There is an <em>action-value</em> policy <math alttext="upper Q">
  <mi>Q</mi>
</math>, which assigns a value to every combination of a state and an action. The higher the value is, the better the action from the point of view of the agent will be. If the agent uses the policy <math alttext="upper Q">
  <mi>Q</mi>
</math> to choose an action, it selects the action with the highest value.</p>

<p>How is the value of an action derived? <a data-type="indexterm" data-primary="direct reward, in QL" id="idm45625281686328"/><a data-type="indexterm" data-primary="discounted value, in QL" id="idm45625281685624"/>The value of an action is composed of its <em>direct reward</em> and the <em>discounted value</em> of the optimal action in the next state. The following is the formal expression:</p>
<div data-type="equation">
<math alttext="upper Q left-parenthesis upper S Subscript t Baseline comma upper A Subscript t Baseline right-parenthesis equals upper R Subscript t plus 1 Baseline plus gamma max Underscript a Endscripts upper Q left-parenthesis upper S Subscript t plus 1 Baseline comma a right-parenthesis" display="block">
  <mrow>
    <mi>Q</mi>
    <mrow>
      <mo>(</mo>
      <msub><mi>S</mi> <mi>t</mi> </msub>
      <mo>,</mo>
      <msub><mi>A</mi> <mi>t</mi> </msub>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <msub><mi>R</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow> </msub>
    <mo>+</mo>
    <mi>γ</mi>
    <munder><mo movablelimits="true" form="prefix">max</mo> <mi>a</mi> </munder>
    <mi>Q</mi>
    <mrow>
      <mo>(</mo>
      <msub><mi>S</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow> </msub>
      <mo>,</mo>
      <mi>a</mi>
      <mo>)</mo>
    </mrow>
  </mrow>
</math>
</div>

<p>Here, <math alttext="upper S Subscript t">
  <msub><mi>S</mi> <mi>t</mi> </msub>
</math> is the state at step (time) <math alttext="t">
  <mi>t</mi>
</math>, <math alttext="upper A Subscript t">
  <msub><mi>A</mi> <mi>t</mi> </msub>
</math> is the action taken at state <math alttext="upper S Subscript t">
  <msub><mi>S</mi> <mi>t</mi> </msub>
</math>, <math alttext="upper R Subscript t plus 1">
  <msub><mi>R</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow> </msub>
</math> is the direct reward of action <math alttext="upper A Subscript t">
  <msub><mi>A</mi> <mi>t</mi> </msub>
</math>, <math alttext="0 less-than gamma less-than 1">
  <mrow>
    <mn>0</mn>
    <mo>&lt;</mo>
    <mi>γ</mi>
    <mo>&lt;</mo>
    <mn>1</mn>
  </mrow>
</math> is a discount factor, and <math alttext="max Underscript a Endscripts upper Q left-parenthesis upper S Subscript t plus 1 Baseline comma a right-parenthesis">
  <mrow>
    <msub><mo movablelimits="true" form="prefix">max</mo> <mi>a</mi> </msub>
    <mi>Q</mi>
    <mrow>
      <mo>(</mo>
      <msub><mi>S</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow> </msub>
      <mo>,</mo>
      <mi>a</mi>
      <mo>)</mo>
    </mrow>
  </mrow>
</math> is the maximum delayed reward given the optimal action from the current policy <math alttext="upper Q">
  <mi>Q</mi>
</math>.</p>

<p>In a simple environment, with only a limited number of possible states, <math alttext="upper Q">
  <mi>Q</mi>
</math> can, for example, be represented by a <em>table</em>, listing for every state-action combination the corresponding value. However, in more interesting or complex settings, such as the 
<span class="keep-together"><code>CartPole</code></span> environment, the number of states is too large for <math alttext="upper Q">
  <mi>Q</mi>
</math> to be written out comprehensively. Therefore, <math alttext="upper Q">
  <mi>Q</mi>
</math> is in general understood to be a <em>function</em>.</p>

<p><a data-type="indexterm" data-primary="neural networks" data-secondary="Q-Learning" id="ix_nn_ql_ch9"/>This is where neural networks come into play. In realistic settings and environments, a closed-form solution for the function <math alttext="upper Q">
  <mi>Q</mi>
</math> might not exist or might be too hard to derive, say, based on dynamic programming. Therefore, QL algorithms generally target <em>approximations</em> only. Neural networks, with their universal approximation capabilities, are a natural choice to accomplish the approximation of <math alttext="upper Q">
  <mi>Q</mi>
</math>.</p>

<p>Another critical element of QL is <em>replay</em>. The QL agent replays a number of experiences (state-action combinations) to update the policy function <math alttext="upper Q">
  <mi>Q</mi>
</math> regularly. This can improve the learning considerably. Furthermore, the QL agent presented in the following—<code>DQLAgent</code>—also alternates between exploration and exploitation during the learning. The alternation is done in a systematic way in that the agent starts with exploration only—in the beginning it could not have learned anything—and slowly but steadily decreases the exploration rate <math alttext="epsilon">
  <mi>ϵ</mi>
</math> until it reaches a minimum level:<sup><a data-type="noteref" id="idm45625283311016-marker" href="ch09.xhtml#idm45625283311016">3</a></sup></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">44</code><code class="p">]</code><code class="p">:</code><code> </code><code class="kn">from</code><code> </code><code class="nn">collections</code><code> </code><code class="kn">import</code><code> </code><code class="n">deque</code><code>
</code><code>         </code><code class="kn">from</code><code> </code><code class="nn">keras.optimizers</code><code> </code><code class="kn">import</code><code> </code><code class="n">Adam</code><code class="p">,</code><code> </code><code class="n">RMSprop</code><code>
</code><code>
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">45</code><code class="p">]</code><code class="p">:</code><code> </code><code class="k">class</code><code> </code><code class="nc">DQLAgent</code><code class="p">:</code><code>
</code><code>             </code><code class="k">def</code><code> </code><code class="nf-Magic">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code><code> </code><code class="n">gamma</code><code class="o">=</code><code class="mf">0.95</code><code class="p">,</code><code> </code><code class="n">hu</code><code class="o">=</code><code class="mi">24</code><code class="p">,</code><code> </code><code class="n">opt</code><code class="o">=</code><code class="n">Adam</code><code class="p">,</code><code>
</code><code>                    </code><code class="n">lr</code><code class="o">=</code><code class="mf">0.001</code><code class="p">,</code><code> </code><code class="n">finish</code><code class="o">=</code><code class="bp">False</code><code class="p">)</code><code class="p">:</code><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">finish</code><code> </code><code class="o">=</code><code> </code><code class="n">finish</code><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">epsilon</code><code> </code><code class="o">=</code><code> </code><code class="mf">1.0</code><code>  </code><a class="co" id="co_reinforcement_learning_CO9-1" href="#callout_reinforcement_learning_CO9-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">epsilon_min</code><code> </code><code class="o">=</code><code> </code><code class="mf">0.01</code><code>  </code><a class="co" id="co_reinforcement_learning_CO9-2" href="#callout_reinforcement_learning_CO9-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">epsilon_decay</code><code> </code><code class="o">=</code><code> </code><code class="mf">0.995</code><code>  </code><a class="co" id="co_reinforcement_learning_CO9-3" href="#callout_reinforcement_learning_CO9-3"><img src="Images/3.png" alt="3" width="12" height="12"/></a><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">gamma</code><code> </code><code class="o">=</code><code> </code><code class="n">gamma</code><code>  </code><a class="co" id="co_reinforcement_learning_CO9-4" href="#callout_reinforcement_learning_CO9-4"><img src="Images/4.png" alt="4" width="12" height="12"/></a><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">batch_size</code><code> </code><code class="o">=</code><code> </code><code class="mi">32</code><code>  </code><a class="co" id="co_reinforcement_learning_CO9-5" href="#callout_reinforcement_learning_CO9-5"><img src="Images/5.png" alt="5" width="12" height="12"/></a><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">max_treward</code><code> </code><code class="o">=</code><code> </code><code class="mi">0</code><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">averages</code><code> </code><code class="o">=</code><code> </code><code class="nb">list</code><code class="p">(</code><code class="p">)</code><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">memory</code><code> </code><code class="o">=</code><code> </code><code class="n">deque</code><code class="p">(</code><code class="n">maxlen</code><code class="o">=</code><code class="mi">2000</code><code class="p">)</code><code>  </code><a class="co" id="co_reinforcement_learning_CO9-6" href="#callout_reinforcement_learning_CO9-6"><img src="Images/6.png" alt="6" width="12" height="12"/></a><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">osn</code><code> </code><code class="o">=</code><code> </code><code class="n">env</code><code class="o">.</code><code class="n">observation_space</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">model</code><code> </code><code class="o">=</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">_build_model</code><code class="p">(</code><code class="n">hu</code><code class="p">,</code><code> </code><code class="n">opt</code><code class="p">,</code><code> </code><code class="n">lr</code><code class="p">)</code><code>
</code><code>
</code><code>             </code><code class="k">def</code><code> </code><code class="nf">_build_model</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code><code> </code><code class="n">hu</code><code class="p">,</code><code> </code><code class="n">opt</code><code class="p">,</code><code> </code><code class="n">lr</code><code class="p">)</code><code class="p">:</code><code>
</code><code>                 </code><code class="n">model</code><code> </code><code class="o">=</code><code> </code><code class="n">Sequential</code><code class="p">(</code><code class="p">)</code><code>
</code><code>                 </code><code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dense</code><code class="p">(</code><code class="n">hu</code><code class="p">,</code><code> </code><code class="n">input_dim</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">osn</code><code class="p">,</code><code>
</code><code>                                 </code><code class="n">activation</code><code class="o">=</code><code class="s1">'</code><code class="s1">relu</code><code class="s1">'</code><code class="p">)</code><code class="p">)</code><code>
</code><code>                 </code><code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dense</code><code class="p">(</code><code class="n">hu</code><code class="p">,</code><code> </code><code class="n">activation</code><code class="o">=</code><code class="s1">'</code><code class="s1">relu</code><code class="s1">'</code><code class="p">)</code><code class="p">)</code><code>
</code><code>                 </code><code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dense</code><code class="p">(</code><code class="n">env</code><code class="o">.</code><code class="n">action_space</code><code class="o">.</code><code class="n">n</code><code class="p">,</code><code> </code><code class="n">activation</code><code class="o">=</code><code class="s1">'</code><code class="s1">linear</code><code class="s1">'</code><code class="p">)</code><code class="p">)</code><code>
</code><code>                 </code><code class="n">model</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">loss</code><code class="o">=</code><code class="s1">'</code><code class="s1">mse</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="n">optimizer</code><code class="o">=</code><code class="n">opt</code><code class="p">(</code><code class="n">lr</code><code class="o">=</code><code class="n">lr</code><code class="p">)</code><code class="p">)</code><code>
</code><code>                 </code><code class="k">return</code><code> </code><code class="n">model</code><code>
</code><code>
</code><code>             </code><code class="k">def</code><code> </code><code class="nf">act</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code><code> </code><code class="n">state</code><code class="p">)</code><code class="p">:</code><code>
</code><code>                 </code><code class="k">if</code><code> </code><code class="n">random</code><code class="o">.</code><code class="n">random</code><code class="p">(</code><code class="p">)</code><code> </code><code class="o">&lt;</code><code class="o">=</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">epsilon</code><code class="p">:</code><code>
</code><code>                     </code><code class="k">return</code><code> </code><code class="n">env</code><code class="o">.</code><code class="n">action_space</code><code class="o">.</code><code class="n">sample</code><code class="p">(</code><code class="p">)</code><code>
</code><code>                 </code><code class="n">action</code><code> </code><code class="o">=</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">state</code><code class="p">)</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code>
</code><code>                 </code><code class="k">return</code><code> </code><code class="n">np</code><code class="o">.</code><code class="n">argmax</code><code class="p">(</code><code class="n">action</code><code class="p">)</code><code>
</code><code>
</code><code>             </code><code class="k">def</code><code> </code><code class="nf">replay</code><code class="p">(</code><code class="bp">self</code><code class="p">)</code><code class="p">:</code><code>
</code><code>                 </code><code class="n">batch</code><code> </code><code class="o">=</code><code> </code><code class="n">random</code><code class="o">.</code><code class="n">sample</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">memory</code><code class="p">,</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">batch_size</code><code class="p">)</code><code>  </code><a class="co" id="co_reinforcement_learning_CO9-7" href="#callout_reinforcement_learning_CO9-7"><img src="Images/7.png" alt="7" width="12" height="12"/></a><code>
</code><code>                 </code><code class="k">for</code><code> </code><code class="n">state</code><code class="p">,</code><code> </code><code class="n">action</code><code class="p">,</code><code> </code><code class="n">reward</code><code class="p">,</code><code> </code><code class="n">next_state</code><code class="p">,</code><code> </code><code class="n">done</code><code> </code><code class="ow">in</code><code> </code><code class="n">batch</code><code class="p">:</code><code>
</code><code>                     </code><code class="k">if</code><code> </code><code class="ow">not</code><code> </code><code class="n">done</code><code class="p">:</code><code>
</code><code>                         </code><code class="n">reward</code><code> </code><code class="o">+</code><code class="o">=</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">gamma</code><code> </code><code class="o">*</code><code> </code><code class="n">np</code><code class="o">.</code><code class="n">amax</code><code class="p">(</code><code>
</code><code>                             </code><code class="bp">self</code><code class="o">.</code><code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">next_state</code><code class="p">)</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="p">)</code><code>  </code><a class="co" id="co_reinforcement_learning_CO9-8" href="#callout_reinforcement_learning_CO9-8"><img src="Images/8.png" alt="8" width="12" height="12"/></a><code>
</code><code>                     </code><code class="n">target</code><code> </code><code class="o">=</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">state</code><code class="p">)</code><code>
</code><code>                     </code><code class="n">target</code><code class="p">[</code><code class="mi">0</code><code class="p">,</code><code> </code><code class="n">action</code><code class="p">]</code><code> </code><code class="o">=</code><code> </code><code class="n">reward</code><code>
</code><code>                     </code><code class="bp">self</code><code class="o">.</code><code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">state</code><code class="p">,</code><code> </code><code class="n">target</code><code class="p">,</code><code> </code><code class="n">epochs</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code><code>
</code><code>                                    </code><code class="n">verbose</code><code class="o">=</code><code class="bp">False</code><code class="p">)</code><code>  </code><a class="co" id="co_reinforcement_learning_CO9-9" href="#callout_reinforcement_learning_CO9-9"><img src="Images/9.png" alt="9" width="12" height="12"/></a><code>
</code><code>                 </code><code class="k">if</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">epsilon</code><code> </code><code class="o">&gt;</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">epsilon_min</code><code class="p">:</code><code>
</code><code>                     </code><code class="bp">self</code><code class="o">.</code><code class="n">epsilon</code><code> </code><code class="o">*</code><code class="o">=</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">epsilon_decay</code><code>  </code><a class="co" id="co_reinforcement_learning_CO9-10" href="#callout_reinforcement_learning_CO9-10"><img src="Images/10.png" alt="10" width="12" height="12"/></a><code>
</code><code>
</code><code>             </code><code class="k">def</code><code> </code><code class="nf">learn</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code><code> </code><code class="n">episodes</code><code class="p">)</code><code class="p">:</code><code>
</code><code>                 </code><code class="n">trewards</code><code> </code><code class="o">=</code><code> </code><code class="p">[</code><code class="p">]</code><code>
</code><code>                 </code><code class="k">for</code><code> </code><code class="n">e</code><code> </code><code class="ow">in</code><code> </code><code class="nb">range</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code><code> </code><code class="n">episodes</code><code> </code><code class="o">+</code><code> </code><code class="mi">1</code><code class="p">)</code><code class="p">:</code><code>
</code><code>                     </code><code class="n">state</code><code> </code><code class="o">=</code><code> </code><code class="n">env</code><code class="o">.</code><code class="n">reset</code><code class="p">(</code><code class="p">)</code><code>
</code><code>                     </code><code class="n">state</code><code> </code><code class="o">=</code><code> </code><code class="n">np</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">state</code><code class="p">,</code><code> </code><code class="p">[</code><code class="mi">1</code><code class="p">,</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">osn</code><code class="p">]</code><code class="p">)</code><code>
</code><code>                     </code><code class="k">for</code><code> </code><code class="n">_</code><code> </code><code class="ow">in</code><code> </code><code class="nb">range</code><code class="p">(</code><code class="mi">5000</code><code class="p">)</code><code class="p">:</code><code>
</code><code>                         </code><code class="n">action</code><code> </code><code class="o">=</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">act</code><code class="p">(</code><code class="n">state</code><code class="p">)</code><code>
</code><code>                         </code><code class="n">next_state</code><code class="p">,</code><code> </code><code class="n">reward</code><code class="p">,</code><code> </code><code class="n">done</code><code class="p">,</code><code> </code><code class="n">info</code><code> </code><code class="o">=</code><code> </code><code class="n">env</code><code class="o">.</code><code class="n">step</code><code class="p">(</code><code class="n">action</code><code class="p">)</code><code>
</code><code>                         </code><code class="n">next_state</code><code> </code><code class="o">=</code><code> </code><code class="n">np</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">next_state</code><code class="p">,</code><code>
</code><code>                                                 </code><code class="p">[</code><code class="mi">1</code><code class="p">,</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">osn</code><code class="p">]</code><code class="p">)</code><code>
</code><code>                         </code><code class="bp">self</code><code class="o">.</code><code class="n">memory</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="p">[</code><code class="n">state</code><code class="p">,</code><code> </code><code class="n">action</code><code class="p">,</code><code> </code><code class="n">reward</code><code class="p">,</code><code>
</code><code>                                              </code><code class="n">next_state</code><code class="p">,</code><code> </code><code class="n">done</code><code class="p">]</code><code class="p">)</code><code>  </code><a class="co" id="co_reinforcement_learning_CO9-11" href="#callout_reinforcement_learning_CO9-11"><img src="Images/11.png" alt="11" width="12" height="12"/></a><code>
</code><code>                         </code><code class="n">state</code><code> </code><code class="o">=</code><code> </code><code class="n">next_state</code><code>
</code><code>                         </code><code class="k">if</code><code> </code><code class="n">done</code><code class="p">:</code><code>
</code><code>                             </code><code class="n">treward</code><code> </code><code class="o">=</code><code> </code><code class="n">_</code><code> </code><code class="o">+</code><code> </code><code class="mi">1</code><code>
</code><code>                             </code><code class="n">trewards</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">treward</code><code class="p">)</code><code>
</code><code>                             </code><code class="n">av</code><code> </code><code class="o">=</code><code> </code><code class="nb">sum</code><code class="p">(</code><code class="n">trewards</code><code class="p">[</code><code class="o">-</code><code class="mi">25</code><code class="p">:</code><code class="p">]</code><code class="p">)</code><code> </code><code class="o">/</code><code> </code><code class="mi">25</code><code>
</code><code>                             </code><code class="bp">self</code><code class="o">.</code><code class="n">averages</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">av</code><code class="p">)</code><code>
</code><code>                             </code><code class="bp">self</code><code class="o">.</code><code class="n">max_treward</code><code> </code><code class="o">=</code><code> </code><code class="nb">max</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">max_treward</code><code class="p">,</code><code> </code><code class="n">treward</code><code class="p">)</code><code>
</code><code>                             </code><code class="n">templ</code><code> </code><code class="o">=</code><code> </code><code class="s1">'</code><code class="s1">episode: {:4d}/{} | treward: {:4d} | </code><code class="s1">'</code><code>
</code><code>                             </code><code class="n">templ</code><code> </code><code class="o">+</code><code class="o">=</code><code> </code><code class="s1">'</code><code class="s1">av: {:6.1f} | max: {:4d}</code><code class="s1">'</code><code>
</code><code>                             </code><code class="k">print</code><code class="p">(</code><code class="n">templ</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">e</code><code class="p">,</code><code> </code><code class="n">episodes</code><code class="p">,</code><code> </code><code class="n">treward</code><code class="p">,</code><code> </code><code class="n">av</code><code class="p">,</code><code>
</code><code>                                                </code><code class="bp">self</code><code class="o">.</code><code class="n">max_treward</code><code class="p">)</code><code class="p">,</code><code> </code><code class="n">end</code><code class="o">=</code><code class="s1">'</code><code class="se">\r</code><code class="s1">'</code><code class="p">)</code><code>
</code><code>                             </code><code class="k">break</code><code>
</code><code>                     </code><code class="k">if</code><code> </code><code class="n">av</code><code> </code><code class="o">&gt;</code><code> </code><code class="mi">195</code><code> </code><code class="ow">and</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">finish</code><code class="p">:</code><code>
</code><code>                         </code><code class="k">break</code><code>
</code><code>                     </code><code class="k">if</code><code> </code><code class="nb">len</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">memory</code><code class="p">)</code><code> </code><code class="o">&gt;</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">batch_size</code><code class="p">:</code><code>
</code><code>                         </code><code class="bp">self</code><code class="o">.</code><code class="n">replay</code><code class="p">(</code><code class="p">)</code><code>  </code><a class="co" id="co_reinforcement_learning_CO9-12" href="#callout_reinforcement_learning_CO9-12"><img src="Images/12.png" alt="12" width="12" height="12"/></a><code>
</code><code>             </code><code class="k">def</code><code> </code><code class="nf">test</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code><code> </code><code class="n">episodes</code><code class="p">)</code><code class="p">:</code><code>
</code><code>                 </code><code class="n">trewards</code><code> </code><code class="o">=</code><code> </code><code class="p">[</code><code class="p">]</code><code>
</code><code>                 </code><code class="k">for</code><code> </code><code class="n">e</code><code> </code><code class="ow">in</code><code> </code><code class="nb">range</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code><code> </code><code class="n">episodes</code><code> </code><code class="o">+</code><code> </code><code class="mi">1</code><code class="p">)</code><code class="p">:</code><code>
</code><code>                     </code><code class="n">state</code><code> </code><code class="o">=</code><code> </code><code class="n">env</code><code class="o">.</code><code class="n">reset</code><code class="p">(</code><code class="p">)</code><code>
</code><code>                     </code><code class="k">for</code><code> </code><code class="n">_</code><code> </code><code class="ow">in</code><code> </code><code class="nb">range</code><code class="p">(</code><code class="mi">5001</code><code class="p">)</code><code class="p">:</code><code>
</code><code>                         </code><code class="n">state</code><code> </code><code class="o">=</code><code> </code><code class="n">np</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">state</code><code class="p">,</code><code> </code><code class="p">[</code><code class="mi">1</code><code class="p">,</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">osn</code><code class="p">]</code><code class="p">)</code><code>
</code><code>                         </code><code class="n">action</code><code> </code><code class="o">=</code><code> </code><code class="n">np</code><code class="o">.</code><code class="n">argmax</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">state</code><code class="p">)</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="p">)</code><code>
</code><code>                         </code><code class="n">next_state</code><code class="p">,</code><code> </code><code class="n">reward</code><code class="p">,</code><code> </code><code class="n">done</code><code class="p">,</code><code> </code><code class="n">info</code><code> </code><code class="o">=</code><code> </code><code class="n">env</code><code class="o">.</code><code class="n">step</code><code class="p">(</code><code class="n">action</code><code class="p">)</code><code>
</code><code>                         </code><code class="n">state</code><code> </code><code class="o">=</code><code> </code><code class="n">next_state</code><code>
</code><code>                         </code><code class="k">if</code><code> </code><code class="n">done</code><code class="p">:</code><code>
</code><code>                             </code><code class="n">treward</code><code> </code><code class="o">=</code><code> </code><code class="n">_</code><code> </code><code class="o">+</code><code> </code><code class="mi">1</code><code>
</code><code>                             </code><code class="n">trewards</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">treward</code><code class="p">)</code><code>
</code><code>                             </code><code class="k">print</code><code class="p">(</code><code class="s1">'</code><code class="s1">episode: {:4d}/{} | treward: {:4d}</code><code class="s1">'</code><code>
</code><code>                                   </code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">e</code><code class="p">,</code><code> </code><code class="n">episodes</code><code class="p">,</code><code> </code><code class="n">treward</code><code class="p">)</code><code class="p">,</code><code> </code><code class="n">end</code><code class="o">=</code><code class="s1">'</code><code class="se">\r</code><code class="s1">'</code><code class="p">)</code><code>
</code><code>                             </code><code class="k">break</code><code>
</code><code>                 </code><code class="k">return</code><code> </code><code class="n">trewards</code></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_reinforcement_learning_CO9-1" href="#co_reinforcement_learning_CO9-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>Initial exploration rate</p></dd>
<dt><a class="co" id="callout_reinforcement_learning_CO9-2" href="#co_reinforcement_learning_CO9-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a></dt>
<dd><p>Minimum exploration rate</p></dd>
<dt><a class="co" id="callout_reinforcement_learning_CO9-3" href="#co_reinforcement_learning_CO9-3"><img src="Images/3.png" alt="3" width="12" height="12"/></a></dt>
<dd><p>Decay rate for exploration rate</p></dd>
<dt><a class="co" id="callout_reinforcement_learning_CO9-4" href="#co_reinforcement_learning_CO9-4"><img src="Images/4.png" alt="4" width="12" height="12"/></a></dt>
<dd><p>Discount factor for delayed reward</p></dd>
<dt><a class="co" id="callout_reinforcement_learning_CO9-5" href="#co_reinforcement_learning_CO9-5"><img src="Images/5.png" alt="5" width="12" height="12"/></a></dt>
<dd><p>Batch size for replay</p></dd>
<dt><a class="co" id="callout_reinforcement_learning_CO9-6" href="#co_reinforcement_learning_CO9-6"><img src="Images/6.png" alt="6" width="12" height="12"/></a></dt>
<dd><p><code>deque</code> collection for limited history</p></dd>
<dt><a class="co" id="callout_reinforcement_learning_CO9-7" href="#co_reinforcement_learning_CO9-7"><img src="Images/7.png" alt="7" width="12" height="12"/></a></dt>
<dd><p>Random selection of history batch for replay</p></dd>
<dt><a class="co" id="callout_reinforcement_learning_CO9-8" href="#co_reinforcement_learning_CO9-8"><img src="Images/8.png" alt="8" width="12" height="12"/></a></dt>
<dd><p><math alttext="upper Q">
  <mi>Q</mi>
</math> value for state-action pair</p></dd>
<dt><a class="co" id="callout_reinforcement_learning_CO9-9" href="#co_reinforcement_learning_CO9-9"><img src="Images/9.png" alt="9" width="12" height="12"/></a></dt>
<dd><p>Update of the neural network for the new action-value pairs</p></dd>
<dt><a class="co" id="callout_reinforcement_learning_CO9-10" href="#co_reinforcement_learning_CO9-10"><img src="Images/10.png" alt="10" width="12" height="12"/></a></dt>
<dd><p>Update of the exploration rate</p></dd>
<dt><a class="co" id="callout_reinforcement_learning_CO9-11" href="#co_reinforcement_learning_CO9-11"><img src="Images/11.png" alt="11" width="12" height="12"/></a></dt>
<dd><p>Storing the new data</p></dd>
<dt><a class="co" id="callout_reinforcement_learning_CO9-12" href="#co_reinforcement_learning_CO9-12"><img src="Images/12.png" alt="12" width="12" height="12"/></a></dt>
<dd><p>Replay to update the policy based on past experiences</p></dd>
</dl>

<p>How does the QL agent perform? As the code that follows shows, it reaches a winning state for <code>CartPole</code> of a total reward of 200. <a data-type="xref" href="#figure_rl_01">Figure 9-2</a> shows the moving average of scores and how it increases over time, although not monotonically. To the contrary, the performance of the agent can significantly decrease at times, as <a data-type="xref" href="#figure_rl_01">Figure 9-2</a> shows. Among other things, the exploration that is taking place throughout leads to random actions that might not necessarily lead to good results in terms of total rewards but may lead to beneficial experiences for updating the policy network:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">In</code> <code class="p">[</code><code class="mi">46</code><code class="p">]:</code> <code class="n">episodes</code> <code class="o">=</code> <code class="mi">1000</code>

<code class="n">In</code> <code class="p">[</code><code class="mi">47</code><code class="p">]:</code> <code class="n">set_seeds</code><code class="p">(</code><code class="mi">100</code><code class="p">)</code>
         <code class="n">agent</code> <code class="o">=</code> <code class="n">DQLAgent</code><code class="p">(</code><code class="n">finish</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>

<code class="n">In</code> <code class="p">[</code><code class="mi">48</code><code class="p">]:</code> <code class="n">agent</code><code class="o">.</code><code class="n">learn</code><code class="p">(</code><code class="n">episodes</code><code class="p">)</code>
         <code class="n">episode</code><code class="p">:</code>  <code class="mi">400</code><code class="o">/</code><code class="mi">1000</code> <code class="o">|</code> <code class="n">treward</code><code class="p">:</code>  <code class="mi">200</code> <code class="o">|</code> <code class="n">av</code><code class="p">:</code>  <code class="mf">195.4</code> <code class="o">|</code> <code class="nb">max</code><code class="p">:</code>  <code class="mi">200</code>
<code class="n">In</code> <code class="p">[</code><code class="mi">49</code><code class="p">]:</code> <code class="n">plt</code><code class="o">.</code><code class="n">figure</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="mi">6</code><code class="p">))</code>
         <code class="n">x</code> <code class="o">=</code> <code class="nb">range</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">agent</code><code class="o">.</code><code class="n">averages</code><code class="p">))</code>
         <code class="n">y</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">polyval</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">polyfit</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">agent</code><code class="o">.</code><code class="n">averages</code><code class="p">,</code> <code class="n">deg</code><code class="o">=</code><code class="mi">3</code><code class="p">),</code> <code class="n">x</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">agent</code><code class="o">.</code><code class="n">averages</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s1">'moving average'</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="s1">'r--'</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s1">'trend'</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s1">'episodes'</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s1">'total reward'</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">legend</code><code class="p">();</code></pre>

<figure class="thumb"><div id="figure_rl_01" class="figure">
<img src="Images/aiif_0902.png" alt="aiif 0902" width="2523" height="1491"/>
<h6><span class="label">Figure 9-2. </span>Average total rewards of <code>DQLAgent</code> for <code>CartPole</code></h6>
</div></figure>

<p>Does the QL agent solve the <code>CartPole</code> problem? In this particular case, it does, given the definition of success by OpenAI Gym:<a data-type="indexterm" data-primary="" data-startref="ix_cartpole_dqlagent" id="idm45625279941208"/><a data-type="indexterm" data-primary="" data-startref="ix_nn_ql_ch9" id="idm45625279940232"/></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">In</code> <code class="p">[</code><code class="mi">50</code><code class="p">]:</code> <code class="n">trewards</code> <code class="o">=</code> <code class="n">agent</code><code class="o">.</code><code class="n">test</code><code class="p">(</code><code class="mi">100</code><code class="p">)</code>
         <code class="n">episode</code><code class="p">:</code>  <code class="mi">100</code><code class="o">/</code><code class="mi">100</code> <code class="o">|</code> <code class="n">treward</code><code class="p">:</code>  <code class="mi">200</code>
<code class="n">In</code> <code class="p">[</code><code class="mi">51</code><code class="p">]:</code> <code class="nb">sum</code><code class="p">(</code><code class="n">trewards</code><code class="p">)</code> <code class="o">/</code> <code class="nb">len</code><code class="p">(</code><code class="n">trewards</code><code class="p">)</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">51</code><code class="p">]:</code> <code class="mf">200.0</code></pre>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Simple Finance Gym"><div class="sect1" id="rl_sf_gym">
<h1>Simple Finance Gym</h1>

<p><a data-type="indexterm" data-primary="financial market setting for RL" id="ix_fin_market_RL"/><a data-type="indexterm" data-primary="OpenAI Gym environment" id="ix_openai_gym_env_3"/>To transfer the QL approach to the financial domain, this section provides a class that mimics an OpenAI Gym environment, but for a financial market as represented by financial time series data. The idea is that, similar to the <code>CartPole</code> environment, four historical prices represent the state of the financial market. An agent can decide, when presented with the state, whether to go long or to go short. In that case, the two environments are comparable since a state is given by four parameters and an agent can take two different actions.</p>

<p>To mimic the OpenAI Gym API, two helper classes are needed—one for the observation space, and one for the action space:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">In</code> <code class="p">[</code><code class="mi">52</code><code class="p">]:</code> <code class="k">class</code> <code class="nc">observation_space</code><code class="p">:</code>
             <code class="k">def</code> <code class="nf-Magic">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">n</code><code class="p">):</code>
                 <code class="bp">self</code><code class="o">.</code><code class="n">shape</code> <code class="o">=</code> <code class="p">(</code><code class="n">n</code><code class="p">,)</code>

<code class="n">In</code> <code class="p">[</code><code class="mi">53</code><code class="p">]:</code> <code class="k">class</code> <code class="nc">action_space</code><code class="p">:</code>
             <code class="k">def</code> <code class="nf-Magic">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">n</code><code class="p">):</code>
                 <code class="bp">self</code><code class="o">.</code><code class="n">n</code> <code class="o">=</code> <code class="n">n</code>
             <code class="k">def</code> <code class="nf">seed</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">seed</code><code class="p">):</code>
                 <code class="k">pass</code>
             <code class="k">def</code> <code class="nf">sample</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
                 <code class="k">return</code> <code class="n">random</code><code class="o">.</code><code class="n">randint</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">n</code> <code class="o">-</code> <code class="mi">1</code><code class="p">)</code></pre>

<p><a data-type="indexterm" data-primary="Finance environment class" id="ix_fin_env_class_ch9"/>The following Python code defines the <code>Finance</code> class. It retrieves end-of-day historical prices for a number of symbols. The major methods of the class are <code>.reset()</code> and <code>.step()</code>. The <code>.step()</code> method checks whether the right action has been taken, defines the reward accordingly, and checks for success or failure. A success is achieved when the agent is able to correctly trade through the whole data set. This can, of course, be defined differently (say, a success is achieved when the agent trades successfully for 1,000 steps). A failure is defined as an accuracy ratio of less than 50% (total rewards divided by total number of steps). However, this is only checked for after a certain number of steps to avoid the high initial variance of this metric:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">54</code><code class="p">]</code><code class="p">:</code><code> </code><code class="k">class</code><code> </code><code class="nc">Finance</code><code class="p">:</code><code>
</code><code>             </code><code class="n">url</code><code> </code><code class="o">=</code><code> </code><code class="s1">'</code><code class="s1">http://hilpisch.com/aiif_eikon_eod_data.csv</code><code class="s1">'</code><code>
</code><code>             </code><code class="k">def</code><code> </code><code class="nf-Magic">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code><code> </code><code class="n">symbol</code><code class="p">,</code><code> </code><code class="n">features</code><code class="p">)</code><code class="p">:</code><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">symbol</code><code> </code><code class="o">=</code><code> </code><code class="n">symbol</code><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">features</code><code> </code><code class="o">=</code><code> </code><code class="n">features</code><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">observation_space</code><code> </code><code class="o">=</code><code> </code><code class="n">observation_space</code><code class="p">(</code><code class="mi">4</code><code class="p">)</code><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">osn</code><code> </code><code class="o">=</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">observation_space</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">action_space</code><code> </code><code class="o">=</code><code> </code><code class="n">action_space</code><code class="p">(</code><code class="mi">2</code><code class="p">)</code><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">min_accuracy</code><code> </code><code class="o">=</code><code> </code><code class="mf">0.475</code><code>  </code><a class="co" id="co_reinforcement_learning_CO10-1" href="#callout_reinforcement_learning_CO10-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">_get_data</code><code class="p">(</code><code class="p">)</code><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">_prepare_data</code><code class="p">(</code><code class="p">)</code><code>
</code><code>             </code><code class="k">def</code><code> </code><code class="nf">_get_data</code><code class="p">(</code><code class="bp">self</code><code class="p">)</code><code class="p">:</code><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">raw</code><code> </code><code class="o">=</code><code> </code><code class="n">pd</code><code class="o">.</code><code class="n">read_csv</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">url</code><code class="p">,</code><code> </code><code class="n">index_col</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code><code>
</code><code>                                        </code><code class="n">parse_dates</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code><code class="o">.</code><code class="n">dropna</code><code class="p">(</code><code class="p">)</code><code>
</code><code>             </code><code class="k">def</code><code> </code><code class="nf">_prepare_data</code><code class="p">(</code><code class="bp">self</code><code class="p">)</code><code class="p">:</code><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">data</code><code> </code><code class="o">=</code><code> </code><code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">raw</code><code class="p">[</code><code class="bp">self</code><code class="o">.</code><code class="n">symbol</code><code class="p">]</code><code class="p">)</code><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">data</code><code class="p">[</code><code class="s1">'</code><code class="s1">r</code><code class="s1">'</code><code class="p">]</code><code> </code><code class="o">=</code><code> </code><code class="n">np</code><code class="o">.</code><code class="n">log</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">data</code><code> </code><code class="o">/</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">shift</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code><code class="p">)</code><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">dropna</code><code class="p">(</code><code class="n">inplace</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">data</code><code> </code><code class="o">=</code><code> </code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">data</code><code> </code><code class="o">-</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="p">)</code><code class="p">)</code><code> </code><code class="o">/</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">std</code><code class="p">(</code><code class="p">)</code><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">data</code><code class="p">[</code><code class="s1">'</code><code class="s1">d</code><code class="s1">'</code><code class="p">]</code><code> </code><code class="o">=</code><code> </code><code class="n">np</code><code class="o">.</code><code class="n">where</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">data</code><code class="p">[</code><code class="s1">'</code><code class="s1">r</code><code class="s1">'</code><code class="p">]</code><code> </code><code class="o">&gt;</code><code> </code><code class="mi">0</code><code class="p">,</code><code> </code><code class="mi">1</code><code class="p">,</code><code> </code><code class="mi">0</code><code class="p">)</code><code>
</code><code>             </code><code class="k">def</code><code> </code><code class="nf">_get_state</code><code class="p">(</code><code class="bp">self</code><code class="p">)</code><code class="p">:</code><code>
</code><code>                 </code><code class="k">return</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">data</code><code class="p">[</code><code class="bp">self</code><code class="o">.</code><code class="n">features</code><code class="p">]</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code>
</code><code>                     </code><code class="bp">self</code><code class="o">.</code><code class="n">bar</code><code> </code><code class="o">-</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">osn</code><code class="p">:</code><code class="bp">self</code><code class="o">.</code><code class="n">bar</code><code class="p">]</code><code class="o">.</code><code class="n">values</code><code>  </code><a class="co" id="co_reinforcement_learning_CO10-2" href="#callout_reinforcement_learning_CO10-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a><code>
</code><code>             </code><code class="k">def</code><code> </code><code class="nf">seed</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code><code> </code><code class="n">seed</code><code class="o">=</code><code class="bp">None</code><code class="p">)</code><code class="p">:</code><code>
</code><code>                 </code><code class="k">pass</code><code>
</code><code>             </code><code class="k">def</code><code> </code><code class="nf">reset</code><code class="p">(</code><code class="bp">self</code><code class="p">)</code><code class="p">:</code><code>  </code><a class="co" id="co_reinforcement_learning_CO10-3" href="#callout_reinforcement_learning_CO10-3"><img src="Images/3.png" alt="3" width="12" height="12"/></a><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">treward</code><code> </code><code class="o">=</code><code> </code><code class="mi">0</code><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">accuracy</code><code> </code><code class="o">=</code><code> </code><code class="mi">0</code><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">bar</code><code> </code><code class="o">=</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">osn</code><code>
</code><code>                 </code><code class="n">state</code><code> </code><code class="o">=</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">data</code><code class="p">[</code><code class="bp">self</code><code class="o">.</code><code class="n">features</code><code class="p">]</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code>
</code><code>                     </code><code class="bp">self</code><code class="o">.</code><code class="n">bar</code><code> </code><code class="o">-</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">osn</code><code class="p">:</code><code class="bp">self</code><code class="o">.</code><code class="n">bar</code><code class="p">]</code><code>
</code><code>                 </code><code class="k">return</code><code> </code><code class="n">state</code><code class="o">.</code><code class="n">values</code><code>
</code><code>             </code><code class="k">def</code><code> </code><code class="nf">step</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code><code> </code><code class="n">action</code><code class="p">)</code><code class="p">:</code><code>
</code><code>                 </code><code class="n">correct</code><code> </code><code class="o">=</code><code> </code><code class="n">action</code><code> </code><code class="o">==</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">data</code><code class="p">[</code><code class="s1">'</code><code class="s1">d</code><code class="s1">'</code><code class="p">]</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="bp">self</code><code class="o">.</code><code class="n">bar</code><code class="p">]</code><code>  </code><a class="co" id="co_reinforcement_learning_CO10-4" href="#callout_reinforcement_learning_CO10-4"><img src="Images/4.png" alt="4" width="12" height="12"/></a><code>
</code><code>                 </code><code class="n">reward</code><code> </code><code class="o">=</code><code> </code><code class="mi">1</code><code> </code><code class="k">if</code><code> </code><code class="n">correct</code><code> </code><code class="k">else</code><code> </code><code class="mi">0</code><code>  </code><a class="co" id="co_reinforcement_learning_CO10-5" href="#callout_reinforcement_learning_CO10-5"><img src="Images/5.png" alt="5" width="12" height="12"/></a><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">treward</code><code> </code><code class="o">+</code><code class="o">=</code><code> </code><code class="n">reward</code><code>  </code><a class="co" id="co_reinforcement_learning_CO10-6" href="#callout_reinforcement_learning_CO10-6"><img src="Images/6.png" alt="6" width="12" height="12"/></a><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">bar</code><code> </code><code class="o">+</code><code class="o">=</code><code> </code><code class="mi">1</code><code>  </code><a class="co" id="co_reinforcement_learning_CO10-7" href="#callout_reinforcement_learning_CO10-7"><img src="Images/7.png" alt="7" width="12" height="12"/></a><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">accuracy</code><code> </code><code class="o">=</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">treward</code><code> </code><code class="o">/</code><code> </code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">bar</code><code> </code><code class="o">-</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">osn</code><code class="p">)</code><code>  </code><a class="co" id="co_reinforcement_learning_CO10-8" href="#callout_reinforcement_learning_CO10-8"><img src="Images/8.png" alt="8" width="12" height="12"/></a><code>
</code><code>                 </code><code class="k">if</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">bar</code><code> </code><code class="o">&gt;</code><code class="o">=</code><code> </code><code class="nb">len</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">data</code><code class="p">)</code><code class="p">:</code><code>  </code><a class="co" id="co_reinforcement_learning_CO10-9" href="#callout_reinforcement_learning_CO10-9"><img src="Images/9.png" alt="9" width="12" height="12"/></a><code>
</code><code>                     </code><code class="n">done</code><code> </code><code class="o">=</code><code> </code><code class="bp">True</code><code>
</code><code>                 </code><code class="k">elif</code><code> </code><code class="n">reward</code><code> </code><code class="o">==</code><code> </code><code class="mi">1</code><code class="p">:</code><code>  </code><a class="co" id="co_reinforcement_learning_CO10-10" href="#callout_reinforcement_learning_CO10-10"><img src="Images/10.png" alt="10" width="12" height="12"/></a><code>
</code><code>                     </code><code class="n">done</code><code> </code><code class="o">=</code><code> </code><code class="bp">False</code><code>
</code><code>                 </code><code class="k">elif</code><code> </code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">accuracy</code><code> </code><code class="o">&lt;</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">min_accuracy</code><code> </code><code class="ow">and</code><code>
</code><code>                       </code><code class="bp">self</code><code class="o">.</code><code class="n">bar</code><code> </code><code class="o">&gt;</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">osn</code><code> </code><code class="o">+</code><code> </code><code class="mi">10</code><code class="p">)</code><code class="p">:</code><code>  </code><a class="co" id="co_reinforcement_learning_CO10-11" href="#callout_reinforcement_learning_CO10-11"><img src="Images/11.png" alt="11" width="12" height="12"/></a><code>
</code><code>                     </code><code class="n">done</code><code> </code><code class="o">=</code><code> </code><code class="bp">True</code><code>
</code><code>                 </code><code class="k">else</code><code class="p">:</code><code>  </code><a class="co" id="co_reinforcement_learning_CO10-12" href="#callout_reinforcement_learning_CO10-12"><img src="Images/12.png" alt="12" width="12" height="12"/></a><code>
</code><code>                     </code><code class="n">done</code><code> </code><code class="o">=</code><code> </code><code class="bp">False</code><code>
</code><code>                 </code><code class="n">state</code><code> </code><code class="o">=</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">_get_state</code><code class="p">(</code><code class="p">)</code><code>
</code><code>                 </code><code class="n">info</code><code> </code><code class="o">=</code><code> </code><code class="p">{</code><code class="p">}</code><code>
</code><code>                 </code><code class="k">return</code><code> </code><code class="n">state</code><code class="p">,</code><code> </code><code class="n">reward</code><code class="p">,</code><code> </code><code class="n">done</code><code class="p">,</code><code> </code><code class="n">info</code></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_reinforcement_learning_CO10-1" href="#co_reinforcement_learning_CO10-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>Defines the minimum accuracy required.</p></dd>
<dt><a class="co" id="callout_reinforcement_learning_CO10-2" href="#co_reinforcement_learning_CO10-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a></dt>
<dd><p>Selects the data defining the state of the financial market.</p></dd>
<dt><a class="co" id="callout_reinforcement_learning_CO10-3" href="#co_reinforcement_learning_CO10-3"><img src="Images/3.png" alt="3" width="12" height="12"/></a></dt>
<dd><p>Resets the environment to its initial values.</p></dd>
<dt><a class="co" id="callout_reinforcement_learning_CO10-4" href="#co_reinforcement_learning_CO10-4"><img src="Images/4.png" alt="4" width="12" height="12"/></a></dt>
<dd><p>Checks whether the agent has chosen the right action (successful trade).</p></dd>
<dt><a class="co" id="callout_reinforcement_learning_CO10-5" href="#co_reinforcement_learning_CO10-5"><img src="Images/5.png" alt="5" width="12" height="12"/></a></dt>
<dd><p>Defines the reward the agent receives.</p></dd>
<dt><a class="co" id="callout_reinforcement_learning_CO10-6" href="#co_reinforcement_learning_CO10-6"><img src="Images/6.png" alt="6" width="12" height="12"/></a></dt>
<dd><p>Adds the reward to the total reward.</p></dd>
<dt><a class="co" id="callout_reinforcement_learning_CO10-7" href="#co_reinforcement_learning_CO10-7"><img src="Images/7.png" alt="7" width="12" height="12"/></a></dt>
<dd><p>Moves the environment one step forward.</p></dd>
<dt><a class="co" id="callout_reinforcement_learning_CO10-8" href="#co_reinforcement_learning_CO10-8"><img src="Images/8.png" alt="8" width="12" height="12"/></a></dt>
<dd><p>Calculates the accuracy of successful actions (trades) given all steps (trades).</p></dd>
<dt><a class="co" id="callout_reinforcement_learning_CO10-9" href="#co_reinforcement_learning_CO10-9"><img src="Images/9.png" alt="9" width="12" height="12"/></a></dt>
<dd><p>If the agent reaches the end of the data set, success is achieved.</p></dd>
<dt><a class="co" id="callout_reinforcement_learning_CO10-10" href="#co_reinforcement_learning_CO10-10"><img src="Images/10.png" alt="10" width="12" height="12"/></a></dt>
<dd><p>If the agent takes the right action, it can move on.</p></dd>
<dt><a class="co" id="callout_reinforcement_learning_CO10-11" href="#co_reinforcement_learning_CO10-11"><img src="Images/11.png" alt="11" width="12" height="12"/></a></dt>
<dd><p>If, after some initial steps, the accuracy drops under the minimum level, the episode ends (failure).</p></dd>
<dt><a class="co" id="callout_reinforcement_learning_CO10-12" href="#co_reinforcement_learning_CO10-12"><img src="Images/12.png" alt="12" width="12" height="12"/></a></dt>
<dd><p>For the remaining cases, the agent can move on.</p></dd>
</dl>

<p>Instances of the <code>Finance</code> class behave like an environment of the OpenAI Gym. 
<span class="keep-together">In particular,</span> in this base case, the instance behaves exactly like the <code>CartPole</code> 
<span class="keep-together">environment:</span></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">55</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">env</code><code> </code><code class="o">=</code><code> </code><code class="n">Finance</code><code class="p">(</code><code class="s1">'</code><code class="s1">EUR=</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="s1">'</code><code class="s1">EUR=</code><code class="s1">'</code><code class="p">)</code><code>  </code><a class="co" id="co_reinforcement_learning_CO11-1" href="#callout_reinforcement_learning_CO11-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>
</code><code>
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">56</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">env</code><code class="o">.</code><code class="n">reset</code><code class="p">(</code><code class="p">)</code><code>
</code><code class="n">Out</code><code class="p">[</code><code class="mi">56</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">array</code><code class="p">(</code><code class="p">[</code><code class="mf">1.819</code><code> </code><code class="p">,</code><code> </code><code class="mf">1.8579</code><code class="p">,</code><code> </code><code class="mf">1.7749</code><code class="p">,</code><code> </code><code class="mf">1.8579</code><code class="p">]</code><code class="p">)</code><code>
</code><code>
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">57</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">a</code><code> </code><code class="o">=</code><code> </code><code class="n">env</code><code class="o">.</code><code class="n">action_space</code><code class="o">.</code><code class="n">sample</code><code class="p">(</code><code class="p">)</code><code>
</code><code>         </code><code class="n">a</code><code>
</code><code class="n">Out</code><code class="p">[</code><code class="mi">57</code><code class="p">]</code><code class="p">:</code><code> </code><code class="mi">0</code><code>
</code><code>
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">58</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">env</code><code class="o">.</code><code class="n">step</code><code class="p">(</code><code class="n">a</code><code class="p">)</code><code>
</code><code class="n">Out</code><code class="p">[</code><code class="mi">58</code><code class="p">]</code><code class="p">:</code><code> </code><code class="p">(</code><code class="n">array</code><code class="p">(</code><code class="p">[</code><code class="mf">1.8579</code><code class="p">,</code><code> </code><code class="mf">1.7749</code><code class="p">,</code><code> </code><code class="mf">1.8579</code><code class="p">,</code><code> </code><code class="mf">1.947</code><code> </code><code class="p">]</code><code class="p">)</code><code class="p">,</code><code> </code><code class="mi">0</code><code class="p">,</code><code> </code><code class="bp">False</code><code class="p">,</code><code> </code><code class="p">{</code><code class="p">}</code><code class="p">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_reinforcement_learning_CO11-1" href="#co_reinforcement_learning_CO11-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>Specifies which symbol and which type of feature (symbol or log return) to be used to define the data representing the state</p></dd>
</dl>

<p>Can the <code>DQLAgent</code>, as developed for the <code>CartPole</code> game, learn to trade in a financial market? Yes, it can, as the following code illustrates. However, although the agent improves its trading skill (on average) over the training episodes, the results are not too impressive (see <a data-type="xref" href="#figure_rl_02">Figure 9-3</a>):</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">In</code> <code class="p">[</code><code class="mi">59</code><code class="p">]:</code> <code class="n">set_seeds</code><code class="p">(</code><code class="mi">100</code><code class="p">)</code>
         <code class="n">agent</code> <code class="o">=</code> <code class="n">DQLAgent</code><code class="p">(</code><code class="n">gamma</code><code class="o">=</code><code class="mf">0.5</code><code class="p">,</code> <code class="n">opt</code><code class="o">=</code><code class="n">RMSprop</code><code class="p">)</code>

<code class="n">In</code> <code class="p">[</code><code class="mi">60</code><code class="p">]:</code> <code class="n">episodes</code> <code class="o">=</code> <code class="mi">1000</code>

<code class="n">In</code> <code class="p">[</code><code class="mi">61</code><code class="p">]:</code> <code class="n">agent</code><code class="o">.</code><code class="n">learn</code><code class="p">(</code><code class="n">episodes</code><code class="p">)</code>
         <code class="n">episode</code><code class="p">:</code> <code class="mi">1000</code><code class="o">/</code><code class="mi">1000</code> <code class="o">|</code> <code class="n">treward</code><code class="p">:</code> <code class="mi">2511</code> <code class="o">|</code> <code class="n">av</code><code class="p">:</code> <code class="mf">1012.7</code> <code class="o">|</code> <code class="nb">max</code><code class="p">:</code> <code class="mi">2511</code>
<code class="n">In</code> <code class="p">[</code><code class="mi">62</code><code class="p">]:</code> <code class="n">agent</code><code class="o">.</code><code class="n">test</code><code class="p">(</code><code class="mi">3</code><code class="p">)</code>
         <code class="n">episode</code><code class="p">:</code>    <code class="mi">3</code><code class="o">/</code><code class="mi">3</code> <code class="o">|</code> <code class="n">treward</code><code class="p">:</code> <code class="mi">2511</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">62</code><code class="p">]:</code> <code class="p">[</code><code class="mi">2511</code><code class="p">,</code> <code class="mi">2511</code><code class="p">,</code> <code class="mi">2511</code><code class="p">]</code>

<code class="n">In</code> <code class="p">[</code><code class="mi">63</code><code class="p">]:</code> <code class="n">plt</code><code class="o">.</code><code class="n">figure</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="mi">6</code><code class="p">))</code>
         <code class="n">x</code> <code class="o">=</code> <code class="nb">range</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">agent</code><code class="o">.</code><code class="n">averages</code><code class="p">))</code>
         <code class="n">y</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">polyval</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">polyfit</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">agent</code><code class="o">.</code><code class="n">averages</code><code class="p">,</code> <code class="n">deg</code><code class="o">=</code><code class="mi">3</code><code class="p">),</code> <code class="n">x</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">agent</code><code class="o">.</code><code class="n">averages</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s1">'moving average'</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="s1">'r--'</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s1">'regression'</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s1">'episodes'</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s1">'total reward'</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">legend</code><code class="p">();</code></pre>

<figure class="thumb"><div id="figure_rl_02" class="figure">
<img src="Images/aiif_0903.png" alt="aiif 0903" width="2523" height="1491"/>
<h6><span class="label">Figure 9-3. </span>Average total rewards of <code>DQLAgent</code> for <code>Finance</code></h6>
</div></figure>
<div data-type="note" epub:type="note"><h1>General RL Agents</h1>
<p>This section provides a class for a financial market environment that mimics the API of an OpenAI Gym environment. It also applies, without any changes to the agent itself, the QL agent to the new financial market environment. Although the performance of the agent in this new environment might not be impressive, it illustrates that the approach of RL, as introduced in this chapter, is rather general. RL agents can in general learn from different environments they interact with. This explains to some extent why AlphaZero from DeepMind is able to master not only the game of Go but also chess and shogi, as discussed in <a data-type="xref" href="ch02.xhtml#superintelligence">Chapter 2</a>.<a data-type="indexterm" data-primary="" data-startref="ix_openai_gym_env_3" id="idm45625283214008"/></p>
</div>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Better Finance Gym"><div class="sect1" id="rl_bf_gym">
<h1>Better Finance Gym</h1>

<p>The idea in the previous section is to develop a simple class that allows RL within a financial market setting. The major goal in that section is to replicate the API of an OpenAI Gym environment. However, there is no need to restrict such an environment to a single type of feature to describe the state of the financial market nor to use only four lags. This section introduces an improved <code>Finance</code> class that allows for multiple features, a flexible number of lags, and specific start and end points for the base data set used. This, among other things, allows the use of one part of the data set for learning and another one for validation or testing. The Python code presented in the following also allows the use of leverage. This might be helpful when intraday data is considered with relatively small absolute returns:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">64</code><code class="p">]</code><code class="p">:</code><code> </code><code class="k">class</code><code> </code><code class="nc">Finance</code><code class="p">:</code><code>
</code><code>             </code><code class="n">url</code><code> </code><code class="o">=</code><code> </code><code class="s1">'</code><code class="s1">http://hilpisch.com/aiif_eikon_eod_data.csv</code><code class="s1">'</code><code>
</code><code>             </code><code class="k">def</code><code> </code><code class="nf-Magic">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code><code> </code><code class="n">symbol</code><code class="p">,</code><code> </code><code class="n">features</code><code class="p">,</code><code> </code><code class="n">window</code><code class="p">,</code><code> </code><code class="n">lags</code><code class="p">,</code><code>
</code><code>                          </code><code class="n">leverage</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code><code> </code><code class="n">min_performance</code><code class="o">=</code><code class="mf">0.85</code><code class="p">,</code><code>
</code><code>                          </code><code class="n">start</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code><code> </code><code class="n">end</code><code class="o">=</code><code class="bp">None</code><code class="p">,</code><code> </code><code class="n">mu</code><code class="o">=</code><code class="bp">None</code><code class="p">,</code><code> </code><code class="n">std</code><code class="o">=</code><code class="bp">None</code><code class="p">)</code><code class="p">:</code><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">symbol</code><code> </code><code class="o">=</code><code> </code><code class="n">symbol</code><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">features</code><code> </code><code class="o">=</code><code> </code><code class="n">features</code><code>  </code><a class="co" id="co_reinforcement_learning_CO12-1" href="#callout_reinforcement_learning_CO12-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">n_features</code><code> </code><code class="o">=</code><code> </code><code class="nb">len</code><code class="p">(</code><code class="n">features</code><code class="p">)</code><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">window</code><code> </code><code class="o">=</code><code> </code><code class="n">window</code><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">lags</code><code> </code><code class="o">=</code><code> </code><code class="n">lags</code><code>  </code><a class="co" id="co_reinforcement_learning_CO12-2" href="#callout_reinforcement_learning_CO12-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">leverage</code><code> </code><code class="o">=</code><code> </code><code class="n">leverage</code><code>  </code><a class="co" id="co_reinforcement_learning_CO12-3" href="#callout_reinforcement_learning_CO12-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">min_performance</code><code> </code><code class="o">=</code><code> </code><code class="n">min_performance</code><code>  </code><a class="co" id="co_reinforcement_learning_CO12-4" href="#callout_reinforcement_learning_CO12-3"><img src="Images/3.png" alt="3" width="12" height="12"/></a><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">start</code><code> </code><code class="o">=</code><code> </code><code class="n">start</code><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">end</code><code> </code><code class="o">=</code><code> </code><code class="n">end</code><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">mu</code><code> </code><code class="o">=</code><code> </code><code class="n">mu</code><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">std</code><code> </code><code class="o">=</code><code> </code><code class="n">std</code><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">observation_space</code><code> </code><code class="o">=</code><code> </code><code class="n">observation_space</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">lags</code><code class="p">)</code><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">action_space</code><code> </code><code class="o">=</code><code> </code><code class="n">action_space</code><code class="p">(</code><code class="mi">2</code><code class="p">)</code><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">_get_data</code><code class="p">(</code><code class="p">)</code><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">_prepare_data</code><code class="p">(</code><code class="p">)</code><code>
</code><code>             </code><code class="k">def</code><code> </code><code class="nf">_get_data</code><code class="p">(</code><code class="bp">self</code><code class="p">)</code><code class="p">:</code><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">raw</code><code> </code><code class="o">=</code><code> </code><code class="n">pd</code><code class="o">.</code><code class="n">read_csv</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">url</code><code class="p">,</code><code> </code><code class="n">index_col</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code><code>
</code><code>                                        </code><code class="n">parse_dates</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code><code class="o">.</code><code class="n">dropna</code><code class="p">(</code><code class="p">)</code><code>
</code><code>             </code><code class="k">def</code><code> </code><code class="nf">_prepare_data</code><code class="p">(</code><code class="bp">self</code><code class="p">)</code><code class="p">:</code><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">data</code><code> </code><code class="o">=</code><code> </code><code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">raw</code><code class="p">[</code><code class="bp">self</code><code class="o">.</code><code class="n">symbol</code><code class="p">]</code><code class="p">)</code><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">data</code><code> </code><code class="o">=</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="bp">self</code><code class="o">.</code><code class="n">start</code><code class="p">:</code><code class="p">]</code><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">data</code><code class="p">[</code><code class="s1">'</code><code class="s1">r</code><code class="s1">'</code><code class="p">]</code><code> </code><code class="o">=</code><code> </code><code class="n">np</code><code class="o">.</code><code class="n">log</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">data</code><code> </code><code class="o">/</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">shift</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code><code class="p">)</code><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">dropna</code><code class="p">(</code><code class="n">inplace</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">data</code><code class="p">[</code><code class="s1">'</code><code class="s1">s</code><code class="s1">'</code><code class="p">]</code><code> </code><code class="o">=</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">data</code><code class="p">[</code><code class="bp">self</code><code class="o">.</code><code class="n">symbol</code><code class="p">]</code><code class="o">.</code><code class="n">rolling</code><code class="p">(</code><code>
</code><code>                                                       </code><code class="bp">self</code><code class="o">.</code><code class="n">window</code><code class="p">)</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="p">)</code><code>   </code><a class="co" id="co_reinforcement_learning_CO12-5" href="#callout_reinforcement_learning_CO12-4"><img src="Images/4.png" alt="4" width="12" height="12"/></a><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">data</code><code class="p">[</code><code class="s1">'</code><code class="s1">m</code><code class="s1">'</code><code class="p">]</code><code> </code><code class="o">=</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">data</code><code class="p">[</code><code class="s1">'</code><code class="s1">r</code><code class="s1">'</code><code class="p">]</code><code class="o">.</code><code class="n">rolling</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">window</code><code class="p">)</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="p">)</code><code>  </code><a class="co" id="co_reinforcement_learning_CO12-6" href="#callout_reinforcement_learning_CO12-4"><img src="Images/4.png" alt="4" width="12" height="12"/></a><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">data</code><code class="p">[</code><code class="s1">'</code><code class="s1">v</code><code class="s1">'</code><code class="p">]</code><code> </code><code class="o">=</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">data</code><code class="p">[</code><code class="s1">'</code><code class="s1">r</code><code class="s1">'</code><code class="p">]</code><code class="o">.</code><code class="n">rolling</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">window</code><code class="p">)</code><code class="o">.</code><code class="n">std</code><code class="p">(</code><code class="p">)</code><code>  </code><a class="co" id="co_reinforcement_learning_CO12-7" href="#callout_reinforcement_learning_CO12-4"><img src="Images/4.png" alt="4" width="12" height="12"/></a><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">dropna</code><code class="p">(</code><code class="n">inplace</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code><code>
</code><code>                 </code><code class="k">if</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">mu</code><code> </code><code class="ow">is</code><code> </code><code class="bp">None</code><code class="p">:</code><code>
</code><code>                     </code><code class="bp">self</code><code class="o">.</code><code class="n">mu</code><code> </code><code class="o">=</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="p">)</code><code>  </code><a class="co" id="co_reinforcement_learning_CO12-8" href="#callout_reinforcement_learning_CO12-5"><img src="Images/5.png" alt="5" width="12" height="12"/></a><code>
</code><code>                     </code><code class="bp">self</code><code class="o">.</code><code class="n">std</code><code> </code><code class="o">=</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">std</code><code class="p">(</code><code class="p">)</code><code>  </code><a class="co" id="co_reinforcement_learning_CO12-9" href="#callout_reinforcement_learning_CO12-5"><img src="Images/5.png" alt="5" width="12" height="12"/></a><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">data_</code><code> </code><code class="o">=</code><code> </code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">data</code><code> </code><code class="o">-</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">mu</code><code class="p">)</code><code> </code><code class="o">/</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">std</code><code>  </code><a class="co" id="co_reinforcement_learning_CO12-10" href="#callout_reinforcement_learning_CO12-5"><img src="Images/5.png" alt="5" width="12" height="12"/></a><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">data_</code><code class="p">[</code><code class="s1">'</code><code class="s1">d</code><code class="s1">'</code><code class="p">]</code><code> </code><code class="o">=</code><code> </code><code class="n">np</code><code class="o">.</code><code class="n">where</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">data</code><code class="p">[</code><code class="s1">'</code><code class="s1">r</code><code class="s1">'</code><code class="p">]</code><code> </code><code class="o">&gt;</code><code> </code><code class="mi">0</code><code class="p">,</code><code> </code><code class="mi">1</code><code class="p">,</code><code> </code><code class="mi">0</code><code class="p">)</code><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">data_</code><code class="p">[</code><code class="s1">'</code><code class="s1">d</code><code class="s1">'</code><code class="p">]</code><code> </code><code class="o">=</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">data_</code><code class="p">[</code><code class="s1">'</code><code class="s1">d</code><code class="s1">'</code><code class="p">]</code><code class="o">.</code><code class="n">astype</code><code class="p">(</code><code class="nb">int</code><code class="p">)</code><code>
</code><code>                 </code><code class="k">if</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">end</code><code> </code><code class="ow">is</code><code> </code><code class="ow">not</code><code> </code><code class="bp">None</code><code class="p">:</code><code>
</code><code>                     </code><code class="bp">self</code><code class="o">.</code><code class="n">data</code><code> </code><code class="o">=</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="p">:</code><code class="bp">self</code><code class="o">.</code><code class="n">end</code><code> </code><code class="o">-</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">start</code><code class="p">]</code><code>
</code><code>                     </code><code class="bp">self</code><code class="o">.</code><code class="n">data_</code><code> </code><code class="o">=</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">data_</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="p">:</code><code class="bp">self</code><code class="o">.</code><code class="n">end</code><code> </code><code class="o">-</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">start</code><code class="p">]</code><code>
</code><code>             </code><code class="k">def</code><code> </code><code class="nf">_get_state</code><code class="p">(</code><code class="bp">self</code><code class="p">)</code><code class="p">:</code><code>
</code><code>                 </code><code class="k">return</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">data_</code><code class="p">[</code><code class="bp">self</code><code class="o">.</code><code class="n">features</code><code class="p">]</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="bp">self</code><code class="o">.</code><code class="n">bar</code><code> </code><code class="o">-</code><code>
</code><code>                                         </code><code class="bp">self</code><code class="o">.</code><code class="n">lags</code><code class="p">:</code><code class="bp">self</code><code class="o">.</code><code class="n">bar</code><code class="p">]</code><code>
</code><code>             </code><code class="k">def</code><code> </code><code class="nf">seed</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code><code> </code><code class="n">seed</code><code class="p">)</code><code class="p">:</code><code>
</code><code>                 </code><code class="n">random</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="n">seed</code><code class="p">)</code><code>
</code><code>                 </code><code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="n">seed</code><code class="p">)</code><code>
</code><code>             </code><code class="k">def</code><code> </code><code class="nf">reset</code><code class="p">(</code><code class="bp">self</code><code class="p">)</code><code class="p">:</code><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">treward</code><code> </code><code class="o">=</code><code> </code><code class="mi">0</code><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">accuracy</code><code> </code><code class="o">=</code><code> </code><code class="mi">0</code><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">performance</code><code> </code><code class="o">=</code><code> </code><code class="mi">1</code><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">bar</code><code> </code><code class="o">=</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">lags</code><code>
</code><code>                 </code><code class="n">state</code><code> </code><code class="o">=</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">data_</code><code class="p">[</code><code class="bp">self</code><code class="o">.</code><code class="n">features</code><code class="p">]</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="bp">self</code><code class="o">.</code><code class="n">bar</code><code class="o">-</code><code>
</code><code>                                 </code><code class="bp">self</code><code class="o">.</code><code class="n">lags</code><code class="p">:</code><code class="bp">self</code><code class="o">.</code><code class="n">bar</code><code class="p">]</code><code>
</code><code>                 </code><code class="k">return</code><code> </code><code class="n">state</code><code class="o">.</code><code class="n">values</code><code>
</code><code>             </code><code class="k">def</code><code> </code><code class="nf">step</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code><code> </code><code class="n">action</code><code class="p">)</code><code class="p">:</code><code>
</code><code>                 </code><code class="n">correct</code><code> </code><code class="o">=</code><code> </code><code class="n">action</code><code> </code><code class="o">==</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">data_</code><code class="p">[</code><code class="s1">'</code><code class="s1">d</code><code class="s1">'</code><code class="p">]</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="bp">self</code><code class="o">.</code><code class="n">bar</code><code class="p">]</code><code>
</code><code>                 </code><code class="n">ret</code><code> </code><code class="o">=</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">data</code><code class="p">[</code><code class="s1">'</code><code class="s1">r</code><code class="s1">'</code><code class="p">]</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="bp">self</code><code class="o">.</code><code class="n">bar</code><code class="p">]</code><code> </code><code class="o">*</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">leverage</code><code>  </code><a class="co" id="co_reinforcement_learning_CO12-11" href="#callout_reinforcement_learning_CO12-6"><img src="Images/6.png" alt="6" width="12" height="12"/></a><code>
</code><code>                 </code><code class="n">reward_1</code><code> </code><code class="o">=</code><code> </code><code class="mi">1</code><code> </code><code class="k">if</code><code> </code><code class="n">correct</code><code> </code><code class="k">else</code><code> </code><code class="mi">0</code><code>
</code><code>                 </code><code class="n">reward_2</code><code> </code><code class="o">=</code><code> </code><code class="nb">abs</code><code class="p">(</code><code class="n">ret</code><code class="p">)</code><code> </code><code class="k">if</code><code> </code><code class="n">correct</code><code> </code><code class="k">else</code><code> </code><code class="o">-</code><code class="nb">abs</code><code class="p">(</code><code class="n">ret</code><code class="p">)</code><code>  </code><a class="co" id="co_reinforcement_learning_CO12-12" href="#callout_reinforcement_learning_CO12-7"><img src="Images/7.png" alt="7" width="12" height="12"/></a><code>
</code><code>                 </code><code class="n">factor</code><code> </code><code class="o">=</code><code> </code><code class="mi">1</code><code> </code><code class="k">if</code><code> </code><code class="n">correct</code><code> </code><code class="k">else</code><code> </code><code class="o">-</code><code class="mi">1</code><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">treward</code><code> </code><code class="o">+</code><code class="o">=</code><code> </code><code class="n">reward_1</code><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">bar</code><code> </code><code class="o">+</code><code class="o">=</code><code> </code><code class="mi">1</code><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">accuracy</code><code> </code><code class="o">=</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">treward</code><code> </code><code class="o">/</code><code> </code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">bar</code><code> </code><code class="o">-</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">lags</code><code class="p">)</code><code>
</code><code>                 </code><code class="bp">self</code><code class="o">.</code><code class="n">performance</code><code> </code><code class="o">*</code><code class="o">=</code><code> </code><code class="n">math</code><code class="o">.</code><code class="n">exp</code><code class="p">(</code><code class="n">reward_2</code><code class="p">)</code><code>  </code><a class="co" id="co_reinforcement_learning_CO12-13" href="#callout_reinforcement_learning_CO12-8"><img src="Images/8.png" alt="8" width="12" height="12"/></a><code>
</code><code>                 </code><code class="k">if</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">bar</code><code> </code><code class="o">&gt;</code><code class="o">=</code><code> </code><code class="nb">len</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">data</code><code class="p">)</code><code class="p">:</code><code>
</code><code>                     </code><code class="n">done</code><code> </code><code class="o">=</code><code> </code><code class="bp">True</code><code>
</code><code>                 </code><code class="k">elif</code><code> </code><code class="n">reward_1</code><code> </code><code class="o">==</code><code> </code><code class="mi">1</code><code class="p">:</code><code>
</code><code>                     </code><code class="n">done</code><code> </code><code class="o">=</code><code> </code><code class="bp">False</code><code>
</code><code>                 </code><code class="k">elif</code><code> </code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">performance</code><code> </code><code class="o">&lt;</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">min_performance</code><code> </code><code class="ow">and</code><code>
</code><code>                       </code><code class="bp">self</code><code class="o">.</code><code class="n">bar</code><code> </code><code class="o">&gt;</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">lags</code><code> </code><code class="o">+</code><code> </code><code class="mi">5</code><code class="p">)</code><code class="p">:</code><code>
</code><code>                     </code><code class="n">done</code><code> </code><code class="o">=</code><code> </code><code class="bp">True</code><code>
</code><code>                 </code><code class="k">else</code><code class="p">:</code><code>
</code><code>                     </code><code class="n">done</code><code> </code><code class="o">=</code><code> </code><code class="bp">False</code><code>
</code><code>                 </code><code class="n">state</code><code> </code><code class="o">=</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">_get_state</code><code class="p">(</code><code class="p">)</code><code>
</code><code>                 </code><code class="n">info</code><code> </code><code class="o">=</code><code> </code><code class="p">{</code><code class="p">}</code><code>
</code><code>                 </code><code class="k">return</code><code> </code><code class="n">state</code><code class="o">.</code><code class="n">values</code><code class="p">,</code><code> </code><code class="n">reward_1</code><code> </code><code class="o">+</code><code> </code><code class="n">reward_2</code><code> </code><code class="o">*</code><code> </code><code class="mi">5</code><code class="p">,</code><code> </code><code class="n">done</code><code class="p">,</code><code> </code><code class="n">info</code></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_reinforcement_learning_CO12-1" href="#co_reinforcement_learning_CO12-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>The features to define the state</p></dd>
<dt><a class="co" id="callout_reinforcement_learning_CO12-2" href="#co_reinforcement_learning_CO12-3"><img src="Images/2.png" alt="2" width="12" height="12"/></a></dt>
<dd><p>The number of lags to be used</p></dd>
<dt><a class="co" id="callout_reinforcement_learning_CO12-3" href="#co_reinforcement_learning_CO12-4"><img src="Images/3.png" alt="3" width="12" height="12"/></a></dt>
<dd><p>The minimum gross performance required</p></dd>
<dt><a class="co" id="callout_reinforcement_learning_CO12-4" href="#co_reinforcement_learning_CO12-5"><img src="Images/4.png" alt="4" width="12" height="12"/></a></dt>
<dd><p>Additional financial features (simple moving average, momentum, rolling 
<span class="keep-together">volatility)</span></p></dd>
<dt><a class="co" id="callout_reinforcement_learning_CO12-5" href="#co_reinforcement_learning_CO12-8"><img src="Images/5.png" alt="5" width="12" height="12"/></a></dt>
<dd><p>Gaussian normalization of the data</p></dd>
<dt><a class="co" id="callout_reinforcement_learning_CO12-6" href="#co_reinforcement_learning_CO12-11"><img src="Images/6.png" alt="6" width="12" height="12"/></a></dt>
<dd><p>The leveraged return for the step</p></dd>
<dt><a class="co" id="callout_reinforcement_learning_CO12-7" href="#co_reinforcement_learning_CO12-12"><img src="Images/7.png" alt="7" width="12" height="12"/></a></dt>
<dd><p>The return-based reward for the step</p></dd>
<dt><a class="co" id="callout_reinforcement_learning_CO12-8" href="#co_reinforcement_learning_CO12-13"><img src="Images/8.png" alt="8" width="12" height="12"/></a></dt>
<dd><p>The gross performance after the step</p></dd>
</dl>

<p>The new <code>Finance</code> class gives more flexibility for the modeling of the financial market environment. The following code shows an example for two features and five lags:<a data-type="indexterm" data-primary="" data-startref="ix_agents_dql" id="idm45625276420824"/><a data-type="indexterm" data-primary="" data-startref="ix_agent_ql_ch9" id="idm45625276419848"/><a data-type="indexterm" data-primary="" data-startref="ix_dql_agent_ch9" id="idm45625276418904"/><a data-type="indexterm" data-primary="" data-startref="ix_ql_dql_agent_ch9" id="idm45625276372888"/><a data-type="indexterm" data-primary="" data-startref="ix_RL_dql_agent_ch9" id="idm45625276371944"/><a data-type="indexterm" data-primary="" data-startref="ix_fin_market_RL" id="idm45625276371000"/></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">In</code> <code class="p">[</code><code class="mi">65</code><code class="p">]:</code> <code class="n">env</code> <code class="o">=</code> <code class="n">Finance</code><code class="p">(</code><code class="s1">'EUR='</code><code class="p">,</code> <code class="p">[</code><code class="s1">'EUR='</code><code class="p">,</code> <code class="s1">'r'</code><code class="p">],</code> <code class="mi">10</code><code class="p">,</code> <code class="mi">5</code><code class="p">)</code>

<code class="n">In</code> <code class="p">[</code><code class="mi">66</code><code class="p">]:</code> <code class="n">a</code> <code class="o">=</code> <code class="n">env</code><code class="o">.</code><code class="n">action_space</code><code class="o">.</code><code class="n">sample</code><code class="p">()</code>
         <code class="n">a</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">66</code><code class="p">]:</code> <code class="mi">0</code>

<code class="n">In</code> <code class="p">[</code><code class="mi">67</code><code class="p">]:</code> <code class="n">env</code><code class="o">.</code><code class="n">reset</code><code class="p">()</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">67</code><code class="p">]:</code> <code class="n">array</code><code class="p">([[</code> <code class="mf">1.7721</code><code class="p">,</code> <code class="o">-</code><code class="mf">1.0214</code><code class="p">],</code>
                <code class="p">[</code> <code class="mf">1.5973</code><code class="p">,</code> <code class="o">-</code><code class="mf">2.4432</code><code class="p">],</code>
                <code class="p">[</code> <code class="mf">1.5876</code><code class="p">,</code> <code class="o">-</code><code class="mf">0.1208</code><code class="p">],</code>
                <code class="p">[</code> <code class="mf">1.6292</code><code class="p">,</code>  <code class="mf">0.6083</code><code class="p">],</code>
                <code class="p">[</code> <code class="mf">1.6408</code><code class="p">,</code>  <code class="mf">0.1807</code><code class="p">]])</code>

<code class="n">In</code> <code class="p">[</code><code class="mi">68</code><code class="p">]:</code> <code class="n">env</code><code class="o">.</code><code class="n">step</code><code class="p">(</code><code class="n">a</code><code class="p">)</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">68</code><code class="p">]:</code> <code class="p">(</code><code class="n">array</code><code class="p">([[</code> <code class="mf">1.5973</code><code class="p">,</code> <code class="o">-</code><code class="mf">2.4432</code><code class="p">],</code>
                 <code class="p">[</code> <code class="mf">1.5876</code><code class="p">,</code> <code class="o">-</code><code class="mf">0.1208</code><code class="p">],</code>
                 <code class="p">[</code> <code class="mf">1.6292</code><code class="p">,</code>  <code class="mf">0.6083</code><code class="p">],</code>
                 <code class="p">[</code> <code class="mf">1.6408</code><code class="p">,</code>  <code class="mf">0.1807</code><code class="p">],</code>
                 <code class="p">[</code> <code class="mf">1.5725</code><code class="p">,</code> <code class="o">-</code><code class="mf">0.9502</code><code class="p">]]),</code>
          <code class="mf">1.0272827803740798</code><code class="p">,</code>
          <code class="bp">False</code><code class="p">,</code>
          <code class="p">{})</code></pre>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45625276178280">
<h5>Different Types of Environments and Data</h5>
<p><a data-type="indexterm" data-primary="CartPole game" data-secondary="versus Finance environment" data-secondary-sortas="Finance environment" id="idm45625276177272"/>It is important to notice that there is a fundamental difference between the <code>CartPole</code> environment and the two versions of the <code>Finance</code> environment. In the <code>CartPole</code> environment, no data is available up front. Only an initial state is chosen with some degree of randomness. Given this state and the action taken by an agent, deterministic transformations are applied to generate new states (data). This is possible since a physical system is simulated that follows physical laws.</p>

<p>The <code>Finance</code> environment, on the other hand, starts with real, historical market data and only presents the available data to the agent in similar fashion as the <code>CartPole</code> environment (that is, step by step and state by state). In this case, the action of the agent does not really influence the environment; the environment instead evolves deterministically, and the agent learns how to behave optimally—trade profitably—in that environment.</p>

<p>In that sense, the <code>Finance</code> environment is more comparable, say, to the problem of finding the fastest way through a labyrinth. In such a case, the data representing the labyrinth is given up front and the agent is only presented with the relevant sub-set of the data (the current state) as it moves around the labyrinth.</p>
</div></aside>
</div></section>













<section data-type="sect1" data-pdf-bookmark="FQL Agent"><div class="sect1" id="rl_fql_agent">
<h1>FQL Agent</h1>

<p><a data-type="indexterm" data-primary="RL (reinforcement learning)" data-secondary="FQLAgent" id="ix_RL_fql_agent_ch9"/><a data-type="indexterm" data-primary="FQLAgent" id="ix_fql_agent"/><a data-type="indexterm" data-primary="QL (Q-learning)" data-secondary="FQLAgent" id="ix_ql_fql_agent_ch9"/><a data-type="indexterm" data-primary="agents" data-secondary="FQLAgent" id="ix_agent_fql"/>Relying on the new <code>Finance</code> environment, this section improves on the simple DQL agent to improve the performance in the financial market context. The <code>FQLAgent</code> class is able to handle multiple features and a flexible number of lags. It also 
<span class="keep-together">distinguishes</span> the learning environment (<code>learn_env</code>) from the validation environment (<code>valid_env</code>). This allows one to gain a more realistic picture of the out-of-sample performance of the agent during training. The basic structure of the class and the RL/QL learning approach is the same for both the <code>DQLAgent</code> class and the 
<span class="keep-together"><code>FQLAgent</code> class:</span></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">In</code> <code class="p">[</code><code class="mi">69</code><code class="p">]:</code> <code class="k">class</code> <code class="nc">FQLAgent</code><code class="p">:</code>
             <code class="k">def</code> <code class="nf-Magic">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">hidden_units</code><code class="p">,</code> <code class="n">learning_rate</code><code class="p">,</code> <code class="n">learn_env</code><code class="p">,</code> <code class="n">valid_env</code><code class="p">):</code>
                 <code class="bp">self</code><code class="o">.</code><code class="n">learn_env</code> <code class="o">=</code> <code class="n">learn_env</code>
                 <code class="bp">self</code><code class="o">.</code><code class="n">valid_env</code> <code class="o">=</code> <code class="n">valid_env</code>
                 <code class="bp">self</code><code class="o">.</code><code class="n">epsilon</code> <code class="o">=</code> <code class="mf">1.0</code>
                 <code class="bp">self</code><code class="o">.</code><code class="n">epsilon_min</code> <code class="o">=</code> <code class="mf">0.1</code>
                 <code class="bp">self</code><code class="o">.</code><code class="n">epsilon_decay</code> <code class="o">=</code> <code class="mf">0.98</code>
                 <code class="bp">self</code><code class="o">.</code><code class="n">learning_rate</code> <code class="o">=</code> <code class="n">learning_rate</code>
                 <code class="bp">self</code><code class="o">.</code><code class="n">gamma</code> <code class="o">=</code> <code class="mf">0.95</code>
                 <code class="bp">self</code><code class="o">.</code><code class="n">batch_size</code> <code class="o">=</code> <code class="mi">128</code>
                 <code class="bp">self</code><code class="o">.</code><code class="n">max_treward</code> <code class="o">=</code> <code class="mi">0</code>
                 <code class="bp">self</code><code class="o">.</code><code class="n">trewards</code> <code class="o">=</code> <code class="nb">list</code><code class="p">()</code>
                 <code class="bp">self</code><code class="o">.</code><code class="n">averages</code> <code class="o">=</code> <code class="nb">list</code><code class="p">()</code>
                 <code class="bp">self</code><code class="o">.</code><code class="n">performances</code> <code class="o">=</code> <code class="nb">list</code><code class="p">()</code>
                 <code class="bp">self</code><code class="o">.</code><code class="n">aperformances</code> <code class="o">=</code> <code class="nb">list</code><code class="p">()</code>
                 <code class="bp">self</code><code class="o">.</code><code class="n">vperformances</code> <code class="o">=</code> <code class="nb">list</code><code class="p">()</code>
                 <code class="bp">self</code><code class="o">.</code><code class="n">memory</code> <code class="o">=</code> <code class="n">deque</code><code class="p">(</code><code class="n">maxlen</code><code class="o">=</code><code class="mi">2000</code><code class="p">)</code>
                 <code class="bp">self</code><code class="o">.</code><code class="n">model</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">_build_model</code><code class="p">(</code><code class="n">hidden_units</code><code class="p">,</code> <code class="n">learning_rate</code><code class="p">)</code>

             <code class="k">def</code> <code class="nf">_build_model</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">hu</code><code class="p">,</code> <code class="n">lr</code><code class="p">):</code>
                 <code class="n">model</code> <code class="o">=</code> <code class="n">Sequential</code><code class="p">()</code>
                 <code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dense</code><code class="p">(</code><code class="n">hu</code><code class="p">,</code> <code class="n">input_shape</code><code class="o">=</code><code class="p">(</code>
                     <code class="bp">self</code><code class="o">.</code><code class="n">learn_env</code><code class="o">.</code><code class="n">lags</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">learn_env</code><code class="o">.</code><code class="n">n_features</code><code class="p">),</code>
                                 <code class="n">activation</code><code class="o">=</code><code class="s1">'relu'</code><code class="p">))</code>
                 <code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dropout</code><code class="p">(</code><code class="mf">0.3</code><code class="p">,</code> <code class="n">seed</code><code class="o">=</code><code class="mi">100</code><code class="p">))</code>
                 <code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dense</code><code class="p">(</code><code class="n">hu</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s1">'relu'</code><code class="p">))</code>
                 <code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dropout</code><code class="p">(</code><code class="mf">0.3</code><code class="p">,</code> <code class="n">seed</code><code class="o">=</code><code class="mi">100</code><code class="p">))</code>
                 <code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dense</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s1">'linear'</code><code class="p">))</code>
                 <code class="n">model</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code>
                     <code class="n">loss</code><code class="o">=</code><code class="s1">'mse'</code><code class="p">,</code>
                     <code class="n">optimizer</code><code class="o">=</code><code class="n">RMSprop</code><code class="p">(</code><code class="n">lr</code><code class="o">=</code><code class="n">lr</code><code class="p">)</code>
                 <code class="p">)</code>
                 <code class="k">return</code> <code class="n">model</code>

             <code class="k">def</code> <code class="nf">act</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">state</code><code class="p">):</code>
                 <code class="k">if</code> <code class="n">random</code><code class="o">.</code><code class="n">random</code><code class="p">()</code> <code class="o">&lt;=</code> <code class="bp">self</code><code class="o">.</code><code class="n">epsilon</code><code class="p">:</code>
                     <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">learn_env</code><code class="o">.</code><code class="n">action_space</code><code class="o">.</code><code class="n">sample</code><code class="p">()</code>
                 <code class="n">action</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">state</code><code class="p">)[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">]</code>
                 <code class="k">return</code> <code class="n">np</code><code class="o">.</code><code class="n">argmax</code><code class="p">(</code><code class="n">action</code><code class="p">)</code>

             <code class="k">def</code> <code class="nf">replay</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
                 <code class="n">batch</code> <code class="o">=</code> <code class="n">random</code><code class="o">.</code><code class="n">sample</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">memory</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">batch_size</code><code class="p">)</code>
                 <code class="k">for</code> <code class="n">state</code><code class="p">,</code> <code class="n">action</code><code class="p">,</code> <code class="n">reward</code><code class="p">,</code> <code class="n">next_state</code><code class="p">,</code> <code class="n">done</code> <code class="ow">in</code> <code class="n">batch</code><code class="p">:</code>
                     <code class="k">if</code> <code class="ow">not</code> <code class="n">done</code><code class="p">:</code>
                         <code class="n">reward</code> <code class="o">+=</code> <code class="bp">self</code><code class="o">.</code><code class="n">gamma</code> <code class="o">*</code> <code class="n">np</code><code class="o">.</code><code class="n">amax</code><code class="p">(</code>
                             <code class="bp">self</code><code class="o">.</code><code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">next_state</code><code class="p">)[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">])</code>
                     <code class="n">target</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">state</code><code class="p">)</code>
                     <code class="n">target</code><code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="n">action</code><code class="p">]</code> <code class="o">=</code> <code class="n">reward</code>
                     <code class="bp">self</code><code class="o">.</code><code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">state</code><code class="p">,</code> <code class="n">target</code><code class="p">,</code> <code class="n">epochs</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code>
                                    <code class="n">verbose</code><code class="o">=</code><code class="bp">False</code><code class="p">)</code>
                 <code class="k">if</code> <code class="bp">self</code><code class="o">.</code><code class="n">epsilon</code> <code class="o">&gt;</code> <code class="bp">self</code><code class="o">.</code><code class="n">epsilon_min</code><code class="p">:</code>
                     <code class="bp">self</code><code class="o">.</code><code class="n">epsilon</code> <code class="o">*=</code> <code class="bp">self</code><code class="o">.</code><code class="n">epsilon_decay</code>

             <code class="k">def</code> <code class="nf">learn</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">episodes</code><code class="p">):</code>
                 <code class="k">for</code> <code class="n">e</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="n">episodes</code> <code class="o">+</code> <code class="mi">1</code><code class="p">):</code>
                     <code class="n">state</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">learn_env</code><code class="o">.</code><code class="n">reset</code><code class="p">()</code>
                     <code class="n">state</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">state</code><code class="p">,</code> <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">learn_env</code><code class="o">.</code><code class="n">lags</code><code class="p">,</code>
                                                <code class="bp">self</code><code class="o">.</code><code class="n">learn_env</code><code class="o">.</code><code class="n">n_features</code><code class="p">])</code>
                     <code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">10000</code><code class="p">):</code>
                         <code class="n">action</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">act</code><code class="p">(</code><code class="n">state</code><code class="p">)</code>
                         <code class="n">next_state</code><code class="p">,</code> <code class="n">reward</code><code class="p">,</code> <code class="n">done</code><code class="p">,</code> <code class="n">info</code> <code class="o">=</code> \
                                         <code class="bp">self</code><code class="o">.</code><code class="n">learn_env</code><code class="o">.</code><code class="n">step</code><code class="p">(</code><code class="n">action</code><code class="p">)</code>
                         <code class="n">next_state</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">next_state</code><code class="p">,</code>
                                         <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">learn_env</code><code class="o">.</code><code class="n">lags</code><code class="p">,</code>
                                          <code class="bp">self</code><code class="o">.</code><code class="n">learn_env</code><code class="o">.</code><code class="n">n_features</code><code class="p">])</code>
                         <code class="bp">self</code><code class="o">.</code><code class="n">memory</code><code class="o">.</code><code class="n">append</code><code class="p">([</code><code class="n">state</code><code class="p">,</code> <code class="n">action</code><code class="p">,</code> <code class="n">reward</code><code class="p">,</code>
                                              <code class="n">next_state</code><code class="p">,</code> <code class="n">done</code><code class="p">])</code>
                         <code class="n">state</code> <code class="o">=</code> <code class="n">next_state</code>
                         <code class="k">if</code> <code class="n">done</code><code class="p">:</code>
                             <code class="n">treward</code> <code class="o">=</code> <code class="n">_</code> <code class="o">+</code> <code class="mi">1</code>
                             <code class="bp">self</code><code class="o">.</code><code class="n">trewards</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">treward</code><code class="p">)</code>
                             <code class="n">av</code> <code class="o">=</code> <code class="nb">sum</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">trewards</code><code class="p">[</code><code class="o">-</code><code class="mi">25</code><code class="p">:])</code> <code class="o">/</code> <code class="mi">25</code>
                             <code class="n">perf</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">learn_env</code><code class="o">.</code><code class="n">performance</code>
                             <code class="bp">self</code><code class="o">.</code><code class="n">averages</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">av</code><code class="p">)</code>
                             <code class="bp">self</code><code class="o">.</code><code class="n">performances</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">perf</code><code class="p">)</code>
                             <code class="bp">self</code><code class="o">.</code><code class="n">aperformances</code><code class="o">.</code><code class="n">append</code><code class="p">(</code>
                                 <code class="nb">sum</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">performances</code><code class="p">[</code><code class="o">-</code><code class="mi">25</code><code class="p">:])</code> <code class="o">/</code> <code class="mi">25</code><code class="p">)</code>
                             <code class="bp">self</code><code class="o">.</code><code class="n">max_treward</code> <code class="o">=</code> <code class="nb">max</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">max_treward</code><code class="p">,</code> <code class="n">treward</code><code class="p">)</code>
                             <code class="n">templ</code> <code class="o">=</code> <code class="s1">'episode: {:2d}/{} | treward: {:4d} | '</code>
                             <code class="n">templ</code> <code class="o">+=</code> <code class="s1">'perf: {:5.3f} | av: {:5.1f} | max: {:4d}'</code>
                             <code class="k">print</code><code class="p">(</code><code class="n">templ</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">e</code><code class="p">,</code> <code class="n">episodes</code><code class="p">,</code> <code class="n">treward</code><code class="p">,</code> <code class="n">perf</code><code class="p">,</code>
                                           <code class="n">av</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">max_treward</code><code class="p">),</code> <code class="n">end</code><code class="o">=</code><code class="s1">'</code><code class="se">\r</code><code class="s1">'</code><code class="p">)</code>
                             <code class="k">break</code>
                     <code class="bp">self</code><code class="o">.</code><code class="n">validate</code><code class="p">(</code><code class="n">e</code><code class="p">,</code> <code class="n">episodes</code><code class="p">)</code>
                     <code class="k">if</code> <code class="nb">len</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">memory</code><code class="p">)</code> <code class="o">&gt;</code> <code class="bp">self</code><code class="o">.</code><code class="n">batch_size</code><code class="p">:</code>
                         <code class="bp">self</code><code class="o">.</code><code class="n">replay</code><code class="p">()</code>
             <code class="k">def</code> <code class="nf">validate</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">e</code><code class="p">,</code> <code class="n">episodes</code><code class="p">):</code>
                 <code class="n">state</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">valid_env</code><code class="o">.</code><code class="n">reset</code><code class="p">()</code>
                 <code class="n">state</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">state</code><code class="p">,</code> <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">valid_env</code><code class="o">.</code><code class="n">lags</code><code class="p">,</code>
                                            <code class="bp">self</code><code class="o">.</code><code class="n">valid_env</code><code class="o">.</code><code class="n">n_features</code><code class="p">])</code>
                 <code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">10000</code><code class="p">):</code>
                     <code class="n">action</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">argmax</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">state</code><code class="p">)[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">])</code>
                     <code class="n">next_state</code><code class="p">,</code> <code class="n">reward</code><code class="p">,</code> <code class="n">done</code><code class="p">,</code> <code class="n">info</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">valid_env</code><code class="o">.</code><code class="n">step</code><code class="p">(</code><code class="n">action</code><code class="p">)</code>
                     <code class="n">state</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">next_state</code><code class="p">,</code> <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">valid_env</code><code class="o">.</code><code class="n">lags</code><code class="p">,</code>
                                            <code class="bp">self</code><code class="o">.</code><code class="n">valid_env</code><code class="o">.</code><code class="n">n_features</code><code class="p">])</code>
                     <code class="k">if</code> <code class="n">done</code><code class="p">:</code>
                         <code class="n">treward</code> <code class="o">=</code> <code class="n">_</code> <code class="o">+</code> <code class="mi">1</code>
                         <code class="n">perf</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">valid_env</code><code class="o">.</code><code class="n">performance</code>
                         <code class="bp">self</code><code class="o">.</code><code class="n">vperformances</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">perf</code><code class="p">)</code>
                         <code class="k">if</code> <code class="n">e</code> <code class="o">%</code> <code class="mi">20</code> <code class="o">==</code> <code class="mi">0</code><code class="p">:</code>
                             <code class="n">templ</code> <code class="o">=</code> <code class="mi">71</code> <code class="o">*</code> <code class="s1">'='</code>
                             <code class="n">templ</code> <code class="o">+=</code> <code class="s1">'</code><code class="se">\n</code><code class="s1">episode: {:2d}/{} | VALIDATION | '</code>
                             <code class="n">templ</code> <code class="o">+=</code> <code class="s1">'treward: {:4d} | perf: {:5.3f} | '</code>
                             <code class="n">templ</code> <code class="o">+=</code> <code class="s1">'eps: {:.2f}</code><code class="se">\n</code><code class="s1">'</code>
                             <code class="n">templ</code> <code class="o">+=</code> <code class="mi">71</code> <code class="o">*</code> <code class="s1">'='</code>
                             <code class="k">print</code><code class="p">(</code><code class="n">templ</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">e</code><code class="p">,</code> <code class="n">episodes</code><code class="p">,</code> <code class="n">treward</code><code class="p">,</code>
                                                <code class="n">perf</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">epsilon</code><code class="p">))</code>
                         <code class="k">break</code></pre>

<p>The following Python code shows that the performance of the <code>FQLAgent</code> is substantially better than that of the simple <code>DQLAgent</code> that solves the <code>CartPole</code> problem. This trading bot seems to learn about trading rather consistently through interacting with the financial market environment (see <a data-type="xref" href="#figure_rl_03">Figure 9-4</a>):</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">In</code> <code class="p">[</code><code class="mi">70</code><code class="p">]:</code> <code class="n">symbol</code> <code class="o">=</code> <code class="s1">'EUR='</code>
         <code class="n">features</code> <code class="o">=</code> <code class="p">[</code><code class="n">symbol</code><code class="p">,</code> <code class="s1">'r'</code><code class="p">,</code> <code class="s1">'s'</code><code class="p">,</code> <code class="s1">'m'</code><code class="p">,</code> <code class="s1">'v'</code><code class="p">]</code>

<code class="n">In</code> <code class="p">[</code><code class="mi">71</code><code class="p">]:</code> <code class="n">a</code> <code class="o">=</code> <code class="mi">0</code>
         <code class="n">b</code> <code class="o">=</code> <code class="mi">2000</code>
         <code class="n">c</code> <code class="o">=</code> <code class="mi">500</code>

<code class="n">In</code> <code class="p">[</code><code class="mi">72</code><code class="p">]:</code> <code class="n">learn_env</code> <code class="o">=</code> <code class="n">Finance</code><code class="p">(</code><code class="n">symbol</code><code class="p">,</code> <code class="n">features</code><code class="p">,</code> <code class="n">window</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">lags</code><code class="o">=</code><code class="mi">6</code><code class="p">,</code>
                          <code class="n">leverage</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">min_performance</code><code class="o">=</code><code class="mf">0.85</code><code class="p">,</code>
                          <code class="n">start</code><code class="o">=</code><code class="n">a</code><code class="p">,</code> <code class="n">end</code><code class="o">=</code><code class="n">a</code> <code class="o">+</code> <code class="n">b</code><code class="p">,</code> <code class="n">mu</code><code class="o">=</code><code class="bp">None</code><code class="p">,</code> <code class="n">std</code><code class="o">=</code><code class="bp">None</code><code class="p">)</code>

<code class="n">In</code> <code class="p">[</code><code class="mi">73</code><code class="p">]:</code> <code class="n">learn_env</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">info</code><code class="p">()</code>
         <code class="o">&lt;</code><code class="k">class</code> <code class="err">'</code><code class="nc">pandas</code><code class="o">.</code><code class="n">core</code><code class="o">.</code><code class="n">frame</code><code class="o">.</code><code class="n">DataFrame</code><code class="s1">'&gt;</code>
         <code class="n">DatetimeIndex</code><code class="p">:</code> <code class="mi">2000</code> <code class="n">entries</code><code class="p">,</code> <code class="mi">2010</code><code class="o">-</code><code class="mo">01</code><code class="o">-</code><code class="mi">19</code> <code class="n">to</code> <code class="mi">2017</code><code class="o">-</code><code class="mi">12</code><code class="o">-</code><code class="mi">26</code>
         <code class="n">Data</code> <code class="n">columns</code> <code class="p">(</code><code class="n">total</code> <code class="mi">5</code> <code class="n">columns</code><code class="p">):</code>
          <code class="c1">#   Column  Non-Null Count  Dtype</code>
         <code class="o">---</code>  <code class="o">------</code>  <code class="o">--------------</code>  <code class="o">-----</code>
          <code class="mi">0</code>   <code class="n">EUR</code><code class="o">=</code>    <code class="mi">2000</code> <code class="n">non</code><code class="o">-</code><code class="n">null</code>   <code class="n">float64</code>
          <code class="mi">1</code>   <code class="n">r</code>       <code class="mi">2000</code> <code class="n">non</code><code class="o">-</code><code class="n">null</code>   <code class="n">float64</code>
          <code class="mi">2</code>   <code class="n">s</code>       <code class="mi">2000</code> <code class="n">non</code><code class="o">-</code><code class="n">null</code>   <code class="n">float64</code>
          <code class="mi">3</code>   <code class="n">m</code>       <code class="mi">2000</code> <code class="n">non</code><code class="o">-</code><code class="n">null</code>   <code class="n">float64</code>
          <code class="mi">4</code>   <code class="n">v</code>       <code class="mi">2000</code> <code class="n">non</code><code class="o">-</code><code class="n">null</code>   <code class="n">float64</code>
         <code class="n">dtypes</code><code class="p">:</code> <code class="n">float64</code><code class="p">(</code><code class="mi">5</code><code class="p">)</code>
         <code class="n">memory</code> <code class="n">usage</code><code class="p">:</code> <code class="mf">93.8</code> <code class="n">KB</code>

<code class="n">In</code> <code class="p">[</code><code class="mi">74</code><code class="p">]:</code> <code class="n">valid_env</code> <code class="o">=</code> <code class="n">Finance</code><code class="p">(</code><code class="n">symbol</code><code class="p">,</code> <code class="n">features</code><code class="p">,</code> <code class="n">window</code><code class="o">=</code><code class="n">learn_env</code><code class="o">.</code><code class="n">window</code><code class="p">,</code>
                          <code class="n">lags</code><code class="o">=</code><code class="n">learn_env</code><code class="o">.</code><code class="n">lags</code><code class="p">,</code> <code class="n">leverage</code><code class="o">=</code><code class="n">learn_env</code><code class="o">.</code><code class="n">leverage</code><code class="p">,</code>
                          <code class="n">min_performance</code><code class="o">=</code><code class="n">learn_env</code><code class="o">.</code><code class="n">min_performance</code><code class="p">,</code>
                          <code class="n">start</code><code class="o">=</code><code class="n">a</code> <code class="o">+</code> <code class="n">b</code><code class="p">,</code> <code class="n">end</code><code class="o">=</code><code class="n">a</code> <code class="o">+</code> <code class="n">b</code> <code class="o">+</code> <code class="n">c</code><code class="p">,</code>
                          <code class="n">mu</code><code class="o">=</code><code class="n">learn_env</code><code class="o">.</code><code class="n">mu</code><code class="p">,</code> <code class="n">std</code><code class="o">=</code><code class="n">learn_env</code><code class="o">.</code><code class="n">std</code><code class="p">)</code>

<code class="n">In</code> <code class="p">[</code><code class="mi">75</code><code class="p">]:</code> <code class="n">valid_env</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">info</code><code class="p">()</code>
         <code class="o">&lt;</code><code class="k">class</code> <code class="err">'</code><code class="nc">pandas</code><code class="o">.</code><code class="n">core</code><code class="o">.</code><code class="n">frame</code><code class="o">.</code><code class="n">DataFrame</code><code class="s1">'&gt;</code>
         <code class="n">DatetimeIndex</code><code class="p">:</code> <code class="mi">500</code> <code class="n">entries</code><code class="p">,</code> <code class="mi">2017</code><code class="o">-</code><code class="mi">12</code><code class="o">-</code><code class="mi">27</code> <code class="n">to</code> <code class="mi">2019</code><code class="o">-</code><code class="mi">12</code><code class="o">-</code><code class="mi">20</code>
         <code class="n">Data</code> <code class="n">columns</code> <code class="p">(</code><code class="n">total</code> <code class="mi">5</code> <code class="n">columns</code><code class="p">):</code>
          <code class="c1">#   Column  Non-Null Count  Dtype</code>
         <code class="o">---</code>  <code class="o">------</code>  <code class="o">--------------</code>  <code class="o">-----</code>
          <code class="mi">0</code>   <code class="n">EUR</code><code class="o">=</code>    <code class="mi">500</code> <code class="n">non</code><code class="o">-</code><code class="n">null</code>    <code class="n">float64</code>
          <code class="mi">1</code>   <code class="n">r</code>       <code class="mi">500</code> <code class="n">non</code><code class="o">-</code><code class="n">null</code>    <code class="n">float64</code>
          <code class="mi">2</code>   <code class="n">s</code>       <code class="mi">500</code> <code class="n">non</code><code class="o">-</code><code class="n">null</code>    <code class="n">float64</code>
          <code class="mi">3</code>   <code class="n">m</code>       <code class="mi">500</code> <code class="n">non</code><code class="o">-</code><code class="n">null</code>    <code class="n">float64</code>
          <code class="mi">4</code>   <code class="n">v</code>       <code class="mi">500</code> <code class="n">non</code><code class="o">-</code><code class="n">null</code>    <code class="n">float64</code>
         <code class="n">dtypes</code><code class="p">:</code> <code class="n">float64</code><code class="p">(</code><code class="mi">5</code><code class="p">)</code>
         <code class="n">memory</code> <code class="n">usage</code><code class="p">:</code> <code class="mf">23.4</code> <code class="n">KB</code>

<code class="n">In</code> <code class="p">[</code><code class="mi">76</code><code class="p">]:</code> <code class="n">set_seeds</code><code class="p">(</code><code class="mi">100</code><code class="p">)</code>
         <code class="n">agent</code> <code class="o">=</code> <code class="n">FQLAgent</code><code class="p">(</code><code class="mi">24</code><code class="p">,</code> <code class="mf">0.0001</code><code class="p">,</code> <code class="n">learn_env</code><code class="p">,</code> <code class="n">valid_env</code><code class="p">)</code>

<code class="n">In</code> <code class="p">[</code><code class="mi">77</code><code class="p">]:</code> <code class="n">episodes</code> <code class="o">=</code> <code class="mi">61</code>

<code class="n">In</code> <code class="p">[</code><code class="mi">78</code><code class="p">]:</code> <code class="n">agent</code><code class="o">.</code><code class="n">learn</code><code class="p">(</code><code class="n">episodes</code><code class="p">)</code>
         <code class="o">=======================================================================</code>
         <code class="n">episode</code><code class="p">:</code> <code class="mi">20</code><code class="o">/</code><code class="mi">61</code> <code class="o">|</code> <code class="n">VALIDATION</code> <code class="o">|</code> <code class="n">treward</code><code class="p">:</code>  <code class="mi">494</code> <code class="o">|</code> <code class="n">perf</code><code class="p">:</code> <code class="mf">1.169</code> <code class="o">|</code> <code class="n">eps</code><code class="p">:</code> <code class="mf">0.68</code>
         <code class="o">=======================================================================</code>
         <code class="o">=======================================================================</code>
         <code class="n">episode</code><code class="p">:</code> <code class="mi">40</code><code class="o">/</code><code class="mi">61</code> <code class="o">|</code> <code class="n">VALIDATION</code> <code class="o">|</code> <code class="n">treward</code><code class="p">:</code>  <code class="mi">494</code> <code class="o">|</code> <code class="n">perf</code><code class="p">:</code> <code class="mf">1.111</code> <code class="o">|</code> <code class="n">eps</code><code class="p">:</code> <code class="mf">0.45</code>
         <code class="o">=======================================================================</code>
         <code class="o">=======================================================================</code>
         <code class="n">episode</code><code class="p">:</code> <code class="mi">60</code><code class="o">/</code><code class="mi">61</code> <code class="o">|</code> <code class="n">VALIDATION</code> <code class="o">|</code> <code class="n">treward</code><code class="p">:</code>  <code class="mi">494</code> <code class="o">|</code> <code class="n">perf</code><code class="p">:</code> <code class="mf">1.089</code> <code class="o">|</code> <code class="n">eps</code><code class="p">:</code> <code class="mf">0.30</code>
         <code class="o">=======================================================================</code>
         <code class="n">episode</code><code class="p">:</code> <code class="mi">61</code><code class="o">/</code><code class="mi">61</code> <code class="o">|</code> <code class="n">treward</code><code class="p">:</code> <code class="mi">1994</code> <code class="o">|</code> <code class="n">perf</code><code class="p">:</code> <code class="mf">1.268</code> <code class="o">|</code> <code class="n">av</code><code class="p">:</code> <code class="mf">1615.1</code> <code class="o">|</code> <code class="nb">max</code><code class="p">:</code> <code class="mi">1994</code>
<code class="n">In</code> <code class="p">[</code><code class="mi">79</code><code class="p">]:</code> <code class="n">agent</code><code class="o">.</code><code class="n">epsilon</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">79</code><code class="p">]:</code> <code class="mf">0.291602079838278</code>

<code class="n">In</code> <code class="p">[</code><code class="mi">80</code><code class="p">]:</code> <code class="n">plt</code><code class="o">.</code><code class="n">figure</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="mi">6</code><code class="p">))</code>
         <code class="n">x</code> <code class="o">=</code> <code class="nb">range</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="nb">len</code><code class="p">(</code><code class="n">agent</code><code class="o">.</code><code class="n">averages</code><code class="p">)</code> <code class="o">+</code> <code class="mi">1</code><code class="p">)</code>
         <code class="n">y</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">polyval</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">polyfit</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">agent</code><code class="o">.</code><code class="n">averages</code><code class="p">,</code> <code class="n">deg</code><code class="o">=</code><code class="mi">3</code><code class="p">),</code> <code class="n">x</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">agent</code><code class="o">.</code><code class="n">averages</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s1">'moving average'</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="s1">'r--'</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s1">'regression'</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s1">'episodes'</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s1">'total reward'</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">legend</code><code class="p">();</code></pre>

<figure class="thumb"><div id="figure_rl_03" class="figure">
<img src="Images/aiif_0904.png" alt="aiif 0904" width="2484" height="1491"/>
<h6><span class="label">Figure 9-4. </span>Average total rewards of <code>FQLAgent</code> for <code>Finance</code></h6>
</div></figure>

<p>An interesting picture also arises for the training and validation performances, as shown in <a data-type="xref" href="#figure_rl_04">Figure 9-5</a>. The training performance shows a high variance, which is due, for example, to the exploration that is going on in addition to the exploitation of the currently optimal policy. In comparison, the validation performance has a much lower variance because it only relies on the exploitation of the currently optimal 
<span class="keep-together">policy:</span><a data-type="indexterm" data-primary="" data-startref="ix_RL_ch9" id="idm45625275685336"/><a data-type="indexterm" data-primary="" data-startref="ix_stat_ineffic_RL_ch9" id="idm45625275684360"/><a data-type="indexterm" data-primary="" data-startref="ix_fin_env_class_ch9" id="idm45625275683416"/><a data-type="indexterm" data-primary="" data-startref="ix_agent_fql" id="idm45625275682472"/><a data-type="indexterm" data-primary="" data-startref="ix_fql_agent" id="idm45625275681528"/><a data-type="indexterm" data-primary="" data-startref="ix_ql_fql_agent_ch9" id="idm45625275680584"/><a data-type="indexterm" data-primary="" data-startref="ix_RL_fql_agent_ch9" id="idm45625275277304"/></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">In</code> <code class="p">[</code><code class="mi">81</code><code class="p">]:</code> <code class="n">plt</code><code class="o">.</code><code class="n">figure</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="mi">6</code><code class="p">))</code>
         <code class="n">x</code> <code class="o">=</code> <code class="nb">range</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="nb">len</code><code class="p">(</code><code class="n">agent</code><code class="o">.</code><code class="n">performances</code><code class="p">)</code> <code class="o">+</code> <code class="mi">1</code><code class="p">)</code>
         <code class="n">y</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">polyval</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">polyfit</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">agent</code><code class="o">.</code><code class="n">performances</code><code class="p">,</code> <code class="n">deg</code><code class="o">=</code><code class="mi">3</code><code class="p">),</code> <code class="n">x</code><code class="p">)</code>
         <code class="n">y_</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">polyval</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">polyfit</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">agent</code><code class="o">.</code><code class="n">vperformances</code><code class="p">,</code> <code class="n">deg</code><code class="o">=</code><code class="mi">3</code><code class="p">),</code> <code class="n">x</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">agent</code><code class="o">.</code><code class="n">performances</code><code class="p">[:],</code> <code class="n">label</code><code class="o">=</code><code class="s1">'training'</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">agent</code><code class="o">.</code><code class="n">vperformances</code><code class="p">[:],</code> <code class="n">label</code><code class="o">=</code><code class="s1">'validation'</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="s1">'r--'</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s1">'regression (train)'</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">y_</code><code class="p">,</code> <code class="s1">'r-.'</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s1">'regression (valid)'</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s1">'episodes'</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s1">'gross performance'</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">legend</code><code class="p">();</code></pre>

<figure class="thumb"><div id="figure_rl_04" class="figure">
<img src="Images/aiif_0905.png" alt="aiif 0905" width="2484" height="1491"/>
<h6><span class="label">Figure 9-5. </span>Training and validation performance of the <code>FQLAgent</code> per episode</h6>
</div></figure>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Conclusions"><div class="sect1" id="idm45625276376376">
<h1>Conclusions</h1>

<p>This chapter discusses reinforcement learning as one of the most successful algorithm classes that AI has to offer. Most of the advances and success stories discussed in <a data-type="xref" href="ch02.xhtml#superintelligence">Chapter 2</a> have their origin in improvements in the field of RL. In this context, neural networks are not rendered useless. To the contrary, they play an important role in approximating the optimal action policy, usually in the form of a policy <math alttext="upper Q">
  <mi>Q</mi>
</math> that, given a certain state, assigns each action a value. The higher the value is, the better the action will be, taking into account both immediate and delayed rewards.</p>

<p>The inclusion of delayed rewards, of course, is relevant in many important contexts. In a gaming context, with multiple actions available in general, it is optimal to choose the one that promises the highest total reward—and probably not just the highest immediate reward. The final total score is what is to be maximized. The same holds true in a financial context. The long-term performance is in general the appropriate goal for trading and investing, not a quick short-term profit that might come at an increased risk of going bankrupt.</p>

<p>The examples in this chapter also demonstrate that the RL approach is rather flexible and general in that it can be applied to different settings equally well. The DQL agent that solves the <code>CartPole</code> problem can also learn how to trade in a financial market, although not too well. Based on improvements of the <code>Finance</code> environment and the FQL agent, the FQL trading bot shows a respectable performance both in-sample (on the training data) and out-of-sample (on the validation data).</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="References"><div class="sect1" id="idm45625275140296">
<h1>References</h1>

<p>Books and papers cited in this chapter:</p>

<ul class="author-date-bib">
<li>
<p>Sutton, Richard S. and Andrew G. Barto. 2018. <em>Reinforcement Learning: An Introduction</em>. Cambridge and London: MIT Press.</p>
</li>
<li>
<p>Watkins, Christopher. 1989. <em>Learning from Delayed Rewards</em>. Ph.D. thesis, University of
Cambridge.</p>
</li>
<li>
<p>Watkins, Christopher and Peter Dayan. 1992. “Q-Learning.” <em>Machine Learning</em> 8 (May): 279-282.</p>
</li>
</ul>
</div></section>







<div data-type="footnotes"><p data-type="footnote" id="idm45625285042408"><sup><a href="ch09.xhtml#idm45625285042408-marker">1</a></sup> See <a href="https://oreil.ly/h-EFL">Deep Reinforcement Learning</a>.</p><p data-type="footnote" id="idm45625283606888"><sup><a href="ch09.xhtml#idm45625283606888-marker">2</a></sup> See, for example, this <a href="https://oreil.ly/84RwE">blog post</a>.</p><p data-type="footnote" id="idm45625283311016"><sup><a href="ch09.xhtml#idm45625283311016-marker">3</a></sup> The implementation is similar to the one found in this <a href="https://oreil.ly/8mI4m">blog post</a>.</p></div></div></section></div>



  </body></html>