["```py\nimport nltk\nimport nltk.data\nnltk.download('punkt')\n```", "```py\nfrom textblob import TextBlob\n```", "```py\nimport spacy\nnlp = spacy.load(\"en_core_web_lg\")\n```", "```py\n#Text to tokenize\ntext = \"This is a tokenize test\"\n```", "```py\nfrom nltk.tokenize import word_tokenize\nword_tokenize(text)\n```", "```py\n['This', 'is', 'a', 'tokenize', 'test']\n```", "```py\nTextBlob(text).words\n```", "```py\nWordList(['This', 'is', 'a', 'tokenize', 'test'])\n```", "```py\ntext = \"S&P and NASDAQ are the two most popular indices in US\"\n\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nstop_words = set(stopwords.words('english'))\ntext_tokens = word_tokenize(text)\ntokens_without_sw= [word for word in text_tokens if not word in stop_words]\n\nprint(tokens_without_sw)\n```", "```py\n['S', '&', 'P', 'NASDAQ', 'two', 'popular', 'indices', 'US']\n```", "```py\ntext = \"It's a Stemming testing\"\n\nparsed_text = word_tokenize(text)\n\n# Initialize stemmer.\nfrom nltk.stem.snowball import SnowballStemmer\nstemmer = SnowballStemmer('english')\n\n# Stem each word.\n[(word, stemmer.stem(word)) for i, word in enumerate(parsed_text)\n if word.lower() != stemmer.stem(parsed_text[i])]\n```", "```py\n[('Stemming', 'stem'), ('testing', 'test')]\n```", "```py\ntext = \"This world has a lot of faces \"\n\nfrom textblob import Word\nparsed_data= TextBlob(text).words\n[(word, word.lemmatize()) for i, word in enumerate(parsed_data)\n if word != parsed_data[i].lemmatize()]\n```", "```py\n[('has', 'ha'), ('faces', 'face')]\n```", "```py\ntext = 'Google is looking at buying U.K. startup for $1 billion'\nTextBlob(text).tags\n```", "```py\n[('Google', 'NNP'),\n ('is', 'VBZ'),\n ('looking', 'VBG'),\n ('at', 'IN'),\n ('buying', 'VBG'),\n ('U.K.', 'NNP'),\n ('startup', 'NN'),\n ('for', 'IN'),\n ('1', 'CD'),\n ('billion', 'CD')]\n```", "```py\ntext = 'Google is looking at buying U.K. startup for $1 billion'\n\nfor entity in nlp(text).ents:\n    print(\"Entity: \", entity.text)\n```", "```py\nEntity:  Google\nEntity:  U.K.\nEntity:  $1 billion\n```", "```py\nfrom spacy import displacy\ndisplacy.render(nlp(text), style=\"ent\", jupyter = True)\n```", "```py\nPython code text = 'Google is looking at buying U.K. startup for $1 billion'\ndoc = nlp(text)\npd.DataFrame([[t.text, t.is_stop, t.lemma_, t.pos_]\n              for t in doc],\n             columns=['Token', 'is_stop_word', 'lemma', 'POS'])\n```", "```py\nsentences = [\n'The stock price of google jumps on the earning data today',\n'Google plunge on China Data!'\n]\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer()\nprint( vectorizer.fit_transform(sentences).todense() )\nprint( vectorizer.vocabulary_ )\n```", "```py\n[[0 1 1 1 1 1 1 0 1 1 2 1]\n [1 1 0 1 0 0 1 1 0 0 0 0]]\n{'the': 10, 'stock': 9, 'price': 8, 'of': 5, 'google': 3, 'jumps':\\\n 4, 'on': 6, 'earning': 2, 'data': 1, 'today': 11, 'plunge': 7,\\\n 'china': 0}\n```", "```py\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\nTFIDF = vectorizer.fit_transform(sentences)\nprint(vectorizer.get_feature_names()[-10:])\nprint(TFIDF.shape)\nprint(TFIDF.toarray())\n```", "```py\n['china', 'data', 'earning', 'google', 'jumps', 'plunge', 'price', 'stock', \\\n'today']\n(2, 9)\n[[0\\.         0.29017021 0.4078241  0.29017021 0.4078241  0.\n  0.4078241  0.4078241  0.4078241 ]\n [0.57615236 0.40993715 0\\.         0.40993715 0\\.         0.57615236\n  0\\.         0\\.         0\\.        ]]\n```", "```py\ndoc = nlp(\"Apple orange cats dogs\")\nprint(\"Vector representation of the sentence for first 10 features: \\n\", \\\ndoc.vector[0:10])\n```", "```py\nVector representation of the sentence for first 10 features:\n [ -0.30732775 0.22351399 -0.110111   -0.367025   -0.13430001\n   0.13790375 -0.24379876 -0.10736975  0.2715925   1.3117325 ]\n```", "```py\nfrom gensim.models import Word2Vec\n\nsentences = [\n['The','stock','price', 'of', 'Google', 'increases'],\n['Google','plunge',' on','China',' Data!']]\n\n# train model\nmodel = Word2Vec(sentences, min_count=1)\n\n# summarize the loaded model\nwords = list(model.wv.vocab)\nprint(words)\nprint(model['Google'][1:5])\n```", "```py\n['The', 'stock', 'price', 'of', 'Google', 'increases', 'plunge', ' on', 'China',\\\n' Data!']\n[-1.7868265e-03 -7.6242397e-04  6.0105987e-05  3.5568199e-03\n]\n```", "```py\nsentences = [\n'The stock price of google jumps on the earning data today',\n'Google plunge on China Data!']\nsentiment = (1, 0)\ndata = pd.DataFrame({'Sentence':sentences,\n        'sentiment':sentiment})\n\n# feature extraction\nfrom sklearn.feature_extraction.text import CountVectorizer\nvect = CountVectorizer().fit(data['Sentence'])\nX_train_vectorized = vect.transform(data['Sentence'])\n\n# Running naive bayes model\nfrom sklearn.naive_bayes import MultinomialNB\nclfrNB = MultinomialNB(alpha=0.1)\nclfrNB.fit(X_train_vectorized, data['sentiment'])\n\n#Testing the model\npreds = clfrNB.predict(vect.transform(['Apple price plunge',\\\n 'Amazon price jumps']))\npreds\n```", "```py\narray([0, 1])\n```", "```py\nsentences = [\n'The stock price of google jumps on the earning data today',\n'Google plunge on China Data!'\n]\n\n#Getting the bag of words\nfrom sklearn.decomposition import LatentDirichletAllocation\nvect=CountVectorizer(ngram_range=(1, 1),stop_words='english')\nsentences_vec=vect.fit_transform(sentences)\n\n#Running LDA on the bag of words.\nfrom sklearn.feature_extraction.text import CountVectorizer\nlda=LatentDirichletAllocation(n_components=3)\nlda.fit_transform(sentences_vec)\n```", "```py\narray([[0.04283242, 0.91209846, 0.04506912],\n       [0.06793339, 0.07059533, 0.86147128]])\n```", "```py\nfrom textblob import TextBlob\nimport spacy\nimport nltk\nimport warnings\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nnltk.download('vader_lexicon')\nnlp = spacy.load(\"en_core_web_lg\")\n```", "```py\ntickers = ['AAPL','MSFT','AMZN','GOOG','FB','WMT','JPM','TSLA','NFLX','ADBE']\nstart = '2010-01-01'\nend = '2018-12-31'\ndf_ticker_return = pd.DataFrame()\nfor ticker in tickers:\n    ticker_yf = yf.Ticker(ticker)\n    if df_ticker_return.empty:\n        df_ticker_return = ticker_yf.history(start = start, end = end)\n        df_ticker_return['ticker']= ticker\n    else:\n        data_temp = ticker_yf.history(start = start, end = end)\n        data_temp['ticker']= ticker\n        df_ticker_return = df_ticker_return.append(data_temp)\ndf_ticker_return.to_csv(r'Data\\Step3.2_ReturnData.csv')\n```", "```py\ndf_ticker_return.head(2)\n```", "```py\nz = zipfile.ZipFile(\"Data/Raw Headline Data.zip\", \"r\")\ntestFile=z.namelist()[10]\nfileData= z.open(testFile).read()\nfileDataSample = json.loads(fileData)['content'][1:500]\nfileDataSample\n```", "```py\n'li class=\"n-box-item date-title\" data-end=\"1305172799\" data-start=\"1305086400\"\ndata-txt=\"Tuesday, December 17, 2019\">Wednesday, May 11,2011</li><li\nclass=\"n-box-item sa-box-item\" data-id=\"76179\" data-ts=\"1305149244\"><div\nclass=\"media media-overflow-fix\"><div class-\"media-left\"><a class=\"box-ticker\"\nhref=\"/symbol/CSCO\" target=\"blank\">CSCO</a></div><div class=\"media-body\"<h4\nclass=\"media-heading\"><a href=\"/news/76179\" sasource=\"on_the_move_news_\nfidelity\" target=\"_blank\">Cisco (NASDAQ:CSCO): Pr'\n```", "```py\ndef jsonParser(json_data):\n    xml_data = json_data['content']\n\n    tree = etree.parse(StringIO(xml_data), parser=etree.HTMLParser())\n\n    headlines = tree.xpath(\"//h4[contains(@class, 'media-heading')]/a/text()\")\n    assert len(headlines) == json_data['count']\n\n    main_tickers = list(map(lambda x: x.replace('/symbol/', ''),\\\n           tree.xpath(\"//div[contains(@class, 'media-left')]//a/@href\")))\n    assert len(main_tickers) == json_data['count']\n    final_headlines = [''.join(f.xpath('.//text()')) for f in\\\n           tree.xpath(\"//div[contains(@class, 'media-body')]/ul/li[1]\")]\n    if len(final_headlines) == 0:\n        final_headlines = [''.join(f.xpath('.//text()')) for f in\\\n           tree.xpath(\"//div[contains(@class, 'media-body')]\")]\n        final_headlines = [f.replace(h, '').split('\\xa0')[0].strip()\\\n                           for f,h in zip (final_headlines, headlines)]\n    return main_tickers, final_headlines\n```", "```py\njsonParser(json.loads(fileData))[1][1]\n```", "```py\n'Cisco Systems (NASDAQ:CSCO) falls further into the red on FQ4\n guidance of $0.37-0.39 vs. $0.42 Street consensus. Sales seen flat\n to +2% vs. 8% Street view. CSCO recently -2.1%.'\n```", "```py\n#Computing the return\ndf_ticker_return['ret_curr'] = df_ticker_return['Close'].pct_change()\n#Computing the event return\ndf_ticker_return['eventRet'] = df_ticker_return['ret_curr']\\\n + df_ticker_return['ret_curr'].shift(-1) + df_ticker_return['ret_curr'].shift(1)\n```", "```py\ncombinedDataFrame = pd.merge(data_df_news, df_ticker_return, how='left', \\\nleft_on=['date','ticker'], right_on=['date','ticker'])\ncombinedDataFrame = combinedDataFrame[combinedDataFrame['ticker'].isin(tickers)]\ndata_df = combinedDataFrame[['ticker','headline','date','eventRet','Close']]\ndata_df = data_df.dropna()\ndata_df.head(2)\n```", "```py\nprint(data_df.shape, data_df.ticker.unique().shape)\n```", "```py\n(2759, 5) (10,)\n```", "```py\ntext = \"Bayer (OTCPK:BAYRY) started the week up 3.5% to €74/share in Frankfurt, \\\ntouching their\nhighest level in 14 months, after the U.S. government said \\\n a $25M glyphosate decision against the\ncompany should be reversed.\"\n\nTextBlob(text).sentiment.polarity\n```", "```py\n0.5\n```", "```py\ndata_df['sentiment_textblob'] = [TextBlob(s).sentiment.polarity for s in \\\ndata_df['headline']]\n```", "```py\ntext = \"Bayer (OTCPK:BAYRY) started the week up 3.5% to €74/share\\\nin Frankfurt, touching their highest level in 14 months, after the\\\nU.S. government said a $25M glyphosate decision against the company\\\nshould be reversed.\"\nTextBlob(text).sentiment_assessments\n```", "```py\nSentiment(polarity=0.5, subjectivity=0.5, assessments=[(['touching'], 0.5, 0.5, \\\nNone)])\n```", "```py\nsentiments_data = pd.read_csv(r'Data\\LabelledNewsData.csv', \\\nencoding=\"ISO-8859-1\")\nsentiments_data.head(1)\n```", "```py\nall_vectors = pd.np.array([pd.np.array([token.vector for token in nlp(s) ]).\\\nmean(axis=0)*pd.np.ones((300))\\\n for s in sentiments_data['headline']])\n```", "```py\n### Create sequence\nvocabulary_size = 20000\ntokenizer = Tokenizer(num_words= vocabulary_size)\ntokenizer.fit_on_texts(sentiments_data['headline'])\nsequences = tokenizer.texts_to_sequences(sentiments_data['headline'])\nX_LSTM = pad_sequences(sequences, maxlen=50)\n```", "```py\nfrom keras.wrappers.scikit_learn import KerasClassifier\ndef create_model(input_length=50):\n    model = Sequential()\n    model.add(Embedding(20000, 300, input_length=50))\n    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer='adam', \\\n    metrics=['accuracy'])\n    return model\nmodel_LSTM = KerasClassifier(build_fn=create_model, epochs=3, verbose=1, \\\n  validation_split=0.4)\nmodel_LSTM.fit(X_train_LSTM, Y_train_LSTM)\n```", "```py\n# stock market lexicon\nsia = SentimentIntensityAnalyzer()\nstock_lex = pd.read_csv('Data/lexicon_data/stock_lex.csv')\nstock_lex['sentiment'] = (stock_lex['Aff_Score'] + stock_lex['Neg_Score'])/2\nstock_lex = dict(zip(stock_lex.Item, stock_lex.sentiment))\nstock_lex = {k:v for k,v in stock_lex.items() if len(k.split(' '))==1}\nstock_lex_scaled = {}\nfor k, v in stock_lex.items():\n    if v > 0:\n        stock_lex_scaled[k] = v / max(stock_lex.values()) * 4\n    else:\n        stock_lex_scaled[k] = v / min(stock_lex.values()) * -4\n\nfinal_lex = {}\nfinal_lex.update(stock_lex_scaled)\nfinal_lex.update(sia.lexicon)\nsia.lexicon = final_lex\n```", "```py\ntext = \"AAPL is trading higher after reporting its October sales\\\nrose 12.6% M/M. It has seen a 20%+ jump in orders\"\n```", "```py\nsia.polarity_scores(text)['compound']\n```", "```py\n0.4535\n```", "```py\nvader_sentiments = pd.np.array([sia.polarity_scores(s)['compound']\\\n for s in data_df['headline']])\n```", "```py\n# buy if current close more than simple moving average (sma)\n# AND sentiment increased by >= 0.5\nif self.dataclose[0] > self.sma[0] and self.sentiment - prev_sentiment >= 0.5:\n  self.order = self.buy()\n\n# sell if current close less than simple moving average(sma)\n# AND sentiment decreased by >= 0.5\nif self.dataclose[0] < self.sma[0] and self.sentiment - prev_sentiment <= -0.5:\n  self.order = self.sell()\n```", "```py\nticker = 'GOOG'\nrun_strategy(ticker, start = '2012-01-01', end = '2018-12-12')\n```", "```py\nStarting Portfolio Value: 100000.00\n2013-01-10, Previous Sentiment 0.08, New Sentiment 0.80 BUY CREATE, 369.36\n2014-07-17, Previous Sentiment 0.73, New Sentiment -0.22 SELL CREATE, 572.16\n2014-07-18, OPERATION PROFIT, GROSS 22177.00, NET 22177.00\n2014-07-18, Previous Sentiment -0.22, New Sentiment 0.77 BUY CREATE, 593.45\n2014-09-12, Previous Sentiment 0.66, New Sentiment -0.05 SELL CREATE, 574.04\n2014-09-15, OPERATION PROFIT, GROSS -1876.00, NET -1876.00\n2015-07-17, Previous Sentiment 0.01, New Sentiment 0.90 BUY CREATE, 672.93\n.\n.\n.\n2018-12-11, Ending Value 149719.00\n```", "```py\nGOOG_ticker= data_df[data_df['ticker'].isin([ticker])]\nNew= list(GOOG_ticker[GOOG_ticker['date'] ==  '2015-07-17']['headline'])\nOld= list(GOOG_ticker[GOOG_ticker['date'] ==  '2015-07-16']['headline'])\nprint(\"Current News:\",New,\"\\n\\n\",\"Previous News:\", Old)\n```", "```py\nCurrent News: [\"Axiom Securities has upgraded Google (GOOG +13.4%, GOOGL +14.8%)\nto Buy following the company's Q2 beat and investor-pleasing comments about\nspending discipline, potential capital returns, and YouTube/mobile growth. MKM\nhas launched coverage at Buy, and plenty of other firms have hiked their targets.\nGoogle's market cap is now above $450B.\"]\n\nPrevious News: [\"While Google's (GOOG, GOOGL) Q2 revenue slightly missed\nestimates when factoring traffic acquisitions costs (TAC), its ex-TAC revenue of\n$14.35B was slightly above a $14.3B consensus. The reason: TAC fell to 21% of ad\nrevenue from Q1's 22% and Q2 2014's 23%. That also, of course, helped EPS beat\nestimates.\", 'Google (NASDAQ:GOOG): QC2 EPS of $6.99 beats by $0.28.']\n```", "```py\nticker = 'FB'\nrun_strategy(ticker, start = '2012-01-01', end = '2018-12-12')\n```", "```py\nStart Portfolio value: 100000.00\nFinal Portfolio Value: 108041.00\nProfit: 8041.00\n```", "```py\nresults_tickers = {}\nfor ticker in tickers:\n    results_tickers[ticker] = run_strategy(ticker, start = '2012-01-01', \\\n    end = '2018-12-12')\npd.DataFrame.from_dict(results_tickers).set_index(\\\n  [pd.Index([\"PerUnitStartPrice\", StrategyProfit'])])\n```", "```py\nresults_tickers = {}\nfor ticker in tickers:\n    results_tickers[ticker] = run_strategy(ticker, start = '2012-01-01', \\\n    end = '2014-12-31')\n```", "```py\nresults_tickers = {}\nfor ticker in tickers:\n    results_tickers[ticker] = run_strategy(ticker, start = '2016-01-01', \\\n    end = '2018-12-31')\n```", "```py\nimport spacy #Custom NER model.\nfrom spacy.util import minibatch, compounding\n```", "```py\nfrom chatterbot import ChatBot\nfrom chatterbot.logic import LogicAdapter\nfrom chatterbot.trainers import ChatterBotCorpusTrainer\nfrom chatterbot.trainers import ListTrainer\n```", "```py\nimport random\nfrom itertools import product\n```", "```py\nchatB = ChatBot(\"Trader\",\n                preprocessors=['chatterbot.preprocessors.clean_whitespace'],\n                logic_adapters=['chatterbot.logic.BestMatch',\n                                'chatterbot.logic.MathematicalEvaluation'])\n\n# Corpus Training\ntrainerCorpus = ChatterBotCorpusTrainer(chatB)\n\n# Train based on English Corpus\ntrainerCorpus.train(\n    \"chatterbot.corpus.english\"\n)\n# Train based on english greetings corpus\ntrainerCorpus.train(\"chatterbot.corpus.english.greetings\")\n\n# Train based on the english conversations corpus\ntrainerCorpus.train(\"chatterbot.corpus.english.conversations\")\n\ntrainerConversation = ListTrainer(chatB)\n# Train based on conversations\n\n# List training\ntrainerConversation.train([\n    'Help!',\n    'Please go to google.com',\n    'What is Bitcoin?',\n    'It is a decentralized digital currency'\n])\n\n# You can train with a second list of data to add response variations\ntrainerConversation.train([\n    'What is Bitcoin?',\n    'Bitcoin is a cryptocurrency.'\n])\n```", "```py\n>Hi\nHow are you doing?\n\n>I am doing well.\nThat is good to hear\n\n>What is 78964 plus 5970\n78964 plus 5970 = 84934\n\n>what is a dollar\ndollar: unit of currency in the united states.\n\n>What is Bitcoin?\nIt is a decentralized digital currency\n\n>Help!\nPlease go to google.com\n\n>Tell me a joke\nDid you hear the one about the mountain goats in the andes? It was \"ba a a a d\".\n\n>What is Bitcoin?\nBitcoin is a cryptocurrency.\n```", "```py\ncompanies = {\n    'AAPL':  ['Apple', 'Apple Inc'],\n    'BAC': ['BAML', 'BofA', 'Bank of America'],\n    'C': ['Citi', 'Citibank'],\n    'DAL': ['Delta', 'Delta Airlines']\n}\n```", "```py\nratios = {\n    'return-on-equity-ttm': ['ROE', 'Return on Equity'],\n    'cash-from-operations-quarterly': ['CFO', 'Cash Flow from Operations'],\n    'pe-ratio-ttm': ['PE', 'Price to equity', 'pe ratio'],\n    'revenue-ttm': ['Sales', 'Revenue'],\n}\n```", "```py\nstring_templates = ['Get me the {ratio} for {company}',\n                   'What is the {ratio} for {company}?',\n                   'Tell me the {ratio} for {company}',\n                  ]\n```", "```py\ncompanies_rev = {}\nfor k, v in companies.items():\n  for ve in v:\n      companies_rev[ve] = k\n  ratios_rev = {}\n  for k, v in ratios.items():\n      \t\tfor ve in v:\n          \t\t\tratios_rev[ve] = k\n  companies_list = list(companies_rev.keys())\n  ratios_list = list(ratios_rev.keys())\n```", "```py\nN_training_samples = 100\ndef get_training_sample(string_templates, ratios_list, companies_list):\n  string_template=string_templates[random.randint(0, len(string_templates)-1)]\n      ratio = ratios_list[random.randint(0, len(ratios_list)-1)]\n      company = companies_list[random.randint(0, len(companies_list)-1)]\n      sent = string_template.format(ratio=ratio,company=company)\n      ents = {\"entities\": [(sent.index(ratio), sent.index(ratio)+\\\n  len(ratio), 'RATIO'),\n                   \t(sent.index(company), sent.index(company)+len(company), \\\n                    'COMPANY')]}\n       return (sent, ents)\n```", "```py\nTRAIN_DATA = [\nget_training_sample(string_templates, ratios_list, companies_list) \\\nfor i in range(N_training_samples)\n]\n```", "```py\nnlp = spacy.blank(\"en\")\n```", "```py\nner = nlp.create_pipe(\"ner\")\nnlp.add_pipe(ner)\n```", "```py\nner.add_label('RATIO')\nner.add_label('COMPANY')\n```", "```py\noptimizer = nlp.begin_training()\nmove_names = list(ner.move_names)\npipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\nother_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\nwith nlp.disable_pipes(*other_pipes):  # only train NER\n     sizes = compounding(1.0, 4.0, 1.001)\n     # batch up the examples using spaCy's minibatch\n     for itn in range(30):\n        random.shuffle(TRAIN_DATA)\n        batches = minibatch(TRAIN_DATA, size=sizes)\n        losses = {}\n        for batch in batches:\n           texts, annotations = zip(*batch)\n           nlp.update(texts, annotations, sgd=optimizer,\n           drop=0.35, losses=losses)\n        print(\"Losses\", losses)\n```", "```py\nfrom chatterbot.conversation import Statement\nclass FinancialRatioAdapter(LogicAdapter):\n    \tdef __init__(self, chatbot, **kwargs):\n        \t\tsuper(FinancialRatioAdapter, self).__init__(chatbot, **kwargs)\n    \tdef process(self, statement, additional_response_selection_parameters):\n      \t\tuser_input = statement.text\n      \t\tdoc = nlp(user_input)\n      \t\tcompany = None\n      \t\tratio = None\n      \t\tconfidence = 0\n      \t\t# We need exactly 1 company and one ratio\n      \t\tif len(doc.ents) == 2:\n      \t\t\tfor ent in doc.ents:\n          \t\t\tif ent.label_ == \"RATIO\":\n              \t\t\t\tratio = ent.text\n              \t\t\tif ratio in ratios_rev:\n                  \t\t\t\tconfidence += 0.5\n          \t\t\tif ent.label_ == \"COMPANY\":\n              \t\t\t\tcompany = ent.text\n              \t\t\t\tif company in companies_rev:\n                  \t\t\t\t\tconfidence += 0.5\n      \t\tif confidence > 0.99: (its found a ratio and company)\n      \t\t\touttext = '''https://www.zacks.com/stock/chart\\\n /{comanpy}/fundamental/{ratio} '''.format(ratio=ratios_rev[ratio]\\\n                  , company=companies_rev[company])\n      \t\t\tconfidence = 1\n      \t\telse:\n      \t\t\touttext = 'Sorry! Could not figure out what the user wants'\n      \t\t\tconfidence = 0\n      \t\toutput_statement = Statement(text=outtext)\n      \t\toutput_statement.confidence = confidence\n      \t\treturn output_statement\n```", "```py\nfrom chatterbot import ChatBot\n```", "```py\nchatbot = ChatBot(\n    \t\t\t\"My ChatterBot\",\n    \t\t\tlogic_adapters=[\n        'financial_ratio_adapter.FinancialRatioAdapter'\n    ]\n)\n```", "```py\nconverse()\n\n>What is ROE for Citibank?\nhttps://www.zacks.com/stock/chart/C/fundamental/return-on-equity-ttm\n\n>Tell me PE for Delta?\nhttps://www.zacks.com/stock/chart/DAL/fundamental/pe-ratio-ttm\n\n>What is Bitcoin?\nIt is a decentralized digital currency\n\n>Help!\nPlease go to google.com\n\n>What is 786940 plus 75869\n786940 plus 75869 = 862809\n\n>Do you like dogs?\nSorry! Could not figure out what the user wants\n```", "```py\nfrom pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\nfrom pdfminer.converter import TextConverter\nfrom pdfminer.layout import LAParams\nfrom pdfminer.pdfpage import PDFPage\nimport re\nfrom io import StringIO\n```", "```py\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n```", "```py\nimport numpy as np\nimport pandas as pd\n```", "```py\ndef convert_pdf_to_txt(path):\n    rsrcmgr = PDFResourceManager()\n    retstr = StringIO()\n    laparams = LAParams()\n    device = TextConverter(rsrcmgr, retstr, laparams=laparams)\n    fp = open(path, 'rb')\n    interpreter = PDFPageInterpreter(rsrcmgr, device)\n    password = \"\"\n    maxpages = 0\n    caching = True\n    pagenos=set()\n\n    for page in PDFPage.get_pages(fp, pagenos,\\\n            maxpages=maxpages, password=password,caching=caching,\\\n            check_extractable=True):\n        interpreter.process_page(page)\n\n    text = retstr.getvalue()\n\n    fp.close()\n    device.close()\n    retstr.close()\n    return text\n```", "```py\nDocument=convert_pdf_to_txt('10K.pdf')\nf=open('Finance10k.txt','w')\nf.write(Document)\nf.close()\nwith open('Finance10k.txt') as f:\n    clean_cont = f.read().splitlines()\n```", "```py\nclean_cont[1:15]\n```", "```py\n[' ',\n '',\n 'SECURITIES AND EXCHANGE COMMISSION',\n ' ',\n '',\n 'Washington, D.C. 20549',\n ' ',\n '',\n '\\xa0',\n 'FORM ',\n '\\xa0',\n '',\n 'QUARTERLY REPORT PURSUANT TO SECTION 13 OR 15(d) OF',\n ' ']\n```", "```py\ndoc=[i.replace('\\xe2\\x80\\x9c', '') for i in clean_cont ]\ndoc=[i.replace('\\xe2\\x80\\x9d', '') for i in doc ]\ndoc=[i.replace('\\xe2\\x80\\x99s', '') for i in doc ]\n\ndocs = [x for x in doc if x != ' ']\ndocss = [x for x in docs if x != '']\nfinancedoc=[re.sub(\"[^a-zA-Z]+\", \" \", s) for s in docss]\n```", "```py\nvect=CountVectorizer(ngram_range=(1, 1),stop_words='english')\nfin=vect.fit_transform(financedoc)\n```", "```py\nlda=LatentDirichletAllocation(n_components=5)\nlda.fit_transform(fin)\nlda_dtf=lda.fit_transform(fin)\n```", "```py\nsorting=np.argsort(lda.components_)[:, ::-1]\nfeatures=np.array(vect.get_feature_names())\n```", "```py\nimport mglearn\nmglearn.tools.print_topics(topics=range(5), feature_names=features,\nsorting=sorting, topics_per_chunk=5, n_words=10)\n```", "```py\ntopic 1       topic 2       topic 3       topic 4       topic 5\n--------      --------      --------      --------      --------\nassets        quarter       loans         securities    value\nbalance       million       mortgage      rate          total\nlosses        risk          loan          investment    income\ncredit        capital       commercial    contracts     net\nperiod        months        total         credit        fair\nderivatives   financial     real          market        billion\nliabilities   management    estate        federal       equity\nderivative    billion       securities    stock         september\nallowance     ended         consumer      debt          december\naverage       september     backed        sales         table\n```", "```py\nfrom __future__ import  print_function\nimport pyLDAvis\nimport pyLDAvis.sklearn\n\nzit=pyLDAvis.sklearn.prepare(lda,fin,vect)\npyLDAvis.show(zit)\n```", "```py\n#Loading the additional packages for word cloud\nfrom os import path\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud,STOPWORDS\n\n#Loading the document and generating the word cloud\nd = path.dirname(__name__)\ntext = open(path.join(d, 'Finance10k.txt')).read()\n\nstopwords = set(STOPWORDS)\nwc = WordCloud(background_color=\"black\", max_words=2000, stopwords=stopwords)\nwc.generate(text)\n\nplt.figure(figsize=(16,13))\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()\n```"]