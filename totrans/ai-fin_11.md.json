["```py\nIn [1]: import os\n        import random\n        import numpy as np\n        import pandas as pd\n        import tensorflow as tf\n        from pprint import pprint\n        from pylab import plt, mpl\n        plt.style.use('seaborn')\n        mpl.rcParams['savefig.dpi'] = 300\n        mpl.rcParams['font.family'] = 'serif'\n        pd.set_option('precision', 4)\n        np.set_printoptions(suppress=True, precision=4)\n        os.environ['PYTHONHASHSEED'] = '0'\n\nIn [2]: def set_seeds(seed=100):  ![1](Images/1.png)\n            random.seed(seed)\n            np.random.seed(seed)\n            tf.random.set_seed(seed)\n        set_seeds()  ![1](Images/1.png)\n```", "```py\nIn [3]: a = np.arange(100)  ![1](Images/1.png)\n        a\nOut[3]: array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n               17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n               34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n               51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n               68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84,\n               85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])\n\nIn [4]: a = a.reshape((len(a), -1))  ![2](Images/2.png)\n\nIn [5]: a.shape  ![2](Images/2.png)\nOut[5]: (100, 1)\n\nIn [6]: a[:5]  ![2](Images/2.png)\nOut[6]: array([[0],\n               [1],\n               [2],\n               [3],\n               [4]])\n```", "```py\nIn [7]: from keras.preprocessing.sequence import TimeseriesGenerator\n        Using TensorFlow backend.\n\nIn [8]: lags = 3\n\nIn [9]: g = TimeseriesGenerator(a, a, length=lags, batch_size=5)  ![1](Images/1.png)\n\nIn [10]: pprint(list(g)[0])  ![1](Images/1.png)\n         (array([[[0],\n                 [1],\n                 [2]],\n\n                [[1],\n                 [2],\n                 [3]],\n\n                [[2],\n                 [3],\n                 [4]],\n\n                [[3],\n                 [4],\n                 [5]],\n\n                [[4],\n                 [5],\n                 [6]]]),\n          array([[3],\n                [4],\n                [5],\n                [6],\n                [7]]))\n```", "```py\nIn [11]: from keras.models import Sequential\n         from keras.layers import SimpleRNN, LSTM, Dense\n\nIn [12]: model = Sequential()\n         model.add(SimpleRNN(100, activation='relu',\n                             input_shape=(lags, 1)))  ![1](Images/1.png)\n         model.add(Dense(1, activation='linear'))\n         model.compile(optimizer='adagrad', loss='mse',\n                       metrics=['mae'])\n\nIn [13]: model.summary()  ![2](Images/2.png)\n         Model: \"sequential_1\"\n         _________________________________________________________________\n         Layer (type)                 Output Shape              Param #\n         =================================================================\n         simple_rnn_1 (SimpleRNN)     (None, 100)               10200\n         _________________________________________________________________\n         dense_1 (Dense)              (None, 1)                 101\n         =================================================================\n         Total params: 10,301\n         Trainable params: 10,301\n         Non-trainable params: 0\n         _________________________________________________________________\n\nIn [14]: %%time\n         model.fit_generator(g, epochs=1000, steps_per_epoch=5,\n                             verbose=False)  ![3](Images/3.png)\n         CPU times: user 17.4 s, sys: 3.9 s, total: 21.3 s\n         Wall time: 30.8 s\n\nOut[14]: <keras.callbacks.callbacks.History at 0x7f7f079058d0>\n```", "```py\nIn [15]: res = pd.DataFrame(model.history.history)\n\nIn [16]: res.tail(3)\nOut[16]:        loss     mae\n         997  0.0001  0.0109\n         998  0.0007  0.0211\n         999  0.0001  0.0101\n\nIn [17]: res.iloc[10:].plot(figsize=(10, 6), style=['--', '--']);\n```", "```py\nIn [18]: x = np.array([21, 22, 23]).reshape((1, lags, 1))\n         y = model.predict(x, verbose=False)  ![1](Images/1.png)\n         int(round(y[0, 0]))\nOut[18]: 24\n\nIn [19]: x = np.array([87, 88, 89]).reshape((1, lags, 1))\n         y = model.predict(x, verbose=False)  ![1](Images/1.png)\n         int(round(y[0, 0]))\nOut[19]: 90\n\nIn [20]: x = np.array([187, 188, 189]).reshape((1, lags, 1))\n         y = model.predict(x, verbose=False)  ![2](Images/2.png)\n         int(round(y[0, 0]))\nOut[20]: 190\n\nIn [21]: x = np.array([1187, 1188, 1189]).reshape((1, lags, 1))\n         y = model.predict(x, verbose=False)  ![3](Images/3.png)\n         int(round(y[0, 0]))\nOut[21]: 1194\n```", "```py\nIn [22]: def transform(x):\n             y = 0.05 * x ** 2 + 0.2 * x + np.sin(x) + 5  ![1](Images/1.png)\n             y += np.random.standard_normal(len(x)) * 0.2  ![2](Images/2.png)\n             return y\n\nIn [23]: x = np.linspace(-2 * np.pi, 2 * np.pi, 500)\n         a = transform(x)\n\nIn [24]: plt.figure(figsize=(10, 6))\n         plt.plot(x, a);\n```", "```py\nIn [25]: a = a.reshape((len(a), -1))\n\nIn [26]: a[:5]\nOut[26]: array([[5.6736],\n                [5.68  ],\n                [5.3127],\n                [5.645 ],\n                [5.7118]])\n\nIn [27]: lags = 5\n\nIn [28]: g = TimeseriesGenerator(a, a, length=lags, batch_size=5)\n\nIn [29]: model = Sequential()\n         model.add(SimpleRNN(500, activation='relu', input_shape=(lags, 1)))\n         model.add(Dense(1, activation='linear'))\n         model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n\nIn [30]: model.summary()\n         Model: \"sequential_2\"\n         _________________________________________________________________\n         Layer (type)                 Output Shape              Param #\n         =================================================================\n         simple_rnn_2 (SimpleRNN)     (None, 500)               251000\n         _________________________________________________________________\n         dense_2 (Dense)              (None, 1)                 501\n         =================================================================\n         Total params: 251,501\n         Trainable params: 251,501\n         Non-trainable params: 0\n         _________________________________________________________________\n\nIn [31]: %%time\n         model.fit_generator(g, epochs=500,\n                             steps_per_epoch=10,\n                             verbose=False)\n         CPU times: user 1min 6s, sys: 14.6 s, total: 1min 20s\n         Wall time: 23.1 s\n\nOut[31]: <keras.callbacks.callbacks.History at 0x7f7f09c11810>\n```", "```py\nIn [32]: x = np.linspace(-6 * np.pi, 6 * np.pi, 1000)  ![1](Images/1.png)\n         d = transform(x)\n\nIn [33]: g_ = TimeseriesGenerator(d, d, length=lags, batch_size=len(d))  ![1](Images/1.png)\n\nIn [34]: f = list(g_)[0][0].reshape((len(d) - lags, lags, 1))  ![1](Images/1.png)\n\nIn [35]: y = model.predict(f, verbose=False)  ![2](Images/2.png)\n\nIn [36]: plt.figure(figsize=(10, 6))\n         plt.plot(x[lags:], d[lags:], label='data', alpha=0.75)\n         plt.plot(x[lags:], y, 'r.', label='pred', ms=3)\n         plt.axvline(-2 * np.pi, c='g', ls='--')\n         plt.axvline(2 * np.pi, c='g', ls='--')\n         plt.text(-15, 22, 'out-of-sample')\n         plt.text(-2, 22, 'in-sample')\n         plt.text(10, 22, 'out-of-sample')\n         plt.legend();\n```", "```py\nIn [37]: url = 'http://hilpisch.com/aiif_eikon_id_eur_usd.csv'\n\nIn [38]: symbol = 'EUR_USD'\n\nIn [39]: raw = pd.read_csv(url, index_col=0, parse_dates=True)\n\nIn [40]: def generate_data():\n             data = pd.DataFrame(raw['CLOSE'])  ![1](Images/1.png)\n             data.columns = [symbol]  ![2](Images/2.png)\n             data = data.resample('30min', label='right').last().ffill()  ![3](Images/3.png)\n             return data\n\nIn [41]: data = generate_data()\n\nIn [42]: data = (data - data.mean()) / data.std()  ![4](Images/4.png)\n\nIn [43]: p = data[symbol].values  ![5](Images/5.png)\n\nIn [44]: p = p.reshape((len(p), -1))  ![5](Images/5.png)\n```", "```py\nIn [45]: lags = 5\n\nIn [46]: g = TimeseriesGenerator(p, p, length=lags, batch_size=5)\n\nIn [47]: def create_rnn_model(hu=100, lags=lags, layer='SimpleRNN',\n                                    features=1, algorithm='estimation'):\n             model = Sequential()\n             if layer is 'SimpleRNN':\n                 model.add(SimpleRNN(hu, activation='relu',\n                                     input_shape=(lags, features)))  ![1](Images/1.png)\n             else:\n                 model.add(LSTM(hu, activation='relu',\n                                input_shape=(lags, features)))  ![1](Images/1.png)\n             if algorithm == 'estimation':\n                 model.add(Dense(1, activation='linear'))  ![2](Images/2.png)\n                 model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n             else:\n                 model.add(Dense(1, activation='sigmoid'))  ![2](Images/2.png)\n                 model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n             return model\n\nIn [48]: model = create_rnn_model()\n\nIn [49]: %%time\n         model.fit_generator(g, epochs=500, steps_per_epoch=10,\n                             verbose=False)\n         CPU times: user 20.8 s, sys: 4.66 s, total: 25.5 s\n         Wall time: 11.2 s\n\nOut[49]: <keras.callbacks.callbacks.History at 0x7f7ef6716590>\n```", "```py\nIn [50]: y = model.predict(g, verbose=False)\n\nIn [51]: data['pred'] = np.nan\n         data['pred'].iloc[lags:] = y.flatten()\n\nIn [52]: data[[symbol, 'pred']].plot(\n                     figsize=(10, 6), style=['b', 'r-.'],\n                     alpha=0.75);\n```", "```py\nIn [53]: data[[symbol, 'pred']].iloc[50:100].plot(\n                     figsize=(10, 6), style=['b', 'r-.'],\n                     alpha=0.75);\n```", "```py\nIn [54]: data = generate_data()\n\nIn [55]: data['r'] = np.log(data / data.shift(1))\n\nIn [56]: data.dropna(inplace=True)\n\nIn [57]: data = (data - data.mean()) / data.std()\n\nIn [58]: r = data['r'].values\n\nIn [59]: r = r.reshape((len(r), -1))\n\nIn [60]: g = TimeseriesGenerator(r, r, length=lags, batch_size=5)\n\nIn [61]: model = create_rnn_model()\n\nIn [62]: %%time\n         model.fit_generator(g, epochs=500, steps_per_epoch=10,\n                             verbose=False)\n         CPU times: user 20.4 s, sys: 4.2 s, total: 24.6 s\n         Wall time: 11.3 s\n\nOut[62]: <keras.callbacks.callbacks.History at 0x7f7ef47a8dd0>\n```", "```py\nIn [63]: y = model.predict(g, verbose=False)\n\nIn [64]: data['pred'] = np.nan\n         data['pred'].iloc[lags:] = y.flatten()\n         data.dropna(inplace=True)\n\nIn [65]: data[['r', 'pred']].iloc[50:100].plot(\n                     figsize=(10, 6), style=['b', 'r-.'],\n                     alpha=0.75);\n         plt.axhline(0, c='grey', ls='--')\n```", "```py\nIn [66]: from sklearn.metrics import accuracy_score\n\nIn [67]: accuracy_score(np.sign(data['r']), np.sign(data['pred']))\nOut[67]: 0.6806532093445226\n```", "```py\nIn [68]: split = int(len(r) * 0.8)  ![1](Images/1.png)\n\nIn [69]: train = r[:split]  ![1](Images/1.png)\n\nIn [70]: test = r[split:]  ![1](Images/1.png)\n\nIn [71]: g = TimeseriesGenerator(train, train, length=lags, batch_size=5)  ![2](Images/2.png)\n\nIn [72]: set_seeds()\n         model = create_rnn_model(hu=100)\n\nIn [73]: %%time\n         model.fit_generator(g, epochs=100, steps_per_epoch=10, verbose=False)  ![2](Images/2.png)\n         CPU times: user 5.67 s, sys: 1.09 s, total: 6.75 s\n         Wall time: 2.95 s\n\nOut[73]: <keras.callbacks.callbacks.History at 0x7f7ef5482dd0>\n\nIn [74]: g_ = TimeseriesGenerator(test, test, length=lags, batch_size=5)  ![3](Images/3.png)\n\nIn [75]: y = model.predict(g_)  ![3](Images/3.png)\n\nIn [76]: accuracy_score(np.sign(test[lags:]), np.sign(y))  ![3](Images/3.png)\nOut[76]: 0.6708428246013668\n```", "```py\nIn [77]: data = generate_data()\n\nIn [78]: data['r'] = np.log(data / data.shift(1))\n\nIn [79]: window = 20\n         data['mom'] = data['r'].rolling(window).mean()  ![1](Images/1.png)\n         data['vol'] = data['r'].rolling(window).std()  ![2](Images/2.png)\n\nIn [80]: data.dropna(inplace=True)\n```", "```py\nIn [81]: split = int(len(data) * 0.8)\n\nIn [82]: train = data.iloc[:split].copy()\n\nIn [83]: mu, std = train.mean(), train.std()  ![1](Images/1.png)\n\nIn [84]: train = (train - mu) / std  ![2](Images/2.png)\n\nIn [85]: test = data.iloc[split:].copy()\n\nIn [86]: test = (test - mu) / std  ![3](Images/3.png)\n\nIn [87]: g = TimeseriesGenerator(train.values, train['r'].values,\n                                 length=lags, batch_size=5)  ![4](Images/4.png)\n\nIn [88]: set_seeds()\n         model = create_rnn_model(hu=100, features=len(data.columns),\n                                  layer='SimpleRNN')\n\nIn [89]: %%time\n         model.fit_generator(g, epochs=100, steps_per_epoch=10,\n                             verbose=False)  ![4](Images/4.png)\n         CPU times: user 5.24 s, sys: 1.08 s, total: 6.32 s\n         Wall time: 2.73 s\n\nOut[89]: <keras.callbacks.callbacks.History at 0x7f7ef313c950>\n\nIn [90]: g_ = TimeseriesGenerator(test.values, test['r'].values,\n                                  length=lags, batch_size=5)  ![5](Images/5.png)\n\nIn [91]: y = model.predict(g_).flatten()  ![5](Images/5.png)\n\nIn [92]: accuracy_score(np.sign(test['r'].iloc[lags:]), np.sign(y))  ![5](Images/5.png)\nOut[92]: 0.37299771167048057\n```", "```py\nIn [93]: set_seeds()\n         model = create_rnn_model(hu=50,\n                     features=len(data.columns),\n                     layer='LSTM',\n                     algorithm='classification')  ![1](Images/1.png)\n\nIn [94]: train_y = np.where(train['r'] > 0, 1, 0)  ![2](Images/2.png)\n\nIn [95]: np.bincount(train_y)  ![3](Images/3.png)\nOut[95]: array([2374, 1142])\n\nIn [96]: def cw(a):\n             c0, c1 = np.bincount(a)\n             w0 = (1 / c0) * (len(a)) / 2\n             w1 = (1 / c1) * (len(a)) / 2\n             return {0: w0, 1: w1}\n\nIn [97]: g = TimeseriesGenerator(train.values, train_y,\n                                 length=lags, batch_size=5)\n\nIn [98]: %%time\n         model.fit_generator(g, epochs=5, steps_per_epoch=10,\n                             verbose=False, class_weight=cw(train_y))\n         CPU times: user 1.25 s, sys: 159 ms, total: 1.41 s\n         Wall time: 947 ms\n\nOut[98]: <keras.callbacks.callbacks.History at 0x7f7ef43baf90>\n\nIn [99]: test_y = np.where(test['r'] > 0, 1, 0)  ![4](Images/4.png)\n\nIn [100]: g_ = TimeseriesGenerator(test.values, test_y,\n                                   length=lags, batch_size=5)\n\nIn [101]: y = np.where(model.predict(g_, batch_size=None) > 0.5, 1, 0).flatten()\n\nIn [102]: np.bincount(y)\nOut[102]: array([492, 382])\n\nIn [103]: accuracy_score(test_y[lags:], y)\nOut[103]: 0.6498855835240275\n```", "```py\nIn [104]: from keras.layers import Dropout\n\nIn [105]: def create_deep_rnn_model(hl=2, hu=100, layer='SimpleRNN',\n                                    optimizer='rmsprop', features=1,\n                                    dropout=False, rate=0.3, seed=100):\n              if hl <= 2: hl = 2  ![1](Images/1.png)\n              if layer == 'SimpleRNN':\n                  layer = SimpleRNN\n              else:\n                  layer = LSTM\n              model = Sequential()\n              model.add(layer(hu, input_shape=(lags, features),\n                               return_sequences=True,\n                              ))  ![2](Images/2.png)\n              if dropout:\n                  model.add(Dropout(rate, seed=seed))  ![3](Images/3.png)\n              for _ in range(2, hl):\n                  model.add(layer(hu, return_sequences=True))\n                  if dropout:\n                      model.add(Dropout(rate, seed=seed))  ![3](Images/3.png)\n              model.add(layer(hu))  ![4](Images/4.png)\n              model.add(Dense(1, activation='sigmoid'))  ![5](Images/5.png)\n              model.compile(optimizer=optimizer,\n                            loss='binary_crossentropy',\n                            metrics=['accuracy'])\n              return model\n\nIn [106]: set_seeds()\n          model = create_deep_rnn_model(\n                      hl=2, hu=50, layer='SimpleRNN',\n                      features=len(data.columns),\n                      dropout=True, rate=0.3)  ![1](Images/1.png)\n\nIn [107]: %%time\n          model.fit_generator(g, epochs=200, steps_per_epoch=10,\n                              verbose=False, class_weight=cw(train_y))\n          CPU times: user 14.2 s, sys: 2.85 s, total: 17.1 s\n          Wall time: 7.09 s\n\nOut[107]: <keras.callbacks.callbacks.History at 0x7f7ef6428790>\n\nIn [108]: y = np.where(model.predict(g_, batch_size=None) > 0.5, 1, 0).flatten()\n\nIn [109]: np.bincount(y)\nOut[109]: array([550, 324])\n\nIn [110]: accuracy_score(test_y[lags:], y)\nOut[110]: 0.6430205949656751\n```"]