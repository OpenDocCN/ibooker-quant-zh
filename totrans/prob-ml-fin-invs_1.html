<html><head></head><body><section class="pagenumrestart" data-pdf-bookmark="Chapter 1. The Need for Probabilistic Machine Learning" data-type="chapter" epub:type="chapter"><div class="chapter" id="the_need_for_probabilistic_machine_lear">&#13;
<h1><span class="label">Chapter 1. </span>The Need for Probabilistic <span class="keep-together">Machine Learning</span></h1>&#13;
<blockquote data-type="epigraph" epub:type="epigraph">&#13;
<p>Essentially, all models are wrong, but some are useful. However, the approximate nature of the model must always be borne in mind.</p>&#13;
<p>—George Box, eminent statistician</p>&#13;
</blockquote>&#13;
<p>A map will enable you to go from one geographic location to another. It is a very useful mathematical model for navigating the physical world. It becomes even more useful if you automate it into a GPS system using artificial intelligence (AI) technologies. However, neither the mathematical model nor the AI-powered GPS system will ever be able to capture the human experience and richness of the terrain it represents. That’s because all models have to simplify the complexities of the real world, thus enabling us to focus on some of the features of a phenomenon that interest us.</p>&#13;
<p>George Box, an eminent statistician,<a contenteditable="false" data-primary="Box, George" data-type="indexterm" id="id258"/><a contenteditable="false" data-primary="models" data-secondary="wrong but useful" data-type="indexterm" id="id259"/><a contenteditable="false" data-primary="goal of financial modeling" data-type="indexterm" id="id260"/><a contenteditable="false" data-primary="models" data-secondary="goal of financial modeling" data-type="indexterm" id="id261"/><a contenteditable="false" data-primary="financial theory" data-secondary="models wrong but useful" data-type="indexterm" id="id262"/> famously said, “all models are wrong, but some are useful.” This deeply insightful quip is our mantra. We accept that all models are wrong because they are inadequate and incomplete representations of reality. Our goal is to build financial systems based on models and supporting technologies that enable useful inferences and predictions for decision making and risk management in the face of endemic uncertainty, incomplete information, and inexact measurements.</p>&#13;
<p>All financial models, whether derived theoretically or discovered empirically by humans and machines, are not only wrong but<a contenteditable="false" data-primary="models" data-secondary="wrong but useful" data-tertiary="types of errors" data-type="indexterm" id="id263"/><a contenteditable="false" data-primary="financial theory" data-secondary="models wrong but useful" data-tertiary="types of errors" data-type="indexterm" id="id264"/> are also at the mercy of three types of errors. In this chapter, we explain this trifecta of errors with an example from consumer credit and explore it using Python code. This exemplifies our claim that inaccuracies of financial models are features, not bugs. After all, we are dealing with people, not particles or pendulums.</p>&#13;
<p>Finance is not an accurate<a contenteditable="false" data-primary="financial theory" data-secondary="flaws of" data-tertiary="inexact instead of precise" data-type="indexterm" id="id265"/><a contenteditable="false" data-primary="physics" data-secondary="finance is not physics" data-tertiary="inexact instead of precise" data-type="indexterm" id="id266"/><a contenteditable="false" data-primary="probabilistic machine learning (PML)" data-secondary="finance is not physics" data-tertiary="inexact instead of precise" data-type="indexterm" id="id267"/> physical science like physics, dealing with precise estimates and predictions, as academia will have us believe. It is an inexact social study grappling with a range of values with varying plausibilities that change continually, often abruptly.</p>&#13;
<p>We conclude the chapter by explaining why AI in general and probabilistic machine learning (ML) in particular offers the most useful and promising theoretical framework and technologies for developing the next generation of systems for finance and investing.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="what_is_a_modelquestion_mark">&#13;
<h1>What Is a Model?</h1>&#13;
<p>AI systems are based on models.<a contenteditable="false" data-primary="models" data-secondary="about" data-type="indexterm" id="id268"/><a contenteditable="false" data-primary="AI" data-secondary="AI systems based on models" data-type="indexterm" id="id269"/> A model maps functional relationships among its inputs and outputs variables based on assumptions and constraints. In general, input variables are called independent variables and output variables are called dependent variables.</p>&#13;
<p>In high school, you learned that the equation of any line in the XY plane can be expressed as y = mx + b, where m is the slope and b is the y-intercept of the line. <a contenteditable="false" data-primary="variables" data-secondary="output/dependent" data-type="indexterm" id="id270"/><a contenteditable="false" data-primary="output/dependent variables" data-type="indexterm" id="id271"/><a contenteditable="false" data-primary="input/independent variables" data-type="indexterm" id="id272"/><a contenteditable="false" data-primary="independent/input variables" data-type="indexterm" id="id273"/><a contenteditable="false" data-primary="dependent/output variables" data-type="indexterm" id="id274"/><a contenteditable="false" data-primary="variables" data-secondary="input/independent" data-type="indexterm" id="id275"/>For example, if you assume that consumer spending—the output/dependent variable y—has a linear relationship with personal income—the input/independent variable x—the equation for the line is called a model for consumer spending. <a contenteditable="false" data-primary="parameters of a model" data-type="indexterm" id="id276"/><a contenteditable="false" data-primary="models" data-secondary="about" data-tertiary="parameters" data-type="indexterm" id="id277"/><a contenteditable="false" data-primary="models" data-secondary="about" data-tertiary="input/independent variables" data-type="indexterm" id="id278"/><a contenteditable="false" data-primary="models" data-secondary="about" data-tertiary="output/dependent variables" data-type="indexterm" id="id279"/>Moreover, the slope m and the intercept b are referred to as the model’s parameters. They are treated as constants, and their specific values define unique functional relationships or models.</p>&#13;
<p>Depending on the type of functional relationships,<a contenteditable="false" data-primary="models" data-secondary="about" data-tertiary="deterministic models" data-type="indexterm" id="id280"/><a contenteditable="false" data-primary="deterministic models versus probabilistic" data-type="indexterm" id="id281"/><a contenteditable="false" data-primary="models" data-secondary="about" data-tertiary="probabilistic models" data-type="indexterm" id="id282"/><a contenteditable="false" data-primary="probabilistic financial models" data-secondary="deterministic versus" data-type="indexterm" id="id283"/><a contenteditable="false" data-primary="models" data-secondary="probabilistic financial models" data-tertiary="about" data-type="indexterm" id="id284"/> the parameters, and the nature of inputs and outputs variables, models may be classified as deterministic or probabilistic. In a deterministic model, there are no uncertainties about the type of functional relationships, the parameters, or the inputs or outputs of the model. The exact opposite is true for probabilistic models discussed in this book.</p>&#13;
</div></aside>&#13;
<section data-pdf-bookmark="Finance Is Not Physics" data-type="sect1"><div class="sect1" id="finance_is_not_physics">&#13;
<h1>Finance Is Not Physics</h1>&#13;
<p>Adam Smith, generally recognized<a contenteditable="false" data-primary="Smith, Adam" data-type="indexterm" id="id285"/><a contenteditable="false" data-primary="financial theory" data-secondary="flaws of" data-tertiary="finance is not physics" data-type="indexterm" id="ch01-phys"/><a contenteditable="false" data-primary="probabilistic machine learning (PML)" data-secondary="finance is not physics" data-type="indexterm" id="ch01-phys2"/><a contenteditable="false" data-primary="physics" data-secondary="finance is not physics" data-type="indexterm" id="ch01-phys3"/> as the founder of modern economics, was in awe of Newton’s laws of mechanics and gravitation.<sup><a data-type="noteref" href="ch01.html#ch01fn1" id="ch01fn1-marker">1</a></sup> Since then, economists have endeavored to make their discipline into a mathematical science like physics. They aspire to formulate theories that accurately explain and predict the economic activities of human beings at the micro and macro levels. This desire gathered momentum in the early 20th century with economists like Irving Fisher and culminated in the econophysics movement of the late 20th century.</p>&#13;
<p>Despite all the complicated mathematics of modern finance, its theories are woefully inadequate, almost pitiful, especially when compared to those of physics. For instance, physics can predict the motion of the moon and the electrons in your computer with jaw-dropping precision. These predictions can be calculated by any physicist, at any time, anywhere on the planet. By contrast, market participants—traders, investors, analysts, finance executives—have trouble explaining the causes of daily market movements or predicting the price of an asset at any time, anywhere in the world.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="the_political_economics_of_misrepresent">&#13;
<h1>The Political Economics of Misrepresenting a Nobel Prize</h1>&#13;
<p>In his will, Alfred Nobel did not<a contenteditable="false" data-primary="Nobel, Alfred" data-type="indexterm" id="id286"/><a contenteditable="false" data-primary="Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel" data-type="indexterm" id="id287"/><a contenteditable="false" data-primary="“Nobel Prize in Economics”" data-primary-sortas="Nobel Prize" data-type="indexterm" id="id288"/><a contenteditable="false" data-primary="Newton, Isaac" data-type="indexterm" id="id289"/> create a prize in economics or mathematics or any other discipline besides physics, chemistry, medicine, literature, and peace. The Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel, now commonly and mistakenly referred to as the Nobel Prize in Economics, was created by the Swedish Central Bank in 1968. The central bank funds the award in perpetuity and pays the Nobel Foundation to administer it like it does the Nobel prizes willed by its benefactor.</p>&#13;
<p>By elevating the status of economics to that of the natural sciences and by buying the ongoing support of the prestigious Nobel Foundation, the Swedish central bank was able to gain independence in its decision making from the country’s politicians to pursue its market-friendly policies. Economic policy decisions were to be left to the economic “scientists,” just as health policy decisions were left to medical scientists.<sup><a data-type="noteref" href="ch01.html#ch01fn2" id="ch01fn2-marker">2</a></sup> However, by doing this, the Foundation disregards the will of Alfred Nobel and misrepresents the fundamental nature of economics as a social science.</p>&#13;
<p>In his 1974 acceptance speech, Friedrich Hayek,<a contenteditable="false" data-primary="Hayek, Friedrich" data-type="indexterm" id="id290"/> a pioneer of libertarian economics and advocate for free-market policies, clearly understood how the newly established economics prize could be misused when he said, “The Nobel Prize confers on an individual an authority which in economics no man ought to possess...This does not matter in the natural sciences. Here the influence exercised by an individual is chiefly an influence on his fellow experts; and they will soon cut him down to size if he exceeds his competence. But the influence of the economist that mainly matters is an influence over laymen: politicians, journalists, civil servants and the public generally.”<sup><a data-type="noteref" href="ch01.html#ch01fn3" id="ch01fn3-marker">3</a></sup></p>&#13;
</div></aside>&#13;
<p class="pagebreak-before">Perhaps finance is harder than physics. Unlike particles and pendulums, people are complex, emotional, creative beings with free will and latent cognitive biases. They tend to behave inconsistently and continually react to the actions of others in unpredictable ways. Furthermore, market participants profit by beating or gaming the systems that they operate in.</p>&#13;
<p>After losing a fortune on his investment<a contenteditable="false" data-primary="Newton, Isaac" data-type="indexterm" id="id291"/> in the South Sea Company, Newton remarked, “I can calculate the movement of the stars, but not the madness of men.”<sup><a data-type="noteref" href="ch01.html#ch01fn4" id="ch01fn4-marker">4</a></sup> Note that Newton was not a novice investor. He served as the warden of the Mint in England for almost 31 years, helping put the British pound on the gold standard, where it would stay for over two centuries.<a contenteditable="false" data-primary="" data-startref="ch01-phys" data-type="indexterm" id="id292"/><a contenteditable="false" data-primary="" data-startref="ch01-phys2" data-type="indexterm" id="id293"/><a contenteditable="false" data-primary="" data-startref="ch01-phys3" data-type="indexterm" id="id294"/></p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="All Financial Models Are Wrong, Most Are Useless" data-type="sect1"><div class="sect1" id="all_financial_models_are_wrongcomma_mos">&#13;
<h1>All Financial Models Are Wrong, Most Are Useless</h1>&#13;
<p>Some academics have even argued<a contenteditable="false" data-primary="financial theory" data-secondary="models wrong, even dangerous" data-type="indexterm" id="id295"/><a contenteditable="false" data-primary="models" data-secondary="wrong, even dangerous" data-type="indexterm" id="id296"/><a contenteditable="false" data-primary="financial theory" data-secondary="models wrong, even dangerous" data-see="dangers of conventional statistical methods" data-tertiary="conventional statistics" data-type="indexterm" id="id297"/><a contenteditable="false" data-primary="financial theory" data-secondary="models wrong, even dangerous" data-see="dangers of conventional AI" data-tertiary="conventional AI" data-type="indexterm" id="id298"/> that theoretical financial models are not only wrong but also dangerous. The veneer of a physical science lulls adherents of economic models into a false sense of certainty about the accuracy of their predictive powers.<sup><a data-type="noteref" href="ch01.html#ch01fn5" id="ch01fn5-marker">5</a></sup> This blind faith has led to many disastrous consequences for their adherents and for society at large.<sup><a data-type="noteref" href="ch01.html#ch01fn6" id="ch01fn6-marker">6</a></sup> <a contenteditable="false" data-primary="Long-Term Capital Management (LTCM)" data-type="indexterm" id="id299"/><a contenteditable="false" data-primary="LCTM (Long-Term Capital Management)" data-type="indexterm" id="id300"/><a contenteditable="false" data-primary="models" data-secondary="wrong, even dangerous" data-tertiary="Long-Term Capital Management" data-type="indexterm" id="id301"/><a contenteditable="false" data-primary="financial theory" data-secondary="models wrong, even dangerous" data-tertiary="Long-Term Capital Management" data-type="indexterm" id="id302"/>Nothing better exemplifies the dangerous consequences of academic arrogance and blind faith in analytical financial models than the spectacular disaster of LTCM, discussed in the sidebar.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="the_disaster_of_long_term_capital_manag">&#13;
<h1>The Disaster of Long-Term Capital Management (LTCM)</h1>&#13;
<p>LTCM was a hedge fund founded<a contenteditable="false" data-primary="Scholes, Myron" data-type="indexterm" id="id303"/><a contenteditable="false" data-primary="Black-Scholes-Merton option pricing formula" data-type="indexterm" id="id304"/> in 1994 by Wall Street veterans and academics Myron Scholes and Robert Merton, inventors of the famous Black-Scholes-Merton option pricing formula. The LTCM team was so confident in its investment models, overseen by two future “Nobel laureates,” that it leveraged its portfolios to dangerously high levels. The team intended to magnify the tiny profits that LTCM was making on its various investments. In the first four years, LTCM had very impressive annual returns and had to turn away investor money.</p>&#13;
<p>However, the unpredictable complexity<a contenteditable="false" data-primary="Russian government default on local currency bonds" data-type="indexterm" id="id305"/> of social systems reared its ugly head in 1998, when the Russian government defaulted on its domestic local currency bonds. Such an event was not anticipated by LTCM’s models, since a government can always print more money rather than default on its debt. This shocked global markets and led to the rapid collapse of LTCM—and showed that leverage magnifies losses, as it does gains. To prevent the crisis of LTCM from spreading and crashing the global financial markets, the Federal Reserve and a consortium of large banks bailed out LTCM. See <a data-type="xref" href="#the_disaster_of_ltcmsource_for_the_imag">Figure 1-1</a>, which compares the value of $1,000 invested separately in LTCM, Dow Jones (DJIA), and US Treasury bonds.</p>&#13;
</div></aside>&#13;
<figure><div class="figure" id="the_disaster_of_ltcmsource_for_the_imag">&#13;
<img alt="The disaster of LTCM" src="assets/pmlf_0101.png"/>&#13;
<h6><span class="label">Figure 1-1. </span>The epic disaster of Long Term Capital Management (LTCM)<sup><a data-type="noteref" href="ch01.html#ch01fn7" id="ch01fn7-marker">7</a></sup></h6>&#13;
</div></figure>&#13;
<p class="pagebreak-before">Taking a diametrically different approach<a contenteditable="false" data-primary="Renaissance Technologies" data-type="indexterm" id="id306"/> from hedge funds like LTCM, Renaissance Technologies, the most successful hedge fund in history, has put its critical views of financial theories into practice. Instead of hiring people with a finance or Wall Street background, the company prefers to hire physicists, mathematicians, statisticians, and computer scientists. It trades the markets using quantitative models based on nonfinancial theories such as information theory, data science, and machine learning.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="The Trifecta of Modeling Errors" data-type="sect1"><div class="sect1" id="the_trifecta_of_modeling_errors">&#13;
<h1>The Trifecta of Modeling Errors</h1>&#13;
<p>Whether financial models are based<a contenteditable="false" data-primary="models" data-secondary="wrong but useful" data-tertiary="types of errors" data-type="indexterm" id="ch01-threety"/><a contenteditable="false" data-primary="financial theory" data-secondary="models wrong but useful" data-tertiary="types of errors" data-type="indexterm" id="ch01-threety2"/> on academic theories or empirical data-mining strategies, they are all subject to the trifecta of modeling errors. Errors in analysis and forecasting may arise from any of the following modeling issues: using an inappropriate functional form, inputting inaccurate parameters, or failing to adapt to structural changes in the market.<sup><a data-type="noteref" href="ch01.html#ch01fn8" id="ch01fn8-marker">8</a></sup> </p>&#13;
<section data-pdf-bookmark="Errors in Model Specification" data-type="sect2"><div class="sect2" id="errors_in_model_specification">&#13;
<h2>Errors in Model Specification</h2>&#13;
<p>Almost all financial theories use<a contenteditable="false" data-primary="financial theory" data-secondary="models wrong but useful" data-tertiary="model specification errors" data-type="indexterm" id="id307"/><a contenteditable="false" data-primary="models" data-secondary="wrong but useful" data-tertiary="model specification errors" data-type="indexterm" id="id308"/><a contenteditable="false" data-primary="normal (Gaussian) distribution" data-secondary="financial theories using" data-type="indexterm" id="id309"/><a contenteditable="false" data-primary="Gaussian distribution" data-see="normal distribution" data-type="indexterm" id="id310"/><a contenteditable="false" data-primary="Black-Scholes-Merton option pricing formula" data-secondary="normal distribution basis" data-type="indexterm" id="id311"/><a contenteditable="false" data-primary="modern portfolio theory (MPT)" data-secondary="based on normal distribution" data-type="indexterm" id="id312"/> the Gaussian or normal distribution in their models. For instance, the normal distribution is the foundation upon which Markowitz’s modern portfolio theory and Black-Scholes-Merton option pricing theory are built.<sup><a data-type="noteref" href="ch01.html#ch01fn9" id="ch01fn9-marker">9</a></sup> However, it is a well-documented fact in academic research that stocks, bonds, currencies, and commodities have fat-tailed return distributions that are distinctly non-Gaussian.<sup><a data-type="noteref" href="ch01.html#ch01fn10" id="ch01fn10-marker">10</a></sup> In other words, extreme events occur far more frequently than predicted by the normal distribution. In <a data-type="xref" href="ch03.html#quantifying_output_uncertainty_with_mon">Chapter 3</a> and <a data-type="xref" href="ch04.html#the_dangers_of_conventional_statistical">Chapter 4</a>, we will actually do financial data analysis in Python to demonstrate the non-Gaussian structure of equity return distributions.</p>&#13;
<p>If asset price returns were normally distributed, none of the following financial disasters would occur within the age of the universe: Black Monday, the Mexican peso crisis, the Asian currency crisis, the bankruptcy of LTCM, or the Flash Crash. “Mini flash crashes” of individual stocks occur with even higher frequency than these macro events.</p>&#13;
<p class="pagebreak-before">Yet, finance textbooks, programs, and professionals continue to use the normal distribution in their asset valuation and risk models because of its simplicity and analytical tractability. These reasons are no longer justifiable given today’s advanced algorithms and computational resources. <a contenteditable="false" data-primary="drunkard’s search and normal distribution" data-type="indexterm" id="id313"/>This reluctance to abandon the normal distribution is a clear example of “the drunkard’s search”: a principle derived from a joke about a drunkard who loses his key in the darkness of a park but frantically searches for it under a lamppost because that’s where the light is.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Errors in Model Parameter Estimates" data-type="sect2"><div class="sect2" id="errors_in_model_parameter_estimates">&#13;
<h2>Errors in Model Parameter Estimates</h2>&#13;
<p>Errors of this type may arise<a contenteditable="false" data-primary="financial theory" data-secondary="models wrong but useful" data-tertiary="model parameter estimate errors" data-type="indexterm" id="ch01-mest"/><a contenteditable="false" data-primary="models" data-secondary="wrong but useful" data-tertiary="model parameter estimate errors" data-type="indexterm" id="ch01-mest2"/><a contenteditable="false" data-primary="parameters of a model" data-secondary="errors from estimating" data-type="indexterm" id="ch01-mest3"/><a contenteditable="false" data-primary="interest rates" data-secondary="errors from estimating" data-type="indexterm" id="ch01-mest4"/><a contenteditable="false" data-primary="credit card interest rate estimation errors" data-type="indexterm" id="ch01-mest5"/><a contenteditable="false" data-primary="the Fed (Federal Open Market Committee) interest rate estimation errors" data-primary-sortas="Fed" data-type="indexterm" id="ch01-mest6"/> because market participants have access to different levels of information with varying speeds of delivery. They also have different levels of sophistication in processing abilities and different cognitive biases. Moreover, these parameters are generally estimated from past data, which may not represent current market conditions accurately. These factors lead to profound epistemic uncertainty about model parameters.</p>&#13;
<p>Let’s consider a specific example of interest rates. Fundamental to the valuation of any financial asset, interest rates are used to discount uncertain future cash flows of the asset and estimate its value in the present. At the consumer level, for example, credit cards have variable interest rates pegged to a benchmark called the prime rate. This rate generally changes in lockstep with the federal funds rate, an interest rate of seminal importance to the US and world economies.</p>&#13;
<p>Let’s imagine that you would like to estimate the interest rate on your credit card one year from now. Suppose the current prime rate is 2% and your credit card company charges you 10% plus prime. Given the strength of the current economy, you believe that the Federal Reserve is more likely to raise interest rates than not. Based on our current information, we know that the Fed will meet eight times in the next 12 months and will either raise the federal funds rate by 0.25% or leave it at the previous level.</p>&#13;
<p>In the following Python code example, <a contenteditable="false" data-primary="interest rates" data-secondary="binomial distribution to model" data-type="indexterm" id="id314"/><a contenteditable="false" data-primary="binomial distribution to model interest rates" data-type="indexterm" id="id315"/>we use the binomial distribution to model your credit card’s interest rate at the end of the 12-month period. Specifically, we’ll use the following parameters for our range of estimates about the probability of the Fed raising the federal funds rate by 0.25% at each meeting: <code>fed_meetings</code> = 8 (number of trials or meetings); <code>probability_raises</code> = [0.6, 0.7,0 .8, 0.9]:</p>&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Import binomial distribution from sciPy library</code>&#13;
<code class="kn">from</code> <code class="nn">scipy.stats</code> <code class="kn">import</code> <code class="n">binom</code>&#13;
<code class="c1"># Import matplotlib library for drawing graphs</code>&#13;
<code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="k">as</code> <code class="nn">plt</code>&#13;
&#13;
<code class="c1"># Total number of meetings of the Federal Open Market Committee (FOMC) in any </code>&#13;
<code class="c1"># year</code>&#13;
<code class="n">fed_meetings</code> <code class="o">=</code> <code class="mi">8</code>&#13;
<code class="c1"># Range of total interest rate increases at the end of the year</code>&#13;
<code class="n">total_increases</code> <code class="o">=</code> <code class="nb">list</code><code class="p">(</code><code class="nb">range</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="n">fed_meetings</code> <code class="o">+</code> <code class="mi">1</code><code class="p">))</code>&#13;
<code class="c1"># Probability that the FOMC will raise rates at any given meeting</code>&#13;
<code class="n">probability_raises</code> <code class="o">=</code> <code class="p">[</code><code class="mf">0.6</code><code class="p">,</code> <code class="mf">0.7</code><code class="p">,</code> <code class="mf">0.8</code><code class="p">,</code> <code class="mf">0.9</code><code class="p">]</code>&#13;
&#13;
<code class="n">fig</code><code class="p">,</code> <code class="n">axs</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="mi">8</code><code class="p">))</code>&#13;
&#13;
<code class="k">for</code> <code class="n">i</code><code class="p">,</code> <code class="n">ax</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">axs</code><code class="o">.</code><code class="n">flatten</code><code class="p">()):</code>&#13;
    <code class="c1"># Use the probability mass function to calculate probabilities of total </code>&#13;
    <code class="c1"># raises in eight meetings</code>&#13;
    <code class="c1"># Based on FOMC bias for raising rates at each meeting</code>&#13;
    <code class="n">prob_dist</code> <code class="o">=</code> <code class="n">binom</code><code class="o">.</code><code class="n">pmf</code><code class="p">(</code><code class="n">k</code><code class="o">=</code><code class="n">total_increases</code><code class="p">,</code> <code class="n">n</code><code class="o">=</code><code class="n">fed_meetings</code><code class="p">,</code> &#13;
    <code class="n">p</code><code class="o">=</code><code class="n">probability_raises</code><code class="p">[</code><code class="n">i</code><code class="p">])</code>&#13;
    <code class="c1"># How each 25 basis point increase in the federal funds rate affects your </code>&#13;
    <code class="c1"># credit card interest rate</code>&#13;
    <code class="n">cc_rate</code> <code class="o">=</code> <code class="p">[</code><code class="n">j</code> <code class="o">*</code> <code class="mf">0.25</code> <code class="o">+</code> <code class="mi">12</code> <code class="k">for</code> <code class="n">j</code> <code class="ow">in</code> <code class="n">total_increases</code><code class="p">]</code>&#13;
&#13;
    <code class="c1"># Plot the results for different FOMC probability</code>&#13;
    <code class="n">ax</code><code class="o">.</code><code class="n">hist</code><code class="p">(</code><code class="n">cc_rate</code><code class="p">,</code> <code class="n">weights</code><code class="o">=</code><code class="n">prob_dist</code><code class="p">,</code> <code class="n">bins</code><code class="o">=</code><code class="n">fed_meetings</code><code class="p">,</code> <code class="n">alpha</code><code class="o">=</code><code class="mf">0.5</code><code class="p">,</code> &#13;
    <code class="n">label</code><code class="o">=</code><code class="n">probability_raises</code><code class="p">[</code><code class="n">i</code><code class="p">])</code>&#13;
    <code class="n">ax</code><code class="o">.</code><code class="n">legend</code><code class="p">()</code>&#13;
    <code class="n">ax</code><code class="o">.</code><code class="n">set_ylabel</code><code class="p">(</code><code class="s1">'Probability of credit card rate'</code><code class="p">)</code>&#13;
    <code class="n">ax</code><code class="o">.</code><code class="n">set_xlabel</code><code class="p">(</code><code class="s1">'Predicted range of credit card rates after 12 months'</code><code class="p">)</code>&#13;
    <code class="n">ax</code><code class="o">.</code><code class="n">set_title</code><code class="p">(</code><code class="sa">f</code><code class="s1">'Probability of raising rates at each meeting: </code><code class="w"/>&#13;
    <code class="p">{</code><code class="n">probability_raises</code><code class="p">[</code><code class="n">i</code><code class="p">]}</code><code class="s1">')</code><code class="w"/>&#13;
&#13;
<code class="c1"># Adjust spacing between subplots</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">tight_layout</code><code class="p">()</code>&#13;
&#13;
<code class="c1"># Show the plot</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>&#13;
<p>In <a data-type="xref" href="#probability_distribution_of_credit_card">Figure 1-2</a>, notice<a contenteditable="false" data-primary="probability distributions" data-secondary="errors in model parameter estimations" data-type="indexterm" id="id316"/> how the probability distribution for your credit card rate in 12 months depends critically on your estimate about the probability of the Fed raising rates at each of the eight meetings. You can see that for every increase of 0.1 in your estimate of the Fed raising rates at each meeting, the expected interest rate for your credit card in 12 months increases by about 0.2%.</p>&#13;
<figure><div class="figure" id="probability_distribution_of_credit_card">&#13;
<img alt="Probability distribution of credit card rates depends on your parameter estimates." src="assets/pmlf_0102.png"/>&#13;
<h6><span class="label">Figure 1-2. </span>Probability distribution of credit card rates depends on your parameter <span class="keep-together">estimates</span></h6>&#13;
</div></figure>&#13;
<p>Even if all market participants used the binomial distribution in their models, it’s easy to see how they could disagree about the future prime rate because of the differences in their estimates about the Fed raising rates at each meeting. Indeed, this parameter is hard to estimate. Many institutions have dedicated analysts, including previous employees of the Fed, analyzing the Fed’s every document, speech, and event to try to estimate this parameter. This is because the Fed funds rate directly impacts the prices of all financial assets and indirectly impacts the employment and inflation rates in the real economy.</p>&#13;
<p>Recall that we assumed that this parameter, <code>probability_raises</code>, was constant in our model for each of the next eight Fed meetings. How realistic is that? Members of the Federal Open Market Committee (FOMC), the rate-setting body, are not just a set of biased coins. They can and do change their individual biases based on how the economy changes over time. The assumption that the parameter <code>probabil⁠ity_​raises</code> will be constant over the next 12 months is not only unrealistic, but also risky.<a contenteditable="false" data-primary="" data-startref="ch01-mest" data-type="indexterm" id="id317"/><a contenteditable="false" data-primary="" data-startref="ch01-mest2" data-type="indexterm" id="id318"/><a contenteditable="false" data-primary="" data-startref="ch01-mest3" data-type="indexterm" id="id319"/><a contenteditable="false" data-primary="" data-startref="ch01-mest4" data-type="indexterm" id="id320"/><a contenteditable="false" data-primary="" data-startref="ch01-mest5" data-type="indexterm" id="id321"/><a contenteditable="false" data-primary="" data-startref="ch01-mest6" data-type="indexterm" id="id322"/></p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Errors from the Failure of a Model to Adapt to Structural Changes" data-type="sect2"><div class="sect2" id="errors_from_the_failure_of_a_model_to_a">&#13;
<h2>Errors from the Failure of a Model to Adapt to Structural Changes</h2>&#13;
<p>The underlying data-generating stochastic<a contenteditable="false" data-primary="models" data-secondary="wrong but useful" data-tertiary="model failing to adapt to structural changes" data-type="indexterm" id="id323"/><a contenteditable="false" data-primary="financial theory" data-secondary="models wrong but useful" data-tertiary="model failing to adapt to structural changes" data-type="indexterm" id="id324"/> process may vary over time—i.e., the process is not stationary ergodic. This implies that statistical moments of the distribution, like mean and variance, computed from sample financial data taken at a specific moment in time or sampled over a sufficiently long time period do not accurately predict the future statistical moments of the underlying distribution. The concepts of stationarity and ergodicity are very important in finance and will be explained in more detail later in the book.</p>&#13;
<p>We live in a dynamic capitalist economy characterized by technological innovations and changing monetary and fiscal policies. Time-variant distributions for asset values and risks are the rule, not the exception. <a contenteditable="false" data-primary="historical data for parameter values" data-type="indexterm" id="id325"/><a contenteditable="false" data-primary="time-variant distribution parameter values from historical data" data-type="indexterm" id="id326"/>For such distributions, parameter values based on historical data are bound to introduce error into forecasts.</p>&#13;
<p>In our previous example, if the economy were to show signs of slowing down, the Fed might decide to adopt a more neutral stance in its fourth meeting, making you change your <code>probability_raises</code> parameter from 70% to 50% going forward. This change in your parameter will, in turn, change the forecast of your credit card interest rate.</p>&#13;
<p>Sometimes the time-variant distributions and their parameters change continuously or abruptly, as in the Mexican peso crisis. For either continuous or abrupt changes, the models used will need to adapt to evolving market conditions. A new functional form with different parameters might be required to explain and predict asset values and risks in the new market regime.</p>&#13;
<p>Suppose after the fifth meeting in our example, the US economy is hit by an external shock—say a new populist government in Greece decides to default on its debt obligations. Now the Fed may be more likely to cut interest rates than to raise them. <a contenteditable="false" data-primary="probability distributions" data-secondary="errors from model not adapting to structural changes" data-type="indexterm" id="id327"/><a contenteditable="false" data-primary="interest rates" data-secondary="binomial distribution to model" data-type="indexterm" id="id328"/><a contenteditable="false" data-primary="binomial distribution to model interest rates" data-type="indexterm" id="id329"/>Given this structural change in the Fed’s outlook, we will have to change the binomial probability distribution in our model to a trinomial distribution with appropriate parameters.<a contenteditable="false" data-primary="" data-startref="ch01-threety" data-type="indexterm" id="id330"/><a contenteditable="false" data-primary="" data-startref="ch01-threety2" data-type="indexterm" id="id331"/></p>&#13;
</div></section>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Probabilistic Financial Models" data-type="sect1"><div class="sect1" id="probabilistic_financial_models">&#13;
<h1>Probabilistic Financial Models</h1>&#13;
<p>Inaccuracies of financial models<a contenteditable="false" data-primary="financial theory" data-secondary="probabilistic financial models" data-type="indexterm" id="id332"/><a contenteditable="false" data-primary="models" data-secondary="probabilistic financial models" data-type="indexterm" id="id333"/><a contenteditable="false" data-primary="financial theory" data-secondary="flaws of" data-tertiary="inexact instead of precise" data-type="indexterm" id="id334"/><a contenteditable="false" data-primary="models" data-secondary="probabilistic financial models" data-tertiary="model uncertainty quantified" data-type="indexterm" id="id335"/><a contenteditable="false" data-primary="uncertainty quantification and analysis" data-secondary="all models should quantify uncertainty" data-type="indexterm" id="id336"/><a contenteditable="false" data-primary="probabilistic financial models" data-type="indexterm" id="id337"/> are features, not bugs. It is intellectually dishonest and foolishly risky to represent financial estimates as scientifically precise values. <a contenteditable="false" data-primary="models" data-secondary="uncertainty must be quantified" data-type="indexterm" id="id338"/>All models should quantify the uncertainty inherent in financial inferences and predictions to be useful for sound decision making and risk management in the business world. Financial data are noisy and have measurement errors. A model’s appropriate functional form may be unknown or an approximation. Model parameters and outputs may have a range of values with associated plausibilities. <a contenteditable="false" data-primary="probabilistic financial models" data-secondary="model uncertainty quantified" data-type="indexterm" id="id339"/><a contenteditable="false" data-primary="inaccuracies accommodated by probabilistic models" data-type="indexterm" id="id340"/><a contenteditable="false" data-primary="probabilistic financial models" data-secondary="inaccuracies accommodated by" data-type="indexterm" id="id341"/>In other words, we need mathematically sound probabilistic models because they accommodate inaccuracies and quantify uncertainties with logical consistency.</p>&#13;
<p>There are two ways model uncertainty is currently quantified:<a contenteditable="false" data-primary="uncertainty quantification and analysis" data-secondary="output uncertainty via forward propagation" data-tertiary="about" data-type="indexterm" id="id342"/><a contenteditable="false" data-primary="uncertainty quantification and analysis" data-secondary="input uncertainty via inverse propagation" data-type="indexterm" id="id343"/><a contenteditable="false" data-primary="output uncertainty quantified via forward propagation" data-type="indexterm" id="id344"/><a contenteditable="false" data-primary="Monte Carlo simulation (MCS)" data-secondary="output uncertainty quantified via forward propagation" data-type="indexterm" id="id345"/><a contenteditable="false" data-primary="input uncertainty quantified via inverse propagation" data-type="indexterm" id="id346"/><a contenteditable="false" data-primary="inverse propagation quantifying input uncertainty" data-type="indexterm" id="id347"/><a contenteditable="false" data-primary="forward propagation quantifying output uncertainty" data-type="indexterm" id="id348"/><a contenteditable="false" data-primary="output uncertainty quantified via forward propagation" data-secondary="PML models" data-type="indexterm" id="id349"/><a contenteditable="false" data-primary="uncertainty quantification and analysis" data-secondary="output uncertainty via forward propagation" data-tertiary="PML models" data-type="indexterm" id="id350"/> forward propagation for output uncertainty, and inverse propagation for input uncertainty. <a data-type="xref" href="#quantifying_input_and_output_uncertaint">Figure 1-3</a> shows the common types of probabilistic models used in finance today for quantifying both types of uncertainty.</p>&#13;
<figure><div class="figure" id="quantifying_input_and_output_uncertaint">&#13;
<img alt="Quantifying input and output uncertainty with probabilistic models" src="assets/pmlf_0103.png"/>&#13;
<h6><span class="label">Figure 1-3. </span>Quantifying input and output uncertainty with probabilistic models</h6>&#13;
</div></figure>&#13;
<p>In forward uncertainty propagation, uncertainties arising from a model’s inexact parameters and inputs are propagated forward throughout the model to generate the uncertainty of the model’s outputs. <a contenteditable="false" data-primary="scenario analysis" data-type="indexterm" id="id351"/><a contenteditable="false" data-primary="sensitivity analysis" data-type="indexterm" id="id352"/>Most financial analysts use scenario and sensitivity analyses to quantify the uncertainty in their models’ predictions. However, these are basic tools that only consider a few possibilities.</p>&#13;
<p>In scenario analysis, only three cases are built for consideration: best-case, base-case, and worst-case scenarios. Each case has a set value for all the inputs and parameters of a model. Similarly, in sensitivity analysis, only a few inputs or parameters are changed to assess their impact on the model’s total output. For instance, a sensitivity analysis might be conducted on how the value of a company changes with interest rates or future earnings. In <a data-type="xref" href="ch03.html#quantifying_output_uncertainty_with_mon">Chapter 3</a>, we will learn<a contenteditable="false" data-primary="Monte Carlo simulation (MCS)" data-secondary="about" data-type="indexterm" id="id353"/> how to perform Monte Carlo simulations (MCS) using Python and apply it to common financial problems. MCS is one of the most powerful probabilistic numerical tools in all the sciences and is used for analyzing both deterministic and probabilistic systems. It is a set of numerical methods that uses independent random samples from specified input parameter distributions to generate new data that we might observe in the future. This enables us to compute the expected uncertainty of a model, especially when its functional relationships are not analytically tractable.</p>&#13;
<p class="pagebreak-before">In inverse uncertainty propagation, uncertainty of the model’s input parameters is inferred from observed data. This is a harder computational problem than forward propagation because the parameters have to be learned from the data using dependent random sampling. Advanced statistical inference techniques or complex numerical computations are used to calculate confidence intervals or credible intervals of a model’s input parameters. In <a data-type="xref" href="ch04.html#the_dangers_of_conventional_statistical">Chapter 4</a>, we explain the deep flaws and limitations of using p-values and confidence intervals, statistical techniques that are commonly used in financial data analysis today. Later in <a data-type="xref" href="ch06.html#the_dangers_of_conventional_ai_systems">Chapter 6</a>, we explain Markov chain Monte Carlo, an advanced, dependent, random sampling method, which can be used to compute credible intervals to quantify the uncertainty of a model’s input <span class="keep-together">parameters.</span></p>&#13;
<p>We require a comprehensive probabilistic framework that combines both forward and inverse uncertainty propagation seamlessly. We don’t want the piecemeal approach that is currently in practice today. That is, we want our probabilistic models to quantify the uncertainty in the outputs of time-variant stochastic processes, with their inexact input parameters learned from sample data.</p>&#13;
<p>Our probabilistic framework will need to update continually the model outputs or its input parameters—or both—based on materially new datasets. Such models will have to be developed using small datasets, since the underlying environment may have changed too quickly to collect a sizable amount of relevant data. Most importantly, our probabilistic models need to know what they don’t know. When extrapolating from datasets they have never encountered before, they need to provide answers with low confidence levels or wider margins of uncertainty.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Financial AI and ML" data-type="sect1"><div class="sect1" id="financial_ai_and_ml">&#13;
<h1>Financial AI and ML</h1>&#13;
<p>Probabilistic machine learning (ML) meets<a contenteditable="false" data-primary="probabilistic machine learning (PML)" data-secondary="ML and AI explained" data-type="indexterm" id="ch01-mlai"/><a contenteditable="false" data-primary="machine learning (ML) models" data-secondary="description of" data-type="indexterm" id="ch01-mlai2"/><a contenteditable="false" data-primary="AI" data-secondary="description of AI" data-type="indexterm" id="ch01-mlai3"/><a contenteditable="false" data-primary="models" data-secondary="ML and AI explained" data-type="indexterm" id="ch01-mlai4"/> all the previously mentioned requirements for building state-of-the-art, next-generation financial systems.<sup><a data-type="noteref" href="ch01.html#ch01fn11" id="ch01fn11-marker">11</a></sup> But what is probabilistic ML? Before we answer that question, let’s first make sure we understand what we mean by ML in particular and AI in general. It is common to see these terms bandied about as synonyms, even though they are not. ML is a subfield of AI. See <a data-type="xref" href="#ml_is_a_subfield_of_aidot">Figure 1-4</a>.</p>&#13;
<figure><div class="figure" id="ml_is_a_subfield_of_aidot">&#13;
<img alt="ML is a subfield of AI." src="assets/pmlf_0104.png"/>&#13;
<h6><span class="label">Figure 1-4. </span>ML is a subfield of AI</h6>&#13;
</div></figure>&#13;
<p>AI is the general field that tries to automate the cognitive abilities of humans, such as analytical thinking, decision making, and sensory perception. <a contenteditable="false" data-primary="symbolic AI (SAI)" data-type="indexterm" id="id354"/><a contenteditable="false" data-primary="SAI (symbolic AI)" data-type="indexterm" id="id355"/><a contenteditable="false" data-primary="AI" data-secondary="symbolic AI" data-type="indexterm" id="id356"/><a contenteditable="false" data-primary="expert systems described" data-type="indexterm" id="id357"/>In the 20th century, computer scientists developed a subfield of AI called symbolic AI (SAI), which included methodologies and tools to embed into computer systems, symbolic representations of human knowledge in the form of well-defined rules or algorithms.</p>&#13;
<p>SAI systems automate the models specified by domain experts and are aptly called expert systems. For instance, traders, finance executives, and system developers work together to explicitly formulate all the rules and the model’s parameters that are to be automated by their financial and investment management systems. I have managed several such projects for marquee financial institutions at one of my previous companies.</p>&#13;
<p>However, SAI failed in automating complex tasks like image recognition and natural language processing—technologies used extensively in corporate finance and investing today. The rules for these types of expert systems are too complex and require constant updating for different situations. In the latter part of the 20th century, a new AI subfield of ML emerged from the confluence of improved algorithms, abundant data, and cheap computing resources.</p>&#13;
<p>ML turns the SAI paradigm on its head. Instead of experts specifying models to process data, humans with little or no domain expertise provide general-purpose algorithms that learn a model from data samples. More importantly, ML programs continually learn from new datasets and update their models without any human intervention for code maintenance. See the next sidebar for a simple explanation of how parameters are learned from data.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="training_a_linear_ml_system_to_learn">&#13;
<h1>Training a Linear ML System to Learn</h1>&#13;
<p>Recall the deterministic linear model<a contenteditable="false" data-primary="probabilistic machine learning (PML)" data-secondary="training a linear ML system" data-type="indexterm" id="id358"/><a contenteditable="false" data-primary="deterministic models versus probabilistic" data-type="indexterm" id="id359"/><a contenteditable="false" data-primary="probabilistic financial models" data-secondary="deterministic versus" data-type="indexterm" id="id360"/><a contenteditable="false" data-primary="models" data-secondary="about" data-tertiary="deterministic models" data-type="indexterm" id="id361"/><a contenteditable="false" data-primary="models" data-secondary="about" data-tertiary="probabilistic models" data-type="indexterm" id="id362"/><a contenteditable="false" data-primary="models" data-secondary="probabilistic financial models" data-tertiary="training a linear ML system" data-type="indexterm" id="id363"/><a contenteditable="false" data-primary="training a linear ML system" data-type="indexterm" id="id364"/> discussed earlier and expressed by the equation y = mx + b. A unique line crosses at least two distinct points in the XY plane. The two points enable us to solve analytically for the exact values of parameters m and b using simple algebra. Once you have computed the parameters, you can use your model to make accurate predictions; given any point x, you can predict exactly what y will be.</p>&#13;
<p>However, financial models are not deterministic but probabilistic. For instance, if you were to plot a company’s stock price returns on the y-axis and the growth rate of its quarterly earnings on the x-axis, you would see stock returns generally increase with earnings growth of a company. If you assume the relationship between stock price returns and quarterly earnings growth is <em>approximately</em> linear, you can use an analytical statistical technique to solve for the model’s parameters m and b that gives you the line that best fits the company’s sample data. If the linear approximation persists in the future, your model’s predictions are not going to be precise, but they are going to be better than making random guesses or relying on luck.</p>&#13;
<p>Alternatively, you could use ML software to do similar calculations for you. <a contenteditable="false" data-primary="probabilistic machine learning (PML)" data-secondary="training a linear ML system" data-tertiary="independent variable as feature or predictor" data-type="indexterm" id="id365"/><a contenteditable="false" data-primary="independent/input variables" data-secondary="feature or predictor in ML systems" data-type="indexterm" id="id366"/><a contenteditable="false" data-primary="input/independent variables" data-secondary="feature or predictor in ML systems" data-type="indexterm" id="id367"/><a contenteditable="false" data-primary="variables" data-secondary="input/independent" data-tertiary="feature or predictor in ML systems" data-type="indexterm" id="id368"/><a contenteditable="false" data-primary="variables" data-secondary="output/dependent" data-tertiary="target or response in ML systems" data-type="indexterm" id="id369"/><a contenteditable="false" data-primary="output/dependent variables" data-secondary="target or response in ML systems" data-type="indexterm" id="id370"/><a contenteditable="false" data-primary="dependent/output variables" data-secondary="target or response in ML systems" data-type="indexterm" id="id371"/><a contenteditable="false" data-primary="training a linear ML system" data-secondary="independent variable as feature or predictor" data-type="indexterm" id="id372"/><a contenteditable="false" data-primary="training a linear ML system" data-secondary="dependent variable as target or response" data-type="indexterm" id="id373"/><a contenteditable="false" data-primary="probabilistic machine learning (PML)" data-secondary="training a linear ML system" data-tertiary="dependent variable as target or response" data-type="indexterm" id="id374"/>In ML systems, the independent variable x is called a feature or predictor, and the dependent variable y is called the target or response variable. <a contenteditable="false" data-primary="training a linear ML system" data-secondary="“training” as feeding sample data" data-secondary-sortas="training as feeding" data-type="indexterm" id="id375"/>Feeding sample data to the ML system is referred to as training the system. When our linear ML system computes the values of the parameters m and b, we say that the ML system has learned the model from the in-sample data. <a contenteditable="false" data-primary="training a linear ML system" data-secondary="objective of ML systems" data-type="indexterm" id="id376"/><a contenteditable="false" data-primary="predictions" data-secondary="training a linear ML system" data-type="indexterm" id="id377"/>The objective in ML is to predict the target values on out-of-sample data, which the system has not been trained on. This is where predictions become challenging.</p>&#13;
</div></aside>&#13;
<p>We will get into the details of modeling, training, and testing probabilistic ML systems in the second half of the book. <a contenteditable="false" data-primary="Mitchell, Tom" data-type="indexterm" id="id378"/><a contenteditable="false" data-primary="probabilistic machine learning (PML)" data-secondary="ML and AI explained" data-tertiary="ML defined by Tom Mitchell" data-type="indexterm" id="id379"/><a contenteditable="false" data-primary="training a linear ML system" data-secondary="ML defined by Tom Mitchell" data-type="indexterm" id="id380"/><a contenteditable="false" data-primary="probabilistic machine learning (PML)" data-secondary="training a linear ML system" data-tertiary="ML defined by Tom Mitchell" data-type="indexterm" id="id381"/><a contenteditable="false" data-primary="models" data-secondary="ML and AI explained" data-tertiary="ML defined by Tom Mitchell" data-type="indexterm" id="id382"/>Here is a useful definition of ML from Tom Mitchell, an ML pioneer: “A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.”<sup><a data-type="noteref" href="ch01.html#ch01fn12" id="ch01fn12-marker">12</a></sup> See <a data-type="xref" href="#an_ml_model_learns_its_parameters_from">Figure 1-5</a>.<a contenteditable="false" data-primary="probabilistic machine learning (PML)" data-secondary="ML and AI explained" data-tertiary="performance via out-of-sample data" data-type="indexterm" id="id383"/><a contenteditable="false" data-primary="probabilistic machine learning (PML)" data-secondary="ML and AI explained" data-tertiary="parameters from in-sample data" data-type="indexterm" id="id384"/><a contenteditable="false" data-primary="models" data-secondary="ML and AI explained" data-tertiary="performance via out-of-sample data" data-type="indexterm" id="id385"/><a contenteditable="false" data-primary="models" data-secondary="ML and AI explained" data-tertiary="parameters from in-sample data" data-type="indexterm" id="id386"/><a contenteditable="false" data-primary="parameters of a model" data-secondary="in-sample data providing" data-type="indexterm" id="id387"/><a contenteditable="false" data-primary="performance of a model" data-secondary="out-of-sample data providing" data-type="indexterm" id="id388"/></p>&#13;
<figure><div class="figure" id="an_ml_model_learns_its_parameters_from">&#13;
<img alt="An ML model learns its parameters from in-sample data, but its performance is evaluated on out-of-sample data." src="assets/pmlf_0105.png"/>&#13;
<h6><span class="label">Figure 1-5. </span>An ML model learns its parameters from in-sample data, but its performance is evaluated on out-of-sample data</h6>&#13;
</div></figure>&#13;
<p>Performance is measured against a prespecified objective function, such as maximizing annual stock price returns or lowering the mean absolute error of parameter <span class="keep-together">estimates.</span></p>&#13;
<p>ML systems are usually classified<a contenteditable="false" data-primary="probabilistic machine learning (PML)" data-secondary="ML and AI explained" data-tertiary="types of ML systems" data-type="indexterm" id="id389"/><a contenteditable="false" data-primary="models" data-secondary="ML and AI explained" data-tertiary="types of ML systems" data-type="indexterm" id="id390"/><a contenteditable="false" data-primary="probabilistic machine learning (PML)" data-secondary="training a linear ML system" data-tertiary="types of ML systems" data-type="indexterm" id="id391"/><a contenteditable="false" data-primary="training a linear ML system" data-secondary="types of ML systems" data-type="indexterm" id="id392"/> into three types based on how much assistance they need from their human teachers or supervisors.<a contenteditable="false" data-primary="supervised learning ML systems" data-type="indexterm" id="id393"/><a contenteditable="false" data-primary="unsupervised learning ML systems" data-type="indexterm" id="id394"/><a contenteditable="false" data-primary="reinforcement learning ML systems" data-type="indexterm" id="id395"/><a contenteditable="false" data-primary="k-means clustering algorithm as unsupervised learning" data-type="indexterm" id="id396"/><a contenteditable="false" data-primary="linear regression" data-secondary="as supervised learning" data-secondary-sortas="supervised learning" data-type="indexterm" id="id397"/><a contenteditable="false" data-primary="logistic regression as supervised learning" data-type="indexterm" id="id398"/><a contenteditable="false" data-primary="random forests as supervised learning" data-type="indexterm" id="id399"/><a contenteditable="false" data-primary="gradient-boosted machines as supervised learning" data-type="indexterm" id="id400"/><a contenteditable="false" data-primary="deep learning as supervised learning" data-type="indexterm" id="id401"/><a contenteditable="false" data-primary="principal component analysis as unsupervised learning" data-type="indexterm" id="id402"/><a contenteditable="false" data-primary="feedback in reinforcement learning" data-type="indexterm" id="id403"/><a contenteditable="false" data-primary="Q-learning as reinforcement learning" data-type="indexterm" id="id404"/><a contenteditable="false" data-primary="deep Q-learning as reinforcement learning" data-type="indexterm" id="id405"/><a contenteditable="false" data-primary="policy-gradient methods as reinforcement learning" data-type="indexterm" id="id406"/></p>&#13;
<dl>&#13;
<dt>Supervised learning</dt>&#13;
<dd>ML algorithms learn functional relationships from data, which are provided in pairs of inputs and desired outputs. This is the most prevalent form of ML used in research and  industry. Some examples of ML systems include linear regression, logistic regression, random forests, gradient-boosted machines, and deep learning.</dd>&#13;
<dt>Unsupervised learning</dt>&#13;
<dd>ML algorithms are only given input data and learn structural relationships in the data on their own. The K-means clustering algorithm is a commonly used data exploration algorithm used by investment analysts. Principal component analysis is a popular dimensionality reduction algorithm.</dd>&#13;
<dt>Reinforcement learning</dt>&#13;
<dd>An ML algorithm continually updates a policy or set of actions based on feedback from its environment with the goal of maximizing the present value of cumulative rewards. It’s different from supervised learning in that the feedback signal is not a desired output or class, but a reward or penalty. Examples of algorithms are Q-learning, deep Q-learning, and policy gradient methods. Reinforcement learning algorithms are being used in advanced trading applications.</dd>&#13;
</dl>&#13;
<p class="pagebreak-before">In the 21st century, financial data scientists are training ML algorithms to discover complex functional relationships using data from multiple financial and nonfinancial sources. The newly discovered relationships may augment or replace the insights of finance and investment executives. ML programs are able to detect patterns in very high-dimensional datasets, a feat that is difficult if not impossible for humans. They are also able to reduce the dimensions to enable visualizations for humans.</p>&#13;
<p>AI is used in all aspects of the finance and investment process—from idea generation to analysis, execution, portfolio, and risk management. The leading AI-powered systems in finance and investing today use some combination of expert systems and ML-based systems by leveraging the advantages of both types of approaches and expertise. <a contenteditable="false" data-primary="human intelligence (HI)" data-type="indexterm" id="id407"/>Furthermore, AI-powered financial systems continue to leverage human intelligence (HI) for research, development, and maintenance. Humans may also intervene in extreme market conditions, where it may be difficult for AI systems to learn from abrupt changes. So you can think of modern financial systems as a complex combination of SAI + ML + HI.<a contenteditable="false" data-primary="" data-startref="ch01-mlai" data-type="indexterm" id="id408"/><a contenteditable="false" data-primary="" data-startref="ch01-mlai2" data-type="indexterm" id="id409"/><a contenteditable="false" data-primary="" data-startref="ch01-mlai3" data-type="indexterm" id="id410"/><a contenteditable="false" data-primary="" data-startref="ch01-mlai4" data-type="indexterm" id="id411"/></p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Probabilistic ML" data-type="sect1"><div class="sect1" id="probabilistic_ml">&#13;
<h1>Probabilistic ML</h1>&#13;
<p>Probabilistic ML is the next-generation ML<a contenteditable="false" data-primary="probabilistic machine learning (PML)" data-secondary="about" data-type="indexterm" id="id412"/> framework and technology for AI-powered financial and investing systems. Leading technology companies clearly understand the limitations of conventional AI technologies and are developing their probabilistic versions to extend their applicability to more complex problems.</p>&#13;
<p>Google recently introduced<a contenteditable="false" data-primary="TensorFlow Probability (Google)" data-type="indexterm" id="id413"/><a contenteditable="false" data-primary="Google TensorFlow Probability" data-type="indexterm" id="id414"/><a contenteditable="false" data-primary="Pyro extending PyTorch platform" data-type="indexterm" id="id415"/> TensorFlow Probability to extend its established TensorFlow platform. Similarly, Facebook and Uber have introduced Pyro to extend their PyTorch platform. <a contenteditable="false" data-primary="PyMC library" data-secondary="about" data-type="indexterm" id="id416"/><a contenteditable="false" data-primary="probabilistic machine learning (PML)" data-secondary="about" data-tertiary="PyMC library" data-type="indexterm" id="id417"/><a contenteditable="false" data-primary="Stan library" data-secondary="about" data-type="indexterm" id="id418"/><a contenteditable="false" data-primary="probabilistic machine learning (PML)" data-secondary="about" data-tertiary="Stan library" data-type="indexterm" id="id419"/>Currently, the most popular open source probabilistic ML technologies are PyMC and Stan. PyMC is written in Python, and Stan is written in C++. <a contenteditable="false" data-primary="PyMC library" data-secondary="about" data-tertiary="used in this book" data-type="indexterm" id="id420"/><a contenteditable="false" data-primary="probabilistic financial models" data-secondary="PyMC library for assembling probabilistic linear ensembles" data-tertiary="about PyMC library" data-type="indexterm" id="id421"/><a contenteditable="false" data-primary="probabilistic linear ensembles" data-secondary="assembling with PyMC and ArviZ" data-tertiary="about PyMC library" data-type="indexterm" id="id422"/><a contenteditable="false" data-primary="probabilistic machine learning (PML)" data-secondary="PyMC library for assembling probabilistic linear ensembles" data-tertiary="about PyMC library" data-type="indexterm" id="id423"/>In <a data-type="xref" href="ch07.html#probabilistic_machine_learning_with_gen">Chapter 7</a>, we use the PyMC library because it’s part of the Python ecosystem.</p>&#13;
<p>Probabilistic ML as discussed in this book<a contenteditable="false" data-primary="probabilistic financial models" data-secondary="as generative models" data-secondary-sortas="generative models" data-type="indexterm" id="id424"/><a contenteditable="false" data-primary="generative AI" data-secondary="about probabilistic ML" data-type="indexterm" id="id425"/><a contenteditable="false" data-primary="AI" data-secondary="generative AI" data-tertiary="about" data-type="indexterm" id="id426"/><a contenteditable="false" data-primary="AI" data-secondary="generative AI" data-tertiary="about probabilistic ML" data-type="indexterm" id="id427"/><a contenteditable="false" data-primary="probabilistic machine learning (PML)" data-secondary="about" data-tertiary="generative AI" data-type="indexterm" id="id428"/><a contenteditable="false" data-primary="generative AI" data-secondary="about" data-type="indexterm" id="id429"/><a contenteditable="false" data-primary="probabilistic machine learning (PML)" data-secondary="characteristics of" data-type="indexterm" id="ch01-char"/> is based on a generative model. It is categorically different from the conventional ML in use today, such as linear, nonlinear, and deep learning systems, even though these other systems compute probabilistic scores. <a data-type="xref" href="#summary_of_major_characteristics_of_pro">Figure 1-6</a> shows the major differences between the two types of systems.</p>&#13;
<figure><div class="figure" id="summary_of_major_characteristics_of_pro">&#13;
<img alt="Summary of major characteristics of probabilistic ML systems" src="assets/pmlf_0106.png"/>&#13;
<h6><span class="label">Figure 1-6. </span>Summary of major characteristics of probabilistic ML systems</h6>&#13;
</div></figure>&#13;
<section data-pdf-bookmark="Probability Distributions" data-type="sect2"><div class="sect2" id="probability_distributions">&#13;
<h2>Probability Distributions</h2>&#13;
<p>Even though conventional ML systems<a contenteditable="false" data-primary="probabilistic machine learning (PML)" data-secondary="characteristics of" data-tertiary="probability distributions" data-type="indexterm" id="id430"/><a contenteditable="false" data-primary="probability distributions" data-secondary="probabilistic ML system characteristic" data-type="indexterm" id="id431"/> use calibrated probabilities, they only compute the most likely estimates and their associated probabilities as single-point values for inputs and outputs. This works well for domains, such as image recognition, where the data are plentiful and the signal-to-noise ratio is high. As was discussed and demonstrated in the previous sections, a point estimate is an inaccurate and misleading representation of financial reality, where uncertainty is very high. Furthermore, the calibrated probabilities may not be valid probabilities as the unconditional probability distribution of the data is almost never computed by MLE models. This can lead to poor quantification of uncertainty as will be explained in <a data-type="xref" href="ch06.html#the_dangers_of_conventional_ai_systems">Chapter 6</a>.</p>&#13;
<p>Probabilistic ML systems only deal in probability distributions in their computations of input parameters and model outputs. This is a realistic and honest representation of the uncertainty of a financial model’s variables. Furthermore, probability distributions leave the user considerable flexibility in picking the appropriate point estimate, if required, based on their business objectives.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Knowledge Integration" data-type="sect2"><div class="sect2" id="knowledge_integration">&#13;
<h2>Knowledge Integration</h2>&#13;
<p>Conventional ML systems do not<a contenteditable="false" data-primary="probabilistic machine learning (PML)" data-secondary="characteristics of" data-tertiary="knowledge integration" data-type="indexterm" id="id432"/><a contenteditable="false" data-primary="knowledge integration" data-secondary="probabilistic ML system characteristic" data-type="indexterm" id="id433"/> have a theoretically sound framework for incorporating prior knowledge, whether it is well-established scientific knowledge, institutional knowledge, or personal insights. Later in the book, we will see that conventional statisticians sneak in prior knowledge using ad hoc statistical methods, such as null hypothesis, statistical significance levels, and L1 and L2 regularizations, while pounding the table about letting only “the data speak for themselves.”</p>&#13;
<p>It is foolish not to integrate prior knowledge in our personal and professional lives. It is the antithesis of learning and vitiates against the nature of the scientific method. <a contenteditable="false" data-primary="knowledge integration" data-secondary="null hypothesis significance testing prohibiting" data-type="indexterm" id="id434"/><a contenteditable="false" data-primary="null hypothesis significance testing (NHST)" data-secondary="knowledge integration prohibited by" data-type="indexterm" id="id435"/><a contenteditable="false" data-primary="NHST" data-see="null hypothesis significance testing" data-type="indexterm" id="id436"/><a contenteditable="false" data-primary="prior knowledge" data-see="knowledge integration" data-type="indexterm" id="id437"/><a contenteditable="false" data-primary="objectivity per NHST" data-type="indexterm" id="id438"/><a contenteditable="false" data-primary="null hypothesis significance testing (NHST)" data-secondary="prosecutor’s fallacy" data-type="indexterm" id="id439"/>Yet this is the basis of null hypothesis significance testing (NHST), the prevailing statistical methodology in academia, research, and industry since the 1960s. NHST prohibits the inclusion of prior knowledge in experiments based on the bogus claim that objectivity demands that we only let the data speak for themselves. <a contenteditable="false" data-primary="prosecutor’s fallacy" data-secondary="null hypothesis significance testing committing" data-type="indexterm" id="id440"/>By following this specious claim, NHST ends up committing the prosecutor’s fallacy, as we will show in <a data-type="xref" href="ch04.html#the_dangers_of_conventional_statistical">Chapter 4</a>.</p>&#13;
<p>NHST’s definition of objectivity would require us to touch fire everywhere and every time we find it because we cannot incorporate our prior knowledge of what it felt like in similar situations in the past. That is the definition of foolishness, not objectivity. In <a data-type="xref" href="ch04.html#the_dangers_of_conventional_statistical">Chapter 4</a>, we will discuss how and why several metastudies have shown that the majority of published medical research findings based on NHST are false. Yes, you read that right, and it has been an open secret since a seminal paper published in 2005.<sup><a data-type="noteref" href="ch01.html#ch01fn13" id="ch01fn13-marker">13</a></sup></p>&#13;
<p>Fortunately, in this book we don’t have to waste much ink or pixels on this specious argument about objectivity or the proliferation of junk science produced by NHST. Probabilistic ML systems provide a mathematically rigorous framework for incorporating prior knowledge and updating it appropriately with learnings from new information. Representation of prior knowledge is done explicitly so that anyone can challenge it or change it. This is the essence of learning and the basis of the scientific method.</p>&#13;
<p>One of the important implications<a contenteditable="false" data-primary="no free lunch (NFL) theorems" data-secondary="prior knowledge necessary for optimization" data-type="indexterm" id="id441"/><a contenteditable="false" data-primary="knowledge integration" data-secondary="no free lunch requiring prior knowledge" data-type="indexterm" id="id442"/> of the no free lunch (NFL) theorems is that prior domain knowledge is necessary to optimize an algorithm’s performance for a specific problem domain. If we don’t apply our prior domain knowledge, the performance of our unbiased algorithm will be no better than random guessing when averaged across all problem domains. There is no such thing as a free lunch, especially in finance and investing. We will discuss the NFL theorems in detail in the next chapter.</p>&#13;
<p>It is common knowledge that integration of accumulated institutional knowledge into a company’s organization, process, and systems leads to a sustainable competitive advantage in business. Moreover, personal insights and experience with markets can lead to “alpha,” or the generation of exceptional returns in trading and investing, for the fund manager who arrives at a subjectively different viewpoint from the rest of the crowd. That’s how Warren Buffet, one of the greatest investors of all time, made his vast fortune. Markets mock dogmatic and unrealistic definitions of objectivity with lost profits and eventually with financial ruin.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Parameter Inference" data-type="sect2"><div class="sect2" id="parameter_inference">&#13;
<h2>Parameter Inference</h2>&#13;
<p>Almost all conventional ML systems<a contenteditable="false" data-primary="probabilistic machine learning (PML)" data-secondary="characteristics of" data-tertiary="parameter inference" data-type="indexterm" id="id443"/><a contenteditable="false" data-primary="parameters of a model" data-secondary="parameter inference by ML systems" data-type="indexterm" id="id444"/> use equally conventional statistical methodologies, such as p-values and confidence intervals, to estimate the uncertainty of a <span class="keep-together">model’s</span> parameters. As will be explained in <a data-type="xref" href="ch04.html#the_dangers_of_conventional_statistical">Chapter 4</a>, these are deeply flawed—almost scandalous—statistical methodologies that plague the social sciences, including finance and economics. These methodologies adhere to a pious pretense to objectivity and to implicit and unrealistic assumptions, obfuscated by inscrutable statistical jargon, in order to generate solutions that are analytically tractable for a small set of scenarios.</p>&#13;
<p>Probabilistic ML is based on a simple and intuitive definition of probability as logic, and the rigorous calculus of probability theory in general and the inverse probability rule in particular. In the next chapter, we show how<a contenteditable="false" data-primary="“Bayes’s theorem”" data-primary-sortas="Bayes’s theorem" data-secondary="inverse probability rule as proper name" data-type="indexterm" id="id445"/><a contenteditable="false" data-primary="inverse probability rule" data-secondary="Bayes’s theorem misnomer" data-type="indexterm" id="id446"/> the inverse probability rule—mistakenly and mortifyingly known as Bayes’s theorem—is a trivial reformulation of the product rule. It is a logical tautology that is embarrassingly easy to prove. It doesn’t deserve to be called a theorem, given how excruciatingly difficult it is to derive most mathematical theorems.</p>&#13;
<p>However, because of the normalizing constant in the inversion formula, it was previously impossible to invert probabilities analytically, except for simple problems. With the recent advancement of state-of-the-art numerical algorithms, such as Hamiltonian Monte Carlo and automatic differentiation variational inference, probabilistic ML systems are now able to invert probabilities to compute model parameter estimates from in-sample data for almost any real-world problem. More importantly, they are able to quantify parameter uncertainties with mathematically sound credible intervals for any level of confidence. This enables inverse uncertainty propagation.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Generative Ensembles" data-type="sect2"><div class="sect2" id="generative_ensembles">&#13;
<h2>Generative Ensembles</h2>&#13;
<p>Almost all conventional ML systems<a contenteditable="false" data-primary="probabilistic machine learning (PML)" data-secondary="characteristics of" data-tertiary="generative ensembles" data-type="indexterm" id="id447"/><a contenteditable="false" data-primary="generative AI" data-secondary="about probabilistic ML" data-tertiary="generative characteristic" data-type="indexterm" id="id448"/><a contenteditable="false" data-primary="AI" data-secondary="generative AI" data-tertiary="characteristic of probabilistic ML" data-type="indexterm" id="id449"/><a contenteditable="false" data-primary="generative AI" data-secondary="generative ensembles" data-tertiary="about" data-type="indexterm" id="id450"/><a contenteditable="false" data-primary="AI" data-secondary="generative AI" data-tertiary="about generative ensembles" data-type="indexterm" id="id451"/><a contenteditable="false" data-primary="probabilistic machine learning (PML)" data-secondary="generative ensembles" data-tertiary="about" data-type="indexterm" id="id452"/> are based on discriminative models. This type of statistical model only learns a decision boundary from the in-sample data, but not how the data are distributed statistically. Therefore, conventional discriminative ML systems cannot simulate new data and quantify total output uncertainty.</p>&#13;
<p>Probabilistic ML systems are based on generative models. This type of statistical model learns the statistical structure of the data distribution and so can easily and seamlessly simulate new data, including generating data that might be missing or corrupted. Furthermore, the distribution of parameters generates an ensemble of models. <a contenteditable="false" data-primary="probability distributions" data-secondary="probabilistic ML system characteristic" data-tertiary="learning from in-sample data" data-type="indexterm" id="id453"/>Most importantly, these systems are able to simulate two-dimensional output uncertainty based on data variability and input parameter uncertainty, the probability distributions of which they have learned previously from in-sample data. This seamlessly enables forward uncertainty propagation.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Uncertainty Awareness" data-type="sect2"><div class="sect2" id="uncertainty_awareness">&#13;
<h2>Uncertainty Awareness</h2>&#13;
<p>When computing probabilities, a conventional<a contenteditable="false" data-primary="probabilistic machine learning (PML)" data-secondary="characteristics of" data-tertiary="uncertainty awareness" data-type="indexterm" id="id454"/><a contenteditable="false" data-primary="uncertainty quantification and analysis" data-secondary="probability for" data-tertiary="probabilistic AI for" data-type="indexterm" id="id455"/><a contenteditable="false" data-primary="maximum likelihood estimation (MLE)" data-secondary="about" data-type="indexterm" id="id456"/> ML system uses the maximum likelihood estimation (MLE) method. This technique optimizes the parameters of an assumed probability distribution such that the in-sample data are most likely to be observed, given the point estimates for the model’s parameters. As we will see later in the book, MLE leads to wrong inferences and predictions when data are sparse, a common occurrence in finance and investing, especially when a market regime changes abruptly.</p>&#13;
<p>What makes it worse is that these MLE-based ML systems attach horrifyingly high probabilities to these wrong estimates. We are automating the overconfidence of powerful systems that lack basic common sense. This makes conventional ML systems potentially risky and dangerous, especially when used in mission-critical operations by personnel who either don’t understand the fundamentals of these ML systems or have blind faith in them.</p>&#13;
<p>Probabilistic ML systems do not rely<a contenteditable="false" data-primary="probability distributions" data-secondary="probabilistic ML system characteristic" data-tertiary="parameter’s entire probability distribution" data-type="indexterm" id="id457"/> on a single-point estimate, no matter how likely or optimal, but a weighted average of every possible estimate of a parameter’s entire probability distribution. Moreover, the uncertainty of these estimates increases appropriately when systems deal with classes of data they have never seen before in training, or are extrapolating beyond known data ranges. Unlike MLE-based systems, probabilistic ML systems know what they don’t know. This keeps the quantification of uncertainty honest and prevents overconfidence in estimates and predictions.<a contenteditable="false" data-primary="" data-startref="ch01-char" data-type="indexterm" id="id458"/></p>&#13;
</div></section>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="conclusions-id00001">&#13;
<h1>Summary</h1>&#13;
<p>Economics is not a precise predictive science like physics. Not even close. So let’s not pretend otherwise and treat academic theories and models of economics as if they were models of quantum physics, the obfuscating math notwithstanding.</p>&#13;
<p>All financial models, whether based on academic theories or ML strategies, are at the mercy of the trifecta of modeling errors. While this trio of errors can be mitigated with appropriate tools, such as probabilistic ML systems, it cannot be eliminated. There will always be asymmetry of information and cognitive biases. Models of asset values and risks will change over time due to the dynamic nature of capitalism, human behavior, and technological innovation.</p>&#13;
<p>Probabilistic ML technologies are based on a simple and intuitive definition of probability as logic and the rigorous calculus of probability theory. They enable the explicit and systematic integration of prior knowledge that is updated continually with new learnings.</p>&#13;
<p class="pagebreak-before">These systems treat uncertainties and errors in financial and investing systems as features, not bugs. They quantify uncertainty generated from inexact inputs, parameters and outputs of finance, and investing systems as probability distributions, not point estimates. This makes for realistic financial inferences and predictions that are useful for decision making and risk management. Most importantly, these systems become capable of forewarning us when their inferences and predictions are no longer useful in the current market environment.</p>&#13;
<p>There are several reasons why probabilistic ML is the next-generation ML framework and technology for AI-powered financial and investing systems. Its probabilistic framework moves away from flawed statistical methodologies (NHST, p-values, confidence intervals) and the restrictive conventional view of probability as a limiting frequency. It moves us toward an intuitive view of probability as logic and a mathematically rigorous statistical framework that quantifies uncertainty holistically and successfully. Therefore, it enables us to move away from the wrong, idealistic, analytical models of the past toward less wrong, more realistic, numerical models of the future.</p>&#13;
<p>The algorithms used in probabilistic programming are among the most sophisticated algorithms in the AI world, which we will delve into in the second half of the book. In the next three chapters, we will take a deeper dive into why it is very risky to deploy your capital using conventional ML systems, because they are based on orthodox probabilistic and statistical methods that are scandalously flawed.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="References" data-type="sect1"><div class="sect1" id="references-id00012">&#13;
<h1>References</h1>&#13;
<p>Géron, Aurélien. “The Machine Learning Landscape.” In <em>Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow</em>, 1–34. 3rd ed. O’Reilly Media, 2022.</p>&#13;
<p>Hayek, Friedrich von. “Banquet Speech.” Speech given at the Nobel Banquet, Stockholm, Sweden, December 10, 1974. Nobel Prize Outreach AB, 2023, <a href="https://www.nobelprize.org/prizes/economic-sciences/1974/hayek/speech/"><em class="hyperlink">https://www.nobelprize.org/prizes/economic-sciences/1974/hayek/speech/</em></a>.</p>&#13;
<p>Ioannidis, John P. A. “Why Most Published Research Findings Are False.” <em>PLOS Medicine</em> 2, no. 8 (2005): e124. <a href="https://doi.org/10.1371/journal.pmed.0020124"><em class="hyperlink">https://doi.org/10.1371/journal.pmed.0020124</em></a>.</p>&#13;
<p>Offer, Avner, and Gabriel Söderberg. <em>The Nobel Factor: The Prize in Economics, Social Democracy, and the Market Turn</em>. Princeton, NJ: Princeton University Press, 2016.</p>&#13;
<p>Orrell, David, and Paul Wilmott. <em>The Money Formula: Dodgy Finance, Pseudo Science, and How Mathematicians Took Over the Markets</em>. West Sussex, UK: Wiley, 2017.</p>&#13;
<p class="pagebreak-before">Sekerke, Matt. <em>Bayesian Risk Management</em>. Wiley, 2015.</p>&#13;
<p>Simons, Katerina. “Model Error.” <em>New England Economic Review</em> (November 1997): 17–28.</p>&#13;
<p>Thompson, J. R., L.S. Baggett, W. C. Wojciechowski, and E. E. Williams. “Nobels For Nonsense.” <em>Journal of Post Keynesian Economics</em> 29, no. 1 (Autumn 2006): 3–18.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Further Reading" data-type="sect1"><div class="sect1" id="further_reading-id00005">&#13;
<h1>Further Reading</h1>&#13;
<p>Jaynes, E. T. <em>Probability Theory: The Logic of Science</em>. New York: Cambridge University Press, 2003.</p>&#13;
<p>Lopez de Prado, Marcos. <em>Advances in Financial Machine Learning</em>. Hoboken, New Jersey: Wiley, 2018.</p>&#13;
<p>Taleb, Nassim Nicholas. <em>Fooled by Randomness: The Hidden Role of Chance in Life and in the Markets</em>. New York: Random House Trade, 2005.</p>&#13;
</div></section>&#13;
<div data-type="footnotes"><p data-type="footnote" id="ch01fn1"><sup><a href="ch01.html#ch01fn1-marker">1</a></sup> David Orrell and Paul Wilmott, “Going Random,” in <em>The Money Formula: Dodgy Finance, Pseudo Science, and How Mathematicians Took Over the Markets</em> (West Sussex, UK: Wiley, 2017).</p><p data-type="footnote" id="ch01fn2"><sup><a href="ch01.html#ch01fn2-marker">2</a></sup> Avner Offer and G. Söderberg, <em>The Nobel Factor: The Prize in Economics, Social Democracy, and the Market Turn</em> (Princeton, NJ: Princeton University Press, 2016).</p><p data-type="footnote" id="ch01fn3"><sup><a href="ch01.html#ch01fn3-marker">3</a></sup> Friedrich von Hayek, “Banquet Speech,” Nobel Prize Outreach AB, 2023, <a href="https://www.nobelprize.org/prizes/economic-sciences/1974/hayek/speech/"><em class="hyperlink">https://www.nobelprize.org/prizes/economic-sciences/1974/hayek/speech</em></a>.</p><p data-type="footnote" id="ch01fn4"><sup><a href="ch01.html#ch01fn4-marker">4</a></sup> David Orrell and Paul Wilmott, “Early Models,” in <em>The Money Formula: Dodgy Finance, Pseudo Science, and How Mathematicians Took Over the Markets</em> (West Sussex, UK: Wiley, 2017).</p><p data-type="footnote" id="ch01fn5"><sup><a href="ch01.html#ch01fn5-marker">5</a></sup> J. R. Thompson, L.S. Baggett, W. C. Wojciechowski, and E. E. Williams, “Nobels For Nonsense,” <em>Journal of Post Keynesian Economics</em> 29, no. 1 (Autumn 2006): 3–18.</p><p data-type="footnote" id="ch01fn6"><sup><a href="ch01.html#ch01fn6-marker">6</a></sup> Orrell and Wilmott, <em>The Money Formula</em>.</p><p data-type="footnote" id="ch01fn7"><sup><a href="ch01.html#ch01fn7-marker">7</a></sup> Adapted from an image from Wikimedia Commons.</p><p data-type="footnote" id="ch01fn8"><sup><a href="ch01.html#ch01fn8-marker">8</a></sup> Orrell and Wilmott, <em>The Money Formula</em>; M. Sekerke, <em>Bayesian Risk Management</em> (Hoboken, NJ: Wiley, 2015); J. R. Thompson, L. S. Baggett, W. C. Wojciechowski, and E. E. Williams, “Nobels for Nonsense,” <em>Journal of Post Keynesian Economics</em> 29, no. 1 (Autumn 2006): 3–18; and Katerina Simons, “Model Error,” <em>New England Economic Review</em> (November 1997): 17–28.</p><p data-type="footnote" id="ch01fn9"><sup><a href="ch01.html#ch01fn9-marker">9</a></sup> Orrell and Wilmott, <em>The Money Formula</em>; Sekerke, <em>Bayesian Risk Management</em>; and Thompson, Baggett, Wojciechowski, and Williams, “Nobels for Nonsense.”</p><p data-type="footnote" id="ch01fn10"><sup><a href="ch01.html#ch01fn10-marker">10</a></sup> Orrell and Wilmott, <em>The Money Formula</em>; Sekerke, <em>Bayesian Risk Management</em>; and Thompson, Baggett, Wojciechowski, and Williams, “Nobels for Nonsense.”</p><p data-type="footnote" id="ch01fn11"><sup><a href="ch01.html#ch01fn11-marker">11</a></sup> Sekerke, <em>Bayesian Risk Management</em>.</p><p data-type="footnote" id="ch01fn12"><sup><a href="ch01.html#ch01fn12-marker">12</a></sup> Aurélien Géron, “The Machine Learning Landscape,” in <em>Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow</em>, 3rd edition (O’Reilly Media, 2022), 1–34.</p><p data-type="footnote" id="ch01fn13"><sup><a href="ch01.html#ch01fn13-marker">13</a></sup> The paper is John P. A. Ioannidis, “Why Most Published Research Findings Are False,” <em>PLOS Medicine</em> 2, no. 8 (2005): e124, <a href="https://doi.org/10.1371/journal.pmed.0020124"><em class="hyperlink">https://doi.org/10.1371/journal.pmed.0020124</em></a>. See also Julia Belluz, “This Is Why You Shouldn’t Believe That Exciting New Medical Study,” <em>Vox</em>, February 27, 2017, <a href="https://www.vox.com/2015/3/23/8264355/research-study-hype"><em class="hyperlink">https://www.vox.com/2015/3/23/8264355/research-study-hype</em></a>.</p></div></div></section></body></html>