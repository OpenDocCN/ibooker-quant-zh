<html><head></head><body><section data-pdf-bookmark="Chapter 2. Essential Probabilistic Methods for Deep Learning" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch02">&#13;
<h1><span class="label">Chapter 2. </span>Essential Probabilistic Methods <span class="keep-together">for Deep Learning</span></h1>&#13;
&#13;
&#13;
<p><a contenteditable="false" data-primary="deep learning (generally)" data-secondary="essential probabilistic methods for" data-type="indexterm" id="Chapter_2.html0"/><a contenteditable="false" data-primary="probabilistic methods" data-type="indexterm" id="Chapter_2.html1"/>The rise and accessibility of technology have made it possible for everyone to deploy machine learning and deep learning algorithms for data analysis and optimization. But unfortunately, a large number of users do not understand the basics of the different learning models. This makes machine learning nothing short of a mystery box to them, which is a recipe for disaster.</p>&#13;
&#13;
<p>Understanding fundamental concepts in probability, statistics, and math is essential for understanding and mastering data as well as for creating models that seek to interpret and forecast data. This chapter presents the basics of probability that are either directly or indirectly related to the algorithms. Note that you are unlikely to use these probability concepts in your everyday life, but it’s important to know where some algorithms draw their assumptions from.</p>&#13;
&#13;
<section data-pdf-bookmark="A Primer on Probability" data-type="sect1"><div class="sect1" id="id7">&#13;
<h1>A Primer on Probability</h1>&#13;
&#13;
<p><a contenteditable="false" data-primary="probabilistic methods" data-secondary="probability basics" data-type="indexterm" id="Chapter_2.html2"/><a contenteditable="false" data-primary="probability" data-type="indexterm" id="Chapter_2.html3"/><em>Probability </em>is all about describing random variables and random events. The world is filled with randomness, and the best way to find your way through chaos is to try to explain it using probabilistic methods. Granted, the phrase <em>explain chaos</em> may be an oxymoron, as chaos cannot really be explained, but we humans cannot relinquish control over uncertain events. This is why we have developed tools to make sense out of our scary world.</p>&#13;
&#13;
<p>You may wonder what is the use of understanding the basics of probability when trying to develop machine learning algorithms for financial trading. This is a reasonable question, and you must know that the foundations of a discipline do not necessarily resemble it.</p>&#13;
&#13;
<p>For example, to become a pilot you have to study aerodynamics, which is filled with technical concepts that do not resemble the final skill. This is similar to what is being done in this chapter; by studying probabilistic essentials, you give your brain a proper warm-up for what’s to come.</p>&#13;
&#13;
<p>Knowing the utility of what you are learning should give you a motivation boost. Here are some key probability topics that are important for machine learning:</p>&#13;
&#13;
<dl>&#13;
	<dt>Probability distribution functions</dt>&#13;
	<dd>&#13;
	<p><a contenteditable="false" data-primary="probability distribution functions" data-secondary="defined" data-type="indexterm" id="id320"/>The possibility of seeing various outcomes of a random variable is described by a <em>probability distribution</em>. For many machine learning techniques, it is essential to comprehend the features and attributes of typical probability distributions. Probability distribution functions also describe different types of time series data, which in turn helps in choosing the right algorithm. For simplicity and coherence, this topic is discussed in <a data-type="xref" href="ch03.html#ch03">Chapter 3</a>.</p>&#13;
	</dd>&#13;
</dl>&#13;
&#13;
<dl>&#13;
	<dt>Hypothesis testing</dt>&#13;
	<dd>&#13;
	<p><a contenteditable="false" data-primary="hypothesis testing" data-type="indexterm" id="id321"/><em>Hypothesis testing</em> is used to establish whether a population-based assertion is more likely to be correct or incorrect based on a sample of data. Stationarity tests use hypothesis testing and are discussed in <a data-type="xref" href="ch03.html#ch03">Chapter 3</a>.</p>&#13;
	</dd>&#13;
</dl>&#13;
&#13;
<dl>&#13;
	<dt>Decision trees</dt>&#13;
	<dd>&#13;
	<p><a contenteditable="false" data-primary="decision trees" data-secondary="defined" data-type="indexterm" id="id322"/><em>Decision trees</em> are a type of machine learning algorithm that borrows from probabilistic concepts such as conditional probability, a concept covered in this chapter. For more detail on decision trees, see <a data-type="xref" href="ch07.html#ch07">Chapter 7</a>.</p>&#13;
	</dd>&#13;
</dl>&#13;
&#13;
<dl>&#13;
	<dt>Information theory</dt>&#13;
	<dd>&#13;
	<p><a contenteditable="false" data-primary="information theory" data-type="indexterm" id="id323"/><em>Information theory </em>is the complex study of how information is quantified, stored, and transmitted. It is incorporated into numerous machine learning techniques, including decision trees. It is also used in a type of nonlinear correlation measure called the maximal information coefficient, which is discussed in <a data-type="xref" href="ch03.html#ch03">Chapter 3</a>.</p>&#13;
	</dd>&#13;
</dl>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Introduction to Probabilistic Concepts" data-type="sect1"><div class="sect1" id="id8">&#13;
<h1>Introduction to Probabilistic Concepts</h1>&#13;
&#13;
<p><a contenteditable="false" data-primary="probabilistic methods" data-secondary="probabilistic concepts" data-type="indexterm" id="Chapter_2.html4"/>The most basic piece of probabilistic information is a <a contenteditable="false" data-primary="random variable, defined" data-type="indexterm" id="id324"/><em>random variable,</em> which is an uncertain number or outcome. Random variables are used to model events that are considered uncertain, such as the future return of a currency pair.</p>&#13;
&#13;
<p><a contenteditable="false" data-primary="continuous random variable" data-type="indexterm" id="id325"/><a contenteditable="false" data-primary="discrete random variable" data-type="indexterm" id="id326"/>A random variable is either discrete or continuous. A <em>discrete random variable</em> has a finite set of values, while a <em>continuous random variable</em> has values within a certain interval. Consider the following examples to clarify things:</p>&#13;
&#13;
<ul>&#13;
	<li class="pagebreak-before less_space">An example of a discrete random variable would be the result of rolling a die. The outcomes are limited by the following set: {1, 2, 3, 4, 5, 6}.</li>&#13;
	<li>An example of a continuous random variable would be the daily price returns of EURUSD (the exchange rate of one euro expressed in US dollars).</li>&#13;
</ul>&#13;
&#13;
<p class="fix_tracking"><a contenteditable="false" data-primary="probability distribution functions" data-secondary="random variables and" data-type="indexterm" id="id327"/>Random variables are described by <em>probability distributions,</em> which are functions that give the probability of every possible value of these random variables. Generally, a histogram is used to show the probability. Histogram plotting is discussed <span class="keep-together">in <a data-type="xref" href="ch03.html#ch03">Chapter 3</a>.</span></p>&#13;
&#13;
<p>At any moment, the probability that a certain event will unfold is between 0 and 1. This means that probability is assigned to random variables on a scale between 0 and 1 such that a probability of 0 represents zero chance of occurrence and a probability of 1 represents a certainty of occurrence.</p>&#13;
&#13;
<p>You can also think of this in percentage terms, which range from 0% to 100%. Values within the two numbers are valid, which means that you can have a 0.5133 (51.33%) probability of a certain event occurring. Consider rolling a die that has six sides. What is the probability of getting a 3 knowing that the die is not manipulated in <span class="keep-together">any way?</span></p>&#13;
&#13;
<p>As the die has six sides, there are six equal probabilities for every outcome, which means that for any outcome, the probability is found as follows:</p>&#13;
&#13;
<div data-type="equation"><p><math alttext="upper P left-parenthesis x right-parenthesis equals one-sixth equals 0.167">&#13;
  <mrow>&#13;
    <mi>P</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>x</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <mstyle displaystyle="false" scriptlevel="0">&#13;
      <mfrac><mn>1</mn> <mn>6</mn></mfrac>&#13;
    </mstyle>&#13;
    <mo>=</mo>&#13;
    <mn>0</mn>&#13;
    <mo lspace="0%" rspace="0%">.</mo>&#13;
    <mn>167</mn>&#13;
  </mrow>&#13;
</math></p></div>&#13;
&#13;
<p>with <em>P(x)</em> designating the probability of event <em>x</em>. This gives the answer to the question:</p>&#13;
&#13;
<div data-type="equation"><p><math alttext="upper P left-parenthesis 3 right-parenthesis equals one-sixth equals 0.167">&#13;
  <mrow>&#13;
    <mi>P</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mn>3</mn>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <mstyle displaystyle="false" scriptlevel="0">&#13;
      <mfrac><mn>1</mn> <mn>6</mn></mfrac>&#13;
    </mstyle>&#13;
    <mo>=</mo>&#13;
    <mn>0</mn>&#13;
    <mo lspace="0%" rspace="0%">.</mo>&#13;
    <mn>167</mn>&#13;
  </mrow>&#13;
</math></p></div>&#13;
&#13;
<p>When a die is rolled, there can only be one result. It cannot give a 3 and a 4 simultaneously, since one side has to dominate the other. <a contenteditable="false" data-primary="mutual exclusivity" data-type="indexterm" id="id328"/>This is the concept of <em>mutual exclusivity</em>. Mutually exclusive events (such as getting a 3 or getting a 4 in a die roll) eventually sum to 1.</p>&#13;
&#13;
<p>Take a look at the following example:</p>&#13;
&#13;
<div data-type="equation"><p><math alttext="upper P left-parenthesis 1 right-parenthesis equals one-sixth equals 0.167">&#13;
  <mrow>&#13;
    <mi>P</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mn>1</mn>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <mstyle displaystyle="false" scriptlevel="0">&#13;
      <mfrac><mn>1</mn> <mn>6</mn></mfrac>&#13;
    </mstyle>&#13;
    <mo>=</mo>&#13;
    <mn>0</mn>&#13;
    <mo lspace="0%" rspace="0%">.</mo>&#13;
    <mn>167</mn>&#13;
  </mrow>&#13;
</math></p>&#13;
&#13;
<p><math alttext="upper P left-parenthesis 2 right-parenthesis equals one-sixth equals 0.167">&#13;
  <mrow>&#13;
    <mi>P</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mn>2</mn>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <mstyle displaystyle="false" scriptlevel="0">&#13;
      <mfrac><mn>1</mn> <mn>6</mn></mfrac>&#13;
    </mstyle>&#13;
    <mo>=</mo>&#13;
    <mn>0</mn>&#13;
    <mo lspace="0%" rspace="0%">.</mo>&#13;
    <mn>167</mn>&#13;
  </mrow>&#13;
</math></p>&#13;
&#13;
<p><math alttext="upper P left-parenthesis 3 right-parenthesis equals one-sixth equals 0.167">&#13;
  <mrow>&#13;
    <mi>P</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mn>3</mn>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <mstyle displaystyle="false" scriptlevel="0">&#13;
      <mfrac><mn>1</mn> <mn>6</mn></mfrac>&#13;
    </mstyle>&#13;
    <mo>=</mo>&#13;
    <mn>0</mn>&#13;
    <mo lspace="0%" rspace="0%">.</mo>&#13;
    <mn>167</mn>&#13;
  </mrow>&#13;
</math></p>&#13;
&#13;
<p><math alttext="upper P left-parenthesis 4 right-parenthesis equals one-sixth equals 0.167">&#13;
  <mrow>&#13;
    <mi>P</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mn>4</mn>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <mstyle displaystyle="false" scriptlevel="0">&#13;
      <mfrac><mn>1</mn> <mn>6</mn></mfrac>&#13;
    </mstyle>&#13;
    <mo>=</mo>&#13;
    <mn>0</mn>&#13;
    <mo lspace="0%" rspace="0%">.</mo>&#13;
    <mn>167</mn>&#13;
  </mrow>&#13;
</math></p>&#13;
&#13;
<p><math alttext="upper P left-parenthesis 5 right-parenthesis equals one-sixth equals 0.167">&#13;
  <mrow>&#13;
    <mi>P</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mn>5</mn>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <mstyle displaystyle="false" scriptlevel="0">&#13;
      <mfrac><mn>1</mn> <mn>6</mn></mfrac>&#13;
    </mstyle>&#13;
    <mo>=</mo>&#13;
    <mn>0</mn>&#13;
    <mo lspace="0%" rspace="0%">.</mo>&#13;
    <mn>167</mn>&#13;
  </mrow>&#13;
</math></p>&#13;
&#13;
<p><math alttext="upper P left-parenthesis 6 right-parenthesis equals one-sixth equals 0.167">&#13;
  <mrow>&#13;
    <mi>P</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mn>6</mn>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <mstyle displaystyle="false" scriptlevel="0">&#13;
      <mfrac><mn>1</mn> <mn>6</mn></mfrac>&#13;
    </mstyle>&#13;
    <mo>=</mo>&#13;
    <mn>0</mn>&#13;
    <mo lspace="0%" rspace="0%">.</mo>&#13;
    <mn>167</mn>&#13;
  </mrow>&#13;
</math></p></div>&#13;
&#13;
<p>Summing all these mutually exclusive events gives 1, which means that the sum of the possible probabilities in a six-sided die is as follows:</p>&#13;
&#13;
<div data-type="equation"><p><math alttext="upper P left-parenthesis 1 right-parenthesis plus upper P left-parenthesis 2 right-parenthesis plus upper P left-parenthesis 3 right-parenthesis plus upper P left-parenthesis 4 right-parenthesis plus upper P left-parenthesis 5 right-parenthesis plus upper P left-parenthesis 6 right-parenthesis equals 1">&#13;
  <mrow>&#13;
    <mi>P</mi>&#13;
    <mo>(</mo>&#13;
    <mn>1</mn>&#13;
    <mo>)</mo>&#13;
    <mo>+</mo>&#13;
    <mi>P</mi>&#13;
    <mo>(</mo>&#13;
    <mn>2</mn>&#13;
    <mo>)</mo>&#13;
    <mo>+</mo>&#13;
    <mi>P</mi>&#13;
    <mo>(</mo>&#13;
    <mn>3</mn>&#13;
    <mo>)</mo>&#13;
    <mo>+</mo>&#13;
    <mi>P</mi>&#13;
    <mo>(</mo>&#13;
    <mn>4</mn>&#13;
    <mo>)</mo>&#13;
    <mo>+</mo>&#13;
    <mi>P</mi>&#13;
    <mo>(</mo>&#13;
    <mn>5</mn>&#13;
    <mo>)</mo>&#13;
    <mo>+</mo>&#13;
    <mi>P</mi>&#13;
    <mo>(</mo>&#13;
    <mn>6</mn>&#13;
    <mo>)</mo>&#13;
    <mo>=</mo>&#13;
    <mn>1</mn>&#13;
  </mrow>&#13;
</math></p></div>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Stating that a random variable has a 0.8 probability of occurring is the same as stating that the same variable has a 0.2 probability of not occurring.</p>&#13;
</div>&#13;
&#13;
<p>Probability measures can be conditional or unconditional. <a contenteditable="false" data-primary="conditional probability" data-type="indexterm" id="id329"/>A <em>conditional probability</em> is when the occurrence of an event impacts the probability that another event occurs. For example, the probability of a sovereign interest rate hike given positive employment data is an example of a conditional probability. The probability of event A given the occurrence of event B is denoted by the mathematical notation <em>P(A|B)</em>.</p>&#13;
&#13;
<p><a contenteditable="false" data-primary="unconditional probability" data-type="indexterm" id="id330"/>In contrast, <em>unconditional probability</em> is not dependent on other events. Taking the example of conditional probability, you can formulate an unconditional probability calculation that measures the probability of an interest rate hike regardless of other economic events.</p>&#13;
&#13;
<p>Probabilities have specific addition and multiplication rules with their own interpretations. Let’s take a look at the formulas before seeing an example. <a contenteditable="false" data-primary="joint probability" data-type="indexterm" id="id331"/>The <em>joint probability</em> of the realization of two events is the probability that they will both occur. It is calculated using the following formula:</p>&#13;
&#13;
<div data-type="equation"><p><math alttext="upper P left-parenthesis upper A upper B right-parenthesis equals upper P left-parenthesis upper A vertical-bar upper B right-parenthesis times upper P left-parenthesis upper B right-parenthesis">&#13;
  <mrow>&#13;
    <mi>P</mi>&#13;
    <mo>(</mo>&#13;
    <mi>A</mi>&#13;
    <mi>B</mi>&#13;
    <mo>)</mo>&#13;
    <mo>=</mo>&#13;
    <mi>P</mi>&#13;
    <mo>(</mo>&#13;
    <mi>A</mi>&#13;
    <mo>|</mo>&#13;
    <mi>B</mi>&#13;
    <mo>)</mo>&#13;
    <mo>×</mo>&#13;
    <mi>P</mi>&#13;
    <mo>(</mo>&#13;
    <mi>B</mi>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math></p></div>&#13;
&#13;
<p>That formula says the probability of occurrence for both A and B is the probability that A occurs given B occurs multiplied by the probability that B occurs. Therefore, the right side of the equation multiplies a conditional probability by an unconditional probability.</p>&#13;
&#13;
<p><a contenteditable="false" data-primary="addition rule" data-type="indexterm" id="id332"/>The <em>addition rule</em> is used to determine the probability that at least one of the two outcomes will occur. This works in two ways: one deals with mutually exclusive events, and the other deals with events that are not mutually exclusive.</p>&#13;
&#13;
<p class="fix_tracking">If the events are not mutually exclusive, then to avoid double counting, the formula is:</p>&#13;
&#13;
<div data-type="equation"><p><math alttext="upper P left-parenthesis upper A o r upper B right-parenthesis equals upper P left-parenthesis upper A right-parenthesis plus upper P left-parenthesis upper B right-parenthesis minus upper P left-parenthesis upper A upper B right-parenthesis">&#13;
  <mrow>&#13;
    <mi>P</mi>&#13;
    <mo>(</mo>&#13;
    <mi>A</mi>&#13;
    <mspace width="0.222222em"/>&#13;
    <mi>o</mi>&#13;
    <mi>r</mi>&#13;
    <mspace width="0.222222em"/>&#13;
    <mi>B</mi>&#13;
    <mo>)</mo>&#13;
    <mo>=</mo>&#13;
    <mi>P</mi>&#13;
    <mo>(</mo>&#13;
    <mi>A</mi>&#13;
    <mo>)</mo>&#13;
    <mo>+</mo>&#13;
    <mi>P</mi>&#13;
    <mo>(</mo>&#13;
    <mi>B</mi>&#13;
    <mo>)</mo>&#13;
    <mo>-</mo>&#13;
    <mi>P</mi>&#13;
    <mo>(</mo>&#13;
    <mi>A</mi>&#13;
    <mi>B</mi>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math></p></div>&#13;
&#13;
<p>If the events are mutually exclusive, then the formula is simplified to the following:</p>&#13;
&#13;
<div data-type="equation"><p><math alttext="upper P left-parenthesis upper A upper B right-parenthesis equals 0">&#13;
  <mrow>&#13;
    <mi>P</mi>&#13;
    <mo>(</mo>&#13;
    <mi>A</mi>&#13;
    <mi>B</mi>&#13;
    <mo>)</mo>&#13;
    <mo>=</mo>&#13;
    <mn>0</mn>&#13;
  </mrow>&#13;
</math></p>&#13;
&#13;
<p><math alttext="upper P left-parenthesis upper A o r upper B right-parenthesis equals upper P left-parenthesis upper A right-parenthesis plus upper P left-parenthesis upper B right-parenthesis minus 0">&#13;
  <mrow>&#13;
    <mi>P</mi>&#13;
    <mo>(</mo>&#13;
    <mi>A</mi>&#13;
    <mspace width="0.222222em"/>&#13;
    <mi>o</mi>&#13;
    <mi>r</mi>&#13;
    <mspace width="0.222222em"/>&#13;
    <mi>B</mi>&#13;
    <mo>)</mo>&#13;
    <mo>=</mo>&#13;
    <mi>P</mi>&#13;
    <mo>(</mo>&#13;
    <mi>A</mi>&#13;
    <mo>)</mo>&#13;
    <mo>+</mo>&#13;
    <mi>P</mi>&#13;
    <mo>(</mo>&#13;
    <mi>B</mi>&#13;
    <mo>)</mo>&#13;
    <mo>-</mo>&#13;
    <mn>0</mn>&#13;
  </mrow>&#13;
</math></p>&#13;
&#13;
<p><math alttext="upper P left-parenthesis upper A o r upper B right-parenthesis equals upper P left-parenthesis upper A right-parenthesis plus upper P left-parenthesis upper B right-parenthesis">&#13;
  <mrow>&#13;
    <mi>P</mi>&#13;
    <mo>(</mo>&#13;
    <mi>A</mi>&#13;
    <mspace width="0.222222em"/>&#13;
    <mi>o</mi>&#13;
    <mi>r</mi>&#13;
    <mspace width="0.222222em"/>&#13;
    <mi>B</mi>&#13;
    <mo>)</mo>&#13;
    <mo>=</mo>&#13;
    <mi>P</mi>&#13;
    <mo>(</mo>&#13;
    <mi>A</mi>&#13;
    <mo>)</mo>&#13;
    <mo>+</mo>&#13;
    <mi>P</mi>&#13;
    <mo>(</mo>&#13;
    <mi>B</mi>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math></p></div>&#13;
&#13;
<p>Notice how in mutually exclusive events, it’s either A or B that can be realized, and therefore, the probability that both of them will occur is zero. To understand why you need to subtract the joint probability of A and B, take a look at <a data-type="xref" href="#figure-2-1">Figure 2-1</a>.</p>&#13;
&#13;
<figure><div class="figure" id="figure-2-1"><img alt="" class="iimagesdlf_0333png" src="assets/dlff_0201.png"/>&#13;
<h6><span class="label">Figure 2-1. </span>The addition rule of probability</h6>&#13;
</div></figure>&#13;
&#13;
<p>Notice how the probability of either A or B occurring while they are mutually exclusive must not include their joint probability. Let’s now look at the concept of independent events.</p>&#13;
&#13;
<p><a contenteditable="false" data-primary="independent events" data-type="indexterm" id="id333"/><em>Independent events</em> are not tied to one another (e.g., rolling the die twice). In this case, the joint probability is calculated as follows:</p>&#13;
&#13;
<div data-type="equation"><p><math alttext="upper P left-parenthesis upper A upper B right-parenthesis equals upper P left-parenthesis upper A right-parenthesis times upper P left-parenthesis upper B right-parenthesis">&#13;
  <mrow>&#13;
    <mi>P</mi>&#13;
    <mo>(</mo>&#13;
    <mi>A</mi>&#13;
    <mi>B</mi>&#13;
    <mo>)</mo>&#13;
    <mo>=</mo>&#13;
    <mi>P</mi>&#13;
    <mo>(</mo>&#13;
    <mi>A</mi>&#13;
    <mo>)</mo>&#13;
    <mo>×</mo>&#13;
    <mi>P</mi>&#13;
    <mo>(</mo>&#13;
    <mi>B</mi>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math></p></div>&#13;
&#13;
<p>Independent events therefore refer to instances where the occurrence of one event has absolutely zero impact on the occurrence of the other event(s). Let’s see an example to validate this concept. Consider a simple coin toss. The probability of getting heads does not depend on what you got in the previous coin toss. Therefore, the probability of getting heads is always 0.50 (50%). To take things further, what is the probability of getting only heads after five coin tosses?</p>&#13;
&#13;
<p>As the probability of each event is independent from the previous or the next one, the formula is as follows:</p>&#13;
&#13;
<div data-type="equation"><p><math alttext="upper P left-parenthesis x right-parenthesis equals 0.50 times 0.50 times 0.50 times 0.50 times 0.50 equals 0.03125 equals 3.125 percent-sign">&#13;
  <mrow>&#13;
    <mi>P</mi>&#13;
    <mo>(</mo>&#13;
    <mi>x</mi>&#13;
    <mo>)</mo>&#13;
    <mo>=</mo>&#13;
    <mn>0</mn>&#13;
    <mo lspace="0%" rspace="0%">.</mo>&#13;
    <mn>50</mn>&#13;
    <mo>×</mo>&#13;
    <mn>0</mn>&#13;
    <mo lspace="0%" rspace="0%">.</mo>&#13;
    <mn>50</mn>&#13;
    <mo>×</mo>&#13;
    <mn>0</mn>&#13;
    <mo lspace="0%" rspace="0%">.</mo>&#13;
    <mn>50</mn>&#13;
    <mo>×</mo>&#13;
    <mn>0</mn>&#13;
    <mo lspace="0%" rspace="0%">.</mo>&#13;
    <mn>50</mn>&#13;
    <mo>×</mo>&#13;
    <mn>0</mn>&#13;
    <mo lspace="0%" rspace="0%">.</mo>&#13;
    <mn>50</mn>&#13;
    <mo>=</mo>&#13;
    <mn>0</mn>&#13;
    <mo lspace="0%" rspace="0%">.</mo>&#13;
    <mn>03125</mn>&#13;
    <mo>=</mo>&#13;
    <mn>3</mn>&#13;
    <mo lspace="0%" rspace="0%">.</mo>&#13;
    <mn>125</mn>&#13;
    <mo lspace="0%" rspace="0%">%</mo>&#13;
  </mrow>&#13;
</math></p></div>&#13;
&#13;
<p><a contenteditable="false" data-primary="expected value of a random variable" data-type="indexterm" id="id334"/>The <em>expected value</em> of a random variable is the weighted average of the different outcomes. Therefore, the expected value is really another way of referring to the mean. Mathematically, the expected value is as follows:</p>&#13;
&#13;
<div data-type="equation"><p><math alttext="upper E left-parenthesis upper X right-parenthesis equals sigma-summation Underscript i equals 1 Overscript n Endscripts left-parenthesis upper P left-parenthesis x Subscript i Baseline right-parenthesis x Subscript i Baseline right-parenthesis">&#13;
  <mrow>&#13;
    <mi>E</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>X</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi> </msubsup>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>P</mi>&#13;
      <mrow>&#13;
        <mo>(</mo>&#13;
        <msub><mi>x</mi> <mi>i</mi> </msub>&#13;
        <mo>)</mo>&#13;
      </mrow>&#13;
      <msub><mi>x</mi> <mi>i</mi> </msub>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
  </mrow>&#13;
</math></p></div>&#13;
&#13;
<p>Take a look at <a data-type="xref" href="#table-2-1">Table 2-1</a> and try to calculate the expected value of the next employment numbers in a certain month of the year.</p>&#13;
&#13;
<table id="table-2-1">&#13;
	<caption><span class="label">Table 2-1. </span>Employment numbers</caption>&#13;
	&#13;
<thead>&#13;
		<tr>&#13;
			<th>Nonfarm payrolls</th>&#13;
			<th>Probability</th>&#13;
		</tr></thead>&#13;
			<tbody>&#13;
		<tr>&#13;
			<td>300,000</td>&#13;
			<td>0.1</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>400,000</td>&#13;
			<td>0.3</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>500,000</td>&#13;
			<td>0.5</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>600,000</td>&#13;
			<td>0.1</td>&#13;
		</tr>&#13;
	</tbody>&#13;
</table>&#13;
&#13;
<p><em>Nonfarm payrolls</em> refer to a monthly report issued by the US Department of Labor that gives information on the total number of paid employees in the nation, excluding those employed in the agriculture sector, as well as those employed by the government and nonprofit organizations.</p>&#13;
&#13;
<p>From <a data-type="xref" href="#table-2-1">Table 2-1</a>, economists assume there is a 50% probability that there will be a 500,000 increase in the total number of paid employees and a 30% probability that there will be a 400,000 increase in the total number of paid employees. The expected value is therefore:</p>&#13;
&#13;
<!--
<div data-type="equation"><p><span class="math-tex" data-type="tex">\(\begin{aligned}E(X) = (300,000 \times 0.1) + (400,000 \times 0.3) + (500,000 \times 0.5) + (600,000 \times 0.1) \\ = 460,000\end{aligned}\)</span></p></div>
-->&#13;
&#13;
<div data-type="equation">&#13;
<math>&#13;
  <mtable displaystyle="true">&#13;
    <mtr>&#13;
      <mtd columnalign="right">&#13;
        <mrow>&#13;
          <mi>E</mi>&#13;
          <mo>(</mo>&#13;
          <mi>X</mi>&#13;
          <mo>)</mo>&#13;
        </mrow>&#13;
      </mtd>&#13;
      <mtd columnalign="left">&#13;
        <mrow>&#13;
          <mo>=</mo>&#13;
          <mo>(</mo>&#13;
          <mn>300</mn>&#13;
          <mo>,</mo>&#13;
          <mn>000</mn>&#13;
          <mo>×</mo>&#13;
          <mn>0</mn>&#13;
          <mo lspace="0%" rspace="0%">.</mo>&#13;
          <mn>1</mn>&#13;
          <mo>)</mo>&#13;
          <mo>+</mo>&#13;
          <mo>(</mo>&#13;
          <mn>400</mn>&#13;
          <mo>,</mo>&#13;
          <mn>000</mn>&#13;
          <mo>×</mo>&#13;
          <mn>0</mn>&#13;
          <mo lspace="0%" rspace="0%">.</mo>&#13;
          <mn>3</mn>&#13;
          <mo>)</mo>&#13;
          <mo>+</mo>&#13;
          <mo>(</mo>&#13;
          <mn>500</mn>&#13;
          <mo>,</mo>&#13;
          <mn>000</mn>&#13;
          <mo>×</mo>&#13;
          <mn>0</mn>&#13;
          <mo lspace="0%" rspace="0%">.</mo>&#13;
          <mn>5</mn>&#13;
          <mo>)</mo>&#13;
          <mo>+</mo>&#13;
          <mo>(</mo>&#13;
          <mn>600</mn>&#13;
          <mo>,</mo>&#13;
          <mn>000</mn>&#13;
          <mo>×</mo>&#13;
          <mn>0</mn>&#13;
          <mo lspace="0%" rspace="0%">.</mo>&#13;
          <mn>1</mn>&#13;
          <mo>)</mo>&#13;
        </mrow>&#13;
      </mtd>&#13;
    </mtr>&#13;
    <mtr>&#13;
      <mtd/>&#13;
      <mtd columnalign="left">&#13;
        <mrow>&#13;
          <mo>=</mo>&#13;
          <mn>460</mn>&#13;
          <mo>,</mo>&#13;
          <mn>000</mn>&#13;
        </mrow>&#13;
      </mtd>&#13;
    </mtr>&#13;
  </mtable>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>Therefore, the number that represents the economists’ consensus is 460,000, as it is the closest weighted value to most forecasts. It is the value that represents the dataset.</p>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>The main takeaways from this section are as follows:</p>&#13;
&#13;
<ul>&#13;
	<li>Probability<em> </em>describes random variables and random events. It is a value between 0 and 1.</li>&#13;
	<li>Probabilities of events may be grouped together to form more complex scenarios.</li>&#13;
	<li>The expected outcome is the weighted average of every <span class="keep-together">probability</span> in the designated universe.<a contenteditable="false" data-primary="" data-startref="Chapter_2.html4" data-type="indexterm" id="id335"/></li>&#13;
</ul>&#13;
</div>&#13;
</div></section>&#13;
&#13;
<section class="pagebreak-before" data-pdf-bookmark="Sampling and Hypothesis Testing" data-type="sect1"><div class="sect1" id="sampling_and_hypothesis_testing">&#13;
<h1 class="less_space">Sampling and Hypothesis Testing</h1>&#13;
&#13;
<p><a contenteditable="false" data-primary="probabilistic methods" data-secondary="sampling and hypothesis testing" data-type="indexterm" id="Chapter_2.html5"/>When populations are large, representative samples are taken so that they become the main describers of data. Take the United States. Its democratic system means that the people hold the right to decide their own fate, but it’s not possible to go to every person and ask them for their detailed opinions on every topic out there. This is why elections are held and representatives are elected to act in the people’s name.</p>&#13;
&#13;
<p><a contenteditable="false" data-primary="sampling" data-secondary="basics" data-type="indexterm" id="Chapter_2.html6"/><em>Sampling </em><a contenteditable="false" data-primary="sampling" data-secondary="defined" data-type="indexterm" id="id336"/>refers to the act of selecting samples of data within a larger population and making conclusions about the statistical properties of the population. There are a few different methods of sampling. The best-known ones are the following:</p>&#13;
&#13;
<dl>&#13;
	<dt>Simple random sampling</dt>&#13;
	<dd>&#13;
	<p><a contenteditable="false" data-primary="simple random sampling" data-type="indexterm" id="id337"/>With simple random sampling, each element in the population has an equal chance of being selected for the sample. This can be a random number generated on a labeled population where each individual has the same probability of being selected.</p>&#13;
	</dd>&#13;
</dl>&#13;
&#13;
<dl>&#13;
	<dt>Stratified sampling</dt>&#13;
	<dd>&#13;
	<p><a contenteditable="false" data-primary="stratified sampling" data-type="indexterm" id="id338"/>With stratified sampling, the population is divided into groups based on some characteristic, and then a simple random sample is taken from each group in proportion to its size.</p>&#13;
	</dd>&#13;
</dl>&#13;
&#13;
<dl>&#13;
	<dt>Cluster sampling</dt>&#13;
	<dd>&#13;
	<p><a contenteditable="false" data-primary="cluster sampling" data-type="indexterm" id="id339"/>With cluster sampling, the population is divided into clusters, and a random sample of clusters is selected. Then, all elements within the selected clusters are included in the sample.</p>&#13;
	</dd>&#13;
</dl>&#13;
&#13;
<dl>&#13;
	<dt>Systematic sampling</dt>&#13;
	<dd>&#13;
	<p><a contenteditable="false" data-primary="systematic sampling, defined" data-type="indexterm" id="id340"/>With systematic sampling, an element is selected by choosing every <em>n</em>th individual from the population, where <em>n</em> is a fixed number. This means that it is not <span class="keep-together">random</span> but specified in advance.</p>&#13;
	</dd>&#13;
</dl>&#13;
&#13;
<p>A rule of thumb is that the more data you acquire, the better the metrics reflect the population. Sampling is extremely important in the world of machine learning, as quite often you are taking samples of data to represent the true population.  <a contenteditable="false" data-primary="in-sample set" data-type="indexterm" id="id341"/><a contenteditable="false" data-primary="out-of-sample set" data-type="indexterm" id="id342"/><a contenteditable="false" data-primary="testing sample" data-type="indexterm" id="id343"/><a contenteditable="false" data-primary="training sample" data-type="indexterm" id="id344"/>For example, when performing a backtest on a trading strategy, you will be required to split the whole dataset into a <em>training sample</em> and a <em>testing sample</em> where the first is the sample of data on which the algorithm understands its structure (also known as the <em>in-sample set</em>), and the second is the sample of data on which the algorithm tests its predictive power (also known as the <em>out-of-sample set</em>).</p>&#13;
&#13;
<p class="fix_tracking"><a contenteditable="false" data-primary="cross validation" data-secondary="sampling and" data-type="indexterm" id="id345"/>Another example of using sampling is <em>cross validation</em>. With this technique, a dataset is divided into two or more subsets. The model is trained using one subset, and its results are tested using the other subset(s). For various subsets of the data, this procedure is repeated numerous times, and then the model’s average performance is determined.</p>&#13;
&#13;
<p>These terms are discussed in more depth in the coming chapters. For now, you should understand that the concept of sampling is very important in machine <span class="keep-together">learning.</span></p>&#13;
&#13;
<p>Sampling is not perfect, and errors may be possible, just as in any other estimation method. <a contenteditable="false" data-primary="sampling error" data-type="indexterm" id="id346"/><em>Sampling error</em> refers to the difference between the statistic of the sample and the statistic of the population (if it’s known). <a contenteditable="false" data-primary="statistic, defined" data-type="indexterm" id="id347"/>A <em>statistic </em>is a metric that describes the analyzed dataset (an example of this would be the mean, a statistic you will see in greater detail in <a data-type="xref" href="ch03.html#ch03">Chapter 3</a>). Now, what is the minimum sample size you should have to be able to make inferences about the population? The rule of thumb is to have a minimum of 30 observations, and the more the merrier. <a contenteditable="false" data-primary="central limit theorem" data-type="indexterm" id="id348"/>This brings the discussion to the <em>central limit theorem</em>, which states that random samples drawn from a population will approach a normal distribution (a probability distribution that is symmetric and bell shaped) as the sample gets larger.</p>&#13;
&#13;
<p>The central limit theorem makes it simple to apply inferences and conclusions as hypothesis testing goes well with a normal distribution. <a contenteditable="false" data-primary="confidence intervals" data-type="indexterm" id="id349"/>Before proceeding to hypothesis testing, let’s look at <em>confidence intervals</em>, which are ranges of values where the population parameter is expected to be. Confidence intervals are generally constructed by adding or subtracting a factor from the point estimate. For example, given a sample mean x̄, a confidence interval can be constructed as follows:</p>&#13;
&#13;
<div data-type="equation"><p><math alttext="x overbar plus-or-minus left-parenthesis reliability factor times standard error right-parenthesis">&#13;
  <mrow>&#13;
    <mover accent="true"><mi>x</mi> <mo>¯</mo></mover>&#13;
    <mo>±</mo>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mtext>reliability</mtext>&#13;
      <mspace width="4.pt"/>&#13;
      <mtext>factor</mtext>&#13;
      <mo>×</mo>&#13;
      <mtext>standard</mtext>&#13;
      <mspace width="4.pt"/>&#13;
      <mtext>error</mtext>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
  </mrow>&#13;
</math></p></div>&#13;
&#13;
<p>Let’s try to understand the calculation step by step. The sample mean is an estimate of the population and is calculated because it is not possible to calculate the population mean. Therefore, by performing a random sample, the assumption is that the sample mean should be equal to the population mean. However, in real life, things may differ, and this is why you should construct a confidence interval using probabilistic <span class="keep-together">methods.</span></p>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p><a contenteditable="false" data-primary="significance level" data-type="indexterm" id="id350"/>The <em>significance level </em>is the threshold of the confidence interval. For example, a confidence interval of 95% means that with 95% confidence, the estimate should lie within a certain range. The remaining 5% probability that it does not is the significance level (generally marked with the letter alpha, α).</p>&#13;
</div>&#13;
&#13;
<p><a contenteditable="false" data-primary="reliability factor" data-type="indexterm" id="id351"/>A <em>reliability f</em><em>actor</em> is a statistical measure that depends on the distribution of the estimate and the probability that it falls within the confidence interval. For the sake of simplicity, let’s assume that the variance of the population is normal and the population is normally distributed. For a significance level of 5% (thus, a confidence interval of 95%), the reliability factor is 1.96 in this case (the way you get this number is less relevant to the discussion).</p>&#13;
&#13;
<p><a contenteditable="false" data-primary="standard error" data-type="indexterm" id="id352"/>The <em>standard error</em> is the standard deviation of the sample. <em>Standard deviation</em> is discussed in greater depth in <a data-type="xref" href="ch03.html#ch03">Chapter 3</a>; for now, just know that it represents the degree of fluctuation of the different values around the mean. Standard error is found using the following formula:</p>&#13;
&#13;
<div data-type="equation"><p><math alttext="s equals StartFraction sigma Over StartRoot n EndRoot EndFraction">&#13;
  <mrow>&#13;
    <mi>s</mi>&#13;
    <mo>=</mo>&#13;
    <mfrac><mi>σ</mi> <msqrt><mi>n</mi></msqrt></mfrac>&#13;
  </mrow>&#13;
</math></p>&#13;
&#13;
<p><math alttext="sigma is the population standard deviation">&#13;
  <mrow>&#13;
    <mi>σ</mi>&#13;
    <mspace width="4.pt"/>&#13;
    <mtext>is</mtext>&#13;
    <mspace width="4.pt"/>&#13;
    <mtext>the</mtext>&#13;
    <mspace width="4.pt"/>&#13;
    <mtext>population</mtext>&#13;
    <mspace width="4.pt"/>&#13;
    <mtext>standard</mtext>&#13;
    <mspace width="4.pt"/>&#13;
    <mtext>deviation</mtext>&#13;
  </mrow>&#13;
</math></p>&#13;
&#13;
<p><math alttext="StartRoot n EndRoot is the square root of the population number">&#13;
  <mrow>&#13;
    <msqrt>&#13;
      <mi>n</mi>&#13;
    </msqrt>&#13;
    <mspace width="4.pt"/>&#13;
    <mtext>is</mtext>&#13;
    <mspace width="4.pt"/>&#13;
    <mtext>the</mtext>&#13;
    <mspace width="4.pt"/>&#13;
    <mtext>square</mtext>&#13;
    <mspace width="4.pt"/>&#13;
    <mtext>root</mtext>&#13;
    <mspace width="4.pt"/>&#13;
    <mtext>of</mtext>&#13;
    <mspace width="4.pt"/>&#13;
    <mtext>the</mtext>&#13;
    <mspace width="4.pt"/>&#13;
    <mtext>population</mtext>&#13;
    <mspace width="4.pt"/>&#13;
    <mtext>number</mtext>&#13;
  </mrow>&#13;
</math></p></div>&#13;
&#13;
<p>It is also worth knowing that for a 1% significance level, the reliability factor is 2.575, and for a 10% significance level, the reliability factor is 1.645. Let’s take a look at a practical example to make sense of all this math.</p>&#13;
&#13;
<p>Consider a population of 100 financial instruments (bonds, currency pairs, stocks, structured products, etc.). The mean annual return of these instruments is 1.4%. Assuming a population standard deviation of 4.34%, what is the confidence interval at a 1% significance level (99% confidence interval) of the mean?</p>&#13;
&#13;
<p>The answer is determined by just plugging the values into the formula as follows:</p>&#13;
&#13;
<div data-type="equation"><p><math alttext="1.4 percent-sign plus-or-minus 2.575 times StartFraction 4.34 percent-sign Over StartRoot 100 EndRoot EndFraction equals 1.4 percent-sign plus-or-minus 1.11 percent-sign">&#13;
  <mrow>&#13;
    <mn>1</mn>&#13;
    <mo lspace="0%" rspace="0%">.</mo>&#13;
    <mn>4</mn>&#13;
    <mo lspace="0%" rspace="0%">%</mo>&#13;
    <mo>±</mo>&#13;
    <mn>2</mn>&#13;
    <mo lspace="0%" rspace="0%">.</mo>&#13;
    <mn>575</mn>&#13;
    <mo>×</mo>&#13;
    <mfrac><mrow><mn>4</mn><mo lspace="0%" rspace="0%">.</mo><mn>34</mn><mo lspace="0%" rspace="0%">%</mo></mrow> <msqrt><mn>100</mn></msqrt></mfrac>&#13;
    <mo>=</mo>&#13;
    <mn>1</mn>&#13;
    <mo lspace="0%" rspace="0%">.</mo>&#13;
    <mn>4</mn>&#13;
    <mo lspace="0%" rspace="0%">%</mo>&#13;
    <mo>±</mo>&#13;
    <mn>1</mn>&#13;
    <mo lspace="0%" rspace="0%">.</mo>&#13;
    <mn>11</mn>&#13;
    <mo lspace="0%" rspace="0%">%</mo>&#13;
  </mrow>&#13;
</math></p></div>&#13;
&#13;
<p>This means that the confidence interval is between (0.29%, 2.51%).</p>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>If the sample size is small and/or the population standard deviation is unknown, a t-distribution may be a better choice than a normal distribution.</p>&#13;
&#13;
<p><a contenteditable="false" data-primary="t-distribution" data-type="indexterm" id="id353"/>The <em>t-distribution</em> is a type of probability distribution used to model the distribution of a sample mean when the sample size is small and/or when the population standard deviation is unknown. It resembles the normal distribution in shape but with heavier tails, which represents the uncertainty associated with smaller sample sizes.<a contenteditable="false" data-primary="" data-startref="Chapter_2.html6" data-type="indexterm" id="id354"/></p>&#13;
</div>&#13;
&#13;
&#13;
&#13;
<p><a contenteditable="false" data-primary="hypothesis testing" data-type="indexterm" id="Chapter_2.html7"/>The next stop is hypothesis testing, a key probabilistic technique of getting conclusions on samples of data. This part is extremely important, as it’s used in a lot of statistical analyses and models.</p>&#13;
&#13;
<p class="pagebreak-before">In statistics, <em>hypothesis testing</em> is a technique for drawing conclusions about a population from a small sample of data. It entails developing two competing hypotheses, the <em>null hypothesis</em> and the <em>alternative hypothesis</em>, about a population parameter and then figuring out which is more likely to be accurate using sample data.</p>&#13;
&#13;
<p>For example, say that a financial analyst is evaluating two portfolios from a risk perspective. They formulate two hypotheses:</p>&#13;
&#13;
<ul>&#13;
	<li>The null hypothesis states that there is no significant difference in the volatility of the two portfolios.</li>&#13;
	<li>The alternative hypothesis states that there is a significant difference in the <span class="keep-together">volatility</span> of the two portfolios.</li>&#13;
</ul>&#13;
&#13;
<p>The hypothesis is then tested using statistical analysis to determine whether the difference in volatility is statistically significant or due to pure chance.</p>&#13;
&#13;
<p>Following the definition of null and alternative hypotheses, a test statistic is computed using the sample data. To assess the result’s significance, the test statistic is then compared to a critical value drawn from a standard distribution. The null hypothesis is rejected and the alternative hypothesis is accepted if the test statistic is inside the crucial zone. The null hypothesis is not rejected and the conclusion that there is insufficient evidence to support the alternative hypothesis is reached if the test statistic does not fall inside the crucial zone.</p>&#13;
&#13;
<p>This is all a fancy way of saying that hypothesis testing basically involves creating two opposing scenarios, running a probability check, and then deciding which scenario is more likely true. Hypothesis testing can take two forms:</p>&#13;
&#13;
<dl>&#13;
 <dt>One-tailed test</dt>&#13;
 <dd><p><a contenteditable="false" data-primary="one-tailed test" data-type="indexterm" id="id355"/>An example of this would be to test if the return on certain financial instruments is greater than zero.</p></dd>&#13;
 &#13;
 <dt>Two-tailed test</dt>&#13;
 <dd><p><a contenteditable="false" data-primary="two-tailed test" data-type="indexterm" id="id356"/>An example of this would be to test if the return on certain financial instruments is different from zero (meaning that it can be either greater than or less than zero). Hypothesis tests are generally two-tailed tests.</p></dd>&#13;
</dl>&#13;
&#13;
<p>The null hypothesis is the one that you want to reject and therefore is tested in the hopes of getting rejected and accepting the alternative scenario. A two-tailed test takes the following general form:</p>&#13;
&#13;
<div data-type="equation"><p><math alttext="upper H 0 colon x equals x 0">&#13;
  <mrow>&#13;
    <msub><mi>H</mi> <mn>0</mn> </msub>&#13;
    <mo>:</mo>&#13;
    <mi>x</mi>&#13;
    <mo>=</mo>&#13;
    <msub><mi>x</mi> <mn>0</mn> </msub>&#13;
  </mrow>&#13;
</math></p>&#13;
&#13;
<p><math alttext="upper H Subscript a Baseline colon x not-equals x 0">&#13;
  <mrow>&#13;
    <msub><mi>H</mi> <mi>a</mi> </msub>&#13;
    <mo>:</mo>&#13;
    <mi>x</mi>&#13;
    <mo>≠</mo>&#13;
    <msub><mi>x</mi> <mn>0</mn> </msub>&#13;
  </mrow>&#13;
</math></p></div>&#13;
&#13;
<p>As the alternative scenario allows for values above and below zero (which is the stated level in the null hypothesis), there should be two critical values. Therefore, the rule of a two-tailed test is to reject the null hypothesis if the test statistic is greater than the upper critical value or less than the lower critical value. For instance, for a normally distributed dataset, the test statistic is compared with the critical values (at 5% significance level) at +1.96 and –1.96. The null hypothesis is rejected if the test statistic falls outside the range of +1.96 and –1.96.</p>&#13;
&#13;
<p>The process of hypothesis testing entails the calculation of the test statistic. This is done by comparing the point estimate of the population parameter with the hypothesized value of the null hypothesis. Both are then scaled by the standard error of the sample. The mathematical representation is as follows:</p>&#13;
&#13;
<div data-type="equation"><p><math alttext="Test statistic equals StartFraction Sample statistic minus Hypothesized value Over Standard error EndFraction">&#13;
  <mrow>&#13;
    <mtext>Test</mtext>&#13;
    <mspace width="4.pt"/>&#13;
    <mtext>statistic</mtext>&#13;
    <mo>=</mo>&#13;
    <mstyle displaystyle="false" scriptlevel="0">&#13;
      <mfrac><mrow><mtext>Sample</mtext><mspace width="4.pt"/><mtext>statistic</mtext><mspace width="4.pt"/><mo>-</mo><mspace width="4.pt"/><mtext>Hypothesized</mtext><mspace width="4.pt"/><mtext>value</mtext></mrow> <mrow><mtext>Standard</mtext><mspace width="4.pt"/><mtext>error</mtext></mrow></mfrac>&#13;
    </mstyle>&#13;
  </mrow>&#13;
</math></p></div>&#13;
&#13;
<p>An important consideration in hypothesis testing is that the sample may not be representative, which leads to errors in describing the population. This gives rise to two types of errors:</p>&#13;
&#13;
<dl>&#13;
	<dt><em>Type I error</em></dt>&#13;
	<dd><p><a contenteditable="false" data-primary="type I/type II error" data-type="indexterm" id="id357"/>This error occurs when rejecting the null hypothesis even though it is true.</p></dd>&#13;
	&#13;
	<dt><em>Type II error</em></dt>&#13;
	<dd><p>This error occurs when failing to reject the null hypothesis even though it is false.</p></dd>&#13;
</dl>&#13;
&#13;
<p>Intuitively, the significance level is the probability of making a type I error. Remember that if α = 5%, then there is a 5% chance of rejecting a true null hypothesis by mistake. An example would make things clearer.</p>&#13;
&#13;
<p>Consider an analyst doing research on the annual returns of a long–short portfolio over a period of 20 years. The mean annual return was 1% with a standard deviation of 2%. The analyst’s opinion is that the mean annual return is not equal to zero, and they want to construct a 95% confidence interval for this and then construct a hypothesis test. You would proceed as follows:</p>&#13;
&#13;
<ol>&#13;
	<li>State the variables. The size of the sample is 20, the standard deviation is 2%, and the mean is 1%.  </li>&#13;
	<li>Calculate the standard error, which in this case is 0.44% as per the formula.</li>&#13;
	<li>Define the critical values for the 95% confidence interval. The critical values are +1.96 and –1.96. To find the confidence interval, add and subtract the margin of error from the sample mean. The confidence interval is therefore (0.13%, 1.86%).</li>&#13;
	<li>Specify the null hypothesis, which is, according to the analyst’s opinion, a two-tailed test. The null hypothesis is that the annual return equals zero. You should reject it if the test statistic is less than –1.96 or greater than +1.96.</li>&#13;
	<li>Using the formula to find the test statistic gives 2.27. Therefore, the null <span class="keep-together">hypothesis</span> is rejected.</li>&#13;
</ol>&#13;
&#13;
<p><a contenteditable="false" data-primary="p-value" data-type="indexterm" id="id358"/>One more important metric to discuss is the p-value. The<em> p-value</em> is the probability of seeing a test statistic more extreme than the one seen in the statistical test given that the null hypothesis is true. Comparing a p-value to a significance level—typically <span class="keep-together">0.05—</span>allows you to understand it. The result is deemed statistically significant, and the null hypothesis is rejected in favor of the alternative hypothesis if the p-value is less than or equal to the significance level.</p>&#13;
&#13;
<p>If the p-value is less than the significance level of 5%, it means that there is a 5% chance you will see a test statistic as extreme as the current one if the null hypothesis is true. Another way of defining the p-value is to consider it as being the smallest significance level for which the null hypothesis can be rejected.<a contenteditable="false" data-primary="" data-startref="Chapter_2.html7" data-type="indexterm" id="id359"/></p>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>The main takeaways from this section are as follows:</p>&#13;
&#13;
<ul>&#13;
	<li>Sampling refers to the collection of data within a population, with the aim of making conclusions about the statistical <span class="keep-together">properties</span> of the aforementioned population.</li>&#13;
	<li>Hypothesis testing is a technique for drawing conclusions about a population from a small sample of data<a contenteditable="false" data-primary="" data-startref="Chapter_2.html5" data-type="indexterm" id="id360"/>.<a contenteditable="false" data-primary="" data-startref="Chapter_2.html3" data-type="indexterm" id="id361"/><a contenteditable="false" data-primary="" data-startref="Chapter_2.html2" data-type="indexterm" id="id362"/></li>&#13;
</ul>&#13;
</div>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="A Primer on Information Theory" data-type="sect1"><div class="sect1" id="id10">&#13;
<h1>A Primer on Information Theory</h1>&#13;
&#13;
<p><a contenteditable="false" data-primary="information theory" data-type="indexterm" id="Chapter_2.html8"/><a contenteditable="false" data-primary="probabilistic methods" data-secondary="information theory basics" data-type="indexterm" id="Chapter_2.html9"/><em>Information theory</em> is a complex field in abstract mathematics that is closely related to probability. It is the study of how information is quantified, stored, and transmitted. There are three conditions of occurrence when it comes to an event:</p>&#13;
&#13;
<dl>&#13;
 <dt>Uncertainty</dt>&#13;
 <dd><p>If the event has not occurred yet</p></dd>&#13;
 &#13;
 <dt>Surprise</dt>&#13;
 <dd><p>If the event has just occurred</p></dd>&#13;
 &#13;
 <dt>Information</dt>&#13;
 <dd><p>If the event has occurred in the past</p></dd>&#13;
</dl>&#13;
&#13;
<p><a contenteditable="false" data-primary="entropy" data-secondary="defined" data-type="indexterm" id="id363"/>One of the key concepts in information theory is <em>entropy</em>, which is the level of uncertainty or randomness in a message or information source and describes the degree to which an event or message is unexpected. <a contenteditable="false" data-primary="information gain" data-secondary="defined" data-type="indexterm" id="id364"/>In contrast, <em>information gain</em> measures the reduction in entropy (surprise) when receiving new information.</p>&#13;
&#13;
<p>Basically, information theory describes the surprise of events. When an event has a low probability of occurrence, it has more surprise and hence more information to provide. Similarly, when an event has a high probability of occurrence, it has less surprise and therefore less information. What you should retain from this is that the amount of information learned from an unlikely event is greater than the amount of information learned from a likely event.</p>&#13;
&#13;
<p>Before starting to dig a little deeper into the field of information theory, it is important to understand what a logarithm is and, for that matter, what an exponent is. A general exponential function takes a certain constant or a variable to a certain power:</p>&#13;
&#13;
<div data-type="equation"><p><math alttext="f left-parenthesis x right-parenthesis equals a Superscript x">&#13;
  <mrow>&#13;
    <mi>f</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>x</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <msup><mi>a</mi> <mi>x</mi> </msup>&#13;
  </mrow>&#13;
</math></p></div>&#13;
&#13;
<p><a contenteditable="false" data-primary="exponent of a number" data-type="indexterm" id="id365"/>In other words, the <em>exponent of a number</em> is the number of times you will multiply it by itself:</p>&#13;
&#13;
<div data-type="equation"><p><math alttext="4 cubed equals 4 times 4 times 4 equals 64">&#13;
  <mrow>&#13;
    <msup><mn>4</mn> <mn>3</mn> </msup>&#13;
    <mo>=</mo>&#13;
    <mn>4</mn>&#13;
    <mo>×</mo>&#13;
    <mn>4</mn>&#13;
    <mo>×</mo>&#13;
    <mn>4</mn>&#13;
    <mo>=</mo>&#13;
    <mn>64</mn>&#13;
  </mrow>&#13;
</math></p></div>&#13;
&#13;
<p><a contenteditable="false" data-primary="logarithm, defined" data-type="indexterm" id="id366"/>A logarithm is the opposite of an exponent, and its aim is to find the exponent—knowing 4 and 64 from the previous example and finding 3:</p>&#13;
&#13;
<div data-type="equation"><p><math alttext="log Subscript 4 Baseline left-parenthesis 64 right-parenthesis equals 3">&#13;
  <mrow>&#13;
    <msub><mo form="prefix">log</mo> <mn>4</mn> </msub>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mn>64</mn>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <mn>3</mn>&#13;
  </mrow>&#13;
</math></p></div>&#13;
&#13;
<p>A logarithm therefore is the answer to how many of one number to multiply to get another number. Since they are literally inverse functions, you can use them together to simplify or even solve for <em>x</em>. Take the following example:</p>&#13;
&#13;
<div data-type="equation"><p><math alttext="log Subscript 4 Baseline left-parenthesis x right-parenthesis equals 3">&#13;
  <mrow>&#13;
    <msub><mo form="prefix">log</mo> <mn>4</mn> </msub>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>x</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <mn>3</mn>&#13;
  </mrow>&#13;
</math></p></div>&#13;
&#13;
<p class="fix_tracking">The objective here is to find <em>x</em> given the logarithmic function. The first step is simply to use the exponential function on one side as you want it to cancel out the logarithm on the right (inverse functions cancel each other out). This gives us the following result:</p>&#13;
&#13;
<div data-type="equation"><p><math alttext="4 Superscript l o g 4 left-parenthesis x right-parenthesis Baseline equals 4 cubed">&#13;
  <mrow>&#13;
    <msup><mn>4</mn> <mrow><mi>l</mi><mi>o</mi><msub><mi>g</mi> <mn>4</mn> </msub><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow> </msup>&#13;
    <mo>=</mo>&#13;
    <msup><mn>4</mn> <mn>3</mn> </msup>&#13;
  </mrow>&#13;
</math></p>&#13;
&#13;
<p><math alttext="x equals 4 cubed">&#13;
  <mrow>&#13;
    <mi>x</mi>&#13;
    <mo>=</mo>&#13;
    <msup><mn>4</mn> <mn>3</mn> </msup>&#13;
  </mrow>&#13;
</math></p>&#13;
&#13;
<p><math alttext="x equals 64">&#13;
  <mrow>&#13;
    <mi>x</mi>&#13;
    <mo>=</mo>&#13;
    <mn>64</mn>&#13;
  </mrow>&#13;
</math></p></div>&#13;
&#13;
<p class="pagebreak-before">Logarithms can have different bases. However, the most used logarithm has a base of 10. In computer science, base 2 logarithms represent bits (binary digits). Therefore, information is represented as bits. The formula of information gain is as follows:</p>&#13;
&#13;
<div data-type="equation"><p><math alttext="upper H left-parenthesis x Subscript i Baseline right-parenthesis equals minus l o g 2 left-parenthesis upper P left-parenthesis x Subscript i Baseline right-parenthesis right-parenthesis">&#13;
  <mrow>&#13;
    <mi>H</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <msub><mi>x</mi> <mi>i</mi> </msub>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <mo>-</mo>&#13;
    <mi>l</mi>&#13;
    <mi>o</mi>&#13;
    <msub><mi>g</mi> <mn>2</mn> </msub>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>P</mi>&#13;
      <mrow>&#13;
        <mo>(</mo>&#13;
        <msub><mi>x</mi> <mi>i</mi> </msub>&#13;
        <mo>)</mo>&#13;
      </mrow>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
  </mrow>&#13;
</math></p></div>&#13;
&#13;
<p>Let’s assume two variables<em> x</em> and <em>y</em>, where<em> x</em> has a probability of 1 (100% and therefore certain) and <em>y</em> has a probability of 0.5 (50% and therefore mostly random). What would be the information value in these two cases? The answer is as follows:</p>&#13;
&#13;
<div data-type="equation"><p><math alttext="upper H left-parenthesis x right-parenthesis equals minus l o g 2 left-parenthesis upper P left-parenthesis 1 right-parenthesis right-parenthesis equals 0">&#13;
  <mrow>&#13;
    <mi>H</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>x</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <mo>-</mo>&#13;
    <mi>l</mi>&#13;
    <mi>o</mi>&#13;
    <msub><mi>g</mi> <mn>2</mn> </msub>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>P</mi>&#13;
      <mrow>&#13;
        <mo>(</mo>&#13;
        <mn>1</mn>&#13;
        <mo>)</mo>&#13;
      </mrow>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <mn>0</mn>&#13;
  </mrow>&#13;
</math></p>&#13;
&#13;
<p><math alttext="upper H left-parenthesis y right-parenthesis equals minus l o g 2 left-parenthesis upper P left-parenthesis 0.5 right-parenthesis right-parenthesis equals 1">&#13;
  <mrow>&#13;
    <mi>H</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>y</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <mo>-</mo>&#13;
    <mi>l</mi>&#13;
    <mi>o</mi>&#13;
    <msub><mi>g</mi> <mn>2</mn> </msub>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>P</mi>&#13;
      <mrow>&#13;
        <mo>(</mo>&#13;
        <mn>0</mn>&#13;
        <mo lspace="0%" rspace="0%">.</mo>&#13;
        <mn>5</mn>&#13;
        <mo>)</mo>&#13;
      </mrow>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <mn>1</mn>&#13;
  </mrow>&#13;
</math></p></div>&#13;
&#13;
<p>So the certain event has an information value of zero, and the one that has a 50-50 chance of realizing has an information value of 1. What about the very unlikely event <em>z</em> that has a probability of 0.05 (5%)?</p>&#13;
&#13;
<div data-type="equation"><p><math alttext="upper H left-parenthesis z right-parenthesis equals minus l o g 2 left-parenthesis upper P left-parenthesis 0.05 right-parenthesis right-parenthesis equals 4.32">&#13;
  <mrow>&#13;
    <mi>H</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>z</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <mo>-</mo>&#13;
    <mi>l</mi>&#13;
    <mi>o</mi>&#13;
    <msub><mi>g</mi> <mn>2</mn> </msub>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>P</mi>&#13;
      <mrow>&#13;
        <mo>(</mo>&#13;
        <mn>0</mn>&#13;
        <mo lspace="0%" rspace="0%">.</mo>&#13;
        <mn>05</mn>&#13;
        <mo>)</mo>&#13;
      </mrow>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <mn>4</mn>&#13;
    <mo lspace="0%" rspace="0%">.</mo>&#13;
    <mn>32</mn>&#13;
  </mrow>&#13;
</math></p></div>&#13;
&#13;
<p>A negative relationship between probability and information is one of the principles of information theory. Entropy and information are related concepts, but they have different meanings and applications.</p>&#13;
&#13;
<p><em>Entropy </em>is a metric used to assess how chaotic or random a system is. Entropy describes how uncertain or unpredictable a signal is. The degree of disorder or unpredictability in the system or communication increases as entropy increases.</p>&#13;
&#13;
<p><em>Information </em>is the decrease in entropy or uncertainty that happens as a result of receiving a signal. A signal’s ability to lessen the receiver’s uncertainty or entropy increases with its informational content.</p>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Entropy is maximized whenever all the events are equally likely.</p>&#13;
</div>&#13;
&#13;
<p><a contenteditable="false" data-primary="entropy" data-secondary="calculating" data-type="indexterm" id="id367"/>Entropy is calculated using the following formula:</p>&#13;
&#13;
<div data-type="equation"><p><math alttext="upper S left-parenthesis x Subscript n Baseline right-parenthesis equals sigma-summation Underscript i equals 1 Overscript n Endscripts left-parenthesis minus l o g 2 left-parenthesis upper P left-parenthesis x Subscript i Baseline right-parenthesis right-parenthesis period left-parenthesis upper P left-parenthesis x Subscript i Baseline right-parenthesis right-parenthesis right-parenthesis">&#13;
  <mrow>&#13;
    <mi>S</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <msub><mi>x</mi> <mi>n</mi> </msub>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi> </msubsup>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mo>-</mo>&#13;
      <mi>l</mi>&#13;
      <mi>o</mi>&#13;
      <msub><mi>g</mi> <mn>2</mn> </msub>&#13;
      <mrow>&#13;
        <mo>(</mo>&#13;
        <mi>P</mi>&#13;
        <mrow>&#13;
          <mo>(</mo>&#13;
          <msub><mi>x</mi> <mi>i</mi> </msub>&#13;
          <mo>)</mo>&#13;
        </mrow>&#13;
        <mo>)</mo>&#13;
      </mrow>&#13;
      <mo lspace="0%" rspace="0%">.</mo>&#13;
      <mrow>&#13;
        <mo>(</mo>&#13;
        <mi>P</mi>&#13;
        <mrow>&#13;
          <mo>(</mo>&#13;
          <msub><mi>x</mi> <mi>i</mi> </msub>&#13;
          <mo>)</mo>&#13;
        </mrow>&#13;
        <mo>)</mo>&#13;
      </mrow>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
  </mrow>&#13;
</math></p></div>&#13;
&#13;
<p>Therefore, entropy is the average of the sum of logarithms times their respective probabilities.</p>&#13;
&#13;
<p>Now let’s discuss the final concept of the section, <em>information gain</em>.  The reduction in entropy caused by changing a dataset is calculated via information gain.</p>&#13;
&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p>Information gain is one of the key concepts you will see in <a data-type="xref" href="ch07.html#ch07">Chapter 7</a> with decision trees, and therefore, you may want to refer back to this section after reading that chapter.</p>&#13;
</div>&#13;
&#13;
<p><a contenteditable="false" data-primary="information gain" data-secondary="calculating" data-type="indexterm" id="id368"/>The typical way to calculate information gain is by comparing the entropy of a dataset before and after a transformation. Recall that entropy is maximized when all the outcomes of a random event have the same probability. This can also be presented as a distribution, where a symmetrical distribution (such as the normal distribution) has high entropy and a skewed distribution has low entropy.</p>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Minimizing entropy is related to maximizing information gain.</p>&#13;
</div>&#13;
&#13;
<p><a contenteditable="false" data-primary="mutual information" data-type="indexterm" id="id369"/>Before closing this introductory section on information theory, let’s look at the concept of <em>mutual information</em>. This measure is calculated between two variables, hence the name <em>mutual</em>, and it measures the reduction in uncertainty of a variable given another variable. The formula for mutual information is as follows:</p>&#13;
&#13;
<div data-type="equation"><p><math alttext="upper M upper I left-parenthesis x comma y right-parenthesis equals upper S left-parenthesis x right-parenthesis minus upper S left-parenthesis x vertical-bar y right-parenthesis">&#13;
  <mrow>&#13;
    <mi>M</mi>&#13;
    <mi>I</mi>&#13;
    <mo>(</mo>&#13;
    <mi>x</mi>&#13;
    <mo>,</mo>&#13;
    <mi>y</mi>&#13;
    <mo>)</mo>&#13;
    <mo>=</mo>&#13;
    <mi>S</mi>&#13;
    <mo>(</mo>&#13;
    <mi>x</mi>&#13;
    <mo>)</mo>&#13;
    <mo>-</mo>&#13;
    <mi>S</mi>&#13;
    <mo>(</mo>&#13;
    <mi>x</mi>&#13;
    <mo>|</mo>&#13;
    <mi>y</mi>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math></p>&#13;
&#13;
&#13;
<ul class="simplelist">&#13;
 <li><p><em>MI</em>(<em>x</em>, <em>y</em>) is the mutual information of <em>x</em> and <em>y</em></p></li>&#13;
 <li><p><em>S</em>(<em>x</em>) is the entropy of <em>x</em></p></li>&#13;
 <li><p><em>S</em>(<em>x</em>|<em>y</em>) is the conditional entropy of <em>x</em> and <em>y</em></p></li>&#13;
</ul>&#13;
</div>&#13;
&#13;
&#13;
<p>Mutual information therefore measures the dependence between variables. The greater the mutual information, the bigger the relationship between the variables (a value of zero represents independent variables). Keep this concept in mind, as you will see it in <a data-type="xref" href="ch03.html#correlation">“Correlation”</a>. This is because mutual information can also be a measure of nonlinear correlation between variables.</p>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>To summarize, here is what you need to retain in information theory to have a basic knowledge of what’s to come:</p>&#13;
&#13;
<ul>&#13;
	<li>Information theory uses concepts from probability to calculate information and entropy, which are used in machine learning models and other calculations (such as correlation).</li>&#13;
	<li>Information is the decrease in entropy or uncertainty that happens as a result of receiving a signal. Entropy is a metric used to assess how chaotic or random a system is.</li>&#13;
	<li>Mutual information is a measure of dependence between two random variables. It can also be used to calculate the <span class="keep-together">correlation</span> between the two.</li>&#13;
	<li>Tools from information theory are used in some machine learning models such as decision trees.<a contenteditable="false" data-primary="" data-startref="Chapter_2.html9" data-type="indexterm" id="id370"/><a contenteditable="false" data-primary="" data-startref="Chapter_2.html8" data-type="indexterm" id="id371"/></li>&#13;
</ul>&#13;
</div>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="id11">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>An understanding of probability presents a basic framework before moving to more advanced topics. This chapter skimmed over the concepts that you may encounter when dealing with machine and deep learning models. It is important to understand how probability is calculated and how hypothesis testing is performed (even though, in reality, algorithms will do this for you).<a contenteditable="false" data-primary="" data-startref="Chapter_2.html1" data-type="indexterm" id="id372"/><a contenteditable="false" data-primary="" data-startref="Chapter_2.html0" data-type="indexterm" id="id373"/></p>&#13;
&#13;
<p>The next chapter is extremely important and presents the statistical knowledge you need, not just for machine learning but also for financial trading and even complex data analysis.</p>&#13;
</div></section>&#13;
&#13;
</div></section></body></html>