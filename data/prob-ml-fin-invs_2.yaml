- en: Chapter 2\. Analyzing and Quantifying Uncertainty
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章 分析和量化不确定性
- en: There are known knowns. These are things we know that we know. There are known
    unknowns. That is to say, there are things that we know we don’t know. But there
    are also unknown unknowns. There are things we don’t know we don’t know.
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 已知的是已知的。这些是我们知道自己知道的事情。已知未知。也就是说，有些我们知道自己不知道的事情。但也有未知的未知。有些事情我们不知道自己不知道。
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: —Donald Rumsfeld, Former US Secretary of Defense
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: —唐纳德·拉姆斯菲尔德，前美国国防部长
- en: The Monty Hall problem, a famous probability brainteaser, is an entertaining
    way to explore the complex and profound nature of uncertainty that we face in
    our personal and professional lives. More pertinently, the solution to the Monty
    Hall problem is essentially a betting strategy. Throughout this chapter, we use
    it to explain many key concepts and pitfalls in probability, statistics, machine
    learning, game theory, finance, and investing.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 蒙提霍尔问题是一个著名的概率智力游戏，是探索我们在个人和职业生活中面临的不确定性复杂和深刻本质的一种娱乐方式。更为相关的是，蒙提霍尔问题的解决方案本质上是一种投注策略。在本章中，我们利用它来解释概率、统计、机器学习、博弈论、金融和投资中的许多关键概念和陷阱。
- en: In this chapter, we will solve the apparent paradox of the Monty Hall problem
    by developing two analytical solutions of differing complexity using the fundamental
    rules of probability theory. We also derive the inverse probability rule that
    is pivotal to probabilistic machine learning. Later in this chapter, we confirm
    these analytical solutions with a Monte Carlo simulation (MCS), one of the most
    powerful numerical techniques that is used extensively in finance and investing.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将通过使用概率论的基本规则开发两种不同复杂度的分析解决蒙提霍尔问题的明显悖论。我们还推导出反向概率规则，这对于概率机器学习至关重要。稍后在本章中，我们将用蒙特卡洛模拟（MCS）确认这些分析解决方案，这是一种在金融和投资中广泛使用的最强大的数值技术之一。
- en: There are three types of uncertainty embedded in the Monty Hall problem that
    we examine. Aleatory uncertainty is the randomness in the observed data (the known
    knowns). Epistemic uncertainty arises from the lack of knowledge about the underlying
    phenomenon (the known unknowns). Ontological uncertainty evolves from the nature
    of human affairs and its inherently unpredictable dynamics (the unknown unknowns).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Monty Hall问题嵌入了三种类型的不确定性，我们进行了探讨。Aleatory不确定性是观察数据中的随机性（已知已知）。Epistemic不确定性源于对潜在现象的不了解（已知未知）。Ontological不确定性则源自人类事务的本质及其固有的不可预测动态（未知未知）。
- en: Probability is used to quantify and analyze uncertainty in a systematic manner.
    In doing so, we reject the vacuous distinction between risk and uncertainty. Probability
    is truly the logic of science. It might be surprising for you to know that we
    can agree on the axioms of probability theory, yet disagree on the meaning of
    probability. We explore the two main schools of thought, the frequentist and epistemic
    views of probability. We find the conventional view of probability, the freque​​ntist
    version, to be a special case of epistemic probability at best and suited to simple
    games of chance. At worst, the frequentist view of probability is based on a facade
    of objective reality that shows an inexcusable ignorance of classical physics
    and common sense.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 概率被用来系统地量化和分析不确定性。在这样做时，我们拒绝了风险和不确定性之间的虚无区别。概率确实是科学的逻辑。也许令你惊讶的是，我们可以同意概率论公理，但在概率的含义上却存在分歧。我们探讨了两种主要的思想流派，频率主义和认识论概率观。我们发现传统的概率观，即频率主义版本，充其量只是认识论概率的一个特例，并且更适合简单的偶然游戏。最糟糕的是，频率主义对概率的观点基于对经典物理学和常识的不可原谅的无知。
- en: The no free lunch (NFL) theorems are a set of impossibility theorems that are
    an algorithmic restatement of the age-old problem of induction within a probabilistic
    framework. We explore how these epistemological concepts have important practical
    implications for probabilistic machine learning, finance, and investing.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: No Free Lunch（NFL）定理是一组不可能定理，它在概率框架内重新阐述了归纳问题的古老问题。我们探讨了这些认识论概念如何在概率机器学习、金融和投资中具有重要的实际意义。
- en: The Monty Hall Problem
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Monty Hall问题
- en: 'The famous Monty Hall problem was originally conceived and solved by an eminent
    statistician, Steve Selvin. The problem as we know it now is based on the popular
    1970s game show *Let’s Make a Deal* and named after its host, Monty Hall. Here
    are the rules of this brainteaser:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 著名的蒙特霍尔问题最初由著名统计学家史蒂夫·塞尔文构思并解决。现在我们所知的问题基于流行的1970年代游戏节目 *Let’s Make a Deal*
    并以主持人蒙特·霍尔的名字命名。以下是这个脑筋急转弯的规则：
- en: There is a car behind one of three doors and goats behind the other two.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 三扇门中有一扇门后面是汽车，另外两扇门后面是山羊。
- en: The objective is to win the car (not a goat!).
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 目标是赢得汽车（而不是山羊！）。
- en: Only Monty knows which door hides the car.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 只有蒙特知道哪扇门藏着汽车。
- en: Monty allows you to choose any one of the three doors.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 蒙特允许你选择三扇门中的任意一扇。
- en: Depending on the door you choose, he opens one of the other two doors that has
    a goat behind it.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据你的选择，他会打开另外两扇门中有一扇门后面是山羊的门。
- en: So let’s play the game. It doesn’t really matter which door you chose because
    the game plays out similarly regardless. Say you chose door 1\. Based on your
    choice of door 1, Monty opens door 3 to show you a goat. See [Figure 2-1](#the_monty_hall_problemsource_for_the_im).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我们来玩这个游戏。无论你选择哪扇门，游戏的进行方式都是类似的。假设你选择了门1。基于你的选择门1，蒙特打开了门3，给你展示了一只山羊。参见 [Figure 2-1](#the_monty_hall_problemsource_for_the_im)。
- en: '![The Monty Hall problem](assets/pmlf_0201.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![蒙特霍尔问题](assets/pmlf_0201.png)'
- en: Figure 2-1\. The Monty Hall problem^([1](ch02.html#ch02fn1))
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-1\. 蒙特霍尔问题^([1](ch02.html#ch02fn1))
- en: 'Now Monty offers you a deal: he gives you the option of sticking with your
    original choice of door 1 or switching to door 2\. Do you switch to door 2 or
    stay with your original decision of door 1? Try to solve this problem before you
    read ahead—it will be worth the trouble.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在蒙特向你提出一个交易：他让你选择是保持原来选择的门1还是换到门2。你会选择换到门2还是保持原来的选择门1？在继续阅读之前尝试解决这个问题——这将是值得的麻烦。
- en: I must admit that when I first came across this problem many years ago, my immediate
    response was that it doesn’t matter whether you stay or switch doors, since now
    it is equally likely that the car is behind either door 1 or door 2\. So I stayed
    with my original choice. Turns out that my choice was wrong.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我必须承认，当我多年前第一次遇到这个问题时，我的直接反应是无论你是保持还是换门，现在汽车被放置在门1或门2后面的概率是相等的。所以我坚持了我最初的选择。结果证明我的选择是错误的。
- en: The optimal strategy is to switch doors because, by opening one of the doors,
    Monty has given you valuable new information which you can use to increase the
    odds of winning the car. After I worked through the solution and realized I was
    wrong, I took comfort in the fact that this problem had stumped thousands of PhD
    statisticians. It had even baffled the great mathematician Paul Erdos, who was
    only convinced that switching doors was a winning strategy after seeing a simulation
    of the solution. The in-depth analysis of the Monty Hall problem in this chapter
    is my “revenge analysis.”
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳策略是换门，因为蒙特开启其中一扇门后，为你提供了宝贵的新信息，你可以利用这些信息增加赢得汽车的几率。在我分析解决方案并意识到我错了之后，我感到欣慰的是，这个问题曾经困扰过成千上万的博士统计学家。这个问题甚至使伟大的数学家保罗·埃尔德什感到困惑，他只有在看到解决方案的模拟之后才确信换门是一个获胜策略。本章对蒙特霍尔问题的深入分析是我的“复仇分析”。
- en: As the following sidebar explains, there may be psychological reasons why people
    don’t switch doors.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如下面的侧边栏解释的那样，人们为什么不换门可能存在心理上的原因。
- en: Before we do a simulation of this problem, let’s try to figure out a solution
    logically by applying the axioms of probability.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们模拟这个问题之前，让我们尝试通过应用概率公理来逻辑推理出解决方案。
- en: Axioms of Probability
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概率公理
- en: Here is a refresher on the axioms, or fundamental rules, of probability. It
    is simply astonishing that the calculus of probability can be derived entirely
    from the following three axioms and a few definitions.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是概率公理或基本规则的复习。令人惊讶的是，概率的计算可以完全从以下三个公理和几个定义中推导出来。
- en: Say S is any scenario (also known as an event). In general, we define S as the
    scenario in which there is a car behind a door. So, S[1] is the specific scenario
    that the car is behind door 1\. We define S[2] and S[3] similarly. The complement
    of S is S′ (not S) and is the scenario in which there is a goat (not a car) behind
    the door.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 S 是任意一种情况（也称为事件）。通常情况下，我们将 S 定义为汽车在门后的情况。因此，S[1] 是指汽车在门1后面的具体情况。类似地，我们类似地定义了
    S[2] 和 S[3]。S 的补集是 S′（不是 S），表示门后是一只山羊而不是汽车的情况。
- en: Scenarios S and S′ are said to be mutually exclusive, since there is either
    a goat or a car, but not both, behind any given door. Since those are the only
    possible scenarios in this game, S and S′ are also said to be collectively exhaustive
    scenarios or events. The set of all possible scenarios is called the sample space.
    Let’s see how we can apply the rules of probability to the Monty Hall game.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 情景S和S'被称为互斥，因为任何给定门后面要么是山羊要么是车，而不会同时是两者。由于这是游戏中唯一可能的情景，S和S'也被称为完全穷尽的情景或事件。所有可能情景的集合称为样本空间。让我们看看如何将概率规则应用到蒙特霍尔游戏中。
- en: 'Axiom 1: P(S) ≥ 0'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '公理 1: P(S) ≥ 0'
- en: Probability of an event or scenario, P(S), is always assigned a nonnegative
    real number. For instance, when Monty shows us that there is no car behind door
    3, P(S[3]) = 0\. An event probability of 0 means the event is impossible or didn’t
    occur.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 事件或情景的概率P(S)总是被赋予非负实数。例如，当蒙特告诉我们门3后面没有车时，P(S[3]) = 0。事件概率为0意味着事件是不可能的或没有发生。
- en: 'Axiom 2: P(S[1]) + P(S[2]) + P(S[3]) = 1'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '公理 2: P(S[1]) + P(S[2]) + P(S[3]) = 1'
- en: What this axiom says is that we are absolutely certain that at least one of
    the scenarios in the sample space will occur. Note that this axiom implies that
    an event probability of 1 means the event will certainly occur or has already
    occurred. We know from the rules of the Monty Hall game that there is only one
    car and it is behind one of the three doors. This means that the scenarios S[1],
    S[2], and S[3] are mutually exclusive and collectively exhaustive. Therefore,
    P(S[1]) + P(S[2]) + P(S[3]) = 1\. Also note that axioms 1 and 2 ensure that probabilities
    always have a value between 0 and 1, inclusive. Furthermore, P(S[1]) + P(not S[1])
    = 1 implies P(S[1]) = 1 – P(not S[1]).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公理的含义是，我们确信样本空间中至少会发生其中一个情景。请注意，这个公理意味着事件概率为1意味着事件肯定会发生或已经发生。根据蒙特霍尔游戏的规则，我们知道只有一辆车，而它在三扇门中的一扇后面。这意味着情景S[1]、S[2]和S[3]是互斥且完全穷尽的。因此，P(S[1])
    + P(S[2]) + P(S[3]) = 1。此外，请注意公理1和公理2确保概率始终在0到1之间，包括边界值。此外，P(S[1]) + P(not S[1])
    = 1暗示了P(S[1]) = 1 – P(not S[1])。
- en: 'Axiom 3: P(S[2] or S[3]) = P(S[2]) + P(S[3])'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '公理 3: P(S[2] 或 S[3]) = P(S[2]) + P(S[3])'
- en: This axiom is known as the sum rule and enables us to compute probabilities
    of two scenarios that are mutually exclusive. Say we want to know the probability
    that the car is either behind door 2 or door 3, i.e., we want to know P(S[2] or
    S[3]). Since the car cannot be behind door 2 and door 3 simultaneously, S[2] and
    S[3] are mutually exclusive, i.e., P(S[2] and S[3]) = 0\. Therefore, P(S[2] or
    S[3]) = P(S[2]) + P(S[3]).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公理被称为求和规则，使我们能够计算两个互斥情景的概率。比如，我们想知道车可能在门2或门3后面的概率，即我们想知道P(S[2] 或 S[3])。由于车不可能同时在门2和门3后面，S[2]和S[3]是互斥的，即P(S[2]
    和 S[3]) = 0。因此，P(S[2] 或 S[3]) = P(S[2]) + P(S[3])。
- en: Probability Distributions Functions
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概率分布函数
- en: A probability mass function (PMF) provides the probability that a discrete variable
    will have a particular value, such as those computed in the Monty Hall problem.
    A PMF only provides discrete and finite values. A cumulative distribution function
    (CDF) enumerates the probability that a variable is less than or equal to a particular
    value. The values of a CDF are always non-decreasing and between 0 and 1 inclusive.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 概率质量函数（PMF）提供了离散变量将具有特定值的概率，例如在蒙特霍尔问题中计算的值。PMF只提供离散且有限的值。累积分布函数（CDF）列举了变量小于或等于特定值的概率。CDF的值始终是非递减的，并且在0到1之间包括边界值。
- en: A probability density function (PDF) provides the probability that a continuous
    variable will fall within a range of values. A PDF can assume infinitely many
    continuous values. However, a PDF assigns a zero probability to any specific point
    estimate. It might seem surprising that a PDF can be greater than 1 at different
    points in the distribution. That’s because a PDF is the derivative or slope of
    the (CDF) and has no constraint on its value not exceeding 1.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 概率密度函数（PDF）提供了连续变量落入数值范围内的概率。PDF 可以取无限多个连续值。然而，PDF 对于任何具体点估计都分配了零概率。或许令人惊讶的是，PDF
    在分布的不同点处可以大于1。这是因为PDF是（CDF）的导数或斜率，其值没有超过1的约束。
- en: We will apply the axioms of probability to solve the Monty Hall problem. It
    is very important to note that in this book, we don’t make the conventional distinction
    between a deterministic variable and a random variable. This is because we interpret
    probability as a dynamic, extrinsic property of the information about an event,
    which may or may not be repeatable or random. The only distinction we make is
    a commonsensical one between a variable and a constant. Events for which we have
    complete information are treated as constants. All other events are treated as
    variables.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将应用概率公理来解决蒙蒂霍尔问题。非常重要的一点是，在本书中，我们不区分确定性变量和随机变量。这是因为我们将概率解释为关于事件信息的动态、外在属性，这些信息可能是可重复的或随机的。我们唯一区分的是一个变量和一个常数的常识性区分。我们完全了解的事件被视为常数。所有其他事件都被视为变量。
- en: For instance, after Monty places a car behind one of the doors and goats behind
    the other two, there is no randomness associated with what entity lies behind
    which door. All such events are now static and nonrandom for both Monty and his
    audience. However, unlike his audience, Monty is certain where the car is placed.
    For Monty, the probability that the car is behind a specific door is a constant,
    namely 1 for the door he chose to place the car behind and 0 for the other two
    doors he chose to place the goats behind. Since we lack any information about
    the location of the car when the game begins, we can treat it as a variable whose
    value we can update dynamically based on new information. For us, these events
    are not deterministic or random. Our probabilities only reflect our lack of information.
    However, we can apply the calculus of probability theory to estimate and update
    our estimates of where the car has been placed. So let’s try to figure that out,
    without further ado.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当蒙蒂在其中一扇门后放置汽车，另外两扇门后放置山羊后，不存在与哪个实体位于哪个门后相关的随机性。对于蒙蒂及其观众来说，所有这样的事件现在都是静态的和非随机的。然而，与他的观众不同，蒙蒂确切地知道汽车放在哪里。对于蒙蒂来说，汽车在特定门后的概率是一个常数，即他选择放置汽车的门为1，选择放置山羊的另外两扇门的概率为0。由于在游戏开始时我们缺乏关于汽车位置的任何信息，我们可以将其视为一个变量，其值可以根据新信息动态更新。对于我们来说，这些事件既不是确定性的也不是随机的。我们的概率只反映了我们缺乏信息。然而，我们可以应用概率理论的计算来估计和更新我们对汽车放置位置的估计。所以让我们试着弄清楚这一点，不再拖延。
- en: 'Since each scenario is mutually exclusive (there is either a goat or a car
    behind each door) and collectively exhaustive (those are all the possible scenarios),
    their probabilities must add up to 1, since at least one of the following scenarios
    must occur:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每种情况是互斥的（每扇门后要么是山羊要么是汽车）且全面的（这些是所有可能的情况），它们的概率必须相加为1，因为以下至少会发生其中之一的情况：
- en: '*P(S[1]) + P(S[2]) + P(S[3]) = 1*                                                              
    *(Equation 2.1)*'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P(S[1]) + P(S[2]) + P(S[3]) = 1*                                                              
    *(方程式 2.1)*'
- en: 'Before we make a choice, the most plausible assumption is that the car is equally
    likely to be behind any one of the three doors. There is nothing in the rules
    of the game to make us think otherwise, and Monty Hall hasn’t given us any hints
    to the contrary. So it is reasonable to assume that *P(S[1])=P(S[2])=P(S[3])*.
    Using Equation 2.1, we get:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们做出选择之前，最合理的假设是汽车同等可能地在三扇门后。游戏规则中没有任何东西让我们认为不是这样，而蒙蒂·霍尔也没有给我们任何相反的提示。因此，假设*P(S[1])=P(S[2])=P(S[3])*是合理的。使用方程式
    2.1，我们得到：
- en: '*3 × P(S[1]) = 1 or P(S[1]) = ⅓*                                                              *(Equation
    2.2)*'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*3 × P(S[1]) = 1 或 P(S[1]) = ⅓*                                                              *(方程式
    2.2)*'
- en: Since *P(S[1]) = P(S[2]) = P(S[3])*, Equation 2.2 implies that it is logical
    to assume that there is a ⅓ probability that the car is behind one of the three
    doors.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 由于*P(S[1]) = P(S[2]) = P(S[3])*，方程式2.2暗示着合理地假设汽车在三扇门后的概率是1/3。
- en: 'By the sum rule, the probability that the car is behind either door 2 or door
    3 is:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 根据总和规则，汽车在2号门或3号门后的概率是：
- en: '*P(S[2] or S[3]) = P(S[2]) + P(S[3]) = ⅓ + ⅓ = ⅔*                                  
    *(Equation 2.3)*'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P(S[2] 或 S[3]) = P(S[2]) + P(S[3]) = ⅓ + ⅓ = ⅔*                                  
    *(方程式 2.3)*'
- en: 'After you choose door 1 and Monty opens door 3, showing you a goat, P(S[3])
    = 0\. Substituting this value in Equation 2.3 and solving for P(S[2]), we get:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 当你选择1号门，蒙蒂打开3号门，并展示给你一只山羊后，P(S[3]) = 0。将这个值代入方程2.3并解出P(S[2])，我们得到：
- en: '*P(S[2]) = P(S[2] or S[3]) – P(S[3]) = ⅔ – 0 = ⅔*                                    
    *(Equation 2.4)*'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P(S[2]) = P(S[2]或S[3]) – P(S[3]) = ⅔ – 0 = ⅔*                                    
    *(方程式 2.4)*'
- en: 'So switching your choice from door 1 to door 2 doubles your chances of winning
    the car: it goes from ⅓ to ⅔. Switching doors is the optimal betting strategy
    in this game. See [Figure 2-2](#a_simple_logical_solution_to_the_monty).'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，将选择从门1换到门2会使您赢得汽车的几率翻倍：从⅓增加到⅔。在这场游戏中，换门是最优的投注策略。参见[图 2-2](#a_simple_logical_solution_to_the_monty)。
- en: '![A simple logical solution to the Monty Hall problem](assets/pmlf_0202.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![蒙特霍尔问题的一个简单逻辑解](assets/pmlf_0202.png)'
- en: Figure 2-2\. A simple logical solution to the Monty Hall problem^([3](ch02.html#ch02fn3))
  id: totrans-50
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-2。蒙特霍尔问题的一个简单逻辑解^([3](ch02.html#ch02fn3))
- en: It is important to note that because of uncertainty, there is still a ⅓ chance
    that you could lose if you switch doors. In general, randomness of results makes
    it hard and frustrating to determine if your investment or trading strategy is
    a winning one or a lucky one. It is much easier to determine a winning strategy
    in the Monty Hall problem because it can be determined analytically or by simulating
    the game many times, as we will do shortly.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，由于不确定性，如果您换门，仍然有⅓的机会会输。总的来说，结果的随机性使得确定您的投资或交易策略是赢家还是赌运成为困难和令人沮丧的事情。在蒙特霍尔问题中确定一个赢的策略要容易得多，因为可以通过分析或模拟多次进行游戏来确定。
- en: Inverting Probabilities
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反转概率
- en: 'Let’s develop a more rigorous analytical solution to the Monty Hall problem.
    To do that, we need to understand conditional probabilities and how to invert
    them. This is the equivalent of understanding the rules of multiplication and
    division of ordinary numbers. Recall that when we condition a probability, we
    revise the plausibility of a scenario or event by incorporating new information
    from the conditioning data. The conditional probability of a scenario H given
    a conditioning dataset D is represented as P(H|D), which reads as the probability
    of H given D and is defined as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对蒙特霍尔问题开发一个更严格的分析解决方案。为此，我们需要理解条件概率及其反转方法。这相当于理解普通数字的乘法和除法规则。回想一下，当我们对概率进行条件设定时，我们通过将来自条件数据的新信息纳入来修正情景或事件的可能性。给定条件数据集D的情景H的条件概率表示为P(H|D)，它读作给定D时H的概率，并定义如下：
- en: '*P(H|D) = P(H and D) / P(D) provided P(D) ≠ 0 since division by 0 is undefined*'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P(H|D) = P(H和D) / P(D)，前提是P(D) ≠ 0，因为除以0是未定义的*'
- en: The division by P(D) ensures that probabilities of all scenarios conditioned
    on D will add up to 1\. Recall that if two events are independent, their joint
    probabilities are the product of their individual probabilities. That is, P(H
    and D) = P(H) × P(D) if knowledge of D does not improve our probability of H,
    and vice versa.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 通过P(D)的除法确保了在D条件下的所有情景的概率将加起来为1。回想一下，如果两个事件是独立的，则它们的联合概率是它们各自概率的乘积。也就是说，如果D的知识不会提高我们对H的概率，反之亦然。
- en: 'The definition of conditional probability of P given H also implies that P(H
    and D) = P(H|D) × P(D). This is called the product rule. We can now derive the
    inverse probability rule from the product rule. We know from the symmetry of the
    joint probability of two events:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: P给定H的条件概率的定义还意味着P(H和D) = P(H|D) × P(D)。这被称为乘法规则。现在我们可以从乘法规则推导出逆概率规则。我们知道从两个事件的联合概率的对称性：
- en: P(H and D) = P(D and H)
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P(H和D) = P(D和H)
- en: P(H|D) × P(D) = P(D|H) × P(H)
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P(H|D) × P(D) = P(D|H) × P(H)
- en: P(H|D) = P(D|H) × P(H) / P(D)
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P(H|D) = P(D|H) × P(H) / P(D)
- en: And that, ladies and gentlemen, is the proof of the famous and wrongly named
    “Bayes’s theorem.” If only all mathematical proofs were so easy! As you can see,
    this alleged theorem is a trivial reformulation of the product rule. It’s as much
    a theorem as multiplying two numbers and solving for one of them in terms of their
    product (for example, H = H × D/D). The hard part and the insightful bit is interpreting
    and applying the formula to invert probabilities and solve complex, real-world
    problems. Since the 1950s, the previously mentioned formula has also been wrongly
    known as Bayes’s theorem. See the following sidebar.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 亲爱的读者们，这就是著名且错误命名为“贝叶斯定理”的证明。如果所有数学证明都这么简单就好了！正如您所见，这个所谓的定理是乘法规则的一个微不足道的改写。它与将两个数相乘并根据它们的乘积解出一个数的规则一样（例如，H
    = H × D/D）。困难和富有洞察力的部分是解释和应用这个公式来反转概率并解决复杂的现实世界问题。自上世纪50年代以来，先前提到的公式也错误地被称为贝叶斯定理。请参阅以下侧栏。
- en: In this book we will correct this blatant injustice and egregious misnomer by
    referring to the rule by its original name, the inverse probability rule. This
    is what the rule was called for over two centuries before R. A. Fisher referred
    to it pejoratively as Bayes’s rule in the middle of the 20th century. I suspect
    that by attaching the name of an amateur mathematician to an incontrovertible
    mathematical rule, Fisher was able to undermine the inverse probability rule so
    that he could commit the prosecutor’s fallacy with impunity under the pious pretense
    of only “letting the data speak for themselves.” Fisher’s “worse than useless”
    statistical inference methodology will be discussed further in [Chapter 4](ch04.html#the_dangers_of_conventional_statistical).
    Also, in this book, we revert to the original name of the rule since it has a
    longer, authenitic tradition, and the alternative of calling the inverse probability
    rule the Laplace-Bernoulli-Moivre-Bayes-Price rule is way too long.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将通过恢复其原始名称，即逆概率规则，来纠正这种公然的不公正和明显的误称。在20世纪中叶，R. A. Fisher将其蔑称为贝叶斯规则之前，这就是这条规则在两个多世纪中的称呼。我怀疑，通过将一个业余数学家的名字附加到一个无可争议的数学规则上，费舍尔能够在虔诚的借口下不受限制地犯下检察官谬误。费舍尔的“比毫无用处更糟糕”的统计推断方法将在第4章进一步讨论。此外，在本书中，我们恢复规则的原始名称，因为它具有更长、更真实的传统，而将逆概率规则称为拉普拉斯-贝努利-莫伊弗-贝叶斯-普赖斯规则太长了。
- en: Furthermore, we will refer to Bayesian statistics as epistemic statistics and
    Bayesian inference as probabilistic inference. Just as frequentist statistics
    interprets probability as the relative limiting frequency of an event, epistemic
    statistics interprets probability as a property of information about an event.
    Hopefully, this will move us away from wrongly attributing this important scientific
    endeavor and body of knowledge to one person whose contributions to this effort
    may be dubious. In fact, there is no evidence to suggest that Bayes was even a
    Bayesian as the term is used today.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将把贝叶斯统计称为认识论统计学，将贝叶斯推断称为概率推断。正如频率统计学解释概率为事件的相对极限频率一样，认识论统计学将概率解释为关于事件信息的属性。希望这将使我们远离错误地将这一重要的科学努力和知识体系归因于一个可能对此努力的贡献存在疑议的人。事实上，并没有证据表明贝叶斯今天被用作这个术语的贝叶斯。
- en: 'Epistemic statistics in general and the inverse probability rule in particular
    are the foundation of probabilistic machine learning, and we will discuss it in
    depth in the second half of this book. For now, let’s apply it to the Monty Hall
    problem and continue with the same definitions of S[1], S[2], and S[3] and their
    related probabilities. Now we define our dataset D, which includes two observations:
    you choose door 1; and based on your choice of door 1, Monty opens door 3 to show
    you a goat. We want to solve for P(S[2]|D), i.e., the probability that the car
    is behind door 2, given dataset D.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，认识论统计学和特别是逆概率规则是概率机器学习的基础，在本书的第二部分我们将深入讨论它。现在，让我们将其应用于蒙提霍尔问题，并继续使用S[1]、S[2]和S[3]及其相关的概率定义。现在我们定义我们的数据集D，其中包括两个观察结果：您选择门1；根据您选择的门1，蒙提打开门3让您看到一只山羊。我们要求解P(S[2]|D)，即给定数据集D时汽车在门2后面的概率。
- en: 'We know from the inverse probability rule that this equals P(D|S[2]) × P(S[2])/P(D).
    The challenging computation is P(D), which is the unconditional or marginal probability
    of seeing the dataset D regardless of which door the car is behind. The rule of
    total probability allows us to compute marginal probabilities from conditional
    probabilities. Specifically, the rule states that the marginal probability of
    D, P(D), is the weighted average probability of realizing D under different scenarios,
    with P(S) giving us the specific probabilities or weights for each scenario in
    the sample space of S:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 根据逆概率规则，我们知道这等于P(D|S[2]) × P(S[2])/P(D)。挑战性的计算是P(D)，即无条件或边际概率，即观察到数据集D的概率，无论汽车停在哪个门后面。全概率规则允许我们从条件概率计算边际概率。具体而言，该规则指出，D的边际概率P(D)是在不同场景下实现D的加权平均概率，其中P(S)给出了S样本空间中每个场景的具体概率或权重：
- en: '*P(D) = P(D|S[1]) × P(S[1]) + P(D|S[2]) × P(S[2]) + P(D|S[3]) × P(S[3])*'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P(D) = P(D|S[1]) × P(S[1]) + P(D|S[2]) × P(S[2]) + P(D|S[3]) × P(S[3])*'
- en: We have estimated the probabilities of the three scenarios at the beginning
    of the Monty Hall game, namely P(S[1]) = P(S[2]) = P(S[3]) = ⅓. These are going
    to be the weights for each possible scenario. Let’s compute the conditional probabilities
    of observing our dataset D. Note that by P(D|S[1]) we mean the probability of
    seeing the dataset D, given that the car is actually behind door 1 and so on.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在蒙提霍尔游戏开始时，我们估计了三种情况的概率，即 P(S[1]) = P(S[2]) = P(S[3]) = ⅓。这些将是每种可能情况的权重。让我们计算观察到我们的数据集
    D 的条件概率。注意，P(D|S[1]) 意味着在车实际上在门 1 后面的情况下看到数据集 D 的概率，依此类推。
- en: If the car is behind door 1 and you pick door 1, there are goats behind the
    other two doors. So Monty can open either door 2 or door 3 to show you a goat.
    Thus, the probability of Monty opening door 3 to show you a goat, given that you
    chose door 1, is ½, or P(D|S[1]) = ½.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果车在门 1 后面，你选了门 1，其他两扇门后面是山羊。所以蒙提可以打开门 2 或门 3 中的任一扇来给你看一只山羊。因此，蒙提打开门 3 给你看一只山羊的概率，假设你选择了门
    1，是 ½，即 P(D|S[1]) = ½。
- en: If the car is behind door 2 and you have chosen door 1, Monty has no choice
    but to open door 3 to show you a goat. So P(D|S[2]) = 1.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果车在门 2 后面而你选择了门 1，蒙提只能打开门 3 来给你看一只山羊。所以 P(D|S[2]) = 1。
- en: If the car is behind door 3, the probability of Monty opening door 3 is zero,
    since he would ruin the game and you would get the car just for showing up. Therefore,
    P(D|S[3]) = 0.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果车在门 3 后面，蒙提打开门 3 的概率为零，因为他会破坏游戏，而你只是为了露个脸就得到了车。因此，P(D|S[3]) = 0。
- en: 'We plug the numbers into the rule of total probability to calculate the marginal
    or unconditional probability of observing the dataset D in the game:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将数字代入总概率法则中，以计算在游戏中观察到数据集 D 的边际或无条件概率：
- en: '*P(D) = P(D|S[1]) × P(S[1]) + P(D|S[2]) × P(S[2]) + P(D|S[3]) × P(S[3])*'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P(D) = P(D|S[1]) × P(S[1]) + P(D|S[2]) × P(S[2]) + P(D|S[3]) × P(S[3])*'
- en: '*P(D) = [½ × ⅓] + [1 × ⅓ ] + [0 × ⅓ ] = ½*'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P(D) = [½ × ⅓] + [1 × ⅓ ] + [0 × ⅓ ] = ½*'
- en: 'Now we have all the probabilities we need to use in the inverse probability
    rule to calculate the probability that the car is behind door 2, given our dataset
    D:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们拥有了计算车在门 2 后面的概率的所有需要使用的概率，假设我们的数据集 D：
- en: '*P(S[2]|D) = P(D|S[2]) × P(S[2]) / P(D)*'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P(S[2]|D) = P(D|S[2]) × P(S[2]) / P(D)*'
- en: '*P(S[2]|D) = [1 × ⅓ ] / ½ = ⅔*'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P(S[2]|D) = [1 × ⅓ ] / ½ = ⅔*'
- en: 'We can similarly compute the probability that the car is behind door 1, given
    our dataset D:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以类似地计算在给定数据集 D 的情况下车在门 1 后面的概率：
- en: '*P(S[1]|D) = P(D|S[1]) × P(S[1]) / P(D)*'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P(S[1]|D) = P(D|S[1]) × P(S[1]) / P(D)*'
- en: '*P(S[1]|D) = [½ × ⅓] / ½ = ⅓*'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P(S[1]|D) = [½ × ⅓] / ½ = ⅓*'
- en: Clearly, we double our chances by switching, since P(S[2]|D) = 2P(S[1]|D) =
    ⅔. Note that there is still a ⅓ chance that you can win by not switching. But
    like trading and investing, your betting strategy should always put the odds in
    your favor.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，通过换门我们的机会增加了一倍，因为 P(S[2]|D) = 2P(S[1]|D) = ⅔。请注意，仍然有 ⅓ 的机会你不换门就能赢。但是就像交易和投资一样，你的投注策略应该始终让赔率对你有利。
- en: Simulating the Solution
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模拟解决方案
- en: 'Still not convinced? Let’s solve the Monty Hall problem by using a powerful
    numerical method called Monte Carlo simulation (MCS), which we mentioned in the
    previous chapter. This powerful computational method is applied by theoreticians
    and practitioners in almost every field, including business and finance. Recall
    that MCS samples randomly from probability distributions to generate numerous
    probable scenarios of a system whose outcomes are uncertain. It is generally used
    to quantify the uncertainty of model outputs. The following MCS code shows how
    switching doors is the optimal betting strategy for this game if played many times:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 仍然不确定？让我们通过使用一个称为蒙特卡罗模拟（MCS）的强大数值方法来解决蒙提霍尔问题，这在前一章中提到过。这种强大的计算方法几乎应用于每个领域的理论家和实践者，包括商业和金融。回想一下，MCS
    从概率分布中随机抽样，以生成系统的许多可能情景，其结果是不确定的。它通常用于量化模型输出的不确定性。下面的 MCS 代码显示了如果多次玩这个游戏，换门是最佳的投注策略：
- en: '[PRE0]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![Image](assets/pmlf_02in01.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/pmlf_02in01.png)'
- en: As you can see from the results of the simulations, switching doors is the winning
    strategy over the long term. The probabilities are approximately the same as in
    the analytical solution if you play the game ten thousand times. The probabilities
    become almost exactly the same as the analytical solution if you play the game
    over a hundred thousand times. We will explore the theoretical reasons for these
    results in particular, and MCS in general, in the next chapter.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从模拟结果中所见，长期来看换门是获胜的策略。如果您玩游戏一万次，概率与分析解几乎相同。如果您玩游戏超过十万次，概率几乎完全与分析解相同。我们将在下一章节中特别探讨这些结果的理论原因，以及蒙特卡罗模拟在一般情况下的用途。
- en: However, it is not clear from the simulation if switching doors is the right
    strategy in the short term (10 trials), especially if the game is played only
    once. We know that Monty is not going to invite us back to play the game again
    regardless of whether we win or lose the car. So can we even talk about probabilities
    for one-off events? But what do we mean by probabilities anyway? Does everyone
    agree on what it means? Let’s explore that now.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，从模拟中并不清楚在短期（10次试验）内是否换门是正确的策略，特别是如果游戏只玩一次的话。我们知道，无论我们赢还是输汽车，蒙蒂都不会再邀请我们回来玩游戏。那么我们甚至能谈论一次性事件的概率吗？但是我们到底是什么意思？大家对概率的含义是否都同意？让我们现在来探讨这个问题。
- en: Meaning of Probability
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概率的含义
- en: Two major schools of thought have been sparring over the fundamental meaning
    of probability—the very soul of statistical inference—for about a century. The
    two camps disagree on not only the fundamental meaning of probability in those
    axioms we enumerated, but also the methods for applying the axioms consistently
    to make inferences. These core differences have led to the development of divergent
    theories of statistical inference and their implementations in practice.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 两大学派关于概率基本含义的争论已经持续了约一个世纪——这是统计推断的灵魂。这两个阵营不仅在我们列举的公理中对概率的基本含义存在分歧，而且在应用这些公理以保持一致性进行推断的方法上也存在分歧。这些核心差异导致了统计推断理论及其实践中的不同发展。
- en: Frequentist Probability
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 频率主义概率
- en: Statisticians who believe that probability is a natural, immutable property
    of an event or physical object, and that it is measured empirically as a long-run
    relative frequency, are called frequentists. Frequentism is the dominant school
    of the statistics of modern times, in both academic research and industrial applications.
    It is also known as orthodox, classical, or conventional statistics.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 认为概率是事件或物理对象的自然、不变属性，并且概率是作为长期相对频率经验性测量的统计学家被称为频率主义者。频率主义是现代统计学的主流学派，无论是在学术研究还是工业应用中都是如此。它也被称为正统、经典或传统统计学。
- en: Orthodox statisticians claim that probability is a naturally occurring attribute
    of an event or physical phenomenon. The probability of an event should be measured
    empirically by repeating similar experiments ad nauseam—either in reality or hypothetically,
    using one’s imagination or using computer simulations. For instance, if an experiment
    is conducted N times and an event E occurs with a frequency of M times, the relative
    frequency M/N approximates the probability of E. As the number of experimental
    trials N approaches infinity, the probability of E equals M/N. [Figure 2-3](#long_run_relative_frequencies_of_the_su)
    shows a histogram of the long-run relative frequencies of the sum of two fair
    dice rolled many times.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 正统统计学家声称概率是事件或物理现象的自然属性。事件的概率应该通过重复类似的实验来经验性地测量——无论是在现实中还是假设地，可以使用想象力或计算机模拟。例如，如果一个实验重复N次，事件E发生了M次，那么相对频率M/N近似于事件E的概率。随着实验次数N趋向无穷大，事件E的概率等于M/N。[图 2-3](#long_run_relative_frequencies_of_the_su)显示了投掷两个公平骰子多次的长期相对频率的直方图。
- en: '![Long-run relative frequencies of the sum of two fair dice](assets/pmlf_0203.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![两个公平骰子的长期相对频率](assets/pmlf_0203.png)'
- en: Figure 2-3\. Long-run relative frequencies of the sum of two fair dice^([5](ch02.html#ch02fn5))
  id: totrans-92
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-3\. 两个公平骰子的长期相对频率^([5](ch02.html#ch02fn5))
- en: Frequentists consider any alternative interpretation of probability as anathema,
    almost blasphemous. However, their definition of probability is ideological and
    not based on scientific experiments. As will be discussed later in the chapter,
    dice and coins don’t have any static, intrinsic, “true” probabilities. For instance,
    coin tosses are not random but based on the laws of physics and can be predicted
    with 100% accuracy using a mechanical coin flipper. These experiments make a mockery
    of the frequentist definition of probability, which shows an egregious ignorance
    of basic physics. In [Chapter 4](ch04.html#the_dangers_of_conventional_statistical),
    we will examine how the frequentist approach to probability and statistics has
    had a profoundly damaging impact on the theory and practice of social sciences
    in general and economic finance in particular, where the majority of the published
    research using their methods is false.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 频率学家认为概率的任何其他解释都是厌恶的，几乎是亵渎的。然而，他们对概率的定义是意识形态的，不基于科学实验。正如本章后面将讨论的那样，骰子和硬币没有任何静态的、固有的、“真实”的概率。例如，抛硬币并不是随机的，而是基于物理定律，可以用机械硬币翻转器100%准确地预测。这些实验嘲笑了频率学派对概率的定义，显示出对基础物理学的极端无知。在[第4章](ch04.html#the_dangers_of_conventional_statistical)中，我们将探讨频率学派对概率和统计方法的方法如何对社会科学一般以及经济金融特别是其中大多数使用其方法的出版研究造成了深远的损害，它们是错误的。
- en: Epistemic Probability
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**认知概率**'
- en: 'The other important school of thought is popularly and mistakenly known as
    Bayesian statistics. As mentioned earlier, this is an egregious misnomer, and
    in this book we will refer to this interpretation as epistemic probability. Probability
    has a simpler, more intuitive meaning in the epistemic school: it is an extension
    of logic and quantifies the degree of plausibility of the event occurring based
    on the current state of knowledge or ignorance. Epistemic probabilities are dynamic,
    mental constructs that are a function of information about events that may or
    may not be random or repeatable.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的思想流派是广为人知并且错误地称为贝叶斯统计学。如前所述，这是一个极其错误的称呼，在本书中我们将这种解释称为认知概率。在认知学派中，概率有一个更简单、更直观的含义：它是逻辑的延伸，并根据当前的知识或无知量化事件发生的可能性程度。认知概率是动态的、心理建构，是事件信息的函数，这些事件可能是随机的或不可重复的。
- en: Probabilities are updated as more information is acquired using the inverse
    probability rule. Most importantly, the plausibility of an event is expressed
    as a probability distribution as opposed to a point estimate. This quantifies
    the degree of plausibility of various outcomes that can occur given the current
    state of knowledge. Point estimates are avoided as much as possible, given the
    uncertainties endemic in life and business. Also, recall that the probability
    of a point estimate is zero for probability density functions. Probabilistic ML
    is based on this school of thought.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 随着获取更多信息，使用反向概率规则更新概率。最重要的是，事件的可能性表达为概率分布，而不是点估计。在生活和商业中存在的不确定性使点估计尽可能被避免。此外，对于概率密度函数，点估计的概率为零。概率机器学习就是基于这种思想流派。
- en: It is important to note that the epistemic interpretation of probability is
    broad and encompasses the frequentist interpretation of probability as a special
    limiting case. For instance, in the Monty Hall problem, we assumed that it is
    equally likely that a car is behind one of three doors; the epistemic and frequentist
    probabilities are both ⅓. Furthermore, both schools of thought would come to the
    same conclusion that switching doors doubles your probability and is a winning
    strategy. However, the epistemic approach did not need any independent and identically
    distributed trials of the game to estimate the probabilities of the two strategies.
    Similarly, for simple games of chance such as dice and cards, both schools of
    probability give you the same results, but frequentists need to imagine resampling
    the data, or actually conduct simulations.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，概率的认知解释是广泛的，并将频率学派对概率解释作为一个特殊的极限情况。例如，在蒙蒂·霍尔问题中，我们假设汽车可能在三扇门后的任一扇门后，认知和频率概率都是⅓。此外，两种思想流派都得出了同样的结论，即换门可以增加你的概率，并且是一种获胜的策略。然而，认知方法不需要对游戏进行任何独立和相同分布的试验来估计两种策略的概率。同样地，对于诸如骰子和牌的简单游戏，两种概率学派都给出了相同的结果，但频率学派需要想象重新采样数据，或者实际进行模拟。
- en: Over the last century, frequentist ideologues have disparaged and tried to destroy
    the epistemic school of thought in covert and overt ways. Amongst other things,
    they labeled epistemic probabilities subjective, which in science is often a pejorative
    term. All models, especially those in the social and economic sciences, have assumptions
    that are subjective by definition, as they involve making design choices among
    many available options.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的一个世纪里，频率学派的理论信徒们以明示或隐晦的方式贬低并试图摧毁认知学派的思想。在其他事情中，他们将认知概率标记为主观的，在科学中通常是一个贬义词。所有的模型，特别是社会和经济科学中的模型，其假设都是根据定义主观的，因为它们涉及在众多可用选项中进行设计选择。
- en: In finance and investing, subjective probabilities are the norm and are desirable,
    as they may lead to a competitive advantage. Subjective probabilities prevent
    the herding and groupthink that occurs when everyone follows the same “objective”
    inference or prediction about an event. What epistemic statistics will ensure
    is that our subjective probabilities are coherent and consistent with probability
    theory. If we are irrational or incoherent about the subjective probabilities
    underlying our investment strategy, other market participants will exploit these
    inconsistencies and make a Dutch book of bets against us. This concept is the
    sports-betting equivalent of a riskless arbitrage opportunity where we lose money
    on our trades or investments to other market participants no matter what the outcomes
    are.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在金融和投资中，主观概率是常态，并且是可取的，因为它们可能导致竞争优势。主观概率可以防止当每个人都遵循同样“客观”的推断或事件预测时发生的赶羊效应和群体思维。认知统计学将确保我们的主观概率与概率理论一致和一贯。如果我们在投资策略的主观概率上是不理性或不一致的，其他市场参与者将利用这些不一致性，并通过反对我们的交易或投资来进行荷兰式套利。这个概念相当于体育博彩中的无风险套利机会，无论结果如何，我们在交易或投资中都会输给其他市场参与者。
- en: The frequentist theory of statistics has been sold to academia and industry
    as a scientifically rigorous, efficient, robust, and objective school of thought.
    Nothing could be further from the truth. Frequentists use the maximum likelihood
    estimation (MLE) method to learn the optimal values of their model parameters.
    In their fervor to “only let the data speak” and make their inferences bias-free,
    they don’t explicitly apply any prior knowledge or base rates to their inferences.
    They then claim that they have bias-free algorithms that are optimal for any class
    of problems. But the NFL theorems clearly state that this is an impossibility.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 统计学的频率派理论被作为科学严谨、高效、健壮和客观的思想销售给了学术界和工业界。这与事实相去甚远。频率学派使用最大似然估计（MLE）方法来学习他们模型参数的最优值。在他们热衷于“只让数据说话”并使推断无偏的过程中，他们并未明确应用任何先验知识或基础率到他们的推断中。然后他们声称他们拥有无偏算法，适用于任何类别的问题。但是NFL定理清楚地表明这是不可能的。
- en: The NFL theorems prove mathematically that the claim that an algorithm is both
    bias-free and optimal for all problem domains is false. That’s a free lunch, and
    not allowed in ML, search, and optimization. If an algorithm is actually bias-free
    as claimed, the NFL theorems tell us that it will not be optimal for all problem
    domains. It will have high variance on different datasets, and its performance
    will be no better than random guessing when averaged across all target distributions.
    The risk is that it will be worse than random guessing, which is what has occurred
    in the social and economic sciences, where the majority of research findings are
    false (see [Chapter 4](ch04.html#the_dangers_of_conventional_statistical) for
    references). If the frequentist’s defense is that all target distributions are
    not equally likely in this world, they need to realize that any selection of a
    subset of target distributions is a foolish admission of bias, because that involves
    making a subjective choice.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: NFL定理在数学上证明了声称一个算法在所有问题域中既无偏又最优的说法是错误的。这是一个免费的午餐，在机器学习、搜索和优化中是不允许的。如果一个算法实际上是无偏的，NFL定理告诉我们它在不同数据集上会有很高的方差，并且其性能在所有目标分布上的平均表现不会比随机猜测好。风险在于它比随机猜测还要糟糕，这在社会和经济科学中已经发生，那里的大多数研究发现都是错误的（参见[第四章](ch04.html#the_dangers_of_conventional_statistical)的引用）。如果频率学派的辩护是，这个世界上并不是所有的目标分布都是同等可能的，他们需要意识到选择一组目标分布的行为是对偏见的愚蠢承认，因为这涉及到做出主观选择。
- en: But we don’t even have to use the sophisticated mathematics and logic of the
    NFL theorems to expose the deep flaws of the frequentist framework. In [Chapter 4](ch04.html#the_dangers_of_conventional_statistical),
    we will examine why frequentist inference methods are “worse than useless,” because
    they violate the rules of multiplication and division of probabilities (the product
    and inverse probability rules) and use the statistical skullduggery of the prosecutor’s
    fallacy in their hypothesis testing methodology.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，我们甚至不必使用NFL定理的复杂数学和逻辑来揭示频率主义框架的深刻缺陷。在[第四章](ch04.html#the_dangers_of_conventional_statistical)中，我们将探讨频率主义推断方法为什么“不仅无用”，因为它们违反了概率的乘法和除法规则（乘积和逆概率规则），并在其假设检验方法中使用了检察官谬误的统计诡计。
- en: Despite the vigorous efforts of frequentists, epistemic statistics has been
    proven to be theoretically sound and experimentally verified. As a matter of fact,
    it is actually the frequentist version of probability that fails miserably, exposing
    its frailties and ad hoceries when subjected to complex statistical phenomena.
    For instance, the frequentist approach cannot be logically applied to image processing
    and reconstruction, where the sampling distribution of any measurement is always
    constant. Probabilistic algorithms dominate the field of image processing, leveraging
    their broader, epistemic foundation.^([6](ch02.html#ch02fn6))
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管频率主义者竭力努力，认识论统计已被证明在理论上是合理的，并经过实验证实。事实上，当暴露于复杂的统计现象时，频率主义概率实际上是失败的，揭示了其在处理图像处理和重建等领域时的脆弱性和特设性，其中任何测量的抽样分布总是恒定的。概率算法主导着图像处理领域，利用其更广泛的认识论基础。^([6](ch02.html#ch02fn6))
- en: A summary of the differences between the frequentist and epistemic statistics
    have been outlined in [Table 2-1](#summary_of_the_differences_between_freq). Each
    of the differences have been or will be explained in this book and supported by
    plenty of scholarly references.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在[表2-1](#summary_of_the_differences_between_freq)中概述了频率主义和认识论统计之间的差异。这些差异每一个都已经或将在本书中进行解释，并得到了大量学术参考资料的支持。
- en: Table 2-1\. Summary of the differences between frequentist and epistemic statistics
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 表2-1\. 频率主义和认识论统计之间差异的总结
- en: '| Dimension | Frequentist statistics | Epistemic statistics |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 维度 | 频率主义统计学 | 认识论统计学'
- en: '| --- | --- | --- |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Probability | An intrinsic, static property of the long-run relative frequency
    of an event or object. Not supported by physics experiments. | A dynamic, extrinsic
    property of the information about any event, which may or may not be repeatable
    or random. |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 概率 | 事件或对象的长期相对频率的固有静态属性。不受物理实验支持。 | 任何事件信息的动态外在属性，可能是可重复或随机的。 |'
- en: '| Data | Sources of variance that are treated as random variables. Inferences
    don’t work on small datasets. | Known and treated as constants. Size of the dataset
    is irrelevant. |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 数据 | 被视为随机变量的方差来源。推断不能用于小数据集。 | 被视为常数的已知数据。数据集的大小无关紧要。 |'
- en: '| Parameters | Treated as unknown or unknowable constants. | Treated as unknown
    variables. |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 被视为未知或不可知的常数。 | 被视为未知变量。 |'
- en: '| Models | There is one “true” model with optimal parameters that explain the
    data. | There are many explanatory models with varying plausibilities. |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 存在一个解释数据的最佳参数的“真实”模型。 | 存在多个解释模型，其可信度各异。 |'
- en: '| Model types | Discriminative models—only learn a decision boundary. Cannot
    generate new data. | Generative models—learn the underlying structure of the data.
    Simulates new data. |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 模型类型 | 判别模型——只学习决策边界。不能生成新数据。 | 生成模型——学习数据的基本结构。模拟新数据。 |'
- en: '| Model assumptions | Implicit in null hypotheses, significance levels, regularizations,
    and asymptotic limits. | Most important model assumptions are explicitly stated
    and quantified as prior probabilities. |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 模型假设 | 隐含在零假设、显著性水平、正则化和渐近限制中。 | 最重要的模型假设明确陈述，并以先验概率形式量化。 |'
- en: '| Inference method | Maximum likelihood estimation. | Inverse probability rule
    / product rule. |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 推断方法 | 最大似然估计。 | 逆概率法则 / 乘法规则。 |'
- en: '| Hypothesis testing | Null hypothesis significance testing is binary and is
    guilty of the prosecutor’s fallacy. | Degrees of plausibility assigned to many
    different hypotheses. |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 假设检验 | 零假设显著性检验是二元的，并且有检察官谬误。 | 给多个不同假设分配可信度。 |'
- en: '| Uncertainty types | Only deals with aleatory uncertainty. | Deals with aleatory
    and epistemic uncertainties. |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 不确定性类型 | 只处理随机不确定性。 | 处理随机和认识不确定性。 |'
- en: '| Uncertainty quantification | Confidence intervals are epistemologically incoherent
    and mathematically flawed. P-values are guilty of the inverse fallacy. | Credible
    intervals based on logic and common sense. Epistemologically coherent and mathematically
    sound. |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 不确定性量化 | 置信区间在认识论上不一致且在数学上有缺陷。P值犯了反向谬误。 | 基于逻辑和常识的可信区间。在认识论上一致且在数学上合理。 |'
- en: '| Computational complexity | Low. | Medium to high. |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 计算复杂性 | 低。 | 中等到高。 |'
- en: '| No free lunch (NFL) theorems | Violates the NFL theorems. Doesn’t include
    prior knowledge, so algorithms have high variance. When averaged across all possible
    problems, their performance is no better than random guessing. | Consistent with
    NFL theorems. Prior knowledge lowers the variance of algorithms, making them optimal
    for specific problem domains. |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 无免费午餐（NFL）定理 | 违反了无免费午餐定理。不包括先验知识，因此算法具有很高的方差。在所有可能的问题上平均下来，它们的性能不比随机猜测好。
    | 符合无免费午餐定理。先验知识降低了算法的方差，使其在特定问题领域中最优。 |'
- en: '| Scientific methodology | Ideological, unscientific, ad hoc methods under
    a facade of objectivity. Denies the validity of the inverse probability rule /
    product rule. Main cause of false findings in social and economic sciences. |
    Logical and scientific view that systematically integrates prior knowledge based
    on the inverse probability rule / product rule. Explicitly states and quantifies
    all objective and subjective assumptions. |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 科学方法论 | 在客观性表象下的意识形态化、非科学、特定场合下的临时方法。否认反向概率法则/乘法法则的有效性。社会和经济科学中虚假发现的主要原因。
    | 逻辑和科学观点，系统地整合基于反向概率法则/乘法法则的先验知识。明确陈述和量化所有客观和主观假设。 |'
- en: Relative Probability
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相对概率
- en: Are there any objective probabilities in the Monty Hall problem? The car is
    behind door 2, so isn’t the “true” and objective probability of S[2] = 1 a constant?
    Yes, any host in the same position as Monty will assign S[2] = 1, just as any
    participant would assign S[2] = ⅓. However, there is no static, immutable, “true”
    probability of any event in the sense that it has an ontological existence in
    this game independent of the actions of humans. If Monty has the car placed behind
    door 1, S[2] = 0 for him but remains constant at ⅓ for any participant.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 蒙提霍尔问题中是否存在客观概率？汽车在2号门后面，所以 S[2] = 1 的“真实”和客观概率不是一个常数吗？是的，任何与蒙提相同位置的主持人将会分配
    S[2] = 1，就像任何参与者会分配 S[2] = ⅓ 一样。然而，在这个游戏中，没有任何事件的静态、不可变的“真实”概率，即独立于人类行为的本体存在。如果蒙提将汽车放在1号门后面，对他来说
    S[2] = 0，但对任何参与者来说，仍然恒定为 ⅓。
- en: Probabilities depend on the model used, the phase of the game, and the information
    available to the participant or the host. As noted previously, probabilities for
    any participant or host are not subjective but a function of the information they
    have, since any host and any reasonable participant would arrive at the same probabilities
    using basic probability theory or common sense. Probability is a mental construct
    used to quantify uncertainties dynamically.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 概率取决于使用的模型、游戏的阶段以及参与者或主持人可用的信息。如前所述，任何参与者或主持人的概率不是主观的，而是他们拥有的信息的函数，因为任何主持人和任何合理的参与者都会使用基本概率理论或常识得出相同的概率。概率是用来动态量化不确定性的心理构造。
- en: It is analogous to the physics of special relativity, which have been experimentally
    verified since Albert Einstein published his monumental paper in 1905\. The principle
    of relativity states that the laws of physics are invariant across all frames
    of reference that are not accelerating relative to one another. Two observers
    sharing the same frame of reference will always agree on fundamental measurements
    of mass, length, and time. However, they will have different measurements of these
    quantities depending on how their frames of reference are moving relative to one
    another and compared to the speed of light. The principle of relativity has many
    important implications, including the fact that there is no such thing as absolute
    motion. Motion is always relative to a frame of reference.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 它类似于特殊相对论的物理学，自从阿尔伯特·爱因斯坦在1905年发表其重要论文以来，已经通过实验证实。相对论原理表明，物理定律在所有不相对加速的参考系中不变。共享同一参考系的两个观察者将始终就质量、长度和时间的基本测量达成一致。然而，根据他们的参考系如何相对运动及其相对于光速的比较，他们对这些量的测量将有所不同。相对论原理有许多重要的含义，包括没有绝对运动的事实。运动总是相对于某个参考系的。
- en: '[Figure 2-4](#the_principle_of_relativity_using_two_f) shows two frames of
    reference, with the primed coordinate system moving at a constant velocity relative
    to the unprimed coordinate system. Observers O and O′ will agree that the same
    laws of physics apply in their worlds. However, observers O will claim that they
    are stationary and that observers O′ are moving forward along the x-axis at a
    constant velocity (+v). But observers O′ will claim that they are stationary and
    it is observers O that are moving backward along the x''-axis at a constant velocity
    (–v). This means that there is no way to say definitively whether O or O′ is really
    moving. They are both moving relative to each other, but neither one is moving
    in an absolute sense.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2-4](#the_principle_of_relativity_using_two_f) 显示了两个参考系，其中带有撇号的坐标系相对于不带撇号的坐标系以恒定速度运动。观察者
    O 和 O′ 将同意物理法则适用于他们的世界。然而，观察者 O 将声称他们静止不动，观察者 O′ 则声称他们沿着 x 轴正向以恒定速度 (+v) 运动。但观察者
    O′ 将声称他们静止不动，而观察者 O 则是在 x′ 轴负向以恒定速度 (–v) 运动。这意味着无法明确地说 O 还是 O′ 真的在运动。他们彼此相对运动，但在绝对意义上，两者都没有运动。'
- en: '![The principle of relativity using two frames of reference moving at a constant
    velocity relative to each other](assets/pmlf_0204.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![两个相对运动的参考系中的相对论原理](assets/pmlf_0204.png)'
- en: Figure 2-4\. The principle of relativity using two frames of reference moving
    at a constant velocity relative to each other^([7](ch02.html#ch02fn7))
  id: totrans-127
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-4\. 两个相对运动的参考系中的相对论原理^([7](ch02.html#ch02fn7))
- en: I find it useful to think in terms of relative probabilities based on an observer’s
    frame of reference or access to information as opposed to objective or subjective
    probabilities. Regardless, probability theory works in all frames of reference
    we are concerned with. We need it to quantify the profound uncertainties that
    we have to deal with in our daily personal and professional lives. But before
    we quantify uncertainty, we need to understand it in some depth.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我发现根据观察者的参考系或信息获取情况来思考相对概率比绝对或主观概率更有用。不管怎样，我们都需要概率论来处理我们日常个人和专业生活中不得不面对的深刻不确定性。但在量化不确定性之前，我们需要对其有相当深入的理解。
- en: 'Risk Versus Uncertainty: A Useless Distinction'
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 风险与不确定性：一个毫无意义的区分
- en: In conventional statistics and economics literature, a vacuous distinction is
    generally made between risk and uncertainty. Supposedly, risk can only be estimated
    for events that are known and have objective probability distributions and parameters
    associated with them. Probabilities and payoffs can be estimated accurately, and
    risk is computed using various metrics. When there are no objective probability
    distributions or if events are unknown or unknowable, the event is described as
    uncertain and the claim is made that risks cannot be estimated.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的统计学和经济学文献中，通常会将风险和不确定性进行空洞的区分。据称，只能对已知且具有客观概率分布和相关参数的事件进行风险估计。可以准确估计概率和回报，并使用各种指标计算风险。当不存在客观概率分布或者事件是未知或无法预测时，就描述该事件为不确定性，并声称风险无法估计。
- en: In finance and investing, we are not dealing with simple games of chance, such
    as casino games, where the players, rules, and probability distributions are fixed
    and known. Both product and financial markets are quite different from such simple
    games of chance, where event risks can be estimated accurately. As was discussed
    in the previous chapter, unknown market participants may use different probability
    distributions in their models based on their own strategies and assumptions. Even
    for popular, consensus statistical distributions, there is no agreement about
    parameter estimates. Furthermore, because markets are not stationary ergodic,
    these probability distributions and their parameters are continually changing,
    sometimes abruptly, making a mockery of everyone’s estimates and predictions.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在金融和投资中，我们不是在处理像赌场游戏这样的简单游戏，其中玩家、规则和概率分布都是固定和已知的。产品市场和金融市场与这类简单的游戏有很大不同，其中事件风险可以被准确估计。正如前一章讨论的那样，未知的市场参与者可能根据他们自己的策略和假设使用不同的概率分布模型。即使对于流行的、共识的统计分布，也不存在关于参数估计的一致性。此外，由于市场不是平稳遍历的，这些概率分布及其参数正在不断变化，有时会突然变化，从而使每个人的估计和预测都成为笑柄。
- en: So, based on the conventional definition of risk and uncertainty, almost all
    investing and finance is uncertain. In practice, this is a useless distinction
    and stems from useless frequentist statistics and neoclassical economics ideologies
    about objective, academic models, not from the realities of market participants.
    As practitioners, we develop our own subjective models based on our experience,
    expertise, institutional knowledge, and judgment. As a matter of fact, we protect
    our proprietary, subjective models assiduously, since sharing them with the public
    would undermine our competitive advantages.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，基于风险和不确定性的传统定义，几乎所有的投资和金融都是不确定的。在实践中，这是一个无用的区分，源自无用的频率主义统计学和新古典经济学关于客观、学术模型的意识形态，而非市场参与者的现实。作为从业者，我们根据我们的经验、专业知识、机构知识和判断力发展我们自己的主观模型。事实上，我们极为谨慎地保护我们的专有、主观模型，因为与公众分享它们将削弱我们的竞争优势。
- en: Edward Thorp, the greatest quantitative gambler and trader of all time, invented
    an options pricing model much before Fischer Black and Myron Scholes published
    their famous “objective” model in 1973\. Since Thorp’s model was a trade secret
    of his hedge fund, he owed it to his investors not to share his model with the
    general public and fritter away his company’s competitive advantage. Thorp applied
    his subjective, numerical, proprietary options model to generate one of the best
    risk-adjusted returns in history. Black and Scholes applied their “objective,”
    analytical options pricing model to real markets only to experience near financial
    ruin and make a hasty retreat to the refuge of the ivory towers of academia.^([8](ch02.html#ch02fn8))
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Edward Thorp，有史以来最伟大的量化赌徒和交易员，在Fischer Black和Myron Scholes 1973年发表其著名的“客观”模型之前，就发明了一种期权定价模型。由于Thorp的模型是他的对冲基金的商业秘密，他有责任向投资者保密，不与公众分享他的模型，以保持公司的竞争优势。Thorp运用他的主观、数值的专有期权模型，创造了历史上最佳的风险调整回报之一。Black和Scholes应用他们的“客观”、分析的期权定价模型到实际市场中，结果几近破产，匆忙撤退到学术象牙塔的庇护所。
- en: Options market makers and derivatives traders like me generally modify the “objective”
    Black-Scholes pricing model in different ways to correct for its many deep flaws.
    In doing so, we make our options trading models subjective and proprietary. Most
    importantly, we make it useful for successfully trading these complex markets.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 期权市场做市商和衍生品交易员像我一样，通常会以不同的方式修改“客观”的Black-Scholes定价模型，以纠正其许多深层次的缺陷。通过这样做，我们使我们的期权交易模型变得主观和专有。最重要的是，我们使其在成功交易这些复杂市场中变得有用。
- en: The real value of the “objective” Black-Scholes options pricing model is clearly
    not in its accurate pricing of options. It’s common knowledge among academics
    and practitioners alike that it’s not accurate, especially since it erroneously
    treats volatility of the underlying asset as a constant. A classic joke about
    the Black-Scholes model is that you have to put “the wrong number in the wrong
    formula to get the right price.”
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: “客观”的Black-Scholes期权定价模型的真正价值显然不在于其对期权的准确定价。学术界和从业者都普遍认为，它并不准确，尤其是因为错误地将基础资产的波动性视为恒定的。关于Black-Scholes模型的一个经典笑话是，你必须在“错误的公式里输入错误的数字才能得到正确的价格。”
- en: The real value-add of the Black-Scholes model, explaining its enduring popularity
    among practitioners, is in its enabling communication among market participants
    who are generally using their own disparate, proprietary options pricing models.
    It is ironic that despite the Black-Scholes model’s fictitious assumptions and
    market fantasies, it has contributed significantly to the rapid growth of real-world
    options markets. Humans are suckers for good works of fiction in any format. Perhaps
    Black and Scholes should have been awarded a real Nobel Prize in literature.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: Black-Scholes模型的真正增值，在解释其在从业者中持久流行的原因时，是其促进市场参与者之间的沟通，这些参与者通常使用自己不同的专有期权定价模型。具有讽刺意味的是，尽管Black-Scholes模型有虚构的假设和市场幻想，但它在实际期权市场的快速增长中做出了显著贡献。人类对任何格式的优秀虚构作品都情有独钟。也许Black和Scholes应该被授予真正的诺贝尔文学奖。
- en: In epistemic statistics, probabilities are an extension of logic and can be
    assigned to any uncertain event—known, unknown, and unknowable. We do this by
    rejecting point estimates and setting the bar extremely high for assuming any
    event to be a certainty (probability = 1) or an impossibility (probability = 0).
    That’s why in epistemic statistics we deal only with probability distributions.
    Unknowable events are acknowledged by using fat-tailed probability distributions
    like the Cauchy distribution, which has no defined mean or variance, reflecting
    the fact that almost anything is possible during the holding periods of our trades
    and investments.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在认识论统计学中，概率是逻辑的延伸，并且可以分配给任何不确定事件——已知的、未知的和不可知的。我们通过拒绝点估计，并对任何事件被认为是确定性（概率=1）或不可能性（概率=0）设定极高的标准来做到这一点。这就是为什么在认识论统计学中，我们只处理概率分布。通过使用像柯西分布这样的厚尾概率分布来承认不可知事件，该分布没有定义的均值或方差，反映了在我们的交易和投资持有期间几乎任何事情都是可能发生的事实。
- en: Probability estimates are based on our prior knowledge, observed data, and expertise
    in making such estimates. But most importantly, they depend on human judgment,
    common sense, and an understanding of causation, which AI systems are incapable
    of processing. The degree of confidence we have in our estimates and forecasts
    will vary depending on many factors, including the nature of the event, the sources
    of uncertainty, our resources, and our abilities to perform such tasks.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 概率估计基于我们的先验知识、观察数据和在进行这类估计时的专业知识。但更重要的是，它们依赖于人类的判断、常识和对因果关系的理解，而这些是AI系统无法处理的。我们对我们的估计和预测的信心程度将根据许多因素而变化，包括事件的性质、不确定性的来源、我们的资源以及我们执行此类任务的能力。
- en: In finance and investing, we don’t have the luxury of not undertaking such imperfect,
    messy statistical endeavors. We do it knowing full well that these difficult exercises
    are rife with approximations, riddled with potential errors, and susceptible to
    the ravages and ridicule of markets. Dwight Eisenhower, former US general and
    president, explained the value of such exercises when he said, “In preparing for
    battle I have always found that plans are useless, but planning is indispensable.”^([9](ch02.html#ch02fn9))
    The alternative of forsaking such statistical exercises by taking comfort in some
    useless definition of risk and uncertainty is even worse. The worst course of
    action is to be lulled into a false sense of security by some economic ideology
    of objective statistical models or normative theory of human behavior and rationality
    that have no basis in data and the experienced realities of the world.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在金融和投资领域，我们没有奢侈品不进行这种不完美、混乱的统计尝试。我们这样做是充分了解到这些困难的练习充满了近似，充斥着潜在的错误，并且容易受到市场的破坏和嘲笑。德怀特·艾森豪威尔，前美国将军和总统，在解释这些练习的价值时说：“在准备战斗时，我始终发现计划是无用的，但规划是不可或缺的。”^([9](ch02.html#ch02fn9))
    通过对风险和不确定性进行某些无用的定义来放弃这种统计练习的选择甚至更糟糕。最糟糕的行动是被某些经济意识形态或对人类行为和理性的客观统计模型或规范理论的错误安全感所麻痹，这些理论没有基础于数据和世界的实际经验现实。
- en: We reject such useless distinctions between risk and uncertainty. All uncertain
    events are logically and realistically plausible based on an appropriate probability
    distribution and boundary conditions. We know that all models are wrong, including
    the useful ones, and do not pledge any fealty to these shadows of reality.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们拒绝在风险和不确定性之间进行这种无用的区分。根据适当的概率分布和边界条件，所有不确定事件在逻辑上和现实上都是合理的。我们知道所有的模型都是错误的，包括有用的模型，并且不对这些现实的影子效忠。
- en: The Trinity of Uncertainty
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不确定性的三位一体
- en: 'Uncertainty is generally classified into three types based on the source from
    which it arises: aleatory, epistemic, and ontological. These are complex concepts
    that philosophers and scientists worldwide have endeavored to understand and apply
    for millennia. Let’s see how we can use the Monty Hall problem to understand the
    complexities of this trinity of uncertainty. Later, we apply each type of uncertainty
    to various aspects of machine learning that we are faced with in practice.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 不确定性通常根据其来源分为三种类型：伪随机、认知和本体。这些是哲学家和科学家们几千年来努力理解和应用的复杂概念。让我们看看如何使用蒙提·霍尔问题来理解这三种不确定性的复杂性。稍后，我们将这些不确定性类型应用到实际中机器学习的各个方面。
- en: Aleatory Uncertainty
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 伪随机不确定性
- en: Aleatory means “of dice” in Latin. Fundamentally, aleatory uncertainty is the
    irreducible randomness of outcomes. Both the analytical and simulated solutions
    to the Monty Hall problem demonstrated that your strategy of staying or switching
    doors in this game does not guarantee you a win during a single play, or even
    multiple plays, of the game. You could stay with your original choice of door
    1 and have a ⅓ chance of winning the car. You could switch to door 2 and have
    a ⅓ chance of losing the car. Whenever you play the game, you are indeed rolling
    the proverbial dice, since the outcome is uncertain.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: “Aleatory”在拉丁语中意味着“掷骰子”。从根本上讲，伪随机不确定性是结果的不可减少的随机性。蒙提·霍尔问题的分析和模拟解决方案都表明，在这个游戏中，无论你选择保持或换门，单次玩游戏或多次玩游戏都不能保证你赢得汽车。你可以坚持最初选择的1号门，有1/3的几率赢得汽车；或者你可以换到2号门，有1/3的几率输掉汽车。每当你玩这个游戏时，你确实在掷象征性的骰子，因为结果是不确定的。
- en: Actually, it’s more uncertain than rolling dice or tossing a coin, since they
    both have no aleatory uncertainty, only epistemic uncertainty, as explained in
    the next section. Tossing a coin is a canonical example of aleatory uncertainty
    in the current literature on probability and statistics. However, this shows an
    inexcusable ignorance of the laws of classical physics. It has been experimentally
    verified that if you know the initial conditions and other parameters of a coin
    toss, you can predict its outcome with 100% accuracy. That’s because coin tossing
    is physics, not randomness.^([10](ch02.html#ch02fn10))
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，与掷骰子或抛硬币相比，硬币投掷更不确定，因为它们都没有伪随机不确定性，只有认知不确定性，正如下一节所解释的那样。在当前关于概率和统计学的文献中，抛硬币是伪随机不确定性的一个典型例子。然而，这显示了对古典物理定律的无可辩解的无知。已经实验证明，如果你知道硬币投掷的初始条件和其他参数，你可以以100%的准确率预测其结果。这是因为硬币投掷是物理学，不是随机性。^([10](ch02.html#ch02fn10))
- en: Statistician and former magician Persi Diaconis had engineers build him a mechanical
    coin flipper so that he could experiment and study coin tossing. Indeed, he and
    his colleagues verified that there is no randomness in a coin toss with the mechanical
    coin flipper.^([11](ch02.html#ch02fn11)) The randomness of a coin toss arises
    from the inconsistency of initial conditions of human coin flipping and from the
    coin rolling on the ground.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 统计学家和前魔术师佩尔西·迪亚康尼斯（Persi Diaconis）让工程师们为他建造了一个机械硬币投掷器，以便他可以进行实验和研究硬币投掷。事实上，他和他的同事们验证了使用机械硬币投掷器进行硬币投掷时没有随机性。^([11](ch02.html#ch02fn11))
    硬币投掷的随机性来自于人类投掷硬币时初始条件的不一致性以及硬币在地面上滚动的情况。
- en: The uncertainty we observe stems from our lack of precise information or knowledge
    of the physics of the tosses. It is a bad example of aleatory uncertainty. It
    also demonstrates that coins don’t have any intrinsic, immutable, limiting frequency,
    as frequentists will have us believe. You can use the physics of the coin toss
    to make a biased coin honest and vice versa with some practice.^([12](ch02.html#ch02fn12))
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到的不确定性源于我们对投掷物理学的精确信息或知识的缺乏。这是伪随机不确定性的一个糟糕示例。它也表明，硬币没有任何固有的、不可变的、限制频率，正如频率学派想让我们相信的那样。你可以利用硬币投掷的物理学使有偏的硬币变得公正，反之亦然，只需一些练习。^([12](ch02.html#ch02fn12))
- en: Tossing coins and rolling dice are examples of epistemic uncertainty, which
    we will discuss in the next subsection. In contrast to coins or dice, no amount
    of information about the physical characteristics of the doors or their motion
    will reduce the aleatory uncertainty of where the car is in the Monty Hall problem.
    It is a great example of aleatory uncertainty, and why social systems are fundamentally
    different and much harder to predict than physical systems.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 抛硬币和掷骰子是认知不确定性的例子，我们将在下一小节讨论。与硬币或骰子相反，无论对门的物理特性或它们的运动有多少信息，都无法减少蒙蒂·霍尔问题中汽车位置的
    aleatory uncertainty。这是 aleatory uncertainty 的一个很好的例子，也是为什么社会系统与物理系统根本不同且更难预测的原因。
- en: 'In machine learning (ML), aleatory uncertainty is the source of irreducible
    error and is generated because of data. It sets the lower bound on the generalization
    error that can be achieved by any ML model. This endemic noise is generated in
    two distinct ways:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习（ML）中，**aleatory uncertainty** 是由数据生成的不可消除误差的源头。它设定了任何 ML 模型能够达到的泛化误差的下界。这种普遍存在的噪音以两种不同的方式生成：
- en: Measurement uncertainty
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 测量不确定性
- en: It is not always possible to measure data with complete accuracy. For instance,
    when there is high market volatility due to an event, such as the release of an
    economic report, it is almost impossible to capture every transaction or tick
    in real time, leading to missing or delayed tick data. Similarly, data transmission
    errors can lead to missing or corrupt tick data.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 并非总能完全精确地测量数据。例如，当市场因某一事件（如经济报告发布）出现高波动性时，几乎不可能实时捕捉每笔交易或每一次价格变动，导致缺失或延迟的交易数据。同样，数据传输错误可能导致缺失或损坏的价格数据。
- en: Sampling uncertainty
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 抽样不确定性
- en: Every time we take a random data sample from across a population at a particular
    time, or sample a stochastic process at different times, the sample statistics,
    such as mean and variance, will vary from sample to sample. This type of aleatory
    uncertainty is due to the inherent randomness of the sampling process itself and
    the variability of the underlying statistical distribution. For example, consumer
    sentiment surveys taken by different companies result in different statistical
    estimates. Also, the variance of a stock’s price returns also changes over time.
    [Figure 2-5](#a_random_data_sample_taken_from_a_popul) shows how a specific random
    data sample is taken from the population to estimate the population mean and variance.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 每当我们从人口中的随机数据样本，或在不同时间抽样随机过程时，样本统计数据（如均值和方差）会因样本而异。这种 aleatory uncertainty 类型是由抽样过程本身的固有随机性以及基础统计分布的可变性所导致的。例如，由不同公司进行的消费者情绪调查会产生不同的统计估计。此外，股票价格回报的方差也随时间变化。[图 2-5](#a_random_data_sample_taken_from_a_popul)
    显示了如何从总体中获取特定的随机数据样本以估计总体的均值和方差。
- en: '![Sampling uncertainty because a random sample is drawn to estimate the statistical
    properties of its population](assets/pmlf_0205.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![因为随机抽样用于估计其总体的统计特性，所以存在抽样不确定性](assets/pmlf_0205.png)'
- en: Figure 2-5\. Sampling uncertainty because a random sample is drawn to estimate
    the statistical properties of its population^([13](ch02.html#ch02fn13))
  id: totrans-155
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-5\. 因为随机抽样用于估计其总体的统计特性^([13](ch02.html#ch02fn13))
- en: Epistemic Uncertainty
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 认知不确定性
- en: '*Episteme* means “knowledge” in Greek. The epistemic uncertainty of any scenario
    depends on the state of knowledge or ignorance of the person confronting it. Unlike
    aleatory uncertainty, you can reduce epistemic uncertainty by acquiring more knowledge
    and understanding. When Monty opens a door to show you a goat, he is providing
    you with very valuable new information that reduces your epistemic uncertainty.
    Based on this information from Monty’s response to your choice, the probability
    of door 1 having a car behind it remained unchanged at ⅓, but the probability
    for door 2 changed from ⅓ to ⅔, and the probability for door 3 changed from ⅓
    to 0.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '*Episteme* 在希腊语中意为“知识”。任何情境的认知不确定性取决于面对其的个体的知识或无知状态。与 aleatory uncertainty
    不同，你可以通过获取更多的知识和理解来减少 epistemic uncertainty。当蒙蒂打开一扇门展示给你一只山羊时，他为你提供了非常宝贵的新信息，从而降低了你的认知不确定性。基于蒙蒂对你选择的反应，门1
    背后有汽车的概率保持不变为 1/3，但门2 的概率从 1/3 变为 2/3，门3 的概率从 1/3 变为 0。'
- en: 'However, there is no uncertainty for Monty regarding which door the car is
    behind: his probability for each door is either 1 or 0 at all times. He is only
    uncertain about which door you are going to pick. Also, once you pick any door,
    he is certain what he is going to do next. But he is uncertain what you will do
    when offered the deal. Will you stay with your original choice of door 1 or switch
    to door 2, and most likely win the car? Monty’s uncertainties are not epistemic
    but ontological, a fundamentally different nature of uncertainty, which we will
    discuss in the next subsection.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Monty 对于汽车在哪扇门后面没有任何不确定性：他对每扇门的概率始终是 1 或 0。他只对你会选哪扇门感到不确定。而且，一旦你选择了任何一扇门，他就确定接下来会做什么。但是，他不确定你在面对选择时会怎么做。你会留在最初选择的门
    1 还是换到门 2，然后很可能赢得汽车？Monty 的不确定性不是认识论的，而是本体论的，这是一种根本不同的不确定性性质，我们将在下一小节中讨论。
- en: So we can see from this game that the uncertainty of picking the right door
    for you is a function of one’s state of knowledge or “episteme.” It is important
    to note that this is not a subjective belief but a function of information or
    lack of it. Any participant and any host would have the same uncertainties calculated
    earlier in this chapter, and switching doors would still be the winning strategy.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们可以从这个游戏中看到，为了为您选择正确的门而存在的不确定性是一个知识状态或“认识论”的功能。重要的是要注意，这不是主观信念，而是信息或其缺乏的功能。本章早期的任何参与者和任何主持人都将计算出相同的不确定性，而换门仍然是获胜策略。
- en: Note
  id: totrans-160
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: This game is also an example of the *asymmetry of information* that characterizes
    financial deals and markets. Generally speaking, parties to a deal always have
    access to differing amounts of information about various aspects of a deal or
    asset, which leads to uncertainty in their price estimates and deal outcomes.
    Different information processing capabilities and speeds further exacerbate those
    uncertainties.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这个游戏也是描述金融交易和市场的信息不对称性的一个例子。一般来说，交易方总是可以获得关于交易或资产各个方面的不同数量的信息，这导致他们在价格估计和交易结果上存在不确定性。不同的信息处理能力和速度进一步加剧了这些不确定性。
- en: There are many sources of epistemic uncertainty in ML arising from a lack of
    access to knowledge and understanding of the underlying phenomenon.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中存在许多认识论不确定性的来源，这源于对底层现象的知识和理解的缺乏。
- en: 'They can be categorized in the following ways:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 它们可以按以下方式分类：
- en: Data uncertainty
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 数据不确定性
- en: To make valid inferences, we want our data sample to represent the underlying
    population or data-generating process. For instance, auditors sample a subset
    of a company’s transactions or financial records during a particular time period
    to check for compliance with accounting standards. The auditor may fail to detect
    errors or fraud if the sample is unrepresentative of the population of transactions.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行有效推断，我们希望我们的数据样本能够代表底层人口或数据生成过程。例如，审计师在特定时间段内对公司的交易或财务记录的子集进行抽样，以检查是否符合会计准则。如果样本不代表交易的总体，则审计员可能无法检测到错误或欺诈。
- en: Model uncertainty
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 模型不确定性
- en: There is always uncertainty about which model to choose to make inferences and
    predictions. For example, when making financial forecasts, should we use linear
    regression or nonlinear regression models? Or a neural network? Or some other
    model?
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 选择哪种模型来进行推断和预测总是存在不确定性。例如，在进行财务预测时，我们应该使用线性回归还是非线性回归模型？还是神经网络？或者其他一些模型？
- en: Feature uncertainty
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 特征不确定性
- en: Assume we pick a linear model as our first approximation and baseline model
    for financial forecasting. What features are we going to select to make our inferences
    and forecasts? Why did we select those features and leave out others? How many
    are required?
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们选择线性模型作为我们的第一个近似和财务预测的基线模型。我们将选择哪些特征来进行推断和预测？我们为什么选择这些特征而排除其他特征？需要多少个特征？
- en: Algorithmic uncertainty
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 算法不确定性
- en: Now that we have selected features for our linear model, what linear algorithm
    should we use to train the model and learn its parameters? Will it be ridge regression,
    lasso regression, support vector machines, or a probabilistic linear regression
    algorithm?
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经为线性模型选择了特征，接下来应该使用什么线性算法来训练模型并学习其参数呢？岭回归、套索回归、支持向量机还是概率线性回归算法？
- en: Parameter uncertainty
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 参数不确定性
- en: Say we decide to use a market model and a probabilistic linear regression algorithm
    as our baseline model. What probability distributions are we going to assign each
    parameter? Or are the parameters going to be point estimates? What about the hyperparameters—that
    is, parameters of the probability distributions of parameters? Are they going
    to be probability distributions or point estimates?
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们决定使用市场模型和概率线性回归算法作为我们的基准模型。我们将为每个参数分配什么概率分布？或者这些参数将是点估计？超参数呢——即参数的概率分布，它们将是概率分布还是点估计？
- en: Method uncertainty
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 方法不确定性
- en: What numerical method are we going to use in our model to learn its parameters
    from in-sample data? Will we use the Markov chain Monte Carlo (MCMC) method or
    variational inference? A Metropolis sampling method or Hamiltonian Monte Carlo
    sampling method? What values are we going to use for the parameters of the chosen
    numerical methods? How do we justify those parameter values?
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的模型中，我们将使用什么数值方法从样本数据中学习其参数？我们将使用马尔可夫链蒙特卡洛（MCMC）方法还是变分推断？Metropolis 抽样方法还是
    Hamiltonian Monte Carlo 抽样方法？我们将使用哪些值来设置所选数值方法的参数？我们如何证明这些参数值的合理性？
- en: Implementation uncertainty
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 实施不确定性
- en: Assume we decide on using the MCMC method. Which software should we use to implement
    it? PyMC, Pyro, TensorFlow Probability, or Stan? Or are we going to build everything
    from scratch using Python? What about R or C++?
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们决定使用 MCMC 方法。我们应该使用哪个软件来实现它？PyMC、Pyro、TensorFlow Probability 还是 Stan？或者我们将使用
    Python 从头开始构建？R 或者 C++ 呢？
- en: As the previous discussion shows, designing ML models involves making choices
    about the objective function, data sample, model, algorithm, and computational
    resources, among many others. As was mentioned in the previous chapter, our goal
    is to train our ML system so that it minimizes out-of-sample generalization errors
    that are reducible.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面的讨论所示，设计机器学习模型涉及对目标函数、数据样本、模型、算法和计算资源等的选择。正如前一章提到的，我们的目标是训练我们的 ML 系统，以使其最小化可归纳泛化错误。
- en: If we have prior knowledge about the problem domain, we might develop a simple
    system with few parameters because of such knowledge and assumptions. This is
    referred to as bias in ML. The risk is that our prior assumptions of the model
    may be erroneous, leading it to underfit the training data systematically and
    learn no new patterns or signals from it. Consequently, the model is exposed to
    bias errors and performs poorly on unseen test data. On the other hand, if we
    don’t have prior knowledge about the problem domain, we might build a complex
    model with many parameters to adapt and learn as much as possible from the training
    data. The risk there is that the model overfits the training data and learns the
    spurious correlations (noise) as well. This result is that the model introduces
    errors in its predictions and inferences due to minor variations in the data.
    These errors are referred to as variance errors and the model performs poorly
    on unseen test data. [Figure 2-6](#the_bias_variance_trade_off_that_needs) shows
    the bias-variance trade-off that needs to made in developing models that minimize
    reducible generalization errors. This trade-off is made more difficult and dynamic
    when the underlying data distributions are not stationary ergodic, as they are
    in finance and investing problems.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们对问题域有先验知识，我们可能会开发一个简单的系统，因为这些知识和假设。这被称为 ML 中的偏差。风险在于，我们对模型的先验假设可能是错误的，导致它在系统地欠拟合训练数据并未从中学习新的模式或信号。因此，模型暴露于偏差误差，并在未看见的测试数据上表现不佳。另一方面，如果我们对问题域没有先验知识，我们可能会构建一个参数较多的复杂模型，以尽可能地适应和学习训练数据。那里的风险在于，模型可能会过度拟合训练数据并学习到伪相关性（噪音）。这导致模型在其预测和推断中引入误差，这些误差被称为方差误差，并导致模型在未看见的测试数据上表现不佳。[图 2-6](#the_bias_variance_trade_off_that_needs)
    展示了开发模型时需要进行的减少可归纳泛化误差的偏差-方差权衡。当基础数据分布不是稳定的遍历时（就像金融和投资问题中一样），这种权衡变得更加困难和动态。
- en: '![The bias-variance trade-off that needs to be made for developing ML models](assets/pmlf_0206.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![在开发 ML 模型时需要进行的偏差-方差权衡](assets/pmlf_0206.png)'
- en: Figure 2-6\. The bias-variance trade-off that needs to be made when developing
    ML models^([14](ch02.html#ch02fn14))
  id: totrans-181
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-6\. 在开发机器学习模型时需要进行的偏差-方差权衡^([14](ch02.html#ch02fn14))
- en: Ontological Uncertainty
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 本体不确定性
- en: Ontology is the philosophical study of the nature of being and reality. Ontological
    uncertainty generally arises from the future of human affairs being essentially
    unknowable.^([15](ch02.html#ch02fn15))
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 本体论是关于存在和现实本质的哲学研究。本体不确定性通常源于未来的人类事务本质上是不可预测的。^([15](ch02.html#ch02fn15))
- en: To make the Monty Hall game resemble a real-world business deal or a trade,
    we have to dive deeper into the objective of the game, namely winning the car.
    From Monty’s perspective, winning means keeping the car so he can reduce the costs
    of the show while still attracting a large audience. When the game is viewed in
    this way, Monty’s knowledge of the car’s placement behind any one of the doors
    does not decrease his ontological uncertainty about winning the game. This is
    because he doesn’t know which door you’re going to pick and whether you will stay
    or switch doors when given the choice to do so. Since his probabilities are the
    complement of your probabilities, he has a ⅔ chance of keeping the car if you
    don’t switch doors and ⅓ chance of losing the car to you if you do switch doors.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 要使蒙蒂·霍尔游戏类似于现实世界的商业交易或交易，我们必须深入探讨游戏的目标，即赢得汽车。从蒙蒂的角度来看，赢就意味着保住汽车，这样他可以在吸引大量观众的同时降低节目成本。从这种视角看游戏时，蒙蒂对汽车藏在哪扇门后并不减少他在赢得游戏方面的本体不确定性。这是因为他不知道你会选择哪扇门，以及在有选择权时你是留在原门还是换门。由于他的概率是你概率的补集，如果你不换门，他有2/3的机会保住汽车，如果你换门，他有1/3的机会把汽车输给你。
- en: There are other possible ontological uncertainties for you. Say you show up
    to play the game a second time armed with the analysis of the game and the door-switching
    strategy. Monty surprises you by changing the rules and does not open another
    door to show you a goat. Instead he asks you to observe his body language and
    tone of voice for clues to help you make your decision. However, Monty has no
    intention of giving you any helpful clues and wants to reduce your probability
    of winning to ⅓ regardless of your decision to stay or switch doors. Monty does
    this because earlier in the week his producer had threatened to cancel the show,
    since its ratings were falling and it was not making enough money to cover Monty’s
    hefty salary.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 你还有其他可能的本体不确定性。比如你第二次参加游戏时，拿出了对游戏和换门策略的分析。蒙蒂让你大吃一惊，改变了规则，不再打开另一扇门给你看山羊。相反，他让你观察他的身体语言和语调，以便帮助你做出决定。然而，蒙蒂并不打算给你任何有用的线索，想要将你的获胜概率降低到1/3，无论你是留在原门还是换门。蒙蒂这样做是因为本周早些时候他的制片人曾威胁要取消节目，因为收视率下降，且不足以支付蒙蒂高额的薪水。
- en: 'Unexpected changes in business and financial markets are the rule, not the
    exception. Markets don’t send out a memo to participants when undergoing structural
    changes. Companies, deals, and trading strategies fail regularly and spectacularly
    because of these types of changes. It is similar to the way one of Hemingway’s
    characters described how he went bankrupt: “Two ways…Gradually and then suddenly.”^([16](ch02.html#ch02fn16))'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 商业和金融市场的意外变化是常规而非例外。市场在经历结构性变化时不会向参与者发出备忘录。公司、交易和交易策略经常因这些类型的变化而彻底失败。这与海明威笔下的一个角色描述自己如何破产的方式类似：“有两种方式……逐渐地然后突然地。”^([16](ch02.html#ch02fn16))
- en: In ML, ontological uncertainty occurs when there is a structural discontinuity
    in the underlying population or data-generating process, as was discussed in [Chapter 1](ch01.html#the_need_for_probabilistic_machine_lear),
    where we had to change the model from a binomial to a trinomial one. In finance
    and investing, the source of ontological uncertainty is the complexity of human
    activities, such as political elections, monetary and fiscal policy changes, company
    bankruptcies, and technological breakthroughs, to name just a few. Only humans
    can understand causality underlying these changes and use common sense to redesign
    the ML models from scratch to adapt to a new regime. [Figure 2-7](#human_intelligence_supported_by_probabi)
    shows the types of intelligent systems that are used in practice to navigate aleatory,
    epistemic, and ontological uncertainties of finance and investing.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，本体不确定性发生在基础人口或数据生成过程中存在结构性断裂的情况下，正如[第一章](ch01.html#the_need_for_probabilistic_machine_lear)中讨论的那样，我们不得不将模型从二项式更改为三项式。在金融和投资中，本体不确定性的来源是人类活动的复杂性，如政治选举、货币和财政政策变化、公司破产和技术突破等。只有人类能够理解这些变化背后的因果关系，并运用常识从头开始重新设计机器学习模型以适应新的制度。[图2-7](#human_intelligence_supported_by_probabi)显示了在实践中用于航行金融和投资的概率、认知和本体不确定性的智能系统类型。
- en: As you can see, designing models involves understanding different types of uncertainties,
    with each entailing a decision among various design options. Answers to these
    questions require prior knowledge of the problem domain and experience experimenting
    with many different models and algorithms. They cannot be derived from first principles
    of deductive logic or learned only from sample data that are not stationary ergodic.
    This seems obvious to practitioners like me. What might surprise most practitioners,
    as it did me, is that there is a set of mathematical theorems called the no free
    lunch (NFL) theorems that prove the validity of our various approaches.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，设计模型涉及理解不同类型的不确定性，每种都需要在各种设计选项中做出决策。这些问题的答案需要对问题域有先验知识，并有经验尝试许多不同的模型和算法。它们不能从演绎逻辑的第一原理中推导出，也不能仅从非静止遍历的样本数据中学习到。这对像我这样的从业者似乎是显而易见的。最令大多数从业者感到惊讶的事情之一（就像我一样），是有一组数学定理被称为无免费午餐（NFL）定理，证明了我们各种方法的有效性。
- en: '![Human intelligence supported by probabilistic AI systems is the best system
    for navigating the three-dimensional uncertainties of finance and investing.](assets/pmlf_0207.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![由概率AI系统支持的人类智能是航行金融和投资三维不确定性的最佳系统。](assets/pmlf_0207.png)'
- en: Figure 2-7\. Human intelligence supported by probabilistic AI systems is the
    most useful model for navigating the three-dimensional uncertainties of finance
    and investing
  id: totrans-190
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-7. 由概率AI系统支持的人类智能是航行金融和投资三维不确定性的最有用模型
- en: The No Free Lunch Theorems
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无免费午餐定理
- en: 'In 1891, Rudyard Kipling, a Nobel laureate in literature and an English imperialist
    from the province of Bombay, recounted a visit to a saloon in his travel journal
    *American Notes*: “It was the institution of the ‘free lunch’ that I had struck.
    You paid for a drink and got as much as you wanted to eat. For something less
    than a rupee a day a man can feed himself sumptuously in San Francisco, even though
    he be a bankrupt. Remember this if ever you are stranded in these parts.”^([17](ch02.html#ch02fn17))'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 1891年，诺贝尔文学奖获得者、英国帝国主义者鲁德亚德·吉卜林（Rudyard Kipling）在其旅行日记《美国笔记》中回忆了对一家酒吧的访问：“我碰巧看到了‘免费午餐’这个制度。你付一杯酒的钱，就可以吃到想吃的东西。在旧金山，一个人每天花不到一卢比，甚至是一个破产者也可以吃得很丰盛。如果你在这些地方受困，记住这一点。”^([17](ch02.html#ch02fn17))
- en: Fortuitously, I have been “stranded” in these parts for some time now and have
    a few rupees left over from my recent visit to the former province of Bombay.
    Unfortunately, this once popular American tradition of the “free lunch” is no
    longer common and has certainly disappeared from the bars in San Francisco, where
    a couple of hundred rupees might get you some peanuts and a glass of tap water.
    However, the idea that lunch is never free and we eventually pay for it with a
    drink—or personal data, or something else—has persisted and is commonly applied
    in many disciplines, especially economics, finance, and investing.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我很幸运地“滞留”在这些地方已经一段时间了，还剩下一些印度卢比，这是我最近访问过前孟买省时留下的。不幸的是，这种曾经流行的美国传统“免费午餐”现在已不常见，尤其是在旧金山的酒吧里，几百卢比可能只能买到一些花生和一杯自来水。然而，“午餐永远不是免费”的观念一直存在，并且在许多学科，特别是经济学、金融学和投资学中得到了广泛应用。
- en: David Wolpert, an eminent computer scientist and physicist, discovered that
    this idea also applied to machine learning and statistical inference. In 1996
    he shocked both these communities by publishing a paper that proved mathematically
    the impossibility of the existence of a superior ML learning algorithm that can
    solve all problems optimally. Prior knowledge of the problem domain is required
    to select the appropriate learning algorithm and improve its performance.^([18](ch02.html#ch02fn18))
    Wolpert, who was a postdoctoral student at the time of the publication, was subjected
    to ad hominem attacks by industry executives and derision by academics who felt
    threatened by these theorems because they were debunking their specious claims
    of discovering such optimal, bias-free, general purpose ML learning algorithms.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 大卫·沃尔珀特（David Wolpert）是一位杰出的计算机科学家和物理学家，他发现这个思想也适用于机器学习和统计推断。1996年，他发表了一篇论文，通过数学证明了不存在一个能够在所有问题上都最优地解决的优秀机器学习算法的存在。在选择适当的学习算法并提高其性能之前，需要先了解问题域的先验知识。^([18](ch02.html#ch02fn18))
    沃尔珀特当时还是博士后学生，发表这篇论文后，遭到了工业界高管的人身攻击，学术界也对这些定理感到嘲讽，因为它们揭穿了他们关于发现这种最优、无偏、通用机器学习算法的虚假主张。
- en: Wolpert subsequently published another paper with William Macready in 1997 that
    provided a similar proof for search and optimization algorithms.^([19](ch02.html#ch02fn19))
    These theorems are collectively known as the no free lunch (NFL) theorems.Please
    note that prior knowledge and assumptions about the problem domain that is used
    in the selection and design of the learning algorithm is also referred to as bias.
    Furthermore, a problem is defined by a data generating target distribution that
    the algorithm is trying to learn from training data. A cost function is used to
    measure the performance of the learning algorithm on out-of-sample test data.
    These theorems have many important implications for ML that are critical for us
    to understand.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 后来，沃尔珀特与威廉·麦克里迪（William Macready）在1997年发表了另一篇论文，为搜索和优化算法提供了类似的证明。^([19](ch02.html#ch02fn19))
    这些定理被称为“无免费午餐”（NFL）定理。请注意，在选择和设计学习算法时，关于问题域的先验知识和假设也被称为偏见。此外，问题由一个数据生成的目标分布定义，算法试图从训练数据中学习。成本函数用于衡量学习算法在样本外测试数据上的性能。这些定理对我们理解机器学习具有许多重要的影响，对我们非常关键。
- en: One important implication is that the performance of all learning algorithms
    when averaged across all problem domains will be the same. As shown in [Figure 2-8](#the_performance_of_all_three_algorithms),
    each data scientist has a different learning algorithm (A, B, C) whose performance
    is measured on unseen data of four different problem domains (apples, pears, tools,
    and shoes). On the apple problem domain, all three learning algorithms perform
    optimally. So we don’t have a unique optimal learning algorithm for the apple
    domain or any other problem domain for that matter. However, the learning algorithms
    have varying performance measures on each of the other three problem domains.
    None of the learning algorithms performs optimally on all four problem domains.
    Regardless, the performance of all three learning algorithms when considered independently
    and averaged across the four problem domains is the same at 32 / 4 = 8.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的一个推论是，所有学习算法的表现在所有问题领域中平均来看是相同的。如图[2-8](#the_performance_of_all_three_algorithms)所示，每位数据科学家都有一个不同的学习算法（A、B、C），它们在四个不同问题领域（苹果、梨、工具和鞋子）的未见数据上的表现被测量。在苹果问题领域中，这三个学习算法表现得都很好。因此，我们在苹果领域或任何其他问题领域中都没有一个唯一的最优学习算法。然而，这些学习算法在其他三个问题领域中的表现各有不同。在所有四个问题领域中独立考虑并取平均后，这三个学习算法的表现是相同的，即32
    / 4 = 8。
- en: '![The performance of all three algorithms averaged across all four problem
    domains is the same, with a score of 8.](assets/pmlf_0208.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![所有三个算法在所有四个问题领域中平均表现相同，得分为8。](assets/pmlf_0208.png)'
- en: Figure 2-8\. The performance of all three learning algorithms averaged across
    all four problem domains is the same, with a score of 8.^([20](ch02.html#ch02fn20))
  id: totrans-198
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-8\. 所有三个学习算法在所有四个问题领域中平均表现相同，得分为8。^([20](ch02.html#ch02fn20))
- en: This example illustrates the central idea in the NFL theorems that there are
    no mathematical reasons to prefer one learning algorithm over another based on
    expected performance across all problem domains. Since learning algorithms have
    varying performances on different problem domains, we must use our empirical knowledge
    of a specific problem domain to select a learning algorithm that is best aligned
    with the domain’s target function. So how well the learning algorithm performs
    is contingent on the validity of our domain knowledge and assumptions. There are
    no free lunches in ML.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子说明了NFL定理中的核心思想，即没有数学上的理由可以根据在所有问题领域中的预期表现来优先选择一个学习算法。由于学习算法在不同问题领域中的表现各不相同，我们必须利用我们对特定问题领域的经验知识来选择一个与该领域目标函数最匹配的学习算法。因此，学习算法的表现如何取决于我们领域知识和假设的有效性。在机器学习中没有免费的午餐。
- en: If we don’t make the payment of prior knowledge to align our learning algorithm
    with the underlying target function of the problem domain, like the freeloading
    frequentists claim we must do to remain unbiased, the learning algorithm’s predictions
    based on unseen data will be no better than random guessing when averaged over
    all possible target distributions. In fact, the risk is that it might be worse
    than random guessing. So we can’t have our lunch and not pay for it in ML. If
    we bolt for the exit without paying for our lunch, we’ll realize later that what
    we wolfed down was junk food and not a real meal.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不支付先验知识的代价来使我们的学习算法与问题领域的潜在目标函数对齐，就像那些吃白食的频率主义者声称我们必须保持无偏一样，那么基于未见数据的学习算法预测平均来看将不会比随机猜测更好。事实上，风险是可能比随机猜测还要糟糕。因此，在机器学习中我们不能吃了免费午餐而不付出代价。如果我们在不支付午餐费用的情况下冲出去，后来我们会意识到我们所狼吞虎咽的是垃圾食品，而不是一顿真正的饭菜。
- en: The most common criticism of NFL theorems is that all target distributions are
    not equally likely in the real world. This criticism is spurious and misses the
    point of using such a mathematical technique. The reason is that in the bias-free
    world that frequentists fantasize about, we are required to assign equal probability
    to all possible target distributions *by definition*. Any selection of a single
    target distribution from a finite set of all possible target distributions *must
    necessarily* involve making a subjective choice, which, *by definition* of an
    unbiased world, is not allowed. Because we are forbidden in a bias-free world
    from using our prior knowledge of the problem domain to pick a single target distribution,
    the performance of an unbiased algorithm must be averaged over all possible, equally
    likely target distributions. The result is that the unbiased algorithm’s average
    performance on unseen data is reduced to being no better than random guessing.
    The frequentist trick of implicitly selecting a target function while obfuscating
    their biased choice with statistical jargon and a sham ideology of objectivity
    doesn’t stand up to scrutiny.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: NFL定理最常见的批评是现实世界中并非所有目标分布都同等可能。这种批评是无稽之谈，并且忽略了使用这样一种数学技术的目的。原因在于，在频率学家幻想的无偏世界中，我们被要求按定义给所有可能的目标分布分配相等的概率。从所有可能的目标分布的有限集中选择单个目标分布必然涉及做出主观选择，而根据无偏世界的定义，这是不允许的。因为在无偏世界中，我们被禁止利用我们对问题领域的先验知识来选择单一的目标分布，所以无偏算法在未见数据上的平均性能被降低到不会比随机猜测更好。频率学家使用的隐式选择目标函数的伎俩，同时用统计术语和客观性的虚假意识形态掩盖他们的偏见选择，经不起严格的审查。
- en: The most important practical implication of the NFL theorems is that good generalization
    performance of any learning algorithm is always context and usage dependent. If
    we have sound prior knowledge and valid assumptions about our problem domain,
    we should use it to select and align the learning algorithm with the underlying
    structure of our specific problem and the nature of its target function. While
    this may introduce biases into the learning algorithm, it is a payment worth making,
    as it will lead to better performance on our specific problem domain.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: NFL定理的最重要实际影响是，任何学习算法的良好泛化性能始终取决于上下文和使用方式。如果我们对问题领域拥有充分的先验知识和有效的假设，我们应该使用它来选择并对齐学习算法与特定问题的基础结构及其目标函数的本质。虽然这可能会引入偏见到学习算法中，但这是值得付出的代价，因为它将导致在我们特定的问题领域中更好的性能。
- en: But remember that this optimality of performance is because of our “payment”
    of prior knowledge. Since our learning algorithm will be biased toward our problem
    domain, we should expect that it will almost surely perform poorly on other problem
    domains that have divergent underlying target functions, such that its average
    performance across all problem domains will be no better than the performance
    of another learning algorithm.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 但请记住，这种性能的优化是因为我们“付出”了先验知识。由于我们的学习算法将偏向于我们的问题领域，我们应该期望它几乎肯定在具有不同基础目标函数的其他问题领域上表现不佳，因此其在所有问题领域中的平均性能将不会优于另一个学习算法的性能。
- en: But the performance of our learning algorithms on other problem domains is not
    our concern. We are not touting our biased learning algorithms and models as a
    panacea for all problem domains. That would be a violation of the NFL theorems.
    In this book, we are concerned primarily with optimizing our probabilistic machine
    learning algorithms and models for finance and investing.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们的学习算法在其他问题领域的表现并不是我们关心的问题。我们并不吹嘘我们的偏见学习算法和模型是解决所有问题领域的灵丹妙药。那将违反NFL定理。在本书中，我们主要关注优化我们的概率机器学习算法和模型，用于金融和投资。
- en: Most importantly, the NFL theorems are yet another mathematical proof of the
    sham objectivity and deeply flawed foundations of frequentist/conventional statistics.
    The frequentists’ pious pretense of not using prior domain knowledge explicitly
    and making statistical inferences based solely on in-sample data is simply wrong
    and has had serious consequences for all the social sciences. It has led to base
    rate fallacies and a proliferation of junk studies whose results are no better
    than random guessing. We will discuss this further in [Chapter 4](ch04.html#the_dangers_of_conventional_statistical).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的是，NFL定理是关于频率主义/传统统计的虚伪客观性和深层次缺陷的又一个数学证明。频率主义者不使用明确的先验领域知识，仅基于样本内数据进行统计推断的虔诚假装是错误的，并对所有社会科学造成了严重后果。它导致基本比率谬误和大量无效研究的泛滥，其结果不比随机猜测好。我们将在[第四章](ch04.html#the_dangers_of_conventional_statistical)进一步讨论此问题。
- en: Investing and the Problem of Induction
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 投资与归纳问题
- en: Inductive reasoning synthesizes information from past observations to formulate
    general hypotheses that will continue to be plausible in the future. Simply put,
    induction makes inferences about the general population distribution based on
    an analysis of a random data sample. [Figure 2-9](#the_use_of_inductive_and_deductive_reas)
    shows how deductive and inductive reasoning are used in the scientific method.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 归纳推理综合从过去的观察中获取信息，以制定在未来仍然可能成立的一般假设。简而言之，归纳推理基于对随机数据样本进行分析来推断一般人群分布的推理。[图2-9](#the_use_of_inductive_and_deductive_reas)显示了演绎和归纳推理在科学方法中的应用。
- en: '![The use of inductive and deductive reasoning in the scientific method](assets/pmlf_0209.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![在科学方法中使用归纳和演绎推理](assets/pmlf_0209.png)'
- en: Figure 2-9\. The use of inductive and deductive reasoning in the scientific
    method^([21](ch02.html#ch02fn21))
  id: totrans-209
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-9。在科学方法中使用归纳和演绎推理^([21](ch02.html#ch02fn21))
- en: 'This is generally what we do in finance and investing. We analyze past data
    to detect patterns (such as trends) or formulate causal relationships between
    variables (such as earnings and price returns) or human behavior (such as fear
    and greed). We try to come up with a cogent thesis as to why these historical
    patterns and causal relationships will most likely persist in the future. Such
    financial analysis is generally divided into two types:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常是我们在金融和投资领域所做的。我们分析过去的数据来检测模式（如趋势）或制定变量（如收益和价格回报）或人类行为（如恐惧和贪婪）之间的因果关系。我们试图提出一个关于为什么这些历史模式和因果关系在未来很可能会持续存在的有力论点。这样的金融分析通常分为两种类型：
- en: Technical analysis
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 技术分析
- en: This is the study of historical patterns of an asset’s price and volume data
    during any time period based on its market dynamics of supply and demand. Patterns
    and statistical indicators are correlated with the asset’s future uptrends, downtrends,
    or trendless (sideways) price movements. See [Figure 2-10](#a_double_bottom_technical_pattern_predi)
    for a technical pattern called a double bottom, which is a signal that indicates
    the asset will rally in the future after it is confirmed that it has found support
    at the second bottom price.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这是根据供需市场动态对任何时间段内资产价格和交易量数据的历史模式进行研究。模式和统计指标与资产未来的上涨、下跌或横盘价格走势相关联。参见[图2-10](#a_double_bottom_technical_pattern_predi)，这是一个名为双底的技术模式信号，表明资产在确认第二底部价格找到支撑后将来会上涨。
- en: Generally speaking, technical analysts are not concerned with the nature of
    the asset or what is causing its prices to change at any given time. This is because
    all necessary information is assumed to be reflected in the price and volume data
    of the asset. Technical analysts are only concerned with detecting patterns in
    historical prices and volumes, computing statistical indicators, and using their
    correlations to predict future price changes of the asset. Technical investing
    and trading strategies assume that historical price and volume patterns and their
    related price correlations will repeat in the future because human behavior and
    the market dynamics of supply and demand on which they are based don’t essentially
    change.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，技术分析师不关心资产的性质或是在任何给定时间内导致其价格变动的原因。这是因为所有必要的信息被认为都反映在资产的价格和交易量数据中。技术分析师只关心在历史价格和交易量中检测模式、计算统计指标，并利用它们的相关性来预测资产未来的价格变动。技术投资和交易策略假设历史价格和交易量模式及其相关价格将在未来重复，因为这些基于人类行为以及供需市场动态的基础通常不会发生本质性变化。
- en: '![A double bottom technical pattern predicts that the price of the asset will
    rise in the future.](assets/pmlf_0210.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![双底技术模式预测资产价格将来会上涨。](assets/pmlf_0210.png)'
- en: Figure 2-10\. A double bottom technical pattern predicts that the price of the
    asset will rise in the future.
  id: totrans-215
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-10\. 双底技术模式预测资产价格将来会上涨。
- en: Fundamental analysis
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 基本分析
- en: This is the study of financial, economic, and human behavior. Analysts study
    historical company financial statements and economic, industry, and consumer reports.
    Using past empirical data, statistical analysis, or academic financial theories,
    analysts formulate causal mechanisms between features (or risk factors) and the
    fundamental value of an asset. Fundamental analysts are mainly concerned with
    the nature of the asset that they analyze and the underlying causal mechanisms
    that determine its valuation.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对金融、经济和人类行为的研究。分析师们研究历史上的公司财务报表以及经济、行业和消费者报告。利用过去的实证数据、统计分析或学术金融理论，分析师们构建了特征（或风险因素）与资产基本价值之间的因果机制。基本分析师主要关注他们分析的资产的本质以及决定其估值的潜在因果机制。
- en: For instance, the discounted cash flow (DCF) model is used extensively for valuing
    assets, such as the equity and debt of a company or the value of its factory.
    Fundamental analysts forecast the cash flows that a company or capital project
    is expected to generate in the future (typically three to five years) and discount
    them back to the present using an interest rate to account for the opportunity
    cost of the company’s capital. They also forecast macroeconomic variables like
    tax rates, inflation rates, gross domestic product, and currency exchange rates,
    among others. The fundamental principle of the DCF model is that cash tomorrow
    is worth less than cash today and must be discounted accordingly. This assumes
    that interest rates are always positive and cash can be lent out to earn interest
    at the appropriate rate. By forgoing the opportunity to lend out their cash at
    the appropriate rate, an investor incurs an opportunity cost that is reflected
    in the discount rate. See [Figure 2-11](#cash_flows_need_to_be_discounted_becaus).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，折现现金流（DCF）模型被广泛用于估值资产，如公司的股本和债务或其工厂的价值。基本分析师预测公司或资本项目未来（通常是三到五年）预计产生的现金流，并根据利率将其贴现回现在，以考虑公司资本的机会成本。他们还预测宏观经济变量，如税率、通货膨胀率、国内生产总值和货币汇率等。DCF模型的基本原则是明天的现金比今天的现金少，因此必须相应地贴现。这假设利率始终为正，并且可以将现金出借以按适当利率赚取利息。通过放弃将现金以适当利率放贷的机会，投资者承担的机会成本反映在贴现率中。参见[图
    2-11](#cash_flows_need_to_be_discounted_becaus)。
- en: '![Cash flows need to be discounted because cash tomorrow is worth less than
    cash today, assuming interest rates are positive.](assets/pmlf_0211.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![现金流需要贴现，因为假设利率为正，明天的现金比今天的现金少。](assets/pmlf_0211.png)'
- en: Figure 2-11\. Cash flows need to be discounted because cash tomorrow is worth
    less than cash today, assuming interest rates are positive
  id: totrans-220
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-11\. 现金流需要贴现，因为假设利率为正，明天的现金比今天的现金少。
- en: The DCF model is very sensitive to minor changes in the discount rate and the
    projected growth rate of its cash flows. This is why the interest rate set by
    central bankers is pivotal to the valuation of all assets, as it directly impacts
    an investor’s cost of capital. Fundamental trading and investing strategies assume
    that the formulated causal mechanisms between features (risk factors) and asset
    valuation will persist into the future.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: DCF模型对贴现率和现金流预期增长率的微小变化非常敏感。这就是为什么央行设定的利率对所有资产的估值至关重要，因为它直接影响投资者的资本成本。基本交易和投资策略假设特征（风险因素）与资产估值之间的构建的因果机制将持续存在。
- en: All other methods, such as quantitative analysis or machine learning, use some
    combination of technical and fundamental analysis. Regardless, how do we know
    that the patterns of technical analysis and causal relationships of fundamental
    analysis that have been observed thus far will continue to persist in the future?
    Well, because the past has resembled the future so far. But that is exactly what
    we are trying to prove in the first place! This circular reasoning is generally
    referred to as the problem of induction.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 所有其他方法，如量化分析或机器学习，都使用技术分析和基本分析的某种组合。不过，我们怎么知道迄今观察到的技术分析模式和基本分析的因果关系会在未来继续存在呢？嗯，因为过去与未来相似。但这正是我们首先要证明的！这种循环推理通常被称为归纳问题。
- en: This is a confounding metaphysical problem that has been investigated by the
    Carvaka school of Indian philosophy, at least 2,400 years before David Hume articulated
    the problem in the 18th century for a Western audience.^([22](ch02.html#ch02fn22))
    We can ignore the problem of induction in physics (see Sidebar). However, we cannot
    do that in the social sciences, especially finance and investing, where it is
    a clear and present danger to any extrapolation of the past into the future. That’s
    because human beings have free will, emotions, and creativity, and they react
    to one another’s actions in unpredictable ways. Sometimes history repeats itself,
    sometimes it rhymes, and sometimes it makes no sense at all. That’s why Securities
    and Exchange Commission (SEC) in the United States mandates all marketing materials
    in the investment management industry to have a disclaimer that states past returns
    are no guarantee of future results. This trite but true statement is merely echoing
    the age-old problem of induction. Unlike the physical universe, social systems
    don’t need infinite time and space to generate seemingly impossible events. An
    average lifetime is more than enough to witness a few of these mind-boggling events.
    For instance, in the last decade, over 15 trillion dollars’ worth of bonds were
    issued with negative interest rates in Japan and Europe! Negative interest rates
    contradict common sense, fundamental principles of finance, and the foundation
    of the DCF model.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个令人困惑的形而上学问题，早在西方人们18世纪为了西方观众时，印度哲学卡尔瓦卡学派就已经研究了至少2400年。^([22](ch02.html#ch02fn22))
    我们可以在物理学中忽略归纳问题（参见侧边栏）。然而，在社会科学中，特别是金融和投资领域，这是对将过去推广到未来的任何逻辑的一个明显且存在的危险。这是因为人类有自由意志、情感和创造力，并且他们对彼此的行为做出不可预测的反应。有时历史会重演，有时会押韵，有时则毫无意义。这就是为什么美国证券交易委员会（SEC）要求所有投资管理行业的营销材料都必须声明，过去的回报不能保证未来的结果。这句陈词滥调但却屡见不鲜的声明只是回应了古老的归纳问题。与物理宇宙不同，社会系统不需要无限的时间和空间来产生看似不可能的事件。平均寿命已经足够让我们见证其中一些令人难以置信的事件。例如，在过去的十年中，日本和欧洲已经发行了超过15万亿美元的负利率债券！负利率违背了常识、金融基本原理和DCF模型的基础。
- en: The Problem of Induction, NFL Theorems, and Probabilistic Machine Learning
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 归纳问题、NFL定理和概率机器学习
- en: Inductive inference is foundational to ML All ML models are built on the assumption
    that patterns discovered in past training data will persist in future unseen data.
    It is important to note that the NFL theorems are a brilliant algorithmic restatement
    of the problem of induction within a probabilistic framework. In both frameworks,
    we have to use prior knowledge or assumptions about the problem domain to make
    predictions that are better than chance on unseen data. More importantly, this
    knowledge cannot be acquired from in-sample data alone or from principles of deductive
    logic or theorems of mathematics. Prior knowledge based on past empirical observations
    and assumptions about the underlying structural unity of the observed phenomenon
    or data-generating target function are required. It is only when we apply our
    prior knowledge about a problem domain can we expect to optimize our learning
    algorithm for making predictions on unseen data that will be much better than
    random guessing.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 归纳推理是机器学习的基础。所有的机器学习模型都建立在这样一个假设之上：在过去的训练数据中发现的模式将在未来的未见数据中持续存在。需要注意的是，NFL定理是概率框架内归纳问题的杰出算法重新表述。在这两个框架中，我们必须使用关于问题领域的先验知识或假设，以便在未见数据上做出比随机猜测更好的预测。更重要的是，这种知识不能仅仅从样本内数据或演绎逻辑的原理或数学定理中获取。必须基于过去的经验观察和对观察现象或数据生成目标函数结构统一的假设。只有当我们应用关于问题领域的先验知识时，我们才能期望优化我们的学习算法，以便在未见数据上做出比随机猜测更好的预测。
- en: 'Most importantly, epistemic statistics embraces the problem of induction zealously
    and directly answers its central question: can we ever be sure that the knowledge
    that we have acquired from past observations is valid and will continue to be
    valid in the future? Of course not—the resounding answer is, we can almost never
    be sure. Learning continually from uncertain and incomplete information is the
    foundation on which epistemic statistics and probabilistic inference is built.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的是，认识论统计热衷于归纳问题，并直接回答其核心问题：我们能否确定我们从过去的观察中获得的知识是有效的，并且将来仍将有效？当然不能—坚定的答案是，我们几乎永远不能确定。从不确定和不完整信息中持续学习是认识论统计和概率推理建立的基础。
- en: Consequently, as we will see in [Chapter 5](ch05.html#the_probabilistic_machine_learning_fram),
    epistemic statistics provides a probabilistic framework for machine learning that
    systematically integrates prior knowledge and keeps updating it with new observations
    because we can never be certain that the validity of our knowledge will continue
    to persist into the future. Almost all knowledge is uncertain to a greater or
    lesser degree and is best represented as a probability distribution, not a point
    estimate. Forecasts about the future (predictions) and the past (retrodictions)
    are also generated from these models as predictive probability distributions.
    Its biggest challenge, however, is dealing with ontological uncertainty, for which
    human intelligence is crucial.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，正如我们将在[第5章](ch05.html#the_probabilistic_machine_learning_fram)中看到的那样，认识论统计为机器学习提供了一个概率框架，系统地整合先验知识，并随着新观察结果更新，因为我们永远不能确定我们的知识的有效性将持续到未来。几乎所有的知识都以不同程度的不确定性存在，并最好表示为概率分布，而不是点估计。从这些模型中生成关于未来（预测）和过去（回溯预测）的预测概率分布。然而，它面临的最大挑战是处理本体不确定性，这是人类智能至关重要的。
- en: Summary
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we used the famous Monty Hall problem to review the fundamental
    rules of probability theory and apply them to solve the Monty Hall problem. We
    also realized how embarassingly easy it is to derive the inverse probability rule
    that is pivotal to epistemic statistics and probabilistic machine learning. Furthermore,
    we used the Monty Hall game to explore the profound complexities of aleatory,
    epistemic, and ontological uncertainty that pervade our lives and businesses.
    A better understanding of the three types of uncertainty and the meaning of probability
    will enable us to analyze and develop appropriate models for our probabilistic
    ML systems to solve the difficult problems we face in finance and investing.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们使用了著名的蒙提霍尔问题来复习概率论的基本规则，并应用这些规则来解决蒙提霍尔问题。我们还意识到了推导逆概率规则的惊人简单，这对认识论统计和概率机器学习至关重要。此外，我们利用蒙提霍尔游戏探讨了贯穿我们生活和业务的偶然性、认识论和本体不确定性的深刻复杂性。对三种不确定性类型及概率含义的更好理解将使我们能够分析并为我们在金融和投资领域面临的困难问题开发适当的模型。
- en: We know for a fact that even a physical object like a coin has no intrinsic
    probability based on long-term frequencies. It depends on initial conditions and
    the physics of the toss. Probabilities are epistemic, not ontological—they are
    a map, not the terrain. It’s about time frequentists stop fooling themselves and
    others with their mind-projection fallacies and give up their pious pretense of
    objectivity and scientific rigor.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们确切知道，即使像硬币这样的物理对象也没有基于长期频率的内在概率。它取决于初始条件和投掷的物理过程。概率是认识论的，而非本体论的——它们是地图，而不是地形。频率主义者们应该停止用他们的心灵投射谬误来愚弄自己和他人，并放弃他们的虔诚假装客观性和科学严谨性。
- en: The NFL theorems can be interpreted as restating the problem of induction for
    machine learning in general and finance and investing in particular. Past performance
    of an algorithm or investment strategy is no guarantee of its future performance.
    The target distribution of the problem domain or the out-of-sample dataset may
    change enough to degrade the performance of the algorithm and investment strategy.
    In other words, it is impossible to have a unique learning algorithm or investment
    strategy that is both bias-free and optimal for all problem domains or market
    environments. If we want an optimal algorithm for our specific problem domain,
    we have to pay for it with assumptions and prior domain knowledge.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: NFL定理可以解释为在一般机器学习和特别是金融与投资中重新表述归纳问题。算法或投资策略的过去表现不是其未来表现的保证。问题域的目标分布或样本外数据集可能会发生足够的变化，从而降低算法和投资策略的性能。换句话说，不可能有一个既无偏差又对所有问题域或市场环境都最优的学习算法或投资策略。如果我们希望为我们特定的问题域找到一个最优算法，就必须通过假设和先前的领域知识来支付代价。
- en: Probabilistic machine learning incorporates the fundamental concepts of the
    problem of induction and NFL theorems within its framework. It systematically
    incorporates prior domain knowledge and continually updates it with new information
    while always expressing the uncertainty about its prior knowledge, inferences,
    retrodictions, and predictions. We will examine this epistemologically sound,
    mathematically rigorous, and commonsensical machine learning framework in [Chapter 5](ch05.html#the_probabilistic_machine_learning_fram).
    In the next chapter, we dive deeper into basic Monte Carlo methods and their applications
    to quantify aleatory and epistemic uncertainty using independent sampling.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 概率机器学习在其框架内结合了归纳问题和NFL定理的基本概念。它系统地整合先前的领域知识，并持续更新新信息，始终表达对先前知识、推断、预测和预测的不确定性。我们将在[第5章](ch05.html#the_probabilistic_machine_learning_fram)中探讨这个认识论上合理、数学上严谨、常识性的机器学习框架。在下一章中，我们将深入研究基本的蒙特卡洛方法及其在独立采样中量化不确定性的应用。
- en: References
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Clark, Matthew P. A., and Brian D. Westerberg. “How Random Is the Toss of a
    Coin?” *Canadian Medical Association Journal* 181, no. 12 (December 8, 2009):
    E306–E308.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 'Clark, Matthew P. A., and Brian D. Westerberg. “How Random Is the Toss of a
    Coin?” *Canadian Medical Association Journal* 181, no. 12 (December 8, 2009):
    E306–E308.'
- en: 'Dale, A. I. *A History of Inverse Probability: From Thomas Bayes to Karl Pearson*.
    New York: Springer, 1999.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 'Dale, A. I. *A History of Inverse Probability: From Thomas Bayes to Karl Pearson*.
    New York: Springer, 1999.'
- en: 'Diaconis, Persi, Susan Holmes, and Richard Montgomery. “Dynamical Bias in the
    Coin Toss.” *Society for Industrial and Applied Mathematics (SIAM) Review* 49,
    no.2 (2007): 211–235.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 'Diaconis, Persi, Susan Holmes, and Richard Montgomery. “Dynamical Bias in the
    Coin Toss.” *Society for Industrial and Applied Mathematics (SIAM) Review* 49,
    no.2 (2007): 211–235.'
- en: 'Hemingway, Ernest. *The Sun Also Rises*. New York: Charles Scribner’s Sons,
    1954.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 'Hemingway, Ernest. *The Sun Also Rises*. New York: Charles Scribner’s Sons,
    1954.'
- en: 'Jaynes, E. T. *Probability Theory: The Logic of Science*. Edited by G. Larry
    Bretthorst, New York: Cambridge University Press, 2003.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 'Jaynes, E. T. *Probability Theory: The Logic of Science*. Edited by G. Larry
    Bretthorst, New York: Cambridge University Press, 2003.'
- en: 'Kahneman, Daniel. *Thinking, Fast and Slow*. New York: Farrar, Straus and Giroux,
    2011.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 'Kahneman, Daniel. *Thinking, Fast and Slow*. New York: Farrar, Straus and Giroux,
    2011.'
- en: 'Kipling, Rudyard. *From Sea to Sea: Letters of Travel, Part II*. New York:
    Charles Scribner’s Sons, 1899.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 'Kipling, Rudyard. *From Sea to Sea: Letters of Travel, Part II*. New York:
    Charles Scribner’s Sons, 1899.'
- en: 'McElreath, Richard. *Statistical Rethinking: A Bayesian Course with Examples
    in R and Stan*. Boca Raton, FL: Chapman and Hall/CRC, 2016.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 'McElreath, Richard. *Statistical Rethinking: A Bayesian Course with Examples
    in R and Stan*. Boca Raton, FL: Chapman and Hall/CRC, 2016.'
- en: McGrath, James. *The Little Book of Big Management Wisdom*. O’Reilly Media,
    2017\. Accessed June 23, 2023\. [*https://www.oreilly.com/library/view/the-little-book/9781292148458/html/chapter-079.html*](https://www.oreilly.com/library/view/the-little-book/9781292148458/html/chapter-079.html).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: McGrath, James，《大管理智慧小书》，O’Reilly Media，2017年。访问于2023年6月23日。[*https://www.oreilly.com/library/view/the-little-book/9781292148458/html/chapter-079.html*](https://www.oreilly.com/library/view/the-little-book/9781292148458/html/chapter-079.html)。
- en: 'Njå, Ove, Øivind Solberg, and Geir Sverre Braut. “Uncertainty—Its Ontological
    Status and Relation to Safety.” In *The Illusion of Risk Control: What Does It
    Take to Live with Uncertainty?* edited by Gilles Motet and Corinne Bieder, 5–21\.
    Cham, Switzerland: SpringerOpen, 2017.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: Njå, Ove, Øivind Solberg 和 Geir Sverre Braut，《不确定性——其本体论地位及其与安全的关系》，收录于《风险控制的错觉：如何与不确定性共存？》，编辑：Gilles
    Motet 和 Corinne Bieder，5–21页。瑞士，香槟：SpringerOpen，2017年。
- en: 'Patterson, Scott. *The Quants: How a New Breed of Math Whizzes Conquered Wall
    Street and Nearly Destroyed It*. New York: Crown Business, 2010.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: Patterson, Scott，《量化交易者：如何新一代数学天才征服华尔街并几乎摧毁它》，纽约：皇冠商业，2010年。
- en: 'Perrett, Roy W. “The Problem of Induction in Indian Philosophy.” *Philosophy
    East and West* 34, no. 2 (1984): 161–74.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: Perrett, Roy W.，《印度哲学中的归纳问题》，《东西方哲学》34卷2期（1984年）：161–74页。
- en: 'Stigler, Stephen M. “Who Discovered Bayes’s Theorem?” *The American Statistician*
    37, no. 4a (1983): 290–296.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: Stigler, Stephen M.，《谁发现了贝叶斯定理？》，《美国统计学家》37卷4期a（1983年）：290–296页。
- en: 'Thaler, Richard. *Misbehaving: The Making of Behavioral Economics*. New York:
    W. W. Norton & Company, 2015.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: Thaler, Richard，《不当行为：行为经济学的形成》（纽约：W·W·诺顿公司，2015年）。
- en: 'Wolpert, David. “The Lack of A Priori Distinctions between Learning Algorithms.”
    *Neural Computation* 8, no. 7 (1996): 1341–90.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: Wolpert, David，《学习算法之间缺乏先验区别》，《神经计算》8卷7期（1996年）：1341–90页。
- en: 'Wolpert, David H., and William G. Macready. “No Free Lunch Theorems for Optimization.”
    *IEEE Transactions on Evolutionary Computation* 1, no. 1 (1997): 67.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: Wolpert, David H. 和 William G. Macready，《优化的无免费午餐定理》，《IEEE进化计算交易》1卷1期（1997年）：67页。
- en: ^([1](ch02.html#ch02fn1-marker)) Adapted from an image on Wikimedia Commons.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch02.html#ch02fn1-marker)) 改编自维基共享资源上的一幅图像。
- en: '^([2](ch02.html#ch02fn2-marker)) Richard Thaler, *Misbehaving: The Making of
    Behavioral Economics* (New York: W. W. Norton & Company, 2015); Daniel Kahneman,
    *Thinking, Fast and Slow* (New York: Farrar, Straus and Giroux, 2011).'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch02.html#ch02fn2-marker)) Richard Thaler，《不当行为：行为经济学的形成》（纽约：W·W·诺顿公司，2015年）；Daniel
    Kahneman，《思考，快与慢》（纽约：法拉尔、斯特劳斯和吉鲁克斯，2011年）。
- en: ^([3](ch02.html#ch02fn3-marker)) Adapted from an image on Wikimedia Commons.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch02.html#ch02fn3-marker)) 改编自维基共享资源上的一幅图像。
- en: '^([4](ch02.html#ch02fn4-marker)) Stephen M. Stigler, “Who Discovered Bayes’s
    Theorem?” *The American Statistician* 37, no. 4a (1983): 290–96; E. T. Jaynes,
    “Historical Digression,” in *Probability Theory: The Logic of Science*, ed. G.
    Larry Bretthorst (New York: Cambridge University Press, 2003), 112–14; A. I. Dale,
    “Inverse Probability,” in *A History of Inverse Probability: From Thomas Bayes
    to Karl Pearson* (New York: Springer, 1999), 1–16.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch02.html#ch02fn4-marker)) Stephen M. Stigler，《谁发现了贝叶斯定理？》，《美国统计学家》37卷4期a（1983年）：290–96页；E.
    T. Jaynes，《历史演变》，收录于《概率论：科学逻辑》，编辑：G. Larry Bretthorst（纽约：剑桥大学出版社，2003年），112–14页；A.
    I. Dale，《逆概率》，收录于《逆概率的历史：从托马斯·贝叶斯到卡尔·皮尔逊》（纽约：斯普林格，1999年），1–16页。
- en: ^([5](ch02.html#ch02fn5-marker)) Adapted from an image on Wikimedia Commons.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch02.html#ch02fn5-marker)) 改编自维基共享资源上的一幅图像。
- en: '^([6](ch02.html#ch02fn6-marker)) Richard McElreath, “The Golem of Prague,”
    in *Statistical Rethinking: A Bayesian Course with Examples in R and Stan* (Boca
    Raton, FL: Chapman and Hall/CRC, 2016), 1–18.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch02.html#ch02fn6-marker)) Richard McElreath，《布拉格的格雷姆》，收录于《统计重新思考：R和Stan实例的贝叶斯课程》（弗罗里达州博卡拉顿：查普曼和霍尔/CRC，2016年），1–18页。
- en: ^([7](ch02.html#ch02fn7-marker)) Adapted from an image on Wikimedia Commons.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch02.html#ch02fn7-marker)) 改编自维基共享资源上的一幅图像。
- en: '^([8](ch02.html#ch02fn8-marker)) Scott Patterson, *The Quants: How a New Breed
    of Math Whizzes Conquered Wall Street and Nearly Destroyed It* (New York: Crown
    Business, 2010).'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch02.html#ch02fn8-marker)) Scott Patterson，《量化交易者：如何新一代数学天才征服华尔街并几乎摧毁它》（纽约：皇冠商业，2010年）。
- en: '^([9](ch02.html#ch02fn9-marker)) Quoted in “Quotation 64: Dwight D. Eisenhower
    on Why Plans Are Useless but Planning Is Essential,” in *The Little Book of Big
    Management Wisdom* by Dr. James McGrath (O’Reilly Media, 2017), [*https://www.oreilly.com/library/view/the-little-book/9781292148458/html/chapter-079.html*](https://www.oreilly.com/library/view/the-little-book/9781292148458/html/chapter-079.html).'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch02.html#ch02fn9-marker)) 引自“引语64：德怀特·D·艾森豪威尔关于计划无用但计划至关重要的理由”，收录于《大管理智慧的小书》（作者：詹姆斯·麦克格拉思博士，O’Reilly
    Media，2017年），[*https://www.oreilly.com/library/view/the-little-book/9781292148458/html/chapter-079.html*](https://www.oreilly.com/library/view/the-little-book/9781292148458/html/chapter-079.html)。
- en: ^([10](ch02.html#ch02fn10-marker)) Jaynes, “How to Cheat at Coin or Die Tossing,”
    in *Probability Theory*, 317–20.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch02.html#ch02fn10-marker)) Jaynes，《如何作弊：硬币或骰子抛掷》，收录于《概率论》，317–20页。
- en: '^([11](ch02.html#ch02fn11-marker)) Persi Diaconis, Susan Holmes, and Richard
    Montgomery, “Dynamical Bias in the Coin Toss,” *Society for Industrial and Applied
    Mathematics (SIAM) Review* 49, no. 2 (2007): 211–35.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch02.html#ch02fn11-marker)) Persi Diaconis、Susan Holmes和Richard Montgomery，《硬币抛掷中的动态偏差》，《工业和应用数学学会评论》49卷2期（2007年）：211–35页。
- en: '^([12](ch02.html#ch02fn12-marker)) Matthew P. A. Clark and Brian D. Westerberg,
    “How Random Is the Toss of a Coin?” *Canadian Medical Association Journal* 181,
    no. 12 (December 8, 2009): E306–E308.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: ^([12](ch02.html#ch02fn12-marker)) Matthew P. A. Clark和Brian D. Westerberg，《抛硬币有多随机？》，《加拿大医学协会杂志》181卷12期（2009年12月8日）：E306–E308页。
- en: ^([13](ch02.html#ch02fn13-marker)) Adapted from an image on Wikimedia Commons.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: ^([13](ch02.html#ch02fn13-marker)) 改编自维基共享资源上的一幅图像。
- en: ^([14](ch02.html#ch02fn14-marker)) Adapted from an image on Wikimedia Commons.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: ^([14](ch02.html#ch02fn14-marker)) 改编自维基共享资源上的一幅图像。
- en: '^([15](ch02.html#ch02fn15-marker)) Ove Njå, Øivind Solberg, and Geir Sverre
    Braut, “Uncertainty—Its Ontological Status and Relation to Safety,” in *The Illusion
    of Risk Control: What Does It Take to Live with Uncertainty?*, ed. Gilles Motet
    and Corinne Bieder (Cham, Switzerland: SpringerOpen, 2017), 5–21.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: ^([15](ch02.html#ch02fn15-marker)) Ove Njå、Øivind Solberg和Geir Sverre Braut，《不确定性——其本体论地位及其与安全性的关系》，收录于《风险控制的幻觉：如何与不确定性共存？》，编辑：Gilles
    Motet和Corinne Bieder（瑞士，Cham：SpringerOpen，2017年），5–21页。
- en: '^([16](ch02.html#ch02fn16-marker)) Ernest Hemingway, *The Sun Also Rises* (New
    York: Charles Scribner’s Sons, 1954), 136.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: ^([16](ch02.html#ch02fn16-marker)) Ernest Hemingway，《太阳照常升起》，（纽约：查尔斯·斯克里布纳子公司，1954年），136页。
- en: '^([17](ch02.html#ch02fn17-marker)) Rudyard Kipling, *From Sea to Sea: Letters
    of Travel, Part II* (New York: Charles Scribner’s Sons, 1899), 39.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: ^([17](ch02.html#ch02fn17-marker)) Rudyard Kipling，《从海到海：旅行信件，第二部分》，（纽约：查尔斯·斯克里布纳子公司，1899年），39页。
- en: '^([18](ch02.html#ch02fn18-marker)) David Wolpert, “The Lack of A Priori Distinctions
    between Learning Algorithms,” *Neural Computation* 8, no. 7 (1996): 1341–90.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: ^([18](ch02.html#ch02fn18-marker)) David Wolpert，《学习算法之间缺乏先验区分》，《神经计算》8卷7期（1996年）：1341–90。
- en: '^([19](ch02.html#ch02fn19-marker)) David H. Wolpert and William G. Macready,
    “No Free Lunch Theorems for Optimization,” *IEEE Transactions on Evolutionary
    Computation* 1, no. 1 (1997): 67.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: ^([19](ch02.html#ch02fn19-marker)) David H. Wolpert和William G. Macready，《优化中的无免费午餐定理》，《IEEE进化计算期刊》1卷1期（1997年）：67页。
- en: ^([20](ch02.html#ch02fn20-marker)) Adapted from an image on Wikimedia Commons.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: ^([20](ch02.html#ch02fn20-marker)) 改编自维基共享资源上的一幅图像。
- en: ^([21](ch02.html#ch02fn21-marker)) Adapted from an image on Wikimedia Commons.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: ^([21](ch02.html#ch02fn21-marker)) 改编自维基共享资源上的一幅图像。
- en: '^([22](ch02.html#ch02fn22-marker)) Roy W. Perrett, “The Problem of Induction
    in Indian Philosophy,” *Philosophy East and West* 34, no. 2 (1984): 161–74.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: ^([22](ch02.html#ch02fn22-marker)) Roy W. Perrett，《印度哲学中的归纳问题》，《东西方哲学》34卷2期（1984年）：161–74。
