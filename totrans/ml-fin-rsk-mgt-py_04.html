<html><head></head><body><section data-pdf-bookmark="Chapter 3. Deep Learning for Time Series Modeling" data-type="chapter" epub:type="chapter"><div class="chapter" id="chapter_3">&#13;
<h1><span class="label">Chapter 3. </span>Deep Learning for Time Series Modeling</h1>&#13;
&#13;
<blockquote data-type="epigram">&#13;
<p> ...Yes, it is true that a Turing machine can compute any computable function given enough memory and enough time, but nature had to solve problems in real time. To do this, it made use of the brain’s neural networks that, like the most powerful computers on the planet, have massively parallel processors. Algorithms that run efficiently on them will eventually win out.</p>&#13;
<p data-type="attribution"> Terrence J. Sejnowski (2018)</p>&#13;
</blockquote>&#13;
&#13;
<p><em>Deep learning</em> has recently become a buzzword for some good reasons, although recent attempts to improve deep learning practices are not the first of their kind. <a data-primary="time series modeling" data-secondary="deep learning for" data-type="indexterm" id="ix_time_series_deep_learn"/><a data-primary="deep learning" data-secondary="time series modeling" data-type="indexterm" id="ix_deep_learn_ch3"/><a data-primary="deep learning" data-type="indexterm" id="idm45737247569456"/>However, it is quite understandable why deep learning has been appreciated for nearly two decades. Deep learning is an abstract concept, which makes it hard to define in few of words.&#13;
<a data-primary="NNs (neural networks)" data-secondary="and deep learning" data-secondary-sortas="deep learning" data-type="indexterm" id="idm45737247568448"/>Unlike a neural network (NN), deep learning has a more complex structure, and hidden layers define the complexity. Therefore, some researchers use the number of hidden layers as a comparison benchmark to distinguish a neural network from deep learning, a useful but not particularly rigorous way to make this distinction. A better definition can clarify the difference.</p>&#13;
&#13;
<p>At a high level, deep learning can be defined:</p>&#13;
<blockquote>&#13;
<p>Deep learning methods are <a data-primary="representation learning" data-type="indexterm" id="idm45737247846416"/>representation-learning<sup><a data-type="noteref" href="ch03.html#idm45737247845584" id="idm45737247845584-marker">1</a></sup> methods with multiple levels of representation, obtained by composing simple but nonlinear modules that each transform the representation at one level (starting with the raw input) into a representation at a higher, slightly more abstract level.</p>&#13;
<p data-type="attribution">Le Cunn et al. (2015)</p>&#13;
</blockquote>&#13;
&#13;
<p>Applications of deep learning date back to the 1940s, when <em>Cybernetics</em> by Norbert Wiener was published. Connectivist thinking then dominated between the 1980s and 1990s. Recent developments in deep learning, such as backpropagation and neural networks, have created the field as we know it. Basically, there have been three waves of deep learning, so we might wonder why deep learning is in its heyday <em>now</em>? Goodfellow et al. (2016) list some plausible reasons, including:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Increasing data sizes</p>&#13;
</li>&#13;
<li>&#13;
<p>Increasing model sizes</p>&#13;
</li>&#13;
<li>&#13;
<p>Increasing accuracy, complexity, and real-world impact</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>It seems like modern technology and data availability have paved the way for an era of deep learning in which new data-driven methods are proposed so that we are able to model time series using unconventional models. This development has given rise to a new wave of deep learning. Two methods stand out in their ability to include longer time periods: the <em>recurrent neural network</em> (RNN) and <em>long short-term memory</em> (LSTM). In this section, we will concentrate on the practicality of these models in Python after briefly discussing the theoretical background.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Recurrent Neural Networks" data-type="sect1"><div class="sect1" id="idm45737247876544">&#13;
<h1>Recurrent Neural Networks</h1>&#13;
&#13;
<p><a data-primary="time series modeling" data-secondary="deep learning for" data-tertiary="recurrent neural networks" data-type="indexterm" id="ix_time_series_deep_learn_rnns"/><a data-primary="NNs (neural networks)" data-secondary="recurrent neural networks" data-type="indexterm" id="ix_nns_rnns"/><a data-primary="RNNs (recurrent neural networks)" data-type="indexterm" id="ix_rnns_ch3"/><a data-primary="deep learning" data-secondary="time series modeling" data-tertiary="recurrent neural networks" data-type="indexterm" id="ix_deep_learn_timeser_rnn"/>An RNN has a neural network structure with at least one feedback connection so that the network can learn sequences. A feedback connection results in a loop, enabling us to unveil the nonlinear characteristics. This type of connection brings us a new and quite useful property: <em>memory</em>.  Thus, an RNN can make use not only of the input data but also the previous outputs, which sounds compelling when it comes to time series modeling.</p>&#13;
&#13;
<p>RNNs come in many forms, such as:</p>&#13;
<dl>&#13;
<dt><a data-primary="one-to-one RNN" data-type="indexterm" id="idm45737247865968"/>One-to-one</dt>&#13;
<dd>&#13;
<p>A one-to-one RNN consists of a single input and a single output, which makes it the most basic type of RNN.</p>&#13;
</dd>&#13;
<dt><a data-primary="one-to-many RNN" data-type="indexterm" id="idm45737247863824"/>One-to-many</dt>&#13;
<dd>&#13;
<p>In this form, an RNN produces multiple outputs for a single input.</p>&#13;
</dd>&#13;
<dt><a data-primary="many-to-one RNN" data-type="indexterm" id="idm45737247861840"/>Many-to-one</dt>&#13;
<dd>&#13;
<p>As opposed to the one-to-many structure, many-to-one has multiple inputs for a single output.</p>&#13;
</dd>&#13;
<dt><a data-primary="many-to-many RNN" data-type="indexterm" id="idm45737247859744"/>Many-to-many</dt>&#13;
<dd>&#13;
<p>This structure has multiple inputs and outputs and is known as the most complicated structure for an RNN.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>A hidden unit in an RNN feeds itself back into the neural network so that the RNN has recurrent layers <a data-primary="feed-forward versus neural network" data-type="indexterm" id="idm45737247857392"/>(unlike a feed-forward neural network) making it a suitable method for modeling time series data. Therefore, in RNNs, activation of a neuron comes from a previous time-step indication that the RNN represents as an accumulating state of the network instance (Buduma and Locascio 2017).</p>&#13;
&#13;
<p>As summarized by Nielsen (2019):</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>RNNs have time steps one at a time in an orderly fashion.</p>&#13;
</li>&#13;
<li>&#13;
<p>The state of the network stays as it is from one time step to another.</p>&#13;
</li>&#13;
<li>&#13;
<p>An RNN updates its state based on the time step.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>These dimensions are illustrated in <a data-type="xref" href="#RNN_dimensions">Figure 3-1</a>. As can be seen, the RNN structure on the right-hand side has a time step, which is the main difference between it and the feed-forward network.</p>&#13;
&#13;
<figure><div class="figure" id="RNN_dimensions">&#13;
<img alt="RNN structure" src="assets/mlfr_0301.png"/>&#13;
<h6><span class="label">Figure 3-1. </span>RNN structure<sup><a data-type="noteref" href="ch03.html#idm45737247426656" id="idm45737247426656-marker">2</a></sup></h6>&#13;
</div></figure>&#13;
&#13;
<p>RNNs have a three-dimensional input, comprised of:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Batch size</p>&#13;
</li>&#13;
<li>&#13;
<p>Time steps</p>&#13;
</li>&#13;
<li>&#13;
<p>Number of features</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p><em>Batch size</em> <a data-primary="batch size input, RNN" data-type="indexterm" id="idm45737247421424"/>denotes the number of observations or number of rows of data. <em>Time steps</em> <a data-primary="time steps input, RNN" data-type="indexterm" id="idm45737247420176"/>are the number of times to feed the model. <a data-primary="number of features input, RNN" data-type="indexterm" id="idm45737248172688"/>Finally, <em>number of features</em> is the number of columns of each sample.</p>&#13;
&#13;
<p>We’ll start with the following code:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">1</code><code class="p">]</code><code class="p">:</code><code> </code><code class="kn">import</code><code> </code><code class="nn">numpy</code><code> </code><code class="kn">as</code><code> </code><code class="nn">np</code><code>&#13;
</code><code>        </code><code class="kn">import</code><code> </code><code class="nn">pandas</code><code> </code><code class="kn">as</code><code> </code><code class="nn">pd</code><code>&#13;
</code><code>        </code><code class="kn">import</code><code> </code><code class="nn">math</code><code>&#13;
</code><code>        </code><code class="kn">import</code><code> </code><code class="nn">datetime</code><code>&#13;
</code><code>        </code><code class="kn">import</code><code> </code><code class="nn">yfinance</code><code> </code><code class="kn">as</code><code> </code><code class="nn">yf</code><code>&#13;
</code><code>        </code><code class="kn">import</code><code> </code><code class="nn">matplotlib.pyplot</code><code> </code><code class="kn">as</code><code> </code><code class="nn">plt</code><code>&#13;
</code><code>        </code><code class="kn">import</code><code> </code><code class="nn">tensorflow</code><code> </code><code class="kn">as</code><code> </code><code class="nn">tf</code><code>&#13;
</code><code>        </code><code class="kn">from</code><code> </code><code class="nn">tensorflow.keras.models</code><code> </code><code class="kn">import</code><code> </code><code class="n">Sequential</code><code>&#13;
</code><code>        </code><code class="kn">from</code><code> </code><code class="nn">tensorflow.keras.callbacks</code><code> </code><code class="kn">import</code><code> </code><code class="n">EarlyStopping</code><code>&#13;
</code><code>        </code><code class="kn">from</code><code> </code><code class="nn">tensorflow.keras.layers</code><code> </code><code class="kn">import</code><code> </code><code class="p">(</code><code class="n">Dense</code><code class="p">,</code><code> </code><code class="n">Dropout</code><code class="p">,</code><code>&#13;
</code><code>                                             </code><code class="n">Activation</code><code class="p">,</code><code> </code><code class="n">Flatten</code><code class="p">,</code><code>&#13;
</code><code>                                             </code><code class="n">MaxPooling2D</code><code class="p">,</code><code> </code><code class="n">SimpleRNN</code><code class="p">)</code><code>&#13;
</code><code>        </code><code class="kn">from</code><code> </code><code class="nn">sklearn.model_selection</code><code> </code><code class="kn">import</code><code> </code><code class="n">train_test_split</code><code>&#13;
</code><code>&#13;
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">2</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">n_steps</code><code> </code><code class="o">=</code><code> </code><code class="mi">13</code><code> </code><a class="co" href="#callout_deep_learning_for_time_series_modeling_CO1-1" id="co_deep_learning_for_time_series_modeling_CO1-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code>        </code><code class="n">n_features</code><code> </code><code class="o">=</code><code> </code><code class="mi">1</code><code> </code><a class="co" href="#callout_deep_learning_for_time_series_modeling_CO1-2" id="co_deep_learning_for_time_series_modeling_CO1-2"><img alt="2" src="assets/2.png"/></a><code>&#13;
</code><code>&#13;
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">3</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">model</code><code> </code><code class="o">=</code><code> </code><code class="n">Sequential</code><code class="p">(</code><code class="p">)</code><code> </code><a class="co" href="#callout_deep_learning_for_time_series_modeling_CO1-3" id="co_deep_learning_for_time_series_modeling_CO1-3"><img alt="3" src="assets/3.png"/></a><code>&#13;
</code><code>        </code><code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">SimpleRNN</code><code class="p">(</code><code class="mi">512</code><code class="p">,</code><code> </code><code class="n">activation</code><code class="o">=</code><code class="s1">'</code><code class="s1">relu</code><code class="s1">'</code><code class="p">,</code><code>&#13;
</code><code>                            </code><code class="n">input_shape</code><code class="o">=</code><code class="p">(</code><code class="n">n_steps</code><code class="p">,</code><code> </code><code class="n">n_features</code><code class="p">)</code><code class="p">,</code><code>&#13;
</code><code>                            </code><code class="n">return_sequences</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code><code class="p">)</code><code> </code><a class="co" href="#callout_deep_learning_for_time_series_modeling_CO1-4" id="co_deep_learning_for_time_series_modeling_CO1-4"><img alt="4" src="assets/4.png"/></a><code>&#13;
</code><code>        </code><code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dropout</code><code class="p">(</code><code class="mf">0.2</code><code class="p">)</code><code class="p">)</code><code> </code><a class="co" href="#callout_deep_learning_for_time_series_modeling_CO1-5" id="co_deep_learning_for_time_series_modeling_CO1-5"><img alt="5" src="assets/5.png"/></a><code>&#13;
</code><code>        </code><code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dense</code><code class="p">(</code><code class="mi">256</code><code class="p">,</code><code> </code><code class="n">activation</code><code> </code><code class="o">=</code><code> </code><code class="s1">'</code><code class="s1">relu</code><code class="s1">'</code><code class="p">)</code><code class="p">)</code><code> </code><a class="co" href="#callout_deep_learning_for_time_series_modeling_CO1-6" id="co_deep_learning_for_time_series_modeling_CO1-6"><img alt="6" src="assets/6.png"/></a><code>&#13;
</code><code>        </code><code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Flatten</code><code class="p">(</code><code class="p">)</code><code class="p">)</code><code> </code><a class="co" href="#callout_deep_learning_for_time_series_modeling_CO1-7" id="co_deep_learning_for_time_series_modeling_CO1-7"><img alt="7" src="assets/7.png"/></a><code>&#13;
</code><code>        </code><code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dense</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code><code> </code><code class="n">activation</code><code class="o">=</code><code class="s1">'</code><code class="s1">linear</code><code class="s1">'</code><code class="p">)</code><code class="p">)</code><code> </code><a class="co" href="#callout_deep_learning_for_time_series_modeling_CO1-8" id="co_deep_learning_for_time_series_modeling_CO1-8"><img alt="8" src="assets/8.png"/></a><code>&#13;
</code><code>&#13;
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">4</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">model</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">optimizer</code><code class="o">=</code><code class="s1">'</code><code class="s1">rmsprop</code><code class="s1">'</code><code class="p">,</code><code>&#13;
</code><code>                      </code><code class="n">loss</code><code class="o">=</code><code class="s1">'</code><code class="s1">mean_squared_error</code><code class="s1">'</code><code class="p">,</code><code>&#13;
</code><code>                      </code><code class="n">metrics</code><code class="o">=</code><code class="p">[</code><code class="s1">'</code><code class="s1">mse</code><code class="s1">'</code><code class="p">]</code><code class="p">)</code><code> </code><a class="co" href="#callout_deep_learning_for_time_series_modeling_CO1-9" id="co_deep_learning_for_time_series_modeling_CO1-9"><img alt="9" src="assets/9.png"/></a><code>&#13;
</code><code>&#13;
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">5</code><code class="p">]</code><code class="p">:</code><code> </code><code class="k">def</code><code> </code><code class="nf">split_sequence</code><code class="p">(</code><code class="n">sequence</code><code class="p">,</code><code> </code><code class="n">n_steps</code><code class="p">)</code><code class="p">:</code><code>&#13;
</code><code>            </code><code class="n">X</code><code class="p">,</code><code> </code><code class="n">y</code><code> </code><code class="o">=</code><code> </code><code class="p">[</code><code class="p">]</code><code class="p">,</code><code> </code><code class="p">[</code><code class="p">]</code><code>&#13;
</code><code>            </code><code class="k">for</code><code> </code><code class="n">i</code><code> </code><code class="ow">in</code><code> </code><code class="nb">range</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">sequence</code><code class="p">)</code><code class="p">)</code><code class="p">:</code><code>&#13;
</code><code>                </code><code class="n">end_ix</code><code> </code><code class="o">=</code><code> </code><code class="n">i</code><code> </code><code class="o">+</code><code> </code><code class="n">n_steps</code><code>&#13;
</code><code>                </code><code class="k">if</code><code> </code><code class="n">end_ix</code><code> </code><code class="o">&gt;</code><code> </code><code class="nb">len</code><code class="p">(</code><code class="n">sequence</code><code class="p">)</code><code> </code><code class="o">-</code><code> </code><code class="mi">1</code><code class="p">:</code><code>&#13;
</code><code>                    </code><code class="k">break</code><code>&#13;
</code><code>                </code><code class="n">seq_x</code><code class="p">,</code><code> </code><code class="n">seq_y</code><code> </code><code class="o">=</code><code> </code><code class="n">sequence</code><code class="p">[</code><code class="n">i</code><code class="p">:</code><code class="n">end_ix</code><code class="p">]</code><code class="p">,</code><code> </code><code class="n">sequence</code><code class="p">[</code><code class="n">end_ix</code><code class="p">]</code><code>&#13;
</code><code>                </code><code class="n">X</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">seq_x</code><code class="p">)</code><code>&#13;
</code><code>                </code><code class="n">y</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">seq_y</code><code class="p">)</code><code>&#13;
</code><code>            </code><code class="k">return</code><code> </code><code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">X</code><code class="p">)</code><code class="p">,</code><code> </code><code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">y</code><code class="p">)</code><code> </code><a class="co" href="#callout_deep_learning_for_time_series_modeling_CO1-10" id="co_deep_learning_for_time_series_modeling_CO1-10"><img alt="10" src="assets/10.png"/></a></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_deep_learning_for_time_series_modeling_CO1-1" id="callout_deep_learning_for_time_series_modeling_CO1-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Defining the number of steps for prediction</p></dd>&#13;
<dt><a class="co" href="#co_deep_learning_for_time_series_modeling_CO1-2" id="callout_deep_learning_for_time_series_modeling_CO1-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>Defining the number of features as 1</p></dd>&#13;
<dt><a class="co" href="#co_deep_learning_for_time_series_modeling_CO1-3" id="callout_deep_learning_for_time_series_modeling_CO1-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>Calling a sequential model to run the RNN</p></dd>&#13;
<dt><a class="co" href="#co_deep_learning_for_time_series_modeling_CO1-4" id="callout_deep_learning_for_time_series_modeling_CO1-4"><img alt="4" src="assets/4.png"/></a></dt>&#13;
<dd><p>Identifying the number of hidden neurons, activation function, and input shape</p></dd>&#13;
<dt><a class="co" href="#co_deep_learning_for_time_series_modeling_CO1-5" id="callout_deep_learning_for_time_series_modeling_CO1-5"><img alt="5" src="assets/5.png"/></a></dt>&#13;
<dd><p>Adding a dropout layer to prevent overfitting</p></dd>&#13;
<dt><a class="co" href="#co_deep_learning_for_time_series_modeling_CO1-6" id="callout_deep_learning_for_time_series_modeling_CO1-6"><img alt="6" src="assets/6.png"/></a></dt>&#13;
<dd><p>Adding one more hidden layer with 256 neurons with the <code>relu</code> activation &#13;
<span class="keep-together">function</span></p></dd>&#13;
<dt><a class="co" href="#co_deep_learning_for_time_series_modeling_CO1-7" id="callout_deep_learning_for_time_series_modeling_CO1-7"><img alt="7" src="assets/7.png"/></a></dt>&#13;
<dd><p>Flattening the model to transform the three-dimensional matrix into a vector</p></dd>&#13;
<dt><a class="co" href="#co_deep_learning_for_time_series_modeling_CO1-8" id="callout_deep_learning_for_time_series_modeling_CO1-8"><img alt="8" src="assets/8.png"/></a></dt>&#13;
<dd><p>Adding an output layer with <code>linear</code> activation function</p></dd>&#13;
<dt><a class="co" href="#co_deep_learning_for_time_series_modeling_CO1-9" id="callout_deep_learning_for_time_series_modeling_CO1-9"><img alt="9" src="assets/9.png"/></a></dt>&#13;
<dd><p>Compiling the RNN model</p></dd>&#13;
<dt><a class="co" href="#co_deep_learning_for_time_series_modeling_CO1-10" id="callout_deep_learning_for_time_series_modeling_CO1-10"><img alt="10" src="assets/10.png"/></a></dt>&#13;
<dd><p>Creating a dependent variable <code>y</code></p></dd>&#13;
</dl>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45737247057344">&#13;
<h5>Activation Functions</h5>&#13;
<p><a data-primary="activation functions, NN" data-type="indexterm" id="idm45737246573216"/>Activation functions are mathematical equations that are used to determine the output in a neural network structure. These tools introduce nonlinearity in the hidden layers so that we are able to model the nonlinear issues.</p>&#13;
&#13;
<p>The following are the most famous activation functions:</p>&#13;
<dl>&#13;
<dt>Sigmoid</dt>&#13;
<dd>&#13;
<div class="openblock">&#13;
<p><a data-primary="sigmoid activation function, NN" data-type="indexterm" id="idm45737246569344"/>This activation function allows us to incorporate a small amount of output as we introduce small changes in the model. It takes values between 0 and 1. The mathematical representation of sigmoid is:</p>&#13;
<div data-type="equation">&#13;
<math display="block">&#13;
  <mrow>&#13;
    <mtext>sigmoid</mtext>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>x</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <mfrac><mn>1</mn> <mrow><mn>1</mn><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo>(</mo><mo>-</mo><msub><mo>∑</mo> <mi>i</mi> </msub><msub><mi>w</mi> <mi>i</mi> </msub><msub><mi>x</mi> <mi>i</mi> </msub><mo>-</mo><mi>b</mi><mo>)</mo></mrow></mfrac>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>where <em>w</em> is weight, <em>x</em> denotes data, <em>b</em> represents bias, and subscript <em>i</em> shows &#13;
<span class="keep-together">features</span>.</p>&#13;
</div>&#13;
&#13;
</dd>&#13;
<dt>Tanh</dt>&#13;
<dd>&#13;
<div class="openblock">&#13;
<p><a data-primary="tanh activation function, NN" data-type="indexterm" id="idm45737247027168"/>If you are handling negative numbers, tanh is your activation function. As opposed to the sigmoid function, it ranges between -1 and 1. The tanh formula is:</p>&#13;
<div data-type="equation">&#13;
<math display="block">&#13;
  <mrow>&#13;
    <mtext>tanh</mtext>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>x</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <mfrac><mrow><mi>s</mi><mi>i</mi><mi>n</mi><mi>h</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow> <mrow><mi>c</mi><mi>o</mi><mi>s</mi><mi>h</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
</div>&#13;
&#13;
</dd>&#13;
<dt>Linear</dt>&#13;
<dd>&#13;
<div class="openblock">&#13;
<p><a data-primary="linear activation function, NN" data-type="indexterm" id="idm45737247014896"/>Using the linear activation function enables us to build linear relationships between independent and dependent variables. The linear activation function takes the inputs and multiplies by the weights to form the outputs proportional to the inputs. It is a convenient activation function for time-series models. Linear activation functions take the form of:</p>&#13;
<div data-type="equation">&#13;
<math display="block">&#13;
  <mrow>&#13;
    <mi>f</mi>&#13;
    <mo>(</mo>&#13;
    <mi>x</mi>&#13;
    <mo>)</mo>&#13;
    <mo>=</mo>&#13;
    <mi>w</mi>&#13;
    <mi>x</mi>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
</div>&#13;
&#13;
</dd>&#13;
<dt>Rectified linear</dt>&#13;
<dd>&#13;
<div class="openblock">&#13;
<p><a data-primary="rectified linear activation function, NN" data-type="indexterm" id="idm45737247006576"/>The rectified linear activation function, known as ReLu, can take 0 if the input &#13;
<span class="keep-together">is zero</span> or below zero. If the input is greater than 0, it goes up in line with <em>x</em>. &#13;
<span class="keep-together">Mathematically</span>:</p>&#13;
<div data-type="equation">&#13;
<math display="block">&#13;
  <mrow>&#13;
    <mtext>ReLu(x)</mtext>&#13;
    <mo>=</mo>&#13;
    <mtext>max</mtext>&#13;
    <mo>(</mo>&#13;
    <mn>0</mn>&#13;
    <mo>,</mo>&#13;
    <mi>x</mi>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
</div>&#13;
&#13;
</dd>&#13;
<dt>Softmax</dt>&#13;
<dd>&#13;
<div class="openblock">&#13;
<p><a data-primary="softmax activation function, NN" data-type="indexterm" id="idm45737246996304"/>Like sigmoid, this activation function is widely applicable to classification problems because softmax converts input into probabilistic distribution proportional to the exponential of the input numbers:</p>&#13;
<div data-type="equation">&#13;
<math display="block">&#13;
  <mrow>&#13;
    <mtext>softmax</mtext>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <msub><mi>x</mi> <mi>i</mi> </msub>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <mfrac><mrow><mi>e</mi><mi>x</mi><mi>p</mi><mo>(</mo><msub><mi>x</mi> <mi>i</mi> </msub><mo>)</mo></mrow> <mrow><msub><mo>∑</mo> <mi>i</mi> </msub><mi>e</mi><mi>x</mi><mi>p</mi><mrow><mo>(</mo><msub><mi>x</mi> <mi>i</mi> </msub><mo>)</mo></mrow></mrow></mfrac>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
</div>&#13;
&#13;
</dd>&#13;
</dl>&#13;
</div></aside>&#13;
&#13;
<p>After configuring the model and generating a dependent variable, let’s extract the data and run the prediction for the stock prices for both Apple and Microsoft:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">6</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">ticker</code><code> </code><code class="o">=</code><code> </code><code class="p">[</code><code class="s1">'</code><code class="s1">AAPL</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="s1">'</code><code class="s1">MSFT</code><code class="s1">'</code><code class="p">]</code><code>&#13;
</code><code>        </code><code class="n">start</code><code> </code><code class="o">=</code><code> </code><code class="n">datetime</code><code class="o">.</code><code class="n">datetime</code><code class="p">(</code><code class="mi">2019</code><code class="p">,</code><code> </code><code class="mi">1</code><code class="p">,</code><code> </code><code class="mi">1</code><code class="p">)</code><code>&#13;
</code><code>        </code><code class="n">end</code><code> </code><code class="o">=</code><code> </code><code class="n">datetime</code><code class="o">.</code><code class="n">datetime</code><code class="p">(</code><code class="mi">2020</code><code class="p">,</code><code> </code><code class="mi">1</code><code> </code><code class="p">,</code><code class="mi">1</code><code class="p">)</code><code>&#13;
</code><code>        </code><code class="n">stock_prices</code><code> </code><code class="o">=</code><code> </code><code class="n">yf</code><code class="o">.</code><code class="n">download</code><code class="p">(</code><code class="n">ticker</code><code class="p">,</code><code class="n">start</code><code class="o">=</code><code class="n">start</code><code class="p">,</code><code> </code><code class="n">end</code><code> </code><code class="o">=</code><code> </code><code class="n">end</code><code class="p">,</code><code> </code><code class="n">interval</code><code class="o">=</code><code class="s1">'</code><code class="s1">1d</code><code class="s1">'</code><code class="p">)</code><code>\&#13;
</code><code>                       </code><code class="o">.</code><code class="n">Close</code><code>&#13;
</code><code>        </code><code class="p">[</code><code class="o">*</code><code class="o">*</code><code class="o">*</code><code class="o">*</code><code class="o">*</code><code class="o">*</code><code class="o">*</code><code class="o">*</code><code class="o">*</code><code class="o">*</code><code class="o">*</code><code class="o">*</code><code class="o">*</code><code class="o">*</code><code class="o">*</code><code class="o">*</code><code class="o">*</code><code class="o">*</code><code class="o">*</code><code class="o">*</code><code class="o">*</code><code class="mi">100</code><code class="o">%</code><code class="o">*</code><code class="o">*</code><code class="o">*</code><code class="o">*</code><code class="o">*</code><code class="o">*</code><code class="o">*</code><code class="o">*</code><code class="o">*</code><code class="o">*</code><code class="o">*</code><code class="o">*</code><code class="o">*</code><code class="o">*</code><code class="o">*</code><code class="o">*</code><code class="o">*</code><code class="o">*</code><code class="o">*</code><code class="o">*</code><code class="o">*</code><code class="o">*</code><code class="o">*</code><code class="p">]</code><code>  </code><code class="mi">2</code><code> </code><code class="n">of</code><code> </code><code class="mi">2</code><code> </code><code class="n">completed</code><code>&#13;
</code><code>&#13;
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">7</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">diff_stock_prices</code><code> </code><code class="o">=</code><code> </code><code class="n">stock_prices</code><code class="o">.</code><code class="n">diff</code><code class="p">(</code><code class="p">)</code><code class="o">.</code><code class="n">dropna</code><code class="p">(</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">8</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">split</code><code> </code><code class="o">=</code><code> </code><code class="nb">int</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">diff_stock_prices</code><code class="p">[</code><code class="s1">'</code><code class="s1">AAPL</code><code class="s1">'</code><code class="p">]</code><code class="o">.</code><code class="n">values</code><code class="p">)</code><code> </code><code class="o">*</code><code> </code><code class="mf">0.95</code><code class="p">)</code><code>&#13;
</code><code>        </code><code class="n">diff_train_aapl</code><code> </code><code class="o">=</code><code> </code><code class="n">diff_stock_prices</code><code class="p">[</code><code class="s1">'</code><code class="s1">AAPL</code><code class="s1">'</code><code class="p">]</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="p">:</code><code class="n">split</code><code class="p">]</code><code>&#13;
</code><code>        </code><code class="n">diff_test_aapl</code><code> </code><code class="o">=</code><code> </code><code class="n">diff_stock_prices</code><code class="p">[</code><code class="s1">'</code><code class="s1">AAPL</code><code class="s1">'</code><code class="p">]</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="n">split</code><code class="p">:</code><code class="p">]</code><code>&#13;
</code><code>        </code><code class="n">diff_train_msft</code><code> </code><code class="o">=</code><code> </code><code class="n">diff_stock_prices</code><code class="p">[</code><code class="s1">'</code><code class="s1">MSFT</code><code class="s1">'</code><code class="p">]</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="p">:</code><code class="n">split</code><code class="p">]</code><code>&#13;
</code><code>        </code><code class="n">diff_test_msft</code><code> </code><code class="o">=</code><code> </code><code class="n">diff_stock_prices</code><code class="p">[</code><code class="s1">'</code><code class="s1">MSFT</code><code class="s1">'</code><code class="p">]</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="n">split</code><code class="p">:</code><code class="p">]</code><code>&#13;
</code><code>&#13;
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">9</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">X_aapl</code><code class="p">,</code><code> </code><code class="n">y_aapl</code><code> </code><code class="o">=</code><code> </code><code class="n">split_sequence</code><code class="p">(</code><code class="n">diff_train_aapl</code><code class="p">,</code><code> </code><code class="n">n_steps</code><code class="p">)</code><code> </code><a class="co" href="#callout_deep_learning_for_time_series_modeling_CO2-1" id="co_deep_learning_for_time_series_modeling_CO2-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code>        </code><code class="n">X_aapl</code><code> </code><code class="o">=</code><code> </code><code class="n">X_aapl</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="p">(</code><code class="n">X_aapl</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="p">,</code><code>  </code><code class="n">X_aapl</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code><code class="p">,</code><code>&#13;
</code><code>                                 </code><code class="n">n_features</code><code class="p">)</code><code class="p">)</code><code> </code><a class="co" href="#callout_deep_learning_for_time_series_modeling_CO2-2" id="co_deep_learning_for_time_series_modeling_CO2-2"><img alt="2" src="assets/2.png"/></a><code>&#13;
</code><code>&#13;
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">10</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">history</code><code> </code><code class="o">=</code><code> </code><code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_aapl</code><code class="p">,</code><code> </code><code class="n">y_aapl</code><code class="p">,</code><code>&#13;
</code><code>                             </code><code class="n">epochs</code><code class="o">=</code><code class="mi">400</code><code class="p">,</code><code> </code><code class="n">batch_size</code><code class="o">=</code><code class="mi">150</code><code class="p">,</code><code> </code><code class="n">verbose</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code><code>&#13;
</code><code>                             </code><code class="n">validation_split</code><code> </code><code class="o">=</code><code> </code><code class="mf">0.10</code><code class="p">)</code><code> </code><a class="co" href="#callout_deep_learning_for_time_series_modeling_CO2-3" id="co_deep_learning_for_time_series_modeling_CO2-3"><img alt="3" src="assets/3.png"/></a><code>&#13;
</code><code>&#13;
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">11</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">start</code><code> </code><code class="o">=</code><code> </code><code class="n">X_aapl</code><code class="p">[</code><code class="n">X_aapl</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code> </code><code class="o">-</code><code> </code><code class="n">n_steps</code><code class="p">]</code><code> </code><a class="co" href="#callout_deep_learning_for_time_series_modeling_CO2-4" id="co_deep_learning_for_time_series_modeling_CO2-4"><img alt="4" src="assets/4.png"/></a><code>&#13;
</code><code>         </code><code class="n">x_input</code><code> </code><code class="o">=</code><code> </code><code class="n">start</code><code> </code><a class="co" href="#callout_deep_learning_for_time_series_modeling_CO2-5" id="co_deep_learning_for_time_series_modeling_CO2-5"><img alt="5" src="assets/5.png"/></a><code>&#13;
</code><code>         </code><code class="n">x_input</code><code> </code><code class="o">=</code><code> </code><code class="n">x_input</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code><code> </code><code class="n">n_steps</code><code class="p">,</code><code> </code><code class="n">n_features</code><code class="p">)</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">12</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">tempList_aapl</code><code> </code><code class="o">=</code><code> </code><code class="p">[</code><code class="p">]</code><code> </code><a class="co" href="#callout_deep_learning_for_time_series_modeling_CO2-6" id="co_deep_learning_for_time_series_modeling_CO2-6"><img alt="6" src="assets/6.png"/></a><code>&#13;
</code><code>         </code><code class="k">for</code><code> </code><code class="n">i</code><code> </code><code class="ow">in</code><code> </code><code class="nb">range</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">diff_test_aapl</code><code class="p">)</code><code class="p">)</code><code class="p">:</code><code>&#13;
</code><code>             </code><code class="n">x_input</code><code> </code><code class="o">=</code><code> </code><code class="n">x_input</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code><code> </code><code class="n">n_steps</code><code class="p">,</code><code> </code><code class="n">n_features</code><code class="p">)</code><code class="p">)</code><code> </code><a class="co" href="#callout_deep_learning_for_time_series_modeling_CO2-7" id="co_deep_learning_for_time_series_modeling_CO2-7"><img alt="7" src="assets/7.png"/></a><code>&#13;
</code><code>             </code><code class="n">yhat</code><code> </code><code class="o">=</code><code> </code><code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">x_input</code><code class="p">,</code><code> </code><code class="n">verbose</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code><code> </code><a class="co" href="#callout_deep_learning_for_time_series_modeling_CO2-8" id="co_deep_learning_for_time_series_modeling_CO2-8"><img alt="8" src="assets/8.png"/></a><code>&#13;
</code><code>             </code><code class="n">x_input</code><code> </code><code class="o">=</code><code> </code><code class="n">np</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">x_input</code><code class="p">,</code><code> </code><code class="n">yhat</code><code class="p">)</code><code>&#13;
</code><code>             </code><code class="n">x_input</code><code> </code><code class="o">=</code><code> </code><code class="n">x_input</code><code class="p">[</code><code class="mi">1</code><code class="p">:</code><code class="p">]</code><code>&#13;
</code><code>             </code><code class="n">tempList_aapl</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">yhat</code><code class="p">)</code><code> </code><a class="co" href="#callout_deep_learning_for_time_series_modeling_CO2-9" id="co_deep_learning_for_time_series_modeling_CO2-9"><img alt="9" src="assets/9.png"/></a><code>&#13;
</code><code>&#13;
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">13</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">X_msft</code><code class="p">,</code><code> </code><code class="n">y_msft</code><code> </code><code class="o">=</code><code> </code><code class="n">split_sequence</code><code class="p">(</code><code class="n">diff_train_msft</code><code class="p">,</code><code> </code><code class="n">n_steps</code><code class="p">)</code><code>&#13;
</code><code>         </code><code class="n">X_msft</code><code> </code><code class="o">=</code><code> </code><code class="n">X_msft</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="p">(</code><code class="n">X_msft</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="p">,</code><code>  </code><code class="n">X_msft</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code><code class="p">,</code><code>&#13;
</code><code>                                  </code><code class="n">n_features</code><code class="p">)</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">14</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">history</code><code> </code><code class="o">=</code><code> </code><code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_msft</code><code class="p">,</code><code> </code><code class="n">y_msft</code><code class="p">,</code><code>&#13;
</code><code>                             </code><code class="n">epochs</code><code class="o">=</code><code class="mi">400</code><code class="p">,</code><code> </code><code class="n">batch_size</code><code class="o">=</code><code class="mi">150</code><code class="p">,</code><code> </code><code class="n">verbose</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code><code>&#13;
</code><code>                             </code><code class="n">validation_split</code><code> </code><code class="o">=</code><code> </code><code class="mf">0.10</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">15</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">start</code><code> </code><code class="o">=</code><code> </code><code class="n">X_msft</code><code class="p">[</code><code class="n">X_msft</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code> </code><code class="o">-</code><code> </code><code class="n">n_steps</code><code class="p">]</code><code>&#13;
</code><code>         </code><code class="n">x_input</code><code> </code><code class="o">=</code><code> </code><code class="n">start</code><code>&#13;
</code><code>         </code><code class="n">x_input</code><code> </code><code class="o">=</code><code> </code><code class="n">x_input</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code><code> </code><code class="n">n_steps</code><code class="p">,</code><code> </code><code class="n">n_features</code><code class="p">)</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">16</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">tempList_msft</code><code> </code><code class="o">=</code><code> </code><code class="p">[</code><code class="p">]</code><code>&#13;
</code><code>         </code><code class="k">for</code><code> </code><code class="n">i</code><code> </code><code class="ow">in</code><code> </code><code class="nb">range</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">diff_test_msft</code><code class="p">)</code><code class="p">)</code><code class="p">:</code><code>&#13;
</code><code>             </code><code class="n">x_input</code><code> </code><code class="o">=</code><code> </code><code class="n">x_input</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code><code> </code><code class="n">n_steps</code><code class="p">,</code><code> </code><code class="n">n_features</code><code class="p">)</code><code class="p">)</code><code>&#13;
</code><code>             </code><code class="n">yhat</code><code> </code><code class="o">=</code><code> </code><code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">x_input</code><code class="p">,</code><code> </code><code class="n">verbose</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code><code>&#13;
</code><code>             </code><code class="n">x_input</code><code> </code><code class="o">=</code><code> </code><code class="n">np</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">x_input</code><code class="p">,</code><code> </code><code class="n">yhat</code><code class="p">)</code><code>&#13;
</code><code>             </code><code class="n">x_input</code><code> </code><code class="o">=</code><code> </code><code class="n">x_input</code><code class="p">[</code><code class="mi">1</code><code class="p">:</code><code class="p">]</code><code>&#13;
</code><code>             </code><code class="n">tempList_msft</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">yhat</code><code class="p">)</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_deep_learning_for_time_series_modeling_CO2-1" id="callout_deep_learning_for_time_series_modeling_CO2-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Calling the <code>split_sequence</code> function to define the lookback period</p></dd>&#13;
<dt><a class="co" href="#co_deep_learning_for_time_series_modeling_CO2-2" id="callout_deep_learning_for_time_series_modeling_CO2-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>Reshaping training data into a three-dimensional case</p></dd>&#13;
<dt><a class="co" href="#co_deep_learning_for_time_series_modeling_CO2-3" id="callout_deep_learning_for_time_series_modeling_CO2-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>Fitting the RNN model to Apple’s stock price</p></dd>&#13;
<dt><a class="co" href="#co_deep_learning_for_time_series_modeling_CO2-4" id="callout_deep_learning_for_time_series_modeling_CO2-4"><img alt="4" src="assets/4.png"/></a></dt>&#13;
<dd><p>Defining the starting point of the prediction for Apple</p></dd>&#13;
<dt><a class="co" href="#co_deep_learning_for_time_series_modeling_CO2-5" id="callout_deep_learning_for_time_series_modeling_CO2-5"><img alt="5" src="assets/5.png"/></a></dt>&#13;
<dd><p>Renaming the variable</p></dd>&#13;
<dt><a class="co" href="#co_deep_learning_for_time_series_modeling_CO2-6" id="callout_deep_learning_for_time_series_modeling_CO2-6"><img alt="6" src="assets/6.png"/></a></dt>&#13;
<dd><p>Creating an empty list to store predictions</p></dd>&#13;
<dt><a class="co" href="#co_deep_learning_for_time_series_modeling_CO2-7" id="callout_deep_learning_for_time_series_modeling_CO2-7"><img alt="7" src="assets/7.png"/></a></dt>&#13;
<dd><p>Reshaping the <code>x_input</code>, which is used for prediction</p></dd>&#13;
<dt><a class="co" href="#co_deep_learning_for_time_series_modeling_CO2-8" id="callout_deep_learning_for_time_series_modeling_CO2-8"><img alt="8" src="assets/8.png"/></a></dt>&#13;
<dd><p>Running prediction for Apple stock</p></dd>&#13;
<dt><a class="co" href="#co_deep_learning_for_time_series_modeling_CO2-9" id="callout_deep_learning_for_time_series_modeling_CO2-9"><img alt="9" src="assets/9.png"/></a></dt>&#13;
<dd><p>Storing <code>yhat</code> into <code>tempList_aapl</code></p></dd>&#13;
</dl>&#13;
&#13;
<p>For the sake of visualization, the following code block is used, resulting in <a data-type="xref" href="#rnn">Figure 3-2</a>:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">17</code><code class="p">]:</code> <code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code><code class="mi">1</code><code class="p">,</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">18</code><code class="p">,</code><code class="mi">15</code><code class="p">))</code>&#13;
         <code class="n">ax</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">diff_test_aapl</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s1">'Actual Stock Price'</code><code class="p">,</code> <code class="n">linestyle</code><code class="o">=</code><code class="s1">'--'</code><code class="p">)</code>&#13;
         <code class="n">ax</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">diff_test_aapl</code><code class="o">.</code><code class="n">index</code><code class="p">,</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">tempList_aapl</code><code class="p">)</code><code class="o">.</code><code class="n">flatten</code><code class="p">(),</code>&#13;
                    <code class="n">linestyle</code><code class="o">=</code><code class="s1">'solid'</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s2">"Prediction"</code><code class="p">)</code>&#13;
         <code class="n">ax</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">set_title</code><code class="p">(</code><code class="s1">'Predicted Stock Price-Apple'</code><code class="p">)</code>&#13;
         <code class="n">ax</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">legend</code><code class="p">(</code><code class="n">loc</code><code class="o">=</code><code class="s1">'best'</code><code class="p">)</code>&#13;
         <code class="n">ax</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">diff_test_msft</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s1">'Actual Stock Price'</code><code class="p">,</code> <code class="n">linestyle</code><code class="o">=</code><code class="s1">'--'</code><code class="p">)</code>&#13;
         <code class="n">ax</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">diff_test_msft</code><code class="o">.</code><code class="n">index</code><code class="p">,</code><code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">tempList_msft</code><code class="p">)</code><code class="o">.</code><code class="n">flatten</code><code class="p">(),</code>&#13;
                    <code class="n">linestyle</code><code class="o">=</code><code class="s1">'solid'</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s2">"Prediction"</code><code class="p">)</code>&#13;
         <code class="n">ax</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code><code class="o">.</code><code class="n">set_title</code><code class="p">(</code><code class="s1">'Predicted Stock Price-Microsoft'</code><code class="p">)</code>&#13;
         <code class="n">ax</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code><code class="o">.</code><code class="n">legend</code><code class="p">(</code><code class="n">loc</code><code class="o">=</code><code class="s1">'best'</code><code class="p">)</code>&#13;
&#13;
&#13;
         <code class="k">for</code> <code class="n">ax</code> <code class="ow">in</code> <code class="n">ax</code><code class="o">.</code><code class="n">flat</code><code class="p">:</code>&#13;
             <code class="n">ax</code><code class="o">.</code><code class="n">set</code><code class="p">(</code><code class="n">xlabel</code><code class="o">=</code><code class="s1">'Date'</code><code class="p">,</code> <code class="n">ylabel</code><code class="o">=</code><code class="s1">'$'</code><code class="p">)</code>&#13;
         <code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>&#13;
&#13;
<p><a data-type="xref" href="#rnn">Figure 3-2</a> shows the stock price prediction results for Apple and Microsoft. Simply eyeballing this, we can readily observe that there is room for improvement in terms of predictive performance of the model in both cases.</p>&#13;
&#13;
<p>Even if we can have satisfactory predictive performance, the drawbacks of the RNN model should not be overlooked. The main drawbacks of the model are:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>The vanishing or exploding gradient problem (please see the following note for a detailed explanation).</p>&#13;
</li>&#13;
<li>&#13;
<p>Training an RNN is a very difficult task as it requires a considerable amount of data.</p>&#13;
</li>&#13;
<li>&#13;
<p>An RNN is unable to process very long sequences when <a data-primary="tanh activation function, NN" data-type="indexterm" id="idm45737245988544"/>the <em>tanh</em> activation function is used.</p>&#13;
</li>&#13;
</ul>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>A <a data-primary="vanishing gradient problem" data-type="indexterm" id="idm45737245985968"/>vanishing gradient is a commonplace problem in deep learning scenarios that are not properly designed. The vanishing gradient problem arises if the gradient tends to get smaller as we conduct the backpropagation. It implies that neurons are learning so slowly that optimization grinds to a halt.</p>&#13;
&#13;
<p>Unlike the vanishing gradient problem, the <a data-primary="exploding gradient problem" data-type="indexterm" id="idm45737245984288"/>exploding gradient problem occurs when small changes in the backpropagation results in huge updates to the weights during the optimization process.</p>&#13;
</div>&#13;
&#13;
<figure><div class="figure" id="rnn">&#13;
<img alt="RNN predictions" src="assets/mlfr_0302.png"/>&#13;
<h6><span class="label">Figure 3-2. </span>RNN prediction results</h6>&#13;
</div></figure>&#13;
&#13;
<p>The drawbacks of RNNs are well stated by Haviv et al. (2019):</p>&#13;
<blockquote>&#13;
<p>This is due to the dependency of the network on its past states, and through them on the entire input history. This ability comes with a cost—RNNs are known to be hard to train (Pascanu et al. 2013a). This difficulty is commonly associated with the vanishing gradient that appears when trying to propagate errors over long times (Hochreiter 1998). When training is successful, the network’s hidden state represents these memories. Understanding how such representation forms throughout training can open new avenues for improving learning of memory-related tasks.<a data-primary="" data-startref="ix_deep_learn_timeser_rnn" data-type="indexterm" id="idm45737245979456"/><a data-primary="" data-startref="ix_nns_rnns" data-type="indexterm" id="idm45737245978464"/><a data-primary="" data-startref="ix_rnns_ch3" data-type="indexterm" id="idm45737245977520"/><a data-primary="" data-startref="ix_time_series_deep_learn_rnns" data-type="indexterm" id="idm45737245976576"/></p></blockquote>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Long-Short Term Memory" data-type="sect1"><div class="sect1" id="idm45737247875680">&#13;
<h1>Long-Short Term Memory</h1>&#13;
&#13;
<p><a data-primary="LSTM (long-short term memory) deep-learning approach" data-type="indexterm" id="ix_long-short_deep_learn"/><a data-primary="time series modeling" data-secondary="deep learning for" data-tertiary="long-short term memory" data-type="indexterm" id="ix_time_series_deep_learn_lstm"/><a data-primary="deep learning" data-secondary="time series modeling" data-tertiary="long-short term memory" data-type="indexterm" id="ix_deep_learn_timeser_long-short"/>The LSTM deep learning approach was developed by Hochreiter and Schmidhuber (1997) and is <a data-primary="GRU (gated recurrent unit)" data-type="indexterm" id="idm45737245969968"/>mainly based on the <em>gated recurrent unit</em> (GRU).</p>&#13;
&#13;
<p>GRU was proposed to deal with the vanishing gradient problem, which is common in neural network structures and occurs when the weight update becomes too small to create a significant change in the network. GRU consists of two gates: <em>update</em> and <em>reset</em>. When an early observation is detected as highly important, then we do not update the hidden state. Similarly, when early observations are not significant, that leads to resetting the state.</p>&#13;
&#13;
<p>As previously discussed, one of the most appealing features of an RNN is its ability to connect past and present information. <a data-primary="long-term dependencies, RNNs" data-type="indexterm" id="idm45737245966400"/>However, this ability turns out to be a failure when <em>long-term dependencies</em> comes into the picture. Long-term dependencies mean that the model learns from early observations.</p>&#13;
&#13;
<p>For instance, let’s examine the following sentence:</p>&#13;
&#13;
<p><em>Countries have their own currencies as in the USA, where people transact with dollars…</em></p>&#13;
&#13;
<p><a data-primary="short-term dependencies, RNNs" data-type="indexterm" id="idm45737245963632"/>In the case of short-term dependencies, it is known that the next predicted word is about a currency, but what if it is asked <em>which</em> currency it’s about? Things get complicated because we might have mentioned various currencies earlier on in the text, implying long-term dependencies. It is necessary to go way back to find something relevant about the countries using dollars.</p>&#13;
&#13;
<p>LSTM tries to attack the weakness of RNN regarding long-term dependencies. LSTM has a quite useful tool to get rid of the unnecessary information so that it works &#13;
<span class="keep-together">more efficiently</span>. LSTM works with gates, enabling it to forget irrelevant data. These &#13;
<span class="keep-together">gates are</span>:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Forget gates</p>&#13;
</li>&#13;
<li>&#13;
<p>Input gates</p>&#13;
</li>&#13;
<li>&#13;
<p>Output gates</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p><a data-primary="forget gates, LSTM" data-type="indexterm" id="idm45737245956576"/>Forget gates are created to sort out the necessary and unnecessary information so that LSTM performs more efficiently than RNN. In doing so, the value of the activation function, <em>sigmoid</em>, becomes zero if the information is irrelevant. Forget gates can be formulated as:</p>&#13;
<div data-type="equation">&#13;
<math display="block">&#13;
  <mrow>&#13;
    <msub><mi>F</mi> <mi>t</mi> </msub>&#13;
    <mo>=</mo>&#13;
    <mi>σ</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <msub><mi>X</mi> <mi>t</mi> </msub>&#13;
      <msub><mi>W</mi> <mi>I</mi> </msub>&#13;
      <mo>+</mo>&#13;
      <msub><mi>h</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow> </msub>&#13;
      <msub><mi>W</mi> <mi>f</mi> </msub>&#13;
      <mo>+</mo>&#13;
      <msub><mi>b</mi> <mi>f</mi> </msub>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>where <math alttext="sigma">&#13;
  <mi>σ</mi>&#13;
</math> is the activation function, <math alttext="h Subscript t minus 1">&#13;
  <msub><mi>h</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow> </msub>&#13;
</math> is the previous hidden state, <math alttext="upper W Subscript upper I">&#13;
  <msub><mi>W</mi> <mi>I</mi> </msub>&#13;
</math> and <math alttext="upper W Subscript f">&#13;
  <msub><mi>W</mi> <mi>f</mi> </msub>&#13;
</math> are weights, and finally, <math alttext="b Subscript f">&#13;
  <msub><mi>b</mi> <mi>f</mi> </msub>&#13;
</math> is the bias parameter in the forget cell.</p>&#13;
&#13;
<p><a data-primary="input gates, LSTM" data-type="indexterm" id="idm45737245918320"/>Input gates are fed by the current timestep, <math alttext="upper X Subscript t">&#13;
  <msub><mi>X</mi> <mi>t</mi> </msub>&#13;
</math>, and the hidden state of the previous timestep, <math alttext="t minus 1">&#13;
  <mrow>&#13;
    <mi>t</mi>&#13;
    <mo>-</mo>&#13;
    <mn>1</mn>&#13;
  </mrow>&#13;
</math>. The goal of input gates is to determine the extent that information should be added to the long-term state. The input gate can be formulated like this:</p>&#13;
<div data-type="equation">&#13;
<math display="block">&#13;
  <mrow>&#13;
    <msub><mi>I</mi> <mi>t</mi> </msub>&#13;
    <mo>=</mo>&#13;
    <mi>σ</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <msub><mi>X</mi> <mi>t</mi> </msub>&#13;
      <msub><mi>W</mi> <mi>I</mi> </msub>&#13;
      <mo>+</mo>&#13;
      <msub><mi>h</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow> </msub>&#13;
      <msub><mi>W</mi> <mi>f</mi> </msub>&#13;
      <mo>+</mo>&#13;
      <msub><mi>b</mi> <mi>I</mi> </msub>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p><a data-primary="output gates, LSTM" data-type="indexterm" id="idm45737245899360"/>Output gates basically determine the extent of the output that should be read, and work as follows:</p>&#13;
<div data-type="equation">&#13;
<math display="block">&#13;
  <mrow>&#13;
    <msub><mi>O</mi> <mi>t</mi> </msub>&#13;
    <mo>=</mo>&#13;
    <mi>σ</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <msub><mi>X</mi> <mi>t</mi> </msub>&#13;
      <msub><mi>W</mi> <mi>o</mi> </msub>&#13;
      <mo>+</mo>&#13;
      <msub><mi>h</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow> </msub>&#13;
      <msub><mi>W</mi> <mi>o</mi> </msub>&#13;
      <mo>+</mo>&#13;
      <msub><mi>b</mi> <mi>I</mi> </msub>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>These gates are not the sole components of LSTM. The other components are:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Candidate memory cell</p>&#13;
</li>&#13;
<li>&#13;
<p>Memory cell</p>&#13;
</li>&#13;
<li>&#13;
<p>Hidden state</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p><a data-primary="candidate memory cell, LSTM" data-type="indexterm" id="idm45737245881568"/>Candidate memory cell determines the extent to which information passes to the cell state. Differently, the activation function in the candidate cell is tanh and takes the following form:</p>&#13;
<div data-type="equation">&#13;
<math display="block">&#13;
  <mrow>&#13;
    <mover accent="true"><msub><mi>C</mi> <mi>t</mi> </msub> <mo>^</mo></mover>&#13;
    <mo>=</mo>&#13;
    <mi>ϕ</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <msub><mi>X</mi> <mi>t</mi> </msub>&#13;
      <msub><mi>W</mi> <mi>c</mi> </msub>&#13;
      <mo>+</mo>&#13;
      <msub><mi>h</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow> </msub>&#13;
      <msub><mi>W</mi> <mi>c</mi> </msub>&#13;
      <mo>+</mo>&#13;
      <msub><mi>b</mi> <mi>c</mi> </msub>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p><a data-primary="memory cell, LSTM" data-type="indexterm" id="idm45737245866368"/>Memory cell allows LSTM to remember or to forget the information:</p>&#13;
<div data-type="equation">&#13;
<math display="block">&#13;
  <mrow>&#13;
    <msub><mi>C</mi> <mi>t</mi> </msub>&#13;
    <mo>=</mo>&#13;
    <msub><mi>F</mi> <mi>t</mi> </msub>&#13;
    <mo>⊙</mo>&#13;
    <mi>C</mi>&#13;
    <mo>+</mo>&#13;
    <mrow>&#13;
      <mi>t</mi>&#13;
      <mo>-</mo>&#13;
      <mn>1</mn>&#13;
    </mrow>&#13;
    <mo>+</mo>&#13;
    <msub><mi>I</mi> <mi>t</mi> </msub>&#13;
    <mo>⊙</mo>&#13;
    <mover accent="true"><msub><mi>C</mi> <mi>t</mi> </msub> <mo>^</mo></mover>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>where <math alttext="circled-dot">&#13;
  <mo>⊙</mo>&#13;
</math> is Hadamard product.</p>&#13;
&#13;
<p><a data-primary="hidden state, LSTM" data-type="indexterm" id="idm45737245851408"/>In this recurrent network, hidden state is a tool to circulate information. Memory cell relates output gate to hidden state:</p>&#13;
<div data-type="equation">&#13;
<math display="block">&#13;
  <mrow>&#13;
    <msub><mi>h</mi> <mi>t</mi> </msub>&#13;
    <mo>=</mo>&#13;
    <mi>ϕ</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <msub><mi>c</mi> <mi>t</mi> </msub>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>⊙</mo>&#13;
    <msub><mi>O</mi> <mi>t</mi> </msub>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p><a data-type="xref" href="#LSTM_structure">Figure 3-3</a> exhibits the LSTM structure.</p>&#13;
&#13;
<figure><div class="figure" id="LSTM_structure">&#13;
<img alt="LSTM_structure" src="assets/mlfr_0303.png"/>&#13;
<h6><span class="label">Figure 3-3. </span>LSTM structure</h6>&#13;
</div></figure>&#13;
&#13;
<p>Now, let’s predict the stock prices using LSTM:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">18</code><code class="p">]</code><code class="p">:</code><code> </code><code class="kn">from</code><code> </code><code class="nn">tensorflow.keras.layers</code><code> </code><code class="kn">import</code><code> </code><code class="n">LSTM</code><code>&#13;
</code><code>&#13;
</code><code>&#13;
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">19</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">n_steps</code><code> </code><code class="o">=</code><code> </code><code class="mi">13</code><code> </code><a class="co" href="#callout_deep_learning_for_time_series_modeling_CO3-1" id="co_deep_learning_for_time_series_modeling_CO3-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code>         </code><code class="n">n_features</code><code> </code><code class="o">=</code><code> </code><code class="mi">1</code><code> </code><a class="co" href="#callout_deep_learning_for_time_series_modeling_CO3-2" id="co_deep_learning_for_time_series_modeling_CO3-2"><img alt="2" src="assets/2.png"/></a><code>&#13;
</code><code>&#13;
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">20</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">model</code><code> </code><code class="o">=</code><code> </code><code class="n">Sequential</code><code class="p">(</code><code class="p">)</code><code>&#13;
</code><code>         </code><code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">LSTM</code><code class="p">(</code><code class="mi">512</code><code class="p">,</code><code> </code><code class="n">activation</code><code class="o">=</code><code class="s1">'</code><code class="s1">relu</code><code class="s1">'</code><code class="p">,</code><code>&#13;
</code><code>                   </code><code class="n">input_shape</code><code class="o">=</code><code class="p">(</code><code class="n">n_steps</code><code class="p">,</code><code> </code><code class="n">n_features</code><code class="p">)</code><code class="p">,</code><code>&#13;
</code><code>                   </code><code class="n">return_sequences</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code><code class="p">)</code><code> </code><a class="co" href="#callout_deep_learning_for_time_series_modeling_CO3-3" id="co_deep_learning_for_time_series_modeling_CO3-3"><img alt="3" src="assets/3.png"/></a><code>&#13;
</code><code>         </code><code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dropout</code><code class="p">(</code><code class="mf">0.2</code><code class="p">)</code><code class="p">)</code><code> </code><a class="co" href="#callout_deep_learning_for_time_series_modeling_CO3-4" id="co_deep_learning_for_time_series_modeling_CO3-4"><img alt="4" src="assets/4.png"/></a><code>&#13;
</code><code>         </code><code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">LSTM</code><code class="p">(</code><code class="mi">256</code><code class="p">,</code><code class="n">activation</code><code class="o">=</code><code class="s1">'</code><code class="s1">relu</code><code class="s1">'</code><code class="p">)</code><code class="p">)</code><code> </code><a class="co" href="#callout_deep_learning_for_time_series_modeling_CO3-5" id="co_deep_learning_for_time_series_modeling_CO3-5"><img alt="5" src="assets/5.png"/></a><code>&#13;
</code><code>         </code><code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Flatten</code><code class="p">(</code><code class="p">)</code><code class="p">)</code><a class="co" href="#callout_deep_learning_for_time_series_modeling_CO3-6" id="co_deep_learning_for_time_series_modeling_CO3-6"><img alt="6" src="assets/6.png"/></a><code>&#13;
</code><code>         </code><code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dense</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code><code> </code><code class="n">activation</code><code class="o">=</code><code class="s1">'</code><code class="s1">linear</code><code class="s1">'</code><code class="p">)</code><code class="p">)</code><code> </code><a class="co" href="#callout_deep_learning_for_time_series_modeling_CO3-7" id="co_deep_learning_for_time_series_modeling_CO3-7"><img alt="7" src="assets/7.png"/></a><code>&#13;
</code><code>&#13;
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">21</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">model</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">optimizer</code><code class="o">=</code><code class="s1">'</code><code class="s1">rmsprop</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="n">loss</code><code class="o">=</code><code class="s1">'</code><code class="s1">mean_squared_error</code><code class="s1">'</code><code class="p">,</code><code>&#13;
</code><code>                       </code><code class="n">metrics</code><code class="o">=</code><code class="p">[</code><code class="s1">'</code><code class="s1">mse</code><code class="s1">'</code><code class="p">]</code><code class="p">)</code><code> </code><a class="co" href="#callout_deep_learning_for_time_series_modeling_CO3-8" id="co_deep_learning_for_time_series_modeling_CO3-8"><img alt="8" src="assets/8.png"/></a><code>&#13;
</code><code>&#13;
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">22</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">history</code><code> </code><code class="o">=</code><code> </code><code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_aapl</code><code class="p">,</code><code> </code><code class="n">y_aapl</code><code class="p">,</code><code>&#13;
</code><code>                             </code><code class="n">epochs</code><code class="o">=</code><code class="mi">400</code><code class="p">,</code><code> </code><code class="n">batch_size</code><code class="o">=</code><code class="mi">150</code><code class="p">,</code><code> </code><code class="n">verbose</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code><code>&#13;
</code><code>                             </code><code class="n">validation_split</code><code> </code><code class="o">=</code><code> </code><code class="mf">0.10</code><code class="p">)</code><code> </code><a class="co" href="#callout_deep_learning_for_time_series_modeling_CO3-9" id="co_deep_learning_for_time_series_modeling_CO3-9"><img alt="9" src="assets/9.png"/></a><code>&#13;
</code><code>&#13;
</code><code class="n">In</code><code> </code><code class="p">[</code><code class="mi">23</code><code class="p">]</code><code class="p">:</code><code> </code><code class="n">start</code><code> </code><code class="o">=</code><code> </code><code class="n">X_aapl</code><code class="p">[</code><code class="n">X_aapl</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code> </code><code class="o">-</code><code> </code><code class="mi">13</code><code class="p">]</code><code>&#13;
</code><code>         </code><code class="n">x_input</code><code> </code><code class="o">=</code><code> </code><code class="n">start</code><code>&#13;
</code><code>         </code><code class="n">x_input</code><code> </code><code class="o">=</code><code> </code><code class="n">x_input</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code><code> </code><code class="n">n_steps</code><code class="p">,</code><code> </code><code class="n">n_features</code><code class="p">)</code><code class="p">)</code></pre>&#13;
<dl class="calloutlist pagebreak-before less_space">&#13;
<dt><a class="co" href="#co_deep_learning_for_time_series_modeling_CO3-1" id="callout_deep_learning_for_time_series_modeling_CO3-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Defining the number of steps for prediction</p></dd>&#13;
<dt><a class="co" href="#co_deep_learning_for_time_series_modeling_CO3-2" id="callout_deep_learning_for_time_series_modeling_CO3-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>Defining the number of feature as 1</p></dd>&#13;
<dt><a class="co" href="#co_deep_learning_for_time_series_modeling_CO3-3" id="callout_deep_learning_for_time_series_modeling_CO3-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>Identifying the number of hidden neurons, the activation function, which is <code>relu</code>, and input shape</p></dd>&#13;
<dt><a class="co" href="#co_deep_learning_for_time_series_modeling_CO3-4" id="callout_deep_learning_for_time_series_modeling_CO3-4"><img alt="4" src="assets/4.png"/></a></dt>&#13;
<dd><p>Adding a dropout layer to prevent overfitting</p></dd>&#13;
<dt><a class="co" href="#co_deep_learning_for_time_series_modeling_CO3-5" id="callout_deep_learning_for_time_series_modeling_CO3-5"><img alt="5" src="assets/5.png"/></a></dt>&#13;
<dd><p>Adding one more hidden layer with 256 neurons, with a <code>relu</code> activation &#13;
<span class="keep-together">function</span></p></dd>&#13;
<dt><a class="co" href="#co_deep_learning_for_time_series_modeling_CO3-6" id="callout_deep_learning_for_time_series_modeling_CO3-6"><img alt="6" src="assets/6.png"/></a></dt>&#13;
<dd><p>Flattening the model to vectorize the three-dimensional matrix</p></dd>&#13;
<dt><a class="co" href="#co_deep_learning_for_time_series_modeling_CO3-7" id="callout_deep_learning_for_time_series_modeling_CO3-7"><img alt="7" src="assets/7.png"/></a></dt>&#13;
<dd><p>Adding an output layer with a <code>linear</code> activation function</p></dd>&#13;
<dt><a class="co" href="#co_deep_learning_for_time_series_modeling_CO3-8" id="callout_deep_learning_for_time_series_modeling_CO3-8"><img alt="8" src="assets/8.png"/></a></dt>&#13;
<dd><p>Compiling LSTM with Root Mean Square Propagation, <code>rmsprop</code>, and mean squared error (MSE), <code>mean_squared_error</code></p></dd>&#13;
<dt><a class="co" href="#co_deep_learning_for_time_series_modeling_CO3-9" id="callout_deep_learning_for_time_series_modeling_CO3-9"><img alt="9" src="assets/9.png"/></a></dt>&#13;
<dd><p>Fitting the LSTM model to Apple’s stock price</p></dd>&#13;
</dl>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p><a data-primary="RMSprop (Root Mean Square Propagation)" data-type="indexterm" id="idm45737246408656"/>Root Mean Square Propagation (<code>RMSProp</code>) is an optimization method in which we calculate the moving average of the squared gradients for each weight. We then find the difference of weight, which is to be used to compute the new weight:</p>&#13;
<div data-type="equation">&#13;
<math display="block">&#13;
  <mrow>&#13;
    <msub><mi>v</mi> <mi>t</mi> </msub>&#13;
    <mo>=</mo>&#13;
    <msub><mi>ρ</mi> <msub><mi>v</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow> </msub> </msub>&#13;
    <mo>+</mo>&#13;
    <mrow>&#13;
      <mn>1</mn>&#13;
      <mo>-</mo>&#13;
      <mi>ρ</mi>&#13;
    </mrow>&#13;
    <msubsup><mi>g</mi> <mi>t</mi> <mn>2</mn> </msubsup>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
<div data-type="equation">&#13;
<math display="block">&#13;
  <mrow>&#13;
    <mi>Δ</mi>&#13;
    <msub><mi>w</mi> <mi>t</mi> </msub>&#13;
    <mo>=</mo>&#13;
    <mo>-</mo>&#13;
    <mfrac><mi>ν</mi> <msqrt><mrow><mi>η</mi><mo>+</mo><mi>ϵ</mi></mrow></msqrt></mfrac>&#13;
    <msub><mi>g</mi> <mi>t</mi> </msub>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
<div data-type="equation">&#13;
<math display="block">&#13;
  <mrow>&#13;
    <msub><mi>w</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow> </msub>&#13;
    <mo>=</mo>&#13;
    <msub><mi>w</mi> <mi>t</mi> </msub>&#13;
    <mo>+</mo>&#13;
    <mi>Δ</mi>&#13;
    <msub><mi>w</mi> <mi>t</mi> </msub>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>Pursuing the same procedure and given the Microsoft stock price, a prediction analysis is carried out:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">24</code><code class="p">]:</code> <code class="n">tempList_aapl</code> <code class="o">=</code> <code class="p">[]</code>&#13;
         <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">diff_test_aapl</code><code class="p">)):</code>&#13;
             <code class="n">x_input</code> <code class="o">=</code> <code class="n">x_input</code><code class="o">.</code><code class="n">reshape</code><code class="p">((</code><code class="mi">1</code><code class="p">,</code> <code class="n">n_steps</code><code class="p">,</code> <code class="n">n_features</code><code class="p">))</code>&#13;
             <code class="n">yhat</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">x_input</code><code class="p">,</code> <code class="n">verbose</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>&#13;
             <code class="n">x_input</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">x_input</code><code class="p">,</code> <code class="n">yhat</code><code class="p">)</code>&#13;
             <code class="n">x_input</code> <code class="o">=</code> <code class="n">x_input</code><code class="p">[</code><code class="mi">1</code><code class="p">:]</code>&#13;
             <code class="n">tempList_aapl</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">yhat</code><code class="p">)</code>&#13;
&#13;
<code class="n">In</code> <code class="p">[</code><code class="mi">25</code><code class="p">]:</code> <code class="n">history</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_msft</code><code class="p">,</code> <code class="n">y_msft</code><code class="p">,</code>&#13;
                             <code class="n">epochs</code><code class="o">=</code><code class="mi">400</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">150</code><code class="p">,</code> <code class="n">verbose</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code>&#13;
                             <code class="n">validation_split</code> <code class="o">=</code> <code class="mf">0.10</code><code class="p">)</code>&#13;
&#13;
<code class="n">In</code> <code class="p">[</code><code class="mi">26</code><code class="p">]:</code> <code class="n">start</code> <code class="o">=</code> <code class="n">X_msft</code><code class="p">[</code><code class="n">X_msft</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code> <code class="o">-</code> <code class="mi">13</code><code class="p">]</code>&#13;
         <code class="n">x_input</code> <code class="o">=</code> <code class="n">start</code>&#13;
         <code class="n">x_input</code> <code class="o">=</code> <code class="n">x_input</code><code class="o">.</code><code class="n">reshape</code><code class="p">((</code><code class="mi">1</code><code class="p">,</code> <code class="n">n_steps</code><code class="p">,</code> <code class="n">n_features</code><code class="p">))</code>&#13;
&#13;
<code class="n">In</code> <code class="p">[</code><code class="mi">27</code><code class="p">]:</code> <code class="n">tempList_msft</code> <code class="o">=</code> <code class="p">[]</code>&#13;
         <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">diff_test_msft</code><code class="p">)):</code>&#13;
             <code class="n">x_input</code> <code class="o">=</code> <code class="n">x_input</code><code class="o">.</code><code class="n">reshape</code><code class="p">((</code><code class="mi">1</code><code class="p">,</code> <code class="n">n_steps</code><code class="p">,</code> <code class="n">n_features</code><code class="p">))</code>&#13;
             <code class="n">yhat</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">x_input</code><code class="p">,</code> <code class="n">verbose</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>&#13;
             <code class="n">x_input</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">x_input</code><code class="p">,</code> <code class="n">yhat</code><code class="p">)</code>&#13;
             <code class="n">x_input</code> <code class="o">=</code> <code class="n">x_input</code><code class="p">[</code><code class="mi">1</code><code class="p">:]</code>&#13;
             <code class="n">tempList_msft</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">yhat</code><code class="p">)</code></pre>&#13;
&#13;
<p>The following code creates the plot (<a data-type="xref" href="#LSTM">Figure 3-4</a>) that shows the prediction results:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">28</code><code class="p">]:</code> <code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">18</code><code class="p">,</code> <code class="mi">15</code><code class="p">))</code>&#13;
         <code class="n">ax</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">diff_test_aapl</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s1">'Actual Stock Price'</code><code class="p">,</code> <code class="n">linestyle</code><code class="o">=</code><code class="s1">'--'</code><code class="p">)</code>&#13;
         <code class="n">ax</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">diff_test_aapl</code><code class="o">.</code><code class="n">index</code><code class="p">,</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">tempList_aapl</code><code class="p">)</code><code class="o">.</code><code class="n">flatten</code><code class="p">(),</code>&#13;
                    <code class="n">linestyle</code><code class="o">=</code><code class="s1">'solid'</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s2">"Prediction"</code><code class="p">)</code>&#13;
         <code class="n">ax</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">set_title</code><code class="p">(</code><code class="s1">'Predicted Stock Price-Apple'</code><code class="p">)</code>&#13;
         <code class="n">ax</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">legend</code><code class="p">(</code><code class="n">loc</code><code class="o">=</code><code class="s1">'best'</code><code class="p">)</code>&#13;
         <code class="n">ax</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">diff_test_msft</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s1">'Actual Stock Price'</code><code class="p">,</code> <code class="n">linestyle</code><code class="o">=</code><code class="s1">'--'</code><code class="p">)</code>&#13;
         <code class="n">ax</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">diff_test_msft</code><code class="o">.</code><code class="n">index</code><code class="p">,</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">tempList_msft</code><code class="p">)</code><code class="o">.</code><code class="n">flatten</code><code class="p">(),</code>&#13;
                    <code class="n">linestyle</code><code class="o">=</code><code class="s1">'solid'</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s2">"Prediction"</code><code class="p">)</code>&#13;
         <code class="n">ax</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code><code class="o">.</code><code class="n">set_title</code><code class="p">(</code><code class="s1">'Predicted Stock Price-Microsoft'</code><code class="p">)</code>&#13;
         <code class="n">ax</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code><code class="o">.</code><code class="n">legend</code><code class="p">(</code><code class="n">loc</code><code class="o">=</code><code class="s1">'best'</code><code class="p">)</code>&#13;
&#13;
         <code class="k">for</code> <code class="n">ax</code> <code class="ow">in</code> <code class="n">ax</code><code class="o">.</code><code class="n">flat</code><code class="p">:</code>&#13;
             <code class="n">ax</code><code class="o">.</code><code class="n">set</code><code class="p">(</code><code class="n">xlabel</code><code class="o">=</code><code class="s1">'Date'</code><code class="p">,</code> <code class="n">ylabel</code><code class="o">=</code><code class="s1">'$'</code><code class="p">)</code>&#13;
         <code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>&#13;
&#13;
<p>LSTM seems to outperform the RNN, particularly in the way it captures the extreme values &#13;
<span class="keep-together">better</span>.<a data-primary="" data-startref="ix_deep_learn_timeser_long-short" data-type="indexterm" id="idm45737246102416"/><a data-primary="" data-startref="ix_long-short_deep_learn" data-type="indexterm" id="idm45737246101488"/><a data-primary="" data-startref="ix_time_series_deep_learn_lstm" data-type="indexterm" id="idm45737244563936"/></p>&#13;
&#13;
<figure><div class="figure" id="LSTM">&#13;
<img alt="LSTM predictions" src="assets/mlfr_0304.png"/>&#13;
<h6><span class="label">Figure 3-4. </span>LSTM prediction results</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Conclusion" data-type="sect1"><div class="sect1" id="idm45737245974704">&#13;
<h1>Conclusion</h1>&#13;
&#13;
<p>This chapter was about predicting stock prices based on deep learning. The models used are RNN and LSTM, which have the ability to process longer time periods. These models do not suggest remarkable improvement but still can be employed to model time series data. LSTM considers, in our case, a 13-step lookback period for prediction. For an extension, it would be a wise approach to include multiple features in the models based on deep learning, which is not allowed in parametric time series models.<a data-primary="" data-startref="ix_deep_learn_ch3" data-type="indexterm" id="idm45737244559936"/><a data-primary="" data-startref="ix_time_series_deep_learn" data-type="indexterm" id="idm45737244559088"/></p>&#13;
&#13;
<p>In the next chapter, we will discuss volatility predictions based on parametric and ML models so that we can compare their performance.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="pagebreak-before less_space" data-pdf-bookmark="References" data-type="sect1"><div class="sect1" id="idm45737244557456">&#13;
<h1>References</h1>&#13;
&#13;
<p>Articles cited in this chapter:</p>&#13;
&#13;
<ul class="author-date-bib">&#13;
<li>&#13;
<p>Ding, Daizong, et al. 2019. “Modeling Extreme Events in Time Series Prediction.” <em>Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>. 1114-1122.</p>&#13;
</li>&#13;
<li>&#13;
<p>Haviv, Doron, Alexander Rivkind, and Omri Barak. 2019. “Understanding and Controlling Memory in Recurrent Neural Networks.” arXiv preprint. arXiv:1902.07275.</p>&#13;
</li>&#13;
<li>&#13;
<p>Hochreiter, Sepp, and Jürgen Schmidhuber. 1997. “Long Short-term Memory.” <em>Neural Computation</em> 9 (8): 1735-1780.</p>&#13;
</li>&#13;
<li>&#13;
<p>LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. 2015. “Deep Learning.” <em>Nature</em> 521, (7553): 436-444.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Books cited in this chapter:</p>&#13;
&#13;
<ul class="author-date-bib">&#13;
<li>&#13;
<p>Buduma, N., and N. Locascio. 2017. <em>Fundamentals of Deep Learning: Designing Next-generation Machine Intelligence Algorithms</em>. Sebastopol: O’Reilly.</p>&#13;
</li>&#13;
<li>&#13;
<p>Goodfellow, I., Y. Bengio, and A. Courville. 2016. <em>Deep Learning</em>. Cambridge, MA: MIT Press.</p>&#13;
</li>&#13;
<li>&#13;
<p>Nielsen, A. 2019. <em>Practical Time Series Analysis: Prediction with Statistics and Machine Learning</em>. Sebastopol: O’Reilly.</p>&#13;
</li>&#13;
<li>&#13;
<p>Patterson, Josh, and Adam Gibson. 2017. <em>Deep Learning: A Practitioner’S Approach</em>. Sebastopol: O’Reilly.</p>&#13;
</li>&#13;
<li>&#13;
<p>Sejnowski, Terrence J. 2018. <em>The Deep Learning Revolution</em>. Cambridge, MA: MIT Press.</p>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<div data-type="footnotes"><p data-type="footnote" id="idm45737247845584"><sup><a href="ch03.html#idm45737247845584-marker">1</a></sup> Representation learning helps us define a concept in a unique way. For instance, if the task is to detect whether something is a circle, then edges play a key role, as a circle has no edge. So using color, shape, and size, we can create a representation for an object. In essence, this is how the human brain works, and we know that deep learning structures are inspired by the brain’s functioning.</p><p data-type="footnote" id="idm45737247426656"><sup><a href="ch03.html#idm45737247426656-marker">2</a></sup> Patterson et. al, 2017. “Deep learning: A practitioner’s approach.”</p></div></div></section></body></html>