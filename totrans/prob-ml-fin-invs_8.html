<html><head></head><body><section data-pdf-bookmark="Chapter 8. Making Probabilistic Decisions with Generative Ensembles" data-type="chapter" epub:type="chapter"><div class="chapter" id="making_probabilistic_decisions_with_gen">&#13;
<h1><span class="label">Chapter 8. </span>Making Probabilistic Decisions with Generative Ensembles</h1>&#13;
&#13;
<blockquote data-type="epigraph" epub:type="epigraph">&#13;
<p>But I realized that the odds as the game progressed actually depended on which cards were still left in the deck and that the edge would shift as play continued, sometimes favoring the casino and sometimes the player.</p>&#13;
&#13;
<p>—Dr. Edward O. Thorp, the greatest quantitative gambler and trader of all time</p>&#13;
</blockquote>&#13;
&#13;
<p>In the previous chapter, we designed,<a contenteditable="false" data-primary="decision making" data-secondary="probabilistic with generative ensembles" data-tertiary="about" data-type="indexterm" id="id1329"/><a contenteditable="false" data-primary="probabilistic decision making with generative ensembles" data-secondary="about" data-type="indexterm" id="id1330"/><a contenteditable="false" data-primary="AI" data-secondary="generative AI" data-tertiary="about generative ensembles" data-type="indexterm" id="id1331"/><a contenteditable="false" data-primary="probabilistic financial models" data-secondary="as generative models" data-secondary-sortas="generative models" data-tertiary="about" data-type="indexterm" id="id1332"/><a contenteditable="false" data-primary="probabilistic machine learning (PML)" data-secondary="generative ensembles" data-tertiary="about" data-type="indexterm" id="id1333"/><a contenteditable="false" data-primary="generative AI" data-secondary="generative ensembles" data-tertiary="about" data-type="indexterm" id="id1334"/><a contenteditable="false" data-primary="Thorp, Edward O." data-type="indexterm" id="id1335"/><a contenteditable="false" data-primary="linear regression" data-secondary="probabilistic versus frequentist/conventional" data-seealso="probabilistic linear ensembles" data-type="indexterm" id="id1336"/><a contenteditable="false" data-primary="probabilistic linear ensembles" data-secondary="frequentist linear regression versus probabilistic" data-type="indexterm" id="id1337"/> developed, trained, and tested a generative ensemble of linear regression lines. Probabilistic linear regression is fundamentally different from frequentist or conventional linear regression, introduced in <a data-type="xref" href="ch04.html#the_dangers_of_conventional_statistical">Chapter 4</a>. For starters, frequentist linear regression produces a single regression line with parameters optimized to fit a noisy financial dataset generated by a stochastic process that is neither stationary nor ergodic. Probabilistic linear regression generates many regression lines, each corresponding to different combinations of possible parameters, which can fit the observed data distribution with various plausibilities while remaining consistent with prior knowledge and model assumptions.</p>&#13;
&#13;
<p>Generative ensembles have the<a contenteditable="false" data-primary="generative AI" data-secondary="generative ensembles" data-tertiary="continually learning and revising parameters" data-type="indexterm" id="id1338"/><a contenteditable="false" data-primary="AI" data-secondary="generative AI" data-tertiary="continually learning and revising parameters" data-type="indexterm" id="id1339"/><a contenteditable="false" data-primary="parameters of a model" data-secondary="parameter inference by ML systems" data-tertiary="probabilistic models" data-type="indexterm" id="id1340"/><a contenteditable="false" data-primary="probabilistic machine learning (PML)" data-secondary="generative ensembles" data-tertiary="continually learning and revising parameters" data-type="indexterm" id="id1341"/> desirable characteristics of being capable of continually learning and revising model parameters from data and explicitly stated past knowledge. <a contenteditable="false" data-primary="generative AI" data-secondary="generative ensembles" data-tertiary="simulating new data and counterfactual knowledge" data-type="indexterm" id="id1342"/><a contenteditable="false" data-primary="probabilistic machine learning (PML)" data-secondary="generative ensembles" data-tertiary="simulating new data and counterfactual knowledge" data-type="indexterm" id="id1343"/><a contenteditable="false" data-primary="AI" data-secondary="generative AI" data-tertiary="simulating new data and counterfactual knowledge" data-type="indexterm" id="id1344"/>What truly distinguishes generative ensembles from their conventional counterparts are their capabilities of seamlessly simulating new data and counterfactual knowledge conditioned on the observed data and model assumptions on which they were trained and tested regardless of the size of the dataset or the ordering of the data.</p>&#13;
&#13;
<p class="pagebreak-before">Generative ensembles do all these activities consistently with their transparent model assumptions and the rigors of probability calculus, while appropriately scaling the aleatory and epistemic uncertainties inherent in such predictions and counterfactual knowledge. Probabilistic models know their limitations and honestly express their ignorance by widening their highest-density intervals in their extrapolations.</p>&#13;
&#13;
<p>In the previous three chapters, we were primarily focused on inferring the distributions of our ensemble’s parameters. In this chapter, we focus our attention on using the simulated outputs of our trained and tested generative ensembles for making financial and investment decisions in the face of three-dimensional uncertainty and incomplete information. In other words, our focus will be on the data-generating posterior predictive distribution of our model instead of the posterior distribution of its parameters. Generally speaking, the ensemble’s outputs are what decision makers understand and need for making their decisions. For instance, the distribution of stock price returns is more meaningful to senior management and clients than the distribution of the alpha and beta parameters of the model used to generate them.</p>&#13;
&#13;
<p>After reviewing the probabilistic inference and prediction framework used in this book, we systematize our approach to decision making by using objective functions. In the first example of probabilistic decision making, we explore how you can use the framework to integrate subjective human behavior with the objectivity of data and rigors of probability calculus. Finance and investing involves people, not particles or pendulums, and a decision-making framework that cannot integrate the intrinsic subjectivity of humanity is utterly useless. This also emphasizes the fact that decision making is both an art and a science in which human common sense and judgment are of paramount importance.</p>&#13;
&#13;
<p>Two loss functions that are commonly<a contenteditable="false" data-primary="loss functions" data-type="indexterm" id="id1345"/><a contenteditable="false" data-primary="probabilistic decision making with generative ensembles" data-secondary="loss functions defined" data-type="indexterm" id="id1346"/><a contenteditable="false" data-primary="decision making" data-secondary="probabilistic with generative ensembles" data-tertiary="loss functions defined" data-type="indexterm" id="id1347"/> used by risk managers and corporate treasurers are value at risk (VaR) and expected shortfall (ES). I introduce a new method of computing these risk measures as an integral part of generative ensembles. To use ensemble averages and its simulated data appropriately, we explore the statistical concept of ergodicity to understand why expected value or ensemble average has severe limitations and doesn’t work as conventional economic theory will have us believe.</p>&#13;
&#13;
<p>Finally, we explore the complex problem of allocating our hard-earned capital to favorable investment opportunities without the risk of financial ruin at any time. We examine the differences between gambling and investing, making decisions regarding one-off investments and a sequence of investments. The two most important capital allocation algorithms, Markowitz’s mean variance and Kelly’s capital growth investment criterion, are applied and their strengths and weaknesses are examined.</p>&#13;
&#13;
<section class="pagebreak-before" data-pdf-bookmark="Probabilistic Inference and Prediction Framework" data-type="sect1"><div class="sect1" id="probabilistic_inference_and_prediction">&#13;
<h1>Probabilistic Inference and Prediction Framework</h1>&#13;
&#13;
<p>Let’s review and summarize the<a contenteditable="false" data-primary="probabilistic decision making with generative ensembles" data-secondary="probabilistic inference and prediction framework" data-type="indexterm" id="ch08-inf"/><a contenteditable="false" data-primary="decision making" data-secondary="probabilistic with generative ensembles" data-tertiary="probabilistic inference and prediction framework" data-type="indexterm" id="ch08-inf2"/> framework we have used in the second half of the book to make inferences about model parameters, retrodictions about in-sample training data distributions, and predictions about out-of-sample test data distributions. We will illustrate this framework by using the debt default example from <a data-type="xref" href="ch05.html#the_probabilistic_machine_learning_fram">Chapter 5</a>—when you were working as an analyst at the hedge fund that invested in high-yielding debt or “junk” bonds:</p>&#13;
&#13;
	&#13;
<ol>&#13;
  <li>&#13;
	<p>Specify all the possible scenarios or event outcomes that can occur in the sample space. The scenarios S<sub>1</sub> and S<sub>2</sub> are the model parameters that we want to estimate:</p>&#13;
&#13;
	<ul>&#13;
		<li>&#13;
		<p>S<sub>1</sub> is the scenario in which XYZ portfolio company defaults on its debt obligations. S<sub>2</sub> is the scenario in which it doesn’t.</p>&#13;
		</li>&#13;
		<li>&#13;
		<p>Scenarios S<sub>1</sub> and S<sub>2</sub> are mutually exclusive and collectively exhaustive, which implies P(default) + P(no default) = 1.</p>&#13;
		</li>&#13;
	</ul>&#13;
  </li>&#13;
	<li>&#13;
    <p>Research and use any and all personal, institutional, scientific, and common knowledge about the problem domain that might help you to design your model and assign prior probabilities to the various parameters in the sample space before observing any new data. This is the prior probability distribution of the model.</p>&#13;
&#13;
	<ul>&#13;
		<li>&#13;
		<p>Your hedge fund management team used its experience, expertise, and institutional knowledge to estimate the following prior probabilities for the parameters S<sub>1</sub> and S<sub>2</sub>:</p>&#13;
&#13;
		<ul class="simplelist">&#13;
			<li>&#13;
			<p>P(default) = 0.10 and P(No default) = 0.90</p>&#13;
			</li>&#13;
		</ul>&#13;
		</li>&#13;
	</ul>&#13;
	</li>&#13;
	<li>&#13;
	<p>Apply similar prior knowledge and domain expertise to specify likelihood functions for each model parameter. Understand what kind of data might be generated from your parametric model.</p>&#13;
&#13;
	<ul>&#13;
		<li>&#13;
		<p>You used your fund’s proprietary ML classification system that leveraged the features of a valuable database about debt defaulters and nondefaulters. In particular, your fund’s analysts have found that companies that eventually default on their debt accumulate 70% negative ratings. However, the companies that do not eventually default only accumulate 40% negative ratings.</p>&#13;
		</li>&#13;
		<li>&#13;
		<p>The likelihood functions of the model are: P(negative | default) = 0.70 and P(negative | no default) = 0.40</p>&#13;
		</li>&#13;
	</ul>&#13;
	</li>&#13;
	<li>&#13;
	<p class="pagebreak-before">Generate data D′ using the model’s prior predictive distribution. The model generates yet-to-be-seen data by averaging the likelihood function over the prior probability distribution of its parameters. The prior predictive distribution serves as an initial model check by simulating data we might have observed in the past based on our current model. The prior predictive distribution is a retrodiction of past data. In general, we can compare the data distribution to our prior knowledge. In particular, we can compare its simulated data to the training data.</p>&#13;
&#13;
	<ul>&#13;
		<li>&#13;
		<p>Based on all your model’s assumptions encoded in your prior probability distribution and the likelihood function, you can expect XYZ portfolio company to generate negative and positive ratings with the following probabilities:</p>&#13;
		</li>&#13;
		</ul>&#13;
		<ul class="simplelist">&#13;
        <li>&#13;
		<p>P(negative) = P(negative | default) P(default) + P(negative | no default) P( no default) = (0.70 × 0.10) + (0.40 × 0.90) = 0.43</p>&#13;
        </li>&#13;
		<li>&#13;
		<p>P(positive) = P(positive | default) P(default) + P(positive | no default) P( no default) = (0.30 × 0.10) + (0.60 × 0.90) = 0.57</p>&#13;
		</li>&#13;
        </ul>&#13;
	</li>&#13;
	<li>&#13;
	<p>Conduct a prior predictive check by observing in-sample data D and comparing it to the simulated data generated in the previous step.</p>&#13;
&#13;
	<ul>&#13;
		<li>&#13;
		<p>If the retrodiction of the data meets your requirements, the model is ready to be trained and you should go to the next step.</p>&#13;
		</li>&#13;
		<li>&#13;
		<p>Otherwise, review the parameters and the functional forms of prior probability distribution and the likelihood function.</p>&#13;
		</li>&#13;
		<li>&#13;
		<p>Repeat steps 2–4 until the model passes your prior predictive check and is ready for training.</p>&#13;
		</li>&#13;
	</ul>&#13;
	</li>&#13;
	<li>&#13;
	<p>Apply the inverse probability rule to update the distributions of model parameters. Our model’s posterior probability distribution updates our prior parameter estimates, given the actual training data.</p>&#13;
&#13;
	<ul>&#13;
		<li>&#13;
		<p>You observed a negative rating and updated the posterior probability of default of XYZ company as follows:</p>&#13;
		</li>&#13;
		</ul>&#13;
		<ul class="simplelist">&#13;
		<li>&#13;
        <p>P(default | negative) = P(negative | default) P(default) / P(negative) = (0.70 × 0.10)/0.43 = 0.16</p>&#13;
        </li>&#13;
		</ul>&#13;
	</li>&#13;
	<li>&#13;
	<p>Generate data D″ using the model’s posterior predictive distribution. The trained model simulates yet-to-be-seen data by averaging the likelihood function over the posterior probability distribution of the updated parameters. The posterior predictive distribution serves as a second model check by retrodicting the in-sample data it was trained on and predicting the out-of-sample or test data distribution we might observe later in testing.</p>&#13;
&#13;
	<ul>&#13;
		<li>&#13;
		<p class="pagebreak-before">Based on all your model’s assumptions encoded in your prior probability distribution, the likelihood function, and the newly observed negative rating, you can expect that XYZ portfolio company will generate new ratings, negative″ and positive″, with the following updated probabilities:</p>&#13;
		</li>&#13;
		</ul>&#13;
		<ul class="simplelist">&#13;
		<li>&#13;
		<p>P(negative″ | negative) = P(negative″ | default) P(default | negative) + P(negative″ | no default) P( no default | negative) = (0.70 × 0.16) + (0.40 × 0.84) = 0.35</p>&#13;
		</li>&#13;
		<li>&#13;
		<p>P(positive″) = 1 − P(negative″) = 0.65</p>&#13;
		</li>&#13;
		</ul>&#13;
	</li>&#13;
</ol>&#13;
&#13;
<p>We are now faced with one of the most important decisions regarding the outputs of our inferences: how are we going to apply its results to make decisions given incomplete information and three-dimensional uncertainty, so that we increase the odds of achieving our objectives?<a contenteditable="false" data-primary="" data-startref="ch08-inf" data-type="indexterm" id="id1348"/><a contenteditable="false" data-primary="" data-startref="ch08-inf2" data-type="indexterm" id="id1349"/></p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Probabilistic Decision-Making Framework" data-type="sect1"><div class="sect1" id="probabilistic_decision_making_framework">&#13;
<h1>Probabilistic Decision-Making Framework</h1>&#13;
&#13;
<p>To make systematic decisions<a contenteditable="false" data-primary="probabilistic decision making with generative ensembles" data-secondary="probabilistic decision-making framework" data-tertiary="about" data-type="indexterm" id="id1350"/><a contenteditable="false" data-primary="decision making" data-secondary="probabilistic with generative ensembles" data-tertiary="probabilistic decision-making framework" data-type="indexterm" id="id1351"/><a contenteditable="false" data-primary="probabilistic decision making with generative ensembles" data-secondary="loss functions defined" data-type="indexterm" id="id1352"/><a contenteditable="false" data-primary="loss functions" data-type="indexterm" id="id1353"/><a contenteditable="false" data-primary="decision making" data-secondary="probabilistic with generative ensembles" data-tertiary="loss functions defined" data-type="indexterm" id="id1354"/> in the face of incomplete information and uncertainty, we need to specify an objective function. A loss function is a specific type of objective function where the objective is to minimize the expected value or weighted average loss of our decisions.<sup><a data-type="noteref" href="ch08.html#ch08fn1" id="ch08fn1-marker">1</a></sup> Simply put, a loss function quantifies our losses for every decision we take based on inferences and predictions we make.</p>&#13;
&#13;
<p>Let’s continue working through our debt default example to understand what a loss function does and how to apply it to outcomes that are simulated by our generative ensembles. We will then generalize it so that we can apply it to any decision-making activity that we might face using any type of objective function.</p>&#13;
&#13;
<section data-pdf-bookmark="Integrating Subjectivity" data-type="sect2"><div class="sect2" id="integrating_subjectivity">&#13;
<h2>Integrating Subjectivity</h2>&#13;
&#13;
<p>The most difficult decisions are<a contenteditable="false" data-primary="probabilistic decision making with generative ensembles" data-secondary="probabilistic decision-making framework" data-tertiary="integrating subjectivity" data-type="indexterm" id="id1355"/><a contenteditable="false" data-primary="decision making" data-secondary="probabilistic with generative ensembles" data-tertiary="integrating subjectivity" data-type="indexterm" id="id1356"/><a contenteditable="false" data-primary="loss functions" data-secondary="probabilistic decision making" data-tertiary="integrating subjectivity" data-type="indexterm" id="id1357"/><a contenteditable="false" data-primary="subjectivity integrated into probabilistic decision making" data-type="indexterm" id="id1358"/> the ones that involve a complex interplay between the objective logic of the situation and the equally rational subjective self-interests of various people involved. Of course, the numbers we assign to any loss function for different decisions can be subjective. In such situations, the absolute numbers of the losses are not important. What is important is that we calibrate the losses consistently to reflect the magnitude of the consequences that would result logically from the various decisions that we make.</p>&#13;
&#13;
<p class="pagebreak-before">Assume that you are working as an analyst at the aforementioned hedge fund. Basically, your job is to excel at data analysis and follow your portfolio manager’s directions, especially regarding her risk limits for any portfolio company’s bonds. The biggest risk you face at your job is getting fired and losing your main source of income. Here is a scenario you might face in your nascent career in investment management:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>Because of two negative ratings in a row, the probability of default for XYZ company bonds is now 25%.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Your portfolio manager has directed you to call a risk management meeting when the probability of default of XYZ portfolio company exceeds 30%, her risk limit, which she swears by based on her experiences and expertise.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>You aspire to be a portfolio manager in the near future and need to demonstrate judgment and the ability to bear risk to your manager and colleagues.</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>The next rating that your ML system assigns XYZ bonds will almost surely not seal the fate of XYZ bonds’ default status. But as you see it, the next rating will have dramatic consequences to your life that could rival any Shakespearean tragedy. The outcomes could range from your getting fired to your getting promoted as a portfolio manager. To call a meeting or not to call a meeting with your portfolio manager before the next rating—that is the question. To help you with your dilemma, we need to specify the probability distribution for the next rating, XYZ’s probability of default breaching the risk limit, and the loss you might experience based on your decision to call or not to call the meeting with your portfolio manager before observing the rating.</p>&#13;
&#13;
<p>Let’s calculate the probability of default for XYZ company if the next rating you observe is a negative one (which would make it three negative ratings in a row):</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>P(3 negatives | default) = 0.70 × 0.70 × 0.70 = 0.343</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>P(3 negatives | no default) = 0.40 × 0.40 × 0.40 = 0.064</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>P(3 negatives) = P(3 negatives | default) P(default) + P(3 negatives | no default) P( no default) = 0.343 × 0.10 + 0.064 × 0.90 = 0.0343 + 0.0576 = 0.0919</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>P(default | 3 negatives) = P(3 negatives | default) P(default) / P(3 negatives) = 0.0343/0.0919 = 0.37</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p class="pagebreak-before">So if the next rating is a negative one, your estimate of the probability of default of XYZ company would be around 37% and would blow past your portfolio manager’s risk limit of 30%. But what is the probability that the next rating for XYZ company is a negative one give that we have already observed 2 negative ratings? We have already computed the posterior predictive distribution for the next rating given two consecutive negative ratings for XYZ company:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>P(negative′ | 2 negatives) = P(negative′ | default) P(default | 2 negatives) + P(negative′ | no default) P(no default | 2 negatives) = (0.7 × 0.25) + (0.4 × 0.75) = 0.475</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>P(positive′ | 2 negatives) = 1 – P(negative′ | 2 negatives) = 0.525</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>It seems that the odds don’t favor calling a meeting with your portfolio manager since there is only a 47.5% probability that the next rating for XYZ company is going to be negative. However, these odds don’t consider the consequences of your decisions on your career and your colleagues. More specifically, we need to figure out the losses you and your portfolio manager might face based on your decision to call or not to call the risk management meeting with her preemptively.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Estimating Losses" data-type="sect2"><div class="sect2" id="estimating_losses">&#13;
<h2>Estimating Losses</h2>&#13;
&#13;
<p>Let’s define a loss function, L(R, D″), that quantifies<a contenteditable="false" data-primary="probabilistic decision making with generative ensembles" data-secondary="probabilistic decision-making framework" data-tertiary="estimating losses" data-type="indexterm" id="ch08-estl"/><a contenteditable="false" data-primary="decision making" data-secondary="probabilistic with generative ensembles" data-tertiary="estimating losses" data-type="indexterm" id="ch08-estl2"/><a contenteditable="false" data-primary="loss functions" data-secondary="probabilistic decision making" data-tertiary="estimating losses" data-type="indexterm" id="ch08-estl3"/> the losses you might experience as a consequence of a decision, R, that you make based on an out-of-sample data prediction, D″.</p>&#13;
&#13;
<p>We now enumerate our outcome data and decision spaces.</p>&#13;
&#13;
<ul class="simplelist">&#13;
	<li>&#13;
	<p>The possible ratings of XYZ bonds are D<sub>1</sub>″ = negative″ and D<sub>2</sub>″ = positive″. Note that these data predictions are mutually exclusive and collectively exhaustive.</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>Based on the predictions of this future, out-of-sample data, D″ given observed data D, your possible decisions, (R,D″), are enumerated here:</p>&#13;
&#13;
<dl>&#13;
	<dt>(R<sub>1</sub>,D<sub>1</sub>″)</dt>&#13;
	<dd>Call a meeting with your portfolio manager based on your prediction that the next rating for XYZ bonds is going to be negative and the company’s probability of default will breach her risk limit of 30%.</dd>&#13;
	<dt>(R<sub>2</sub>,D<sub>2</sub>″)</dt>&#13;
	<dd>Don’t call a meeting with your portfolio manager based on your prediction that the next rating for XYZ bonds will be positive and the company’s probability of default would be well below her risk limit.</dd>&#13;
	<dt class="pagebreak-before">(R<sub>3</sub>,D<sub>2</sub>″)</dt>&#13;
	<dd>Call a meeting with your portfolio manager based on your prediction that the next rating of XYZ company will be positive. Persuade your manager to take advantage of current discounted market prices of XYZ bonds to increase her position size.</dd>&#13;
	<dt>(R<sub>4</sub>,D<sub>1</sub>″)</dt>&#13;
	<dd>Don’t call a meeting with your portfolio manager based on your prediction that the next rating of XYZ bonds will be negative. Clearly, that would be foolish and not an option you would ever consider. We have merely listed it here for completeness.</dd>&#13;
</dl>&#13;
&#13;
<p>Decisions (R<sub>1</sub>,D<sub>1</sub>″), (R<sub>2</sub>, D<sub>2</sub>″), and (R<sub>3</sub>, D<sub>2</sub>″) are the only viable decisions that you can make, and they are mutually exclusive and collectively exhaustive. We need to assign losses to each of these decisions to reflect their consequences to your life.</p>&#13;
&#13;
<p>The possible losses for decision (R<sub>1</sub>, D<sub>1</sub>″)—in which you call a meeting with your portfolio manager to apprise her of the impending breach of her risk limit by XYZ bonds based on your prediction that the next rating will be negative—are as follows:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>One possible outcome is that the next rating of XYZ company turns out to be negative″. This is a great outcome for you and your portfolio manager. You would have shown sound judgment, anticipation, and risk management—some of the most important qualities of an investment manager. Your portfolio manager would have proactively managed her position risk thanks to your brilliant actions. Consequently, you would make significant progress toward your career goals of becoming a portfolio manager.</p>&#13;
&#13;
	<ul>&#13;
		<li>&#13;
		<p>Your loss function reflects this favorable outcome by giving you a reward or a negative loss. Let’s assign it a value of +100 points: L(R<sub>1</sub>,D<sub>1</sub>″ | negative″) = +100</p>&#13;
		</li>&#13;
	</ul>&#13;
	</li>&#13;
	<li>&#13;
	<p>The only other possible outcome is that the next rating of XYZ company turns out to be positive″. This is not a good outcome for you. Your portfolio manager would take some losses on her hedges that she put on to protect her XYZ bonds based on your previous prediction. She might suspect that you panicked since the probability of a negative rating was 47.5%, less than a coin toss. She could conclude that you might not have the grit and gumption it takes to be a portfolio manager. Your dream of becoming a portfolio manager in the near future would gradually fade away. But let’s look at the bright side of such a possible scenario: you would still have your job, and this could turn out to be a good learning experience for you.</p>&#13;
&#13;
	<ul>&#13;
		<li>&#13;
		<p>Your loss function would reflect this by giving you a small loss of say –100 points: L(R<sub>1</sub>,D<sub>1</sub>″ | positive″) = −100</p>&#13;
		</li>&#13;
	</ul>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p class="pagebreak-before">For the decision (R<sub>2</sub>, D<sub>2</sub>″), where you don’t call a meeting based on your prediction of a positive rating for XYZ bonds, your possible losses are as follows:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>One possible outcome is that the next rating of XYZ company turns out to be negative. This is your nightmare scenario. Now the probability that XYZ company is going to default will have blown past your portfolio manager’s risk limit. Market prices of bonds of XYZ company would take a hit. Your manager’s portfolio would start underperforming her peers and her annual bonus would be in jeopardy. Quite possibly, she would be the one to call a meeting with you. You would be wished the very best in the future and politely escorted out of the door by security personnel.</p>&#13;
&#13;
	<ul>&#13;
		<li>&#13;
		<p>This awful outcome is encoded in your loss function by assigning a large loss to it, say −1000 points: L(R<sub>2</sub>, D<sub>2</sub>″ |negative″) = −1000</p>&#13;
		</li>&#13;
	</ul>&#13;
	</li>&#13;
	<li>&#13;
	<p>The only other outcome is that the next rating of XYZ company may turn out to be positive″. This would be a good outcome for you. However, it would not be clear to your manager whether it was good judgment or luck that played a role in your decisions and prediction. After all, the probability that the next rating would be a positive one was just 52.5%, a little better than a coin toss. She might conclude that you were cutting it a bit too close for comfort. Contrast this with her reaction to (R<sub>1</sub>, D<sub>1</sub>″|positive″). Both are inconsistent but rational viewpoints based on subjective attitudes toward risk that change at any given time for whatever reason. But that’s exactly how people and markets can and do behave. We just have to deal with it in the best way we can.</p>&#13;
&#13;
	<ul>&#13;
		<li>&#13;
		<p>Your loss function would reflect this neutral outcome with no loss or 0 points: L(R<sub>2</sub>, D<sub>2</sub>″ | positive″) = 0</p>&#13;
		</li>&#13;
	</ul>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>Finally, the possible losses for the decision (R<sub>1</sub>, D<sub>2</sub>″)—where you call a meeting based on your prediction of a positive rating for XYZ bonds and convince your portfolio manager to increase her position size—are as follows:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>One possible outcome after the meeting with your portfolio manager is that the next rating of XYZ bonds turns out to be positive″ as predicted. This is the best outcome for you. Based on your recommendation, your portfolio manager would have already bought more XYZ bonds at discounted market prices. She would most likely have taken the opportunity to make a quick profit as XYZ bond prices rally on the new positive information. You would have demonstrated predictive capabilities and the smarts to monetize it. This would impress everyone at the fund, especially your fund manager, whose bonus check would surely increase. Now it would seem to only be a matter of time until you would be managing a multimillion-dollar portfolio yourself.</p>&#13;
&#13;
	<ul>&#13;
		<li>&#13;
		<p>The loss function would calibrate this positive reward by giving you a larger reward or negative loss. Let’s assign it a value of +500 points: L(R<sub>1</sub>,D<sub>2</sub>″ | positive″) = +500</p>&#13;
		</li>&#13;
	</ul>&#13;
	</li>&#13;
	<li>&#13;
	<p>The other outcome after the meeting with your portfolio manager is that the next rating of XYZ company turns out to be negative. This would be the worst outcome for you. Now the probability that XYZ company is going to default has blown past your portfolio manager’s risk limit. Market prices of bonds of XYZ company would have taken a big hit while her position size had grown bigger. Your manager’s portfolio performance would be bringing up the rear at the fund, and her job would be at risk. There would be nothing to discuss, and you would be escorted out of the door by security personnel.</p>&#13;
&#13;
	<ul>&#13;
		<li>&#13;
		<p>The loss function calibrates this disastrous outcome by assigning a huge loss of –2000 points: L(R<sub>2</sub>, D<sub>2</sub>″ |negative″) = −2000</p>&#13;
		</li>&#13;
	</ul>&#13;
	</li>&#13;
</ul>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Minimizing Losses" data-type="sect2"><div class="sect2" id="minimizing_losses">&#13;
<h2>Minimizing Losses</h2>&#13;
&#13;
<p>We can now calculate the expected losses for each of the three decisions (R<sub>1</sub>, D<sub>1</sub>″), (R<sub>2</sub>, D<sub>2</sub>″), (R<sub>3</sub>, D<sub>2</sub>″) by averaging over the posterior predictive probability distribution, P(D″ | D), for the next rating of XYZ bonds, given that we have already observed 2 negative ratings:<a contenteditable="false" data-primary="probabilistic decision making with generative ensembles" data-secondary="probabilistic decision-making framework" data-tertiary="minimizing losses" data-type="indexterm" id="id1359"/><a contenteditable="false" data-primary="decision making" data-secondary="probabilistic with generative ensembles" data-tertiary="minimizing losses" data-type="indexterm" id="id1360"/><a contenteditable="false" data-primary="loss functions" data-secondary="probabilistic decision making" data-tertiary="minimizing losses" data-type="indexterm" id="id1361"/><a contenteditable="false" data-primary="" data-startref="ch08-estl" data-type="indexterm" id="id1362"/><a contenteditable="false" data-primary="" data-startref="ch08-estl2" data-type="indexterm" id="id1363"/><a contenteditable="false" data-primary="" data-startref="ch08-estl3" data-type="indexterm" id="id1364"/></p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>E[L(R<sub>1</sub>, D<sub>1</sub>″)] = P(negative″ | 2 negatives) L(R<sub>1</sub>, D<sub>1</sub>″ | negative″) + P(positive″ | 2 negatives) L( R<sub>1</sub>,D<sub>1</sub>″ | negative″) = 0.475 × +100 + 0.525 × –100 = –5 points</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>E[L(R<sub>2</sub>, D<sub>2</sub>″)] = P(negative″ | 2 negatives) L(R<sub>2</sub>, D<sub>2</sub>″ | negative″) + P(positive″ | 2 negatives) L( R<sub>2</sub>, D<sub>2</sub>″ | positive″) = 0.475 × –1000 + 0.525 × 0 = –475 points</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>E[L(R<sub>3</sub>, D<sub>2</sub>″)] = P(negative″ | 2 negatives) L(R<sub>3</sub>, D<sub>2</sub>″ | negative″) + P(positive″ | 2 negatives) L( R<sub>3</sub>, D<sub>2</sub>″ | positive″) = 0.475 × –2000 + 0.525 × +500 = <span class="keep-together">–687.5 points</span></p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>In probabilistic decision making, the best decision you can make is the one that minimizes the expected value of your losses averaged over the consequences of your specific decisions. In the formula for minimizing losses, we have averaged the loss function over the posterior predictive distribution of simulated data. Since E[L(R<sub>1</sub>, D<sub>1</sub>″)] &gt; E[L(R<sub>2</sub>, D<sub>2</sub>″)] &gt; E[L(R<sub>3</sub>, D<sub>2</sub>″)], you should decide on (R<sub>1</sub>, D<sub>1</sub>″). Your best option is to call for a meeting with our portfolio manager as soon as possible and apprise her that XYZ bonds are probably going to blow past her risk limit and that she needs to manage her position appropriately. This choice minimizes your career risks.</p>&#13;
&#13;
<p class="pagebreak-before">It is common knowledge that real-life decision making is an art and a science. Career risks, executive egos, conflicting self-interests, greed, and fear of people are some of the most powerful drivers of financial transactions everywhere in the world—from mundane daily trades to megamergers of the largest companies to the Federal Reserve raising interest rates. You ignore such subjective drivers of decision making at your peril and could miss out on profitable, if not life-changing, opportunities.</p>&#13;
&#13;
<p>At any rate, based on our exercise of minimizing career risks, we can posit that, for discrete distributions, with a posterior predictive distribution P(D″|D) and a loss function L(R, D″), the best decision is the one that minimizes the expected loss, E[L(R)], of predicted outcomes over all possible actions R, as shown here:</p>&#13;
&#13;
<ul class="simplelist">&#13;
<li><math alttext="upper E left-bracket upper L left-parenthesis upper R right-parenthesis right-bracket equals arg min Underscript upper R Endscripts left-parenthesis sigma-summation Underscript i Endscripts upper L left-parenthesis upper R comma upper D Subscript i Superscript Super Superscript double-prime Superscript Baseline right-parenthesis times upper P left-parenthesis upper D Subscript i Superscript Super Superscript double-prime Superscript Baseline vertical-bar upper D right-parenthesis right-parenthesis">&#13;
  <mrow>&#13;
    <mi>E</mi>&#13;
    <mrow>&#13;
      <mo>[</mo>&#13;
      <mi>L</mi>&#13;
      <mrow>&#13;
        <mo>(</mo>&#13;
        <mi>R</mi>&#13;
        <mo>)</mo>&#13;
      </mrow>&#13;
      <mo>]</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <mo form="prefix">arg</mo>&#13;
    <msub><mo form="prefix" movablelimits="true">min</mo> <mi>R</mi> </msub>&#13;
    <mfenced close=")" open="(" separators="">&#13;
      <msub><mo>∑</mo> <mi>i</mi> </msub>&#13;
      <mi>L</mi>&#13;
      <mrow>&#13;
        <mo>(</mo>&#13;
        <mi>R</mi>&#13;
        <mo>,</mo>&#13;
        <msubsup><mi>D</mi> <mi>i</mi> <msup><mrow/> <mrow><mo>'</mo><mo>'</mo></mrow> </msup> </msubsup>&#13;
        <mo>)</mo>&#13;
      </mrow>&#13;
      <mo>×</mo>&#13;
      <mi>P</mi>&#13;
      <mrow>&#13;
        <mo>(</mo>&#13;
        <msubsup><mi>D</mi> <mi>i</mi> <msup><mrow/> <mrow><mo>'</mo><mo>'</mo></mrow> </msup> </msubsup>&#13;
        <mo>|</mo>&#13;
        <mi>D</mi>&#13;
        <mo>)</mo>&#13;
      </mrow>&#13;
    </mfenced>&#13;
  </mrow>&#13;
</math></li>&#13;
</ul>&#13;
&#13;
<p>This expected loss formula for discrete functions can be extended to continuous functions by substituting summation with integration. We can now apply loss functions to continuous distributions that we encounter in our regression ensembles. As before, we minimize the expected loss over all possible actions R as shown here:</p>&#13;
&#13;
<ul class="simplelist">&#13;
<li><math alttext="upper E left-bracket upper L left-parenthesis upper R right-parenthesis right-bracket equals arg min Underscript upper R Endscripts left-parenthesis integral upper L left-parenthesis upper R comma upper D Superscript Super Superscript double-prime Superscript Baseline right-parenthesis times upper P left-parenthesis upper D Superscript Super Superscript double-prime Superscript Baseline vertical-bar upper D right-parenthesis d upper D Superscript double-prime Baseline right-parenthesis">&#13;
  <mrow>&#13;
    <mi>E</mi>&#13;
    <mrow>&#13;
      <mo>[</mo>&#13;
      <mi>L</mi>&#13;
      <mrow>&#13;
        <mo>(</mo>&#13;
        <mi>R</mi>&#13;
        <mo>)</mo>&#13;
      </mrow>&#13;
      <mo>]</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <mo form="prefix">arg</mo>&#13;
    <msub><mo form="prefix" movablelimits="true">min</mo> <mi>R</mi> </msub>&#13;
    <mfenced close=")" open="(" separators="">&#13;
      <mo>∫</mo>&#13;
      <mi>L</mi>&#13;
      <mrow>&#13;
        <mo>(</mo>&#13;
        <mi>R</mi>&#13;
        <mo>,</mo>&#13;
        <msup><mi>D</mi> <msup><mrow/> <mrow><mo>'</mo><mo>'</mo></mrow> </msup> </msup>&#13;
        <mo>)</mo>&#13;
      </mrow>&#13;
      <mo>×</mo>&#13;
      <mi>P</mi>&#13;
      <mrow>&#13;
        <mo>(</mo>&#13;
        <msup><mi>D</mi> <msup><mrow/> <mrow><mo>'</mo><mo>'</mo></mrow> </msup> </msup>&#13;
        <mo>|</mo>&#13;
        <mi>D</mi>&#13;
        <mo>)</mo>&#13;
      </mrow>&#13;
      <mi>d</mi>&#13;
      <msup><mi>D</mi> <msup><mrow/> <mrow><mo>'</mo><mo>'</mo></mrow> </msup> </msup>&#13;
    </mfenced>&#13;
  </mrow>&#13;
</math></li>&#13;
</ul>&#13;
&#13;
<p>These formulas make it look more difficult than it really is in applying our decision framework. What is indeed difficult is understanding and applying the expected value of our ensemble, as we will discuss in the next section.</p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Risk Management" data-type="sect1"><div class="sect1" id="risk_management">&#13;
<h1>Risk Management</h1>&#13;
&#13;
<p>Investors, traders, and corporate<a contenteditable="false" data-primary="probabilistic decision making with generative ensembles" data-secondary="risk management" data-tertiary="about" data-type="indexterm" id="id1365"/><a contenteditable="false" data-primary="risk" data-secondary="risk management" data-type="indexterm" id="id1366"/><a contenteditable="false" data-primary="decision making" data-secondary="probabilistic with generative ensembles" data-tertiary="risk management" data-type="indexterm" id="ch08-rism"/> executives aim to profit from risky undertakings in which financial losses are not only expected but inevitable over the investment’s holding period. The key to success in these probabilistic endeavors is to proactively and systematically manage losses so that they do not overwhelm profits or impair the capital base in any finite time period. In <a data-type="xref" href="ch03.html#quantifying_output_uncertainty_with_mon">Chapter 3</a>, we examined the inadequacies of volatility for risk management. Value at risk (VaR) and expected shortfall (ES) are two risk measures that are used extensively by almost all financial institutions, government regulators, and corporate treasurers of nonfinancial institutions.<sup><a data-type="noteref" href="ch08.html#ch08fn2" id="ch08fn2-marker">2</a></sup> It is very important that practitioners have a strong understanding of the methods used to calculate these measures as they, too, have serious weaknesses that can lead to disastrous mispricing of financial risks. In this section we explore risk management in general and how to apply the aforementioned risk measures to generative ensembles in <span class="keep-together">particular.</span></p>&#13;
&#13;
<section data-pdf-bookmark="Capital Preservation" data-type="sect2"><div class="sect2" id="capital_preservation">&#13;
<h2>Capital Preservation</h2>&#13;
&#13;
<p>Warren Buffett, the greatest discretionary<a contenteditable="false" data-primary="probabilistic decision making with generative ensembles" data-secondary="risk management" data-tertiary="capital preservation" data-type="indexterm" id="id1367"/><a contenteditable="false" data-primary="decision making" data-secondary="probabilistic with generative ensembles" data-tertiary="capital preservation" data-type="indexterm" id="id1368"/><a contenteditable="false" data-primary="risk" data-secondary="risk management" data-tertiary="capital preservation" data-type="indexterm" id="id1369"/><a contenteditable="false" data-primary="Buffet, Warren" data-type="indexterm" id="id1370"/> equity investor of all time, has two well-known rules for making investments in risky assets like equities:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>Rule number one: Don’t lose money.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Rule number two: Don’t forget rule number one.</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>Buffett’s sage advice is that when making a risky investment, we must focus more on managing the ever-present risks affecting the investment than on its potential future returns. Most importantly, we must never lose sight that the primary objective in investing is the return <em>of</em> our capital; the return <em>on</em> our capital is a secondary objective. We shouldn’t go broke before we get our just deserts, should the investment opportunity actually turn out to be a profitable one in the future. Furthermore, even if the current investment doesn’t work out as expected, there will always be others in the future that we can participate in as long as we preserve our capital base. Underlying Buffet’s avuncular precept—borne out of decades of exemplary investing experiences—is the important statistical idea of ergodicity, which we explore next.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Ergodicity" data-type="sect2"><div class="sect2" id="ergodicity">&#13;
<h2>Ergodicity</h2>&#13;
&#13;
<p>Let’s go back to the linear ensemble<a contenteditable="false" data-primary="probabilistic decision making with generative ensembles" data-secondary="risk management" data-tertiary="ergodicity" data-type="indexterm" id="ch08-ergo"/><a contenteditable="false" data-primary="decision making" data-secondary="probabilistic with generative ensembles" data-tertiary="ergodicity" data-type="indexterm" id="ch08-ergo2"/><a contenteditable="false" data-primary="ergodicity" data-type="indexterm" id="ch08-ergo3"/><a contenteditable="false" data-primary="risk" data-secondary="risk management" data-tertiary="ergodicity" data-type="indexterm" id="ch08-ergo4"/> in the previous chapter and analyze the simulated 20,000 posterior predictive samples that our ensemble has generated using our model assumptions and the observed data. It is important to note that the posterior predictive distribution generates a range of possible future outcomes, each of which could have been generated by any combination of parameter values of our ensemble that is consistent with its model assumptions and the data used to train and test it.</p>&#13;
&#13;
<p>While we can easily calculate descriptive statistics of the posterior predictive samples as we do later, we cannot directly associate any sample outcome with specific values of the model parameters. Of course, we can always infer the credible interval of each parameter that might have generated the samples from its marginal posterior distribution, as we did in the previous chapter. Let’s use the following Python code to summarize the predicted excess returns of a hypothetical position in Apple stock:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="c1"># Flatten posterior predictive xdarray into one numpy array of </code>&#13;
<code class="c1"># 20,000 simulated samples.</code>&#13;
<code class="n">simulated_data</code> <code class="o">=</code> <code class="n">target_predicted</code><code class="o">.</code><code class="n">flatten</code><code class="p">()</code>&#13;
&#13;
<code class="c1"># Create a pandas dataframe to analyze the simulated data.</code>&#13;
<code class="n">generated_data</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">simulated_data</code><code class="p">,</code> <code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s2">"Values"</code><code class="p">])</code>&#13;
&#13;
<code class="c1"># Print the summary statistics.</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="n">generated_data</code><code class="o">.</code><code class="n">describe</code><code class="p">()</code><code class="o">.</code><code class="n">round</code><code class="p">(</code><code class="mi">2</code><code class="p">))</code>&#13;
&#13;
<code class="c1"># Plot the predicted samples of Apple's excess returns generated by </code>&#13;
<code class="c1"># tested linear ensemble.</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">hist</code><code class="p">(</code><code class="n">simulated_data</code><code class="p">,</code> <code class="n">bins</code><code class="o">=</code><code class="s1">'auto'</code><code class="p">,</code> <code class="n">density</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="s2">"Apple's excess returns predicted by linear ensemble"</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s1">'Probability density'</code><code class="p">),</code> &#13;
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s1">'Simulated excess returns of Apple'</code><code class="p">);</code>&#13;
&#13;
</pre>&#13;
&#13;
<figure class="informal"><div class="figure"><img alt="Image" src="assets/pmlf_08in01.png"/>&#13;
</div></figure>&#13;
&#13;
<p>It is more important to note that this posterior predictive distribution of daily excess returns does not predict the specific timing or duration of those returns, only the distribution of possible returns in the future based on our ensemble’s model assumptions and data observed during the training and testing periods. Our ensemble average is the expected value of our hypothetical investment in Apple stock. Let’s see if it can help us decide whether to hold, increase, or decrease our position size.</p>&#13;
&#13;
<p class="pagebreak-before">A simple loss function, L(R, D″), is simply the market value of our position size multiplied by the daily excess returns of Apple for each simulated data point from the posterior predictive distribution of the ensemble:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>L(R, D″) = R × D″</p>&#13;
&#13;
	<ul>&#13;
		<li>&#13;
		<p>R is the market value of our investment in Apple stock.</p>&#13;
		</li>&#13;
		<li>&#13;
		<p>D″ is a simulated daily excess return generated by our linear ensemble.</p>&#13;
		</li>&#13;
	</ul>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>In the following Python code, we assume that our hypothetical investment in Apple stock is valued at $100,000 and compute the ensemble average of all simulated excess returns:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="c1">#Market value of position size in the portfolio</code>&#13;
<code class="n">position_size</code> <code class="o">=</code> <code class="mi">100000</code>&#13;
&#13;
<code class="c1">#The loss function is position size * excess returns of Apple </code>&#13;
<code class="c1">#for each prediction. </code>&#13;
<code class="n">losses</code> <code class="o">=</code> <code class="n">simulated_data</code><code class="o">/</code><code class="mi">100</code><code class="o">*</code><code class="n">position_size</code>&#13;
&#13;
<code class="c1">#Expected loss is probability weighted arithmetic mean of all the losses </code>&#13;
<code class="c1">#and profits</code>&#13;
<code class="n">expected_loss</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">losses</code><code class="p">)</code>&#13;
&#13;
<code class="c1">#Range of losses predicted by tested linear ensemble.</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="s2">"Expected loss on investment of $100,000 is $</code><code class="si">{:.0f}</code><code class="s2">, with max possible </code><code class="w"/>&#13;
<code class="n">loss</code> <code class="n">of</code> <code class="err">$</code><code class="p">{:</code><code class="mf">.0</code><code class="n">f</code><code class="p">}</code> <code class="ow">and</code> <code class="nb">max</code> <code class="n">possible</code> <code class="n">profit</code> <code class="n">of</code> <code class="err">$</code><code class="p">{:</code><code class="mf">.0</code><code class="n">f</code><code class="p">}</code><code class="s2">"</code><code class="w"/>&#13;
<code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">expected_loss</code><code class="p">,</code> <code class="n">np</code><code class="o">.</code><code class="n">min</code><code class="p">(</code><code class="n">losses</code><code class="p">),</code> <code class="n">np</code><code class="o">.</code><code class="n">max</code><code class="p">(</code><code class="n">losses</code><code class="p">)))</code>&#13;
&#13;
<code class="n">Expected</code> <code class="n">loss</code> <code class="n">on</code> <code class="n">investment</code> <code class="n">of</code> <code class="err">$</code><code class="mi">100</code><code class="p">,</code><code class="mi">000</code> <code class="ow">is</code> <code class="err">$</code><code class="o">-</code><code class="mi">237</code><code class="p">,</code> <code class="k">with</code> <code class="nb">max</code> <code class="n">possible</code> <code class="n">loss</code> <code class="n">of</code> &#13;
<code class="err">$</code><code class="o">-</code><code class="mi">10253</code> <code class="ow">and</code> <code class="nb">max</code> <code class="n">possible</code> <code class="n">profit</code> <code class="n">of</code> <code class="err">$</code><code class="mi">8286</code></pre>&#13;
&#13;
<p>The expected value of –$237 is almost a rounding error based on an investment of $100,000. This suggests we can expect to experience little or no losses if we hold our position, assuming market conditions remain approximately the same as those encoded in our model and reflected in the observed data used to train and test it. Given the large range of possible daily losses and profits our position might incur over time, from –10.25% to +8.29%, isn’t the expected value of –0.24% misleading and risky? It seems that an ensemble average or expected value is a useless and dangerous statistic for risk management decisions. Let’s dig deeper into the statistical concept of expected value to understand why and how we can apply it appropriately.<sup><a data-type="noteref" href="ch08.html#ch08fn3" id="ch08fn3-marker">3</a></sup></p>&#13;
&#13;
<p>Recall that when we estimate the expected value of any variable, such as an investment, we compute a probability weighted average of all possible outcomes and their respective payoffs. We also assume that the outcomes are independent of one another and are identically distributed, i.e., they are drawn from the same stochastic process. In other words, the expected value of the investment is a probability-weighted arithmetic mean. What is noteworthy is that the expected value has no time dependency and is also referred to as the ensemble average of a stochastic process or system. If you have an ensemble of independent and identically distributed investments or trades you are going to make simultaneously, expected value is a useful tool for decision making. Or if you are running a casino business, you can calculate the expected value of your winnings across all gamblers at any given time.</p>&#13;
&#13;
<p>However, as investors and traders, we only observe a specific path or trajectory that our investment takes over time. We measure the outcomes and payoffs of our investment sequentially as a time average over a finite period. In particular, we may only observe a subset of all the possible outcomes and their respective payoffs as they unfold over time. In the unlikely scenario that our investment’s trajectory realizes every possible predicted outcome and payoff over time, the time average of the trajectory will almost surely converge to the ensemble average. Such a stochastic process is called ergodic. We discussed this briefly in <a data-type="xref" href="ch06.html#the_dangers_of_conventional_ai_systems">Chapter 6</a> in the Markov chain section.</p>&#13;
&#13;
<p>What is special about an ergodic investment process is that the expected value of the investment summarizes the return observed by any investor holding that investment over a sufficiently long period of time. Of course, as was mentioned in <a data-type="xref" href="ch06.html#the_dangers_of_conventional_ai_systems">Chapter 6</a>, this assumes that there is no absorbing state in the Markov chain that truncates the investor’s wealth trajectory. As we will see in this section and the next, investment processes are non-ergodic, and relying on expected values for managing risks or returns can lead to large losses, if not financial ruin.</p>&#13;
&#13;
<p>Even if a process is assumed to be ergodic, the time average of our investment does not take the actual ordering of the sequence of outcomes and payoffs into account. Why should it? After all, it’s just another arithmetic mean. What is noteworthy is that this, too, assumes that investors are passive, buy-and-hold investors. The specific sequence of returns that an investment follows in the market is crucial as it leads to different consequences and decisions for different types of investors. An example will help illustrate this point.</p>&#13;
&#13;
<p>A stock trajectory that has a loss of –10.25% followed by a gain of +8.29% entails different decisions and consequences for an investor than a stock trajectory that has a gain of +8.29% followed by a loss of –10.25%. This is despite the fact that in both these two-step sequences the stock ends up down at –2.81% for a buy and hold investor. This up and down returns sequence is called volatility drag as it drags down the expected returns, an arithmetic mean, to the geometric mean, or compounded returns. If the volatility drag is constant, compounded returns = average return - 1/2 variance of returns. But the risks from the volatility drag for an investor could be very different depending on their investment strategy. Let’s see why.</p>&#13;
&#13;
<p>Assume that for any stock position in their portfolio, an investor has a daily loss limit of –10% and a daily profit limit of +5%. The former stock sequence (–10.25%, +8.29%) will hit the investor’s stop-loss limit order at –10% and force them out of their position. To add insult to injury, the next day the stock comes roaring back +8.29%, while the investor is nursing a –10% <em>realized</em> loss on their investment. Now the investor would be down –7.19% compared to their peers who held onto their position or other investors who had a risk limit of –10.26% or lower. Talk about rubbing salt into our investor’s wounds! It would now be hard for the investor to decide to re-enter their position after such a bruising whiplash.</p>&#13;
&#13;
<p>Let’s now consider what happens if the stock follows the latter sequence (+8.29%, –10.25%). The investor would take a profit of +5% when the stock shoots up +8.29%. They would feel some regret about not selling out of their position at the recent high price. But no one ever times the market perfectly or can do it consistently. However, the next day the investor would feel extremely smart and pleased with themselves when the stock falls –10.25%. They would be outperforming their peers by +7.81% and can gloat about it if they so choose. It would now be quite easy for the investor to re-enter their position in the stock since their break-even price would have been lowered by +5%.</p>&#13;
&#13;
<p>This example demonstrates another reason why volatility, or standard deviation of returns, is a nonsensical measure of risk, as was discussed in <a data-type="xref" href="ch03.html#quantifying_output_uncertainty_with_mon">Chapter 3</a>. Volatility is just another ensemble average and is non-ergodic. In the first trajectory volatility hurts the investor’s returns, and in the second trajectory it helps them.</p>&#13;
&#13;
<p>While the numbers are specific to our probabilistic ensemble, investment trajectories over any time period can be profoundly consequential to most active investors and traders in general. The specific ordering of return sequences impacts an investor’s decisions, experiences, and investment success. The concept of the “average investor” experiencing the expected value of returns on an investment is just another financial fairy tale.<a contenteditable="false" data-primary="" data-startref="ch08-ergo" data-type="indexterm" id="id1371"/><a contenteditable="false" data-primary="" data-startref="ch08-ergo2" data-type="indexterm" id="id1372"/><a contenteditable="false" data-primary="" data-startref="ch08-ergo3" data-type="indexterm" id="id1373"/><a contenteditable="false" data-primary="" data-startref="ch08-ergo4" data-type="indexterm" id="id1374"/></p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Generative Value at Risk" data-type="sect2"><div class="sect2" id="value_at_risk">&#13;
<h2>Generative Value at Risk</h2>&#13;
&#13;
<p>Rather than relying on the ensemble average,<a contenteditable="false" data-primary="value at risk (VaR)" data-type="indexterm" id="ch08-var"/><a contenteditable="false" data-primary="probabilistic decision making with generative ensembles" data-secondary="risk management" data-tertiary="value at risk" data-type="indexterm" id="ch08-var2"/><a contenteditable="false" data-primary="decision making" data-secondary="probabilistic with generative ensembles" data-tertiary="value at risk" data-type="indexterm" id="ch08-var3"/><a contenteditable="false" data-primary="risk" data-secondary="risk management" data-tertiary="value at risk" data-type="indexterm" id="ch08-var4"/><a contenteditable="false" data-primary="loss functions" data-secondary="value at risk (VaR)" data-type="indexterm" id="ch08-var5"/> a popular loss function called value at risk (VaR) can help us make better risk management decisions for any time period. VaR is a percentile measure of a return distribution, representing the value below which a given percentage of the returns (or losses) fall. In other words, VaR is the maximum loss that is expected to be incurred over a specified period of time with a given probability. See <a data-type="xref" href="#value_at_risk_left_parenthesisvarright">Figure 8-1</a>, which shows VaR and conditional VaR (CVaR), which is explained in the next subsection.</p>&#13;
&#13;
<figure><div class="figure" id="value_at_risk_left_parenthesisvarright"><img alt="Value at risk (VaR) with alpha probability and conditional VaR (CVaR), also known as expected shortfall (ES), with 1-alpha probability is shown for a distribution of returns of a hypothetical investment." src="assets/pmlf_0801.png"/>&#13;
<h6><span class="label">Figure 8-1. </span>Value at risk (VaR) with alpha probability and conditional VaR (CVaR), also known as expected shortfall (ES), with 1-alpha probability shown for a distribution of returns of a hypothetical investment<sup><a data-type="noteref" href="ch08.html#ch08fn4" id="ch08fn4-marker">4</a></sup></h6>&#13;
</div></figure>&#13;
&#13;
<p>Unlike volatility, this measure is based on a commonsensical understanding of risk. As an example, say the daily VaR for a portfolio is $100,000, with 99% probability. This means that we estimate that there is a:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>99% probability that the daily loss of the portfolio will not exceed $100,000</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>1% probability that the daily loss will exceed $100,000</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>Generally speaking, the time horizon of VaR is often related to how long a decision maker thinks might be necessary to take an action, such as to liquidate a stock position. Longer time horizons generally produce larger VaR values because there is more uncertainty involved the further out you go into the future.</p>&#13;
&#13;
<p>In <a data-type="xref" href="ch03.html#quantifying_output_uncertainty_with_mon">Chapter 3</a>, we used Monte Carlo simulation to expose the deep flaws of using volatility as a measure of risk. It is common in the industry to use Monte Carlo simulations to estimate VaR for complex investments or portfolios using theoretical or empirical models. The risk estimate is called Monte Carlo VaR. In probabilistic machine learning, this simulation is done seamlessly and epistemologically consistently using the posterior predictive distribution. I use posterior predictive samples to estimate VaR, which I call Generative VaR or GVaR, as follows:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p class="pagebreak-before">Sort N simulated excess returns in descending order of losses.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Take the first M of those losses such that 1 − M/N is the required probability threshold.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>The smallest loss in the subset of M losses is your GVaR.</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>Now let’s use Python to compute the GVaR of our linear ensemble from the losses in the tail of its posterior predictive distribution:<a contenteditable="false" data-primary="" data-startref="ch08-var" data-type="indexterm" id="id1375"/><a contenteditable="false" data-primary="" data-startref="ch08-var2" data-type="indexterm" id="id1376"/><a contenteditable="false" data-primary="" data-startref="ch08-var3" data-type="indexterm" id="id1377"/><a contenteditable="false" data-primary="" data-startref="ch08-var4" data-type="indexterm" id="id1378"/><a contenteditable="false" data-primary="" data-startref="ch08-var5" data-type="indexterm" id="id1379"/></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="c1">#Generate a list the 20 worst daily losses predicted </code>&#13;
<code class="c1"># by tested linear ensemble.</code>&#13;
<code class="n">sorted_returns</code> <code class="o">=</code> <code class="n">generated</code><code class="p">[</code><code class="s1">'Values'</code><code class="p">]</code><code class="o">.</code><code class="n">sort_values</code><code class="p">()</code>&#13;
<code class="n">sorted_returns</code><code class="o">.</code><code class="n">head</code><code class="p">(</code><code class="mi">20</code><code class="p">)</code><code class="o">.</code><code class="n">round</code><code class="p">(</code><code class="mi">2</code><code class="p">)</code></pre>&#13;
&#13;
<figure class="informal"><div class="figure"><img alt="Image" src="assets/pmlf_08in02.png"/>&#13;
</div></figure>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="c1"># Compute the first percentile of returns. </code>&#13;
<code class="n">probability</code> <code class="o">=</code> <code class="mf">0.99</code>&#13;
<code class="n">gvar</code> <code class="o">=</code> <code class="n">sorted_returns</code><code class="o">.</code><code class="n">quantile</code><code class="p">(</code><code class="mi">1</code><code class="o">-</code><code class="n">probability</code><code class="p">)</code>&#13;
&#13;
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"The daily Generative VaR at </code><code class="si">{</code><code class="n">probability</code><code class="si">}</code><code class="s2">% probability is </code><code class="w"/>&#13;
<code class="p">{</code><code class="n">gvar</code><code class="o">/</code><code class="mi">100</code><code class="p">:</code><code class="mf">.2</code><code class="o">%</code><code class="p">}</code> <code class="n">implying</code> <code class="n">a</code> <code class="n">dollar</code> <code class="n">loss</code> <code class="n">of</code> <code class="err">$</code><code class="p">{</code><code class="n">gvar</code><code class="o">/</code><code class="mi">100</code><code class="o">*</code><code class="n">position_size</code><code class="p">:</code><code class="mf">.0</code><code class="n">f</code><code class="p">}</code> <code class="s2">")</code><code class="w"/>&#13;
&#13;
<code class="n">The</code> <code class="n">daily</code> <code class="n">Generative</code> <code class="n">VaR</code> <code class="n">at</code> <code class="mf">0.99</code><code class="o">%</code> <code class="n">probability</code> <code class="ow">is</code> <code class="o">-</code><code class="mf">3.79</code><code class="o">%</code> <code class="n">implying</code> <code class="n">a</code> <code class="n">dollar</code>&#13;
<code class="n">loss</code> <code class="n">of</code> <code class="err">$</code><code class="o">-</code><code class="mi">3789</code> &#13;
</pre>&#13;
</div></section>&#13;
&#13;
<section class="pagebreak-before" data-pdf-bookmark="Generative Expected Shortfall" data-type="sect2"><div class="sect2" id="expected_shortfall">&#13;
<h2>Generative Expected Shortfall</h2>&#13;
&#13;
<p>After the Great Financial Crisis,<a contenteditable="false" data-primary="probabilistic decision making with generative ensembles" data-secondary="risk management" data-tertiary="expected shortfall" data-type="indexterm" id="id1380"/><a contenteditable="false" data-primary="decision making" data-secondary="probabilistic with generative ensembles" data-tertiary="expected shortfall" data-type="indexterm" id="id1381"/><a contenteditable="false" data-primary="expected shortfall (ES)" data-type="indexterm" id="id1382"/><a contenteditable="false" data-primary="risk" data-secondary="risk management" data-tertiary="expected shortfall" data-type="indexterm" id="id1383"/><a contenteditable="false" data-primary="loss functions" data-secondary="expected shortfall" data-type="indexterm" id="id1384"/><a contenteditable="false" data-primary="value at risk (VaR)" data-secondary="expected shortfall" data-type="indexterm" id="id1385"/><a contenteditable="false" data-primary="Great Financial Crisis" data-type="indexterm" id="id1386"/> it became common knowledge that there is a deep flaw in the VaR measure that was used by financial institutions. It doesn’t estimate the heavy losses that can occur in the tail of the distribution beyond VaR’s cutoff point. Expected shortfall (ES), also known as conditional VaR, is a loss function that is commonly used to estimate the rare but extreme losses that might occur in the tail of the return distribution. Refer back to <a data-type="xref" href="#value_at_risk_left_parenthesisvarright">Figure 8-1</a> to see the relationship between VaR and ES. As the name implies, ES is an expected value and is estimated as a weighted average of all the losses after the VaR’s cutoff point. Let’s compute the generative ES of our linear ensemble and compare it to all the worst returns in the tail of the posterior predictive distribution:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="c1"># Filter the returns that fall below the first percentile</code>&#13;
<code class="n">generated_tail</code> <code class="o">=</code> <code class="n">sorted_returns</code><code class="p">[</code><code class="n">sorted_returns</code> <code class="o">&lt;=</code> <code class="n">gvar</code><code class="p">]</code>&#13;
&#13;
<code class="c1"># Expected shortfall is the mean of the tail returns.</code>&#13;
<code class="n">ges</code> <code class="o">=</code> <code class="n">generated_tail</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code>&#13;
&#13;
<code class="c1"># Generated tail risk is the worst possible loss predicted </code>&#13;
<code class="c1"># by the linear ensemble</code>&#13;
<code class="n">gtr</code> <code class="o">=</code> <code class="n">generated_tail</code><code class="o">.</code><code class="n">min</code><code class="p">()</code>&#13;
&#13;
<code class="c1"># Plot a histogram of the worst returns or generated tail risk (GTR)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">hist</code><code class="p">(</code><code class="n">generated_tail</code><code class="p">,</code> <code class="n">bins</code><code class="o">=</code><code class="mi">50</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">axvline</code><code class="p">(</code><code class="n">x</code><code class="o">=</code><code class="n">gvar</code><code class="p">,</code> <code class="n">color</code><code class="o">=</code><code class="s1">'green'</code><code class="p">,</code> <code class="n">linestyle</code><code class="o">=</code><code class="s1">'solid'</code><code class="p">,</code> &#13;
<code class="n">label</code><code class="o">=</code><code class="s1">'Generative Value at Risk'</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">axvline</code><code class="p">(</code><code class="n">x</code><code class="o">=</code><code class="n">ges</code><code class="p">,</code> <code class="n">color</code><code class="o">=</code><code class="s1">'black'</code><code class="p">,</code> <code class="n">linestyle</code><code class="o">=</code><code class="s1">'dashed'</code><code class="p">,</code> &#13;
<code class="n">label</code><code class="o">=</code><code class="s1">'Generative expected shortfall'</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">axvline</code><code class="p">(</code><code class="n">x</code><code class="o">=</code><code class="n">gtr</code><code class="p">,</code> <code class="n">color</code><code class="o">=</code><code class="s1">'red'</code><code class="p">,</code> <code class="n">linestyle</code><code class="o">=</code><code class="s1">'dotted'</code><code class="p">,</code> &#13;
<code class="n">label</code><code class="o">=</code><code class="s1">'Generative tail risk'</code><code class="p">)</code>&#13;
&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s1">'Simulated excess returns of Apple'</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s1">'Frequency of excess returns'</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="s1">'Simulation of the bottom 1</code><code class="si">% e</code><code class="s1">xcess returns of Apple'</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">legend</code><code class="p">(</code><code class="n">loc</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code>&#13;
&#13;
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"The daily Generative VaR at </code><code class="si">{</code><code class="n">probability</code><code class="si">}</code><code class="s2">% probability is </code><code class="w"/>&#13;
<code class="p">{</code><code class="n">gvar</code><code class="o">/</code><code class="mi">100</code><code class="p">:</code><code class="mf">.2</code><code class="o">%</code><code class="p">}</code> <code class="n">implying</code> <code class="n">a</code> <code class="n">dollar</code> <code class="n">loss</code> <code class="n">of</code> <code class="err">$</code><code class="p">{</code><code class="n">gvar</code><code class="o">/</code><code class="mi">100</code><code class="o">*</code><code class="n">position_size</code><code class="p">:</code><code class="mf">.0</code><code class="n">f</code><code class="p">}</code> <code class="s2">")</code><code class="w"/>&#13;
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"The daily Generative expected shortfall at </code><code class="w"/>&#13;
<code class="p">{</code><code class="mi">1</code><code class="o">-</code><code class="n">probability</code><code class="p">:</code><code class="mf">.2</code><code class="p">}</code><code class="o">%</code> <code class="n">probability</code> <code class="ow">is</code> <code class="p">{</code><code class="n">ges</code><code class="o">/</code><code class="mi">100</code><code class="p">:</code><code class="mf">.2</code><code class="o">%</code><code class="p">}</code> <code class="n">implying</code> <code class="n">a</code> <code class="n">dollar</code> <code class="n">loss</code> &#13;
<code class="n">of</code> <code class="err">$</code><code class="p">{</code><code class="n">ges</code><code class="o">/</code><code class="mi">100</code><code class="o">*</code><code class="n">position_size</code><code class="p">:</code><code class="mf">.0</code><code class="n">f</code><code class="p">}</code><code class="s2">")</code><code class="w"/>&#13;
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"The daily Generative tail risk is </code><code class="si">{</code><code class="n">gtr</code><code class="o">/</code><code class="mi">100</code><code class="si">:</code><code class="s2">.2%</code><code class="si">}</code><code class="s2"> </code><code class="w"/>&#13;
<code class="n">implying</code> <code class="n">a</code> <code class="n">dollar</code> <code class="n">loss</code> <code class="n">of</code> <code class="err">$</code><code class="p">{</code><code class="n">gtr</code><code class="o">/</code><code class="mi">100</code><code class="o">*</code><code class="n">position_size</code><code class="p">:</code><code class="mf">.0</code><code class="n">f</code><code class="p">}</code><code class="s2">")</code><code class="w"/>&#13;
&#13;
</pre>&#13;
&#13;
<p> </p>&#13;
&#13;
<figure class="informal"><div class="figure"><img alt="Image" src="assets/pmlf_08in03.png"/>&#13;
</div></figure>&#13;
&#13;
<p>From the loss functions of VaR and ES, we can see that there is a 99% probability that the daily losses on our hypothetical investment in Apple stock is not expected to be lower than –3.79%. Should the loss exceed that GVaR threshold, the GES or daily loss in 1% of the scenarios is not expected to be lower than –4.50%.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Generative Tail Risk" data-type="sect2"><div class="sect2" id="tail_risk">&#13;
<h2>Generative Tail Risk</h2>&#13;
&#13;
<p>The major flaw of ES is that<a contenteditable="false" data-primary="probabilistic decision making with generative ensembles" data-secondary="risk management" data-tertiary="tail risk" data-type="indexterm" id="id1387"/><a contenteditable="false" data-primary="decision making" data-secondary="probabilistic with generative ensembles" data-tertiary="tail risk" data-type="indexterm" id="id1388"/><a contenteditable="false" data-primary="risk" data-secondary="risk management" data-tertiary="tail risk" data-type="indexterm" id="id1389"/> it is yet another expected value or ensemble average that understates the risks due to extreme events. It’s an even more dangerous statistic as it is averaging over a subset of the worst losses of the returns of our regression ensemble in a region of the distribution that is even more non-ergodic and fat-tailed as shown in the previous graph of the simulated losses in the tail of the posterior predictive distribution. For our specific ensemble, the worst loss is over twice the GES. If an extreme loss impairs your capital base, you will not be around to observe the expected shortfall. As a volatility trader who is short volatility quite often, I use the worst loss generated by the posterior predictive distribution— –10.25% for our ensemble—as my shortfall and hedge my trades accordingly. I refer to it as the Generative tail risk (GTR) of the ensemble.</p>&#13;
&#13;
<p class="pagebreak-before">If you own a stock, as most people do, you are in essence short volatility and are making a high-probability bet that the stock is not going to make unexpected moves in the future. Based on your risk preferences, position size, and confidence in your regression ensemble, you might choose a different percentile in the tail of the return distribution as a reference point to manage your tail risk. Consequently, you may decide to hold your stock position, reduce it, or hedge it with options or futures or both. Regardless, you should continue to monitor your investment and the overall market by continually updating your regression ensemble with more recent data as it becomes available. As we have discussed in the latter half of this book, continual probabilistic machine learning is the hallmark of generative ensembles.<a contenteditable="false" data-primary="" data-startref="ch08-rism" data-type="indexterm" id="id1390"/></p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Capital Allocation" data-type="sect1"><div class="sect1" id="capital_allocation">&#13;
<h1>Capital Allocation</h1>&#13;
&#13;
<p>Capital preservation, or return<a contenteditable="false" data-primary="probabilistic decision making with generative ensembles" data-secondary="capital allocation" data-type="indexterm" id="id1391"/><a contenteditable="false" data-primary="capital allocation" data-secondary="about" data-type="indexterm" id="id1392"/><a contenteditable="false" data-primary="decision making" data-secondary="probabilistic with generative ensembles" data-tertiary="capital allocation" data-type="indexterm" id="ch08-allo"/> of our capital, is our primary objective. In the previous section, we explored the tools we can use to manage our risky investments to achieve that objective. Now let’s focus our attention on the second objective: the return on our capital, or capital appreciation. As investors and traders, we have two related, fundamental decisions to make when faced with investing in risky assets in an environment of three-dimensional uncertainty and incomplete information:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>Evaluate and decide if the investment will appreciate in value over a reasonable time period.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Decide what fraction of our hard-earned capital to allocate to that opportunity.</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>Expected value is used extensively for evaluating the attractiveness of investment opportunities. It is applied in almost every situation in finance and investing, from estimating the free cash flows of a company’s capital project to valuing its debt and its outstanding equity. However, like all concepts and tools, expected value has its strengths, weaknesses, and limitations. As we have already discussed in the previous section, expected value as an ensemble average is a complex idea. In this section we continue to deepen our understanding of ensemble averages to see if and where they can be applied appropriately by an investor looking to allocate their capital to increase their wealth without risking financial ruin at any time.</p>&#13;
&#13;
<aside class="pagebreak-before" data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="gambling_a_foolapostrophes_errand_for">&#13;
<h1>Gambling: A Fool’s Errand for the Ages</h1>&#13;
&#13;
<p>Gambling seems to have fallen into<a contenteditable="false" data-primary="gambling in history" data-type="indexterm" id="id1393"/><a contenteditable="false" data-primary="“Dyuta Sukta” (“Ode to Dice”)" data-primary-sortas="Dyuta Sukta" data-type="indexterm" id="id1394"/><a contenteditable="false" data-primary="“Ode to Dice” (“Dyuta Sukta”)" data-primary-sortas="Ode to Dice" data-type="indexterm" id="id1395"/><a contenteditable="false" data-primary="probabilistic decision making with generative ensembles" data-secondary="capital allocation" data-tertiary="gamblers" data-type="indexterm" id="id1396"/><a contenteditable="false" data-primary="decision making" data-secondary="probabilistic with generative ensembles" data-tertiary="gamblers" data-type="indexterm" id="id1397"/><a contenteditable="false" data-primary="capital allocation" data-secondary="gamblers" data-type="indexterm" id="id1398"/> worldwide disrepute at the dawn of civilization. The “Dyuta Sukta” (“Ode to Dice,” Rig Veda, 1700–1100 BCE) is a psychologically insightful ode to the woes of gamblers in ancient India, where it was a socially accepted activity.<sup><a data-type="noteref" href="ch08.html#ch08fn5" id="ch08fn5-marker">5</a></sup> The social ills of gambling compelled Chinese authorities to ban or regulate gambling as small-stakes games for most of its long history there. Ancient Jewish authorities suspected the ethics of gamblers and barred them from testifying in court.</p>&#13;
&#13;
<p>No other story captures the<a contenteditable="false" data-primary="Mahabharata on gambling" data-type="indexterm" id="id1399"/> ignominy and calamitous consequences of gambling as does the <em>Mahabharata</em> (circa 900 BCE), the world’s longest epic poem—it is roughly 10 times longer than the <em>Iliad</em> and <em>Odyssey</em> combined. The seeds of war between two royal families were sown when one of the heroes was invited by his cousin to play a game of dice that were secretly loaded. For complex reasons, the hero was compelled into gambling away his kingdom, his wealth, his four brothers, himself, and finally his wife. The heroes of this great epic plunged from royals to slaves with a few throws of loaded dice. Now that is the epitome of a gambler’s ruin!</p>&#13;
</div></aside>&#13;
&#13;
<section data-pdf-bookmark="Gambler’s Ruin" data-type="sect2"><div class="sect2" id="gamblerapostrophes_ruin">&#13;
<h2>Gambler’s Ruin</h2>&#13;
&#13;
<p>It was not until the 17th century<a contenteditable="false" data-primary="probabilistic decision making with generative ensembles" data-secondary="capital allocation" data-tertiary="gamblers’ ruin" data-type="indexterm" id="id1400"/><a contenteditable="false" data-primary="capital allocation" data-secondary="gamblers’ ruin" data-type="indexterm" id="id1401"/><a contenteditable="false" data-primary="gambler’s ruin" data-type="indexterm" id="id1402"/><a contenteditable="false" data-primary="decision making" data-secondary="probabilistic with generative ensembles" data-tertiary="gamblers’ ruin" data-type="indexterm" id="id1403"/><a contenteditable="false" data-primary="Pascal, Blaise" data-type="indexterm" id="id1404"/> that Blaise Pascal, an eminent mathematician and physicist, working with a French aristocrat to improve his gambling skills, proved mathematically what was known to be true for a few millennia: eventually all gamblers go broke. Gambles are useful probabilistic models and have played a pivotal role in the development of probability and decision theories.<sup><a data-type="noteref" href="ch08.html#ch08fn6" id="ch08fn6-marker">6</a></sup> The classic problem of gambler’s ruin is instructive for investors as it emphasizes that a positive expected value is a necessary condition for making investments.</p>&#13;
&#13;
<p>Suppose you decide to play the following coin-tossing game. You start with $M, and your opponent starts with $N. Each of you bets $1 on the toss of a coin, which turns up heads with probability p and tails with probability q = (1 – p). If it’s heads, you win $1 from your opponent; if it’s tails, you lose $1 to your opponent. The game ends when either player goes broke (i.e., ruined). This is no silly game. It’s a stochastic process called an arithmetic random walk and can be used to model stock prices and collisions of dust particles. It is also a Markov chain, since its future state only depends on its current state and not the path it took to get there.</p>&#13;
&#13;
<p>The gambler’s ruin is a two-part problem in which a gambler makes a series of bets with negative or zero expected value. Using the arithmetic random walk model, it can be shown mathematically that any gambler will almost surely go broke in both of the following scenarios:</p>&#13;
&#13;
<ol>&#13;
	<li>&#13;
	<p>A gambler makes a series of bets, where the probability of success for each bet is less than 50% and payoff equals the amount staked. In such games the gambler will eventually go broke regardless of their betting strategy since their wager always has negative expectation. It doesn’t even matter how big the gambler’s bankroll is compared to their opponents. The gambler’s probability of ruin P(ruin) is:</p>&#13;
&#13;
<ul class="simplelist">&#13;
<li><p><math><mrow><mrow><mrow><mi>P</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>r</mi><mo>⁢</mo><mi>u</mi><mo>⁢</mo><mi>i</mi><mo>⁢</mo><mi>n</mi></mrow><mo>)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mrow><mo>[</mo><mrow><mn>1</mn><mo>−</mo><mrow><mrow><mo>(</mo><mrow><mfrac><mi>p</mi><mi>q</mi></mfrac></mrow><mo>)</mo></mrow><mo>⁢</mo><mi>^</mi><mo>⁢</mo><mi>N</mi></mrow></mrow><mo>]</mo></mrow><mo>/</mo><mrow><mo>[</mo><mrow><mn>1</mn><mo>−</mo><mrow><mrow><mo>(</mo><mrow><mfrac><mi>p</mi><mi>q</mi></mfrac></mrow><mo>)</mo></mrow><mo>⁢</mo><mi>^</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>N</mi><mo>+</mo><mi>M</mi></mrow><mo>)</mo></mrow></mrow></mrow><mo>]</mo></mrow></mrow><mo>⁢</mo><mi>i</mi><mo>⁢</mo><mi>f</mi><mo>⁢</mo><mi>p</mi></mrow></mrow><mo>&lt;</mo><mi>q</mi></mrow></math></p></li>&#13;
</ul>&#13;
	</li>&#13;
	<li>&#13;
	<p>A gambler is given a series of bets where the probability of success of each bet is 50% and the payoff equals the amount staked. These are fair odds, but the gambler’s opponent has a bigger bankroll. Surprisingly, the gambler will eventually go broke even in this scenario, if their opponent has a marginally bigger bankroll. If their opponent has a much larger bankroll, such as a casino dealer, the bumpy road to ruin transforms into a highway with no speed limit. The probability of ruin P(ruin) is:</p>&#13;
<ul class="simplelist">&#13;
<li>&#13;
	<p><math><mrow><mrow><mrow><mi>P</mi><mo>(</mo><mi>r</mi><mi>u</mi><mi>i</mi><mi>n</mi><mo>)</mo><mo>=</mo><mfrac><mrow><mi>N</mi></mrow><mrow><mo>(</mo><mi>M</mi><mo>+</mo><mi>N</mi><mo>)</mo></mrow></mfrac></mrow><mo>⁢</mo><mi>i</mi><mi>f</mi><mo>⁢</mo><mtext> </mtext><mi>P</mi><mo>⁢</mo><mtext> </mtext><mtext> </mtext><mo>=</mo><mtext> </mtext><mi>q</mi><mo>⁢</mo><mtext> </mtext><mtext> </mtext><mn>a</mn><mn>n</mn><mn>d</mn><mo>⁢</mo><mtext> </mtext><mtext> </mtext><mn>M</mn><mo>&lt;</mo><mn>N</mn></mrow></mrow></math></p>&#13;
	</li>&#13;
	</ul>&#13;
	</li>&#13;
</ol>&#13;
&#13;
<p>Note that in the first scenario, we are assuming that the gambler is not able or allowed to count cards or use the physics of the gambling machine, such as a roulette wheel, <a contenteditable="false" data-primary="Thorp, Edward O." data-type="indexterm" id="id1405"/>as the great Ed Thorp did to beat the dealers in Vegas in the 1960s.<sup><a data-type="noteref" href="ch08.html#ch08fn7" id="ch08fn7-marker">7</a></sup></p>&#13;
&#13;
<p>The math is brutally clear. No matter how hard you try or what betting system you invent, because of negative expectations of the bets, gambling is a fool’s errand for the ages. Gamblers will take all kinds of random walks that will zig and zag between gains and losses, but all roads will eventually lead to ruin. Games involving equal odds with equal bankrolls are implausible situations for almost all gamblers. So to avoid going broke, a gambler needs to make bets with positive expected values.</p>&#13;
&#13;
<p>But a gamble with positive expectation is commonly known as an investment. In mathematical models, the sign of the expected value of each bet is the main difference between a gamble and an investment, <a contenteditable="false" data-primary="Kelly, John" data-type="indexterm" id="id1406"/><a contenteditable="false" data-primary="optimal capital growth algorithm" data-type="indexterm" id="id1407"/>according to John Kelly, the inventor of the optimal capital growth algorithm.<sup><a data-type="noteref" href="ch08.html#ch08fn8" id="ch08fn8-marker">8</a></sup> By engaging in positive expectation bets, a <span class="keep-together">degenerate gambler</span> morphs into a respectable investor who can now pursue a statistically feasible path of increasing their wealth while avoiding financial ruin. But what fraction of our capital do we allocate to investments with positive expectations? Does it make sense to go all in when we are offered really favorable odds?</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Expected Valuer’s Ruin" data-type="sect2"><div class="sect2" id="expected_valuerapostrophes_ruin">&#13;
<h2>Expected Valuer’s Ruin</h2>&#13;
&#13;
<p>Say you are confronted by a wealthy<a contenteditable="false" data-primary="probabilistic decision making with generative ensembles" data-secondary="capital allocation" data-tertiary="expected valuer’s ruin" data-type="indexterm" id="ch08-expv"/><a contenteditable="false" data-primary="decision making" data-secondary="probabilistic with generative ensembles" data-tertiary="expected valuer’s ruin" data-type="indexterm" id="ch08-expv2"/><a contenteditable="false" data-primary="expected valuer’s ruin" data-type="indexterm" id="ch08-expv3"/> and powerful adversary who owns a biased coin that has a 76% probability of showing heads. Assume that the probability of this biased coin is known precisely to all, but the physics of any coin toss is always unknown. Your adversary makes you a legally binding offer—an offer you <em>can</em> refuse.</p>&#13;
&#13;
<p>If you stake your entire net worth on a single toss of his coin and it shows heads, he will pay you three times the value of your net worth. But if the coin shows tails, you lose your entire net worth except the clothes you are wearing—it’s not personal, it’s just business. This seems like the wager of a lifetime because it gives you the opportunity to increase your net worth twofold in a blink of an eye:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>Expected value of wager = (3 × net worth × 0.76) – (net worth × 0.24) = 2.04 × net worth.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Payoffs are in multiples of your net worth, which will get estimated in legal proceedings (or not), so bluffing your net worth won’t help.</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>Do you accept this offer, which clearly has a high expected value, with a 76% probability of success but with a nontrivial 24% probability of financial ruin? Tossing a coin once doesn’t really involve time, so can expected value as an ensemble average work here to help us evaluate this opportunity?</p>&#13;
&#13;
<p>Our common sense instinctively raises red flags about relying on any financial rule of maximizing expected value for such high-stakes decision making. It’s as if we were the ones staring down the barrel of Detective “Dirty” Harry’s famous .44 magnum handgun, wondering whether there is a bullet left, and him warning us: “You’ve gotta ask yourself one question: ‘Do I feel lucky?’ Well, do ya, punk?”</p>&#13;
&#13;
<p>Any responsible person, experienced investor, trader, or corporate executive (all of whom, <em>generally speaking</em>, are not punks) would refuse this offer because any investment opportunity that even hints at the possibility of financial ruin is a deal-breaker. This is expressed succinctly in a market maxim that says, “There are old traders, there are bold traders, but there are no old, bold traders.” Or another one that says, “Bulls make money, bears make money, but pigs get slaughtered.” One essential statistical insight of these two aphorisms, built over centuries of collective observations and life experiences, is that maximizing the expected value of investments almost surely leads to heavy losses, if not financial ruin, even if the odds are in your favor.</p>&#13;
&#13;
<p>What about a series of favorable bets where you maximize the expected value of each bet by going all in? Even if your same adversary were to give you a series of independent and identically distributed (i.i.d.) positive expectation bets, it would be ruinous for you to bet everything you have on each successive bet. You don’t need mathematical proof to figure out that it only takes one losing bet to wipe out all the accumulated profits and the initial bankroll of any investor who is an expected value maximizer.</p>&#13;
&#13;
<p>So how should you make your decision in one-off binary opportunities with positive expectations that don’t involve betting your entire net worth? Let’s return to the situation we described in <a data-type="xref" href="ch06.html#the_dangers_of_conventional_ai_systems">Chapter 6</a> regarding ZYX technology company and its earnings expectations. Recall that after observing ZYX successfully beat its earnings expectations in the last three quarters, your model’s prediction was that there was a 76% probability that it would beat its earnings expectations in the next quarter. Assume that you continue to find the probabilistic model useful, and ZYX is going to announce its earnings after the close of trading today.</p>&#13;
&#13;
<p>Based on prices of options traded on ZYX stock, it seems that the market is pricing a 5% move up in the stock price if ZYX beats its earnings expectations. However, the market is also pricing a 15% move down in ZYX stock price if it does not. For the sake of this discussion, assume that these are accurate forecasts of the move in the stock prices after the earnings event. How can you use this market information and your model’s prediction to allocate capital to ZYX stock before the earnings announcement today?</p>&#13;
&#13;
<p>Let’s create an objective function V(F, Y″) where F is the fraction of your total capital you want to invest in ZYX and Y″ is the predicted outcome of an earnings beat. Given your objectives of avoiding any possibility of penury, F must be in the interval [0, 1):</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>Since F cannot equal 1 for any investment, we avoid the expected value maximizing strategy and the gambler’s ruin, as previously discussed.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Furthermore, leveraging your position is not allowed as F &lt; 1. This means you cannot borrow cash from your broker to invest more capital than the cash in your account. When you borrow money from your broker to invest in stocks, you can end up owing more than your initial capital outlay, which is worse than blowing up your account.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Since F cannot be negative, you cannot short stocks. Shorting a stock is an advanced trading technique in which you borrow the shares from your broker to sell the stock with the expectation of buying it at a lower price. It’s buying low and selling high but in reverse order. Note that stock prices have a floor at $0 because of the limited liability of corporate ownership. However, stocks do not have a theoretical upper limit, which many unfortunate investors have realized in bubbles and manias. This is why shorting stocks can be risky and requires <span class="keep-together">expertise</span> and disciplined risk management. Stocks can burst into powerful rallies, called short covering rallies, for the flimsiest of reasons. These rallies can be twice as powerful, since there are buy orders from buyers and buy orders from short sellers, who are rushing to cover their short positions by buying back the stocks they had previously sold short. I have been on the wrong side of such short covering rallies several times, and the phrase “face-ripping rallies” is a fitting description of these experiences.</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>Recall that Y″ is our probabilistic model’s out-of-sample predictions of ZYX’s earnings announcements based on observed in-sample data D, with P(Y<sub>1</sub>″ = 1 | D) = 76% when ZYX beats earnings expectations and P(Y<sub>0</sub>″ = 0 | D) = 24% when it doesn’t. Therefore, the expected value of our objective function, E[V(F)], is the probability weighted average over a profit (W) outcome of 5% and a loss outcome (L) of –15%:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>E[V(F)] = W × F × P(Y<sub>1</sub>″ = 1 | D) + L × F × P(Y<sub>0</sub>″ = 0 | D)</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>E[V(F)] = 0.05 × F × 0.76 – 0.15 × F × 0.24 = 0.002 × F</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>The trade has an edge or positive expectation of about 0.2%.</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>It only takes common sense to see that no single investor will observe an increase of 0.2% in the stock value of ZYX, or any other expected value they might have estimated, after the earnings event. Depending on the actual earnings results of ZYX, each investor who is long on the stock will either incur a 5% gain or –15% loss, or vice versa if they are short on the stock. The expected value we have computed is an ensemble average of all the profits and losses of all ZYX stockholders. It is hard to estimate, and it may or may not be within a reasonable range of your estimate.</p>&#13;
&#13;
<p>But why should we care about the ensemble average anyway in this situation? As you can see, it is a completely useless tool for decision making in such one-off binary events for a single investor. A positive expected value of ZYX’s earnings event sounds great until you get hit with a –15% loss in a high-frequency microsecond, much faster than your eye can even blink. Mike Tyson, a former heavyweight boxing champion, summarized such hopeful positive expectations eloquently when he said, “Everyone has a plan until they get punched in the mouth.”</p>&#13;
&#13;
<p>So what capital allocation algorithm can help you make decisions in one-off binary trades regardless of the amount of capital you are going to allocate to the bet? Unfortunately, there are none. Only your capacity to bear the worst <em>known</em> outcome of your decision, which is subjective by definition, can help you make such one-off binary decisions. We have already discussed how to integrate subjectivity into probabilistic decision making. Say you have a daily loss limit of –10% and daily profit limit of +5% for any position in your portfolio. This makes the decision systematic and much easier to make, especially for automated systems:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>Don’t invest in ZYX before the earnings announcement, since the non-trivial probability of losing –10% in one day conflicts with the risk limits of your objective function.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Don’t short ZYX stock since that conflicts with your objective function.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>If you already have an investment in ZYX stock, you need to recalibrate your position size and hedge it with options or futures such that the daily loss doesn’t exceed –10%. Of course, hedging costs will lower the 5% expected gain, so you will have to recalculate the expected value to make sure it is still positive.</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>Searching for investment opportunities that have positive expected values over a reasonable time horizon is generally the difficult part of any investment strategy. But in this section, we have learned that making investments with positive expectation is a necessary but not a sufficient condition. Therefore, investors are faced with a dilemma:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>If they allocate too much capital to such a favorable opportunity, they risk bankruptcy or making devastating losses.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>If they allocate too little capital, they risk wasting a favorable opportunity.</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>This implies that investors need a capital allocation algorithm that computes a percentage of their total capital to a series of investment opportunities with positive expectation such that it balances two fundamental objectives:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>Avoids financial ruin at all times</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Increases their wealth in a finite time period</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>Some investors have additional objectives that are to be achieved on the capital they manage over a specific time period, generally one year:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>Percentage profits to exceed a defined threshold</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Percentage losses not to exceed a defined threshold</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>These objectives can be encoded in an investor’s objective function that will condition and constrain their capital allocation algorithm. As we have already learned in this chapter so far, applying expected value in investing and finance is neither intuitive nor straightforward, as investment processes are non-ergodic. Let’s explore a capital allocation algorithm that is widely used in academia and in the industry.<a contenteditable="false" data-primary="" data-startref="ch08-expv" data-type="indexterm" id="id1408"/><a contenteditable="false" data-primary="" data-startref="ch08-expv2" data-type="indexterm" id="id1409"/><a contenteditable="false" data-primary="" data-startref="ch08-expv3" data-type="indexterm" id="id1410"/></p>&#13;
</div></section>&#13;
&#13;
<section class="pagebreak-before" data-pdf-bookmark="Modern Portfolio Theory" data-type="sect2"><div class="sect2" id="modern_portfolio_theory">&#13;
<h2>Modern Portfolio Theory</h2>&#13;
&#13;
<p>Modern portfolio theory (MPT), developed<a contenteditable="false" data-primary="probabilistic decision making with generative ensembles" data-secondary="capital allocation" data-tertiary="modern portfolio theory" data-type="indexterm" id="ch08-modt"/><a contenteditable="false" data-primary="capital allocation" data-secondary="modern portfolio theory" data-type="indexterm" id="ch08-modt2"/><a contenteditable="false" data-primary="decision making" data-secondary="probabilistic with generative ensembles" data-tertiary="modern portfolio theory" data-type="indexterm" id="ch08-modt3"/><a contenteditable="false" data-primary="modern portfolio theory (MPT)" data-secondary="capital allocation" data-type="indexterm" id="ch08-modt4"/><a contenteditable="false" data-primary="Markowitz, Harry" data-type="indexterm" id="id1411"/><a contenteditable="false" data-primary="modern portfolio theory (MPT)" data-secondary="about" data-type="indexterm" id="id1412"/><a contenteditable="false" data-primary="volatility" data-secondary="modern portfolio theory" data-type="indexterm" id="id1413"/> by Harry Markovitz in 1952, focuses on quantifying the benefits of diversification using correlations of returns of different assets in a portfolio. It maximizes the expected value of the returns of a portfolio of assets for a given level of variance over a single time period, so volatility (the square root of variance) is used as a constraint on the expected value optimization algorithm. MPT assumes that asset price returns are stationary ergodic and normally distributed.</p>&#13;
&#13;
<p>As we have learned already, these are unrealistic and dangerous assumptions, due to the following factors:<a contenteditable="false" data-primary="dangers of modern portfolio theory" data-type="indexterm" id="id1414"/><a contenteditable="false" data-primary="modern portfolio theory (MPT)" data-secondary="dangers of" data-type="indexterm" id="id1415"/></p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>They ignore skewness and kurtosis of asset price returns, which are known to be asymmetric and fat-tailed even by academics.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Portfolio diversification is reduced or eliminated in periods of extreme market stress, as we saw recently in 2020 and previously in 2008.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>In normal periods, fat-tailed distributions can introduce large errors in correlations among securities in the portfolio.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Portfolio weights can be extremely sensitive to estimates of returns, variances, and covariances. Small changes in return estimates can completely change the composition of the optimal portfolio.</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>MPT portfolios are much riskier than advertised and provide suboptimal returns—diversification leads to “diworsification.” <a contenteditable="false" data-primary="Buffet, Warren" data-type="indexterm" id="id1416"/>Buffet has called MPT “a whole lot of nonsense” and has been laughing all the way to his mega bank ever since.</p>&#13;
&#13;
<p>In an interview, Markowitz admitted<a contenteditable="false" data-primary="Markowitz, Harry" data-type="indexterm" id="id1417"/><a contenteditable="false" data-primary="1/N heuristic" data-type="indexterm" id="id1418"/><a contenteditable="false" data-primary="naive diversification strategy" data-type="indexterm" id="id1419"/> to not using his “Nobel prize–winning” mean-variance algorithm for his own retirement funds! If that is not an indictment of the mean-variance algorithm, I don’t know what else is.<sup><a data-type="noteref" href="ch08.html#ch08fn9" id="ch08fn9-marker">9</a></sup> Instead Markowitz used 1/N heuristic or the naive diversification strategy. This is an investment strategy in which you allocate equal amounts of your capital to each of N investments. This naive diversification portfolio strategy has been shown to outperform mean variance and other complex portfolio strategies.<sup><a data-type="noteref" href="ch08.html#ch08fn10" id="ch08fn10-marker">10</a></sup></p>&#13;
&#13;
<p class="pagebreak-before">We focus our attention on another simpler but<a contenteditable="false" data-primary="capital asset pricing model (CAPM)" data-secondary="modern portfolio theory as basis" data-type="indexterm" id="id1420"/><a contenteditable="false" data-primary="capital asset pricing model (CAPM)" data-secondary="modern portfolio theory as basis" data-tertiary="CAPM equally useless" data-type="indexterm" id="id1421"/><a contenteditable="false" data-primary="dangers of modern portfolio theory" data-secondary="CAPM equally useless" data-type="indexterm" id="id1422"/><a contenteditable="false" data-primary="capital asset pricing model (CAPM)" data-secondary="dangers of" data-type="indexterm" id="id1423"/><a contenteditable="false" data-primary="Sharpe, William" data-type="indexterm" id="id1424"/> equally useless model of MPT to highlight the conceptual blunder of using volatility as a measure of total risk. The capital asset pricing model (CAPM) discussed in <a data-type="xref" href="ch04.html#the_dangers_of_conventional_statistical">Chapter 4</a> was derived from Markovitz’s portfolio theory by his student William Sharpe. It simplifies MPT in terms of thinking about expected return for any risky investment. <a contenteditable="false" data-primary="risk" data-secondary="unsystematic and systematic of CAPM" data-type="indexterm" id="id1425"/><a contenteditable="false" data-primary="unsystematic risk of CAPM" data-type="indexterm" id="id1426"/><a contenteditable="false" data-primary="systematic risk of CAPM" data-type="indexterm" id="id1427"/>According to the CAPM, an asset has two types of risks: unsystematic and systematic. Unsystematic risk is idiosyncratic to the asset concerned and is diversifiable. Systematic risk is market risk that affects all assets and is not diversifiable.</p>&#13;
&#13;
<p>The CAPM builds on the heroic assumptions of MPT that all investors are rational and risk-averse and have the same expectations at the same time given the same information, such that markets are always in equilibrium. Such financial fairy tales rival any you might see in Disney’s Magic Kingdom. At any rate, these Markovitz investors are supposed to create strongly efficient markets and only hold diversified portfolios that will reduce the correlation among assets and eliminate the idiosyncratic risk of any particular asset. Statistically, this implies that in a well-diversified portfolio, the idiosyncratic risk of any particular asset will be zero, as will the expected value of any error term in the regression line. Therefore, Markovitz investors will only pay a premium for systematic risk of an asset, as it cannot be diversified away.</p>&#13;
&#13;
<p>In such strongly efficient markets, all fairly priced investments will plot on a regression line called the security market line with the intercept equal to the risk-free rate and the slope equal to beta, or systematic risk. An asset’s beta gives the magnitude and direction of the movement of the asset with respect to the market. See <a data-type="xref" href="#the_capm_claims_that_as_you_increase_th">Figure 8-2</a> (M is the market portfolio with beta = 1).</p>&#13;
&#13;
<p>The systematic risk term, beta, of the asset’s MM is the same as the one calculated using its CAPM. However, note that an asset’s market model (MM) is different from its CAPM in three important respects:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>The CAPM formulates expected returns of an asset, while its MM formulates realized returns.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>The MM has both an idiosyncratic risk term (alpha) and an error term in its formulation.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Based on MPT, the expected value of alpha is zero since it has been diversified away by rational investors. That is the reason it does not appear in the CAPM.</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<figure><div class="figure" id="the_capm_claims_that_as_you_increase_th"><img alt="The CAPM claims that as you increase the systematic risk of your investment, or beta, its expected return increases linearly. Beta is directly proportional to the volatility of returns of the investment." src="assets/pmlf_0802.png"/>&#13;
<h6><span class="label">Figure 8-2. </span>The CAPM claims that as you increase the systematic risk of your investment, or beta, its expected return increases linearly. Beta is directly proportional to the volatility of returns of the investment<sup><a data-type="noteref" href="ch08.html#ch08fn11" id="ch08fn11-marker">11</a></sup></h6>&#13;
</div></figure>&#13;
&#13;
<p>In simple linear regression, beta quantifies the average change in the target for a unit change in the associated feature. Based on the assumptions of simple linear regression, especially the one about constant variance of the residuals, beta has an analytical formula that is equal to:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>Beta = Rxy × Sy / Sx where:</p>&#13;
&#13;
	<ul>&#13;
		<li>&#13;
		<p>Sy = standard deviation of the target or investment</p>&#13;
		</li>&#13;
		<li>&#13;
		<p>Sx = standard deviation of the feature or market portfolio</p>&#13;
		</li>&#13;
		<li>&#13;
		<p>Rxy = coefficient of correlation between the feature and the target</p>&#13;
		</li>&#13;
	</ul>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>Beta can also be interpreted as the parameter that correlates the volatility of the risky investment with the volatility of the market.<a contenteditable="false" data-primary="" data-startref="ch08-modt" data-type="indexterm" id="id1428"/><a contenteditable="false" data-primary="" data-startref="ch08-modt2" data-type="indexterm" id="id1429"/><a contenteditable="false" data-primary="" data-startref="ch08-modt3" data-type="indexterm" id="id1430"/><a contenteditable="false" data-primary="" data-startref="ch08-modt4" data-type="indexterm" id="id1431"/></p>&#13;
</div></section>&#13;
&#13;
<section class="pagebreak-before" data-pdf-bookmark="Markowitz Investor’s Ruin" data-type="sect2"><div class="sect2" id="markowitz_investorapostrophes_ruin">&#13;
<h2>Markowitz Investor’s Ruin</h2>&#13;
&#13;
<p>As you can see from <a data-type="xref" href="#the_capm_claims_that_as_you_increase_th">Figure 8-2</a>, the CAPM<a contenteditable="false" data-primary="MPT" data-see="modern portfolio theory" data-type="indexterm" id="id1432"/><a contenteditable="false" data-primary="probabilistic decision making with generative ensembles" data-secondary="capital allocation" data-tertiary="MPT/CAPM investor’s ruin" data-type="indexterm" id="ch08-ruin"/><a contenteditable="false" data-primary="capital allocation" data-secondary="MPT/CAPM investor’s ruin" data-type="indexterm" id="ch08-ruin2"/><a contenteditable="false" data-primary="modern portfolio theory (MPT)" data-secondary="dangers of" data-tertiary="investor’s ruin" data-type="indexterm" id="ch08-ruin3"/><a contenteditable="false" data-primary="capital asset pricing model (CAPM)" data-secondary="dangers of" data-tertiary="investor’s ruin" data-type="indexterm" id="ch08-ruin4"/><a contenteditable="false" data-primary="decision making" data-secondary="probabilistic with generative ensembles" data-tertiary="MPT/CAPM investor’s ruin" data-type="indexterm" id="ch08-ruin5"/> claims that you can increase the expected value of returns as much as you want, by selecting risky investments or using leverage or both, as long as you are willing to accept the attendant volatility of the asset’s price returns.</p>&#13;
&#13;
<p>Let’s test these assumptions about the expected value of returns by generating a very large sample of hypothetical trades with the same probabilities, outcomes, and payoffs as ZYX’s earnings event. In the following Python code, we generate 20,000 samples from our posterior predictive distribution. That should be sufficiently large for the law of large numbers (LLN) to kick in and enable the convergence of any asymptotic property of stochastic processes.</p>&#13;
&#13;
<p>In particular, we will calculate our ensemble average by computing the posterior predictive mean across the 20,000 simulated samples. These samples simulate the two outcomes of ZYX’s earnings event. We then provide the same 20,000 outcomes sequentially as a time series to 100 simulated investors. Each simulated investor applies MPT/CAPM theory to their investing process. They allocate anywhere from 1% to 100% of their initial capital of $100,000 to ZYX stock. The profit or loss resulting from each of the simulated outcomes for a specific investor/fraction of the total capital is computed. Our code keeps track of the terminal wealth for each specific fraction/investor iteratively. Finally, we plot the terminal wealth for each fraction/investor and check if the time average of the typical investor equals the ensemble average computed earlier:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="c1">#Fix the random seed so numbers can be reproduced</code>&#13;
<code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="mi">114</code><code class="p">)</code>&#13;
&#13;
<code class="c1">#Number of posterior predictive samples to simulate</code>&#13;
<code class="n">N</code> <code class="o">=</code> <code class="mi">20000</code>&#13;
&#13;
<code class="c1">#Draw 100,000 samples from the model's posterior distribution </code>&#13;
<code class="c1">#of parameter p</code>&#13;
<code class="c1">#Random.choice() selects 100,000 values of p from the </code>&#13;
<code class="c1">#earnings_beat['parameter'] column using the probabilities in the </code>&#13;
<code class="c1">#earnings_beat['posterior'] column.</code>&#13;
<code class="n">posterior_samples</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">choice</code><code class="p">(</code><code class="n">earnings_beat</code><code class="p">[</code><code class="s1">'parameter'</code><code class="p">],</code> &#13;
<code class="n">size</code><code class="o">=</code><code class="mi">100000</code><code class="p">,</code> <code class="n">p</code><code class="o">=</code><code class="n">earnings_beat</code><code class="p">[</code><code class="s1">'posterior'</code><code class="p">])</code>&#13;
&#13;
<code class="c1">#Draw a smaller subset of N random samples from the </code>&#13;
<code class="c1">#posterior samples of parameter p</code>&#13;
<code class="n">posterior_samples_n</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">choice</code><code class="p">(</code><code class="n">posterior_samples</code><code class="p">,</code> <code class="n">size</code><code class="o">=</code><code class="n">N</code><code class="p">)</code>&#13;
&#13;
<code class="c1">#Generate N random simulated outcomes by using the model's likelihood</code>&#13;
<code class="c1">#function and posterior samples of the parameter p</code>&#13;
<code class="c1">#Likelihood function is the Bernoulli distribution, a special case </code>&#13;
<code class="c1">#of the binomial distribution where number of trials n=1</code>&#13;
<code class="c1">#Simulated data are the data generated from the posterior </code>&#13;
<code class="c1">#predictive distribution of the model</code>&#13;
<code class="n">simulated_data</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">binomial</code><code class="p">(</code><code class="n">n</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">p</code><code class="o">=</code><code class="n">posterior_samples_n</code><code class="p">)</code>&#13;
&#13;
<code class="c1">#Plot the simulated data of earnings outcomes y=0 and y=1</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">figure</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">8</code><code class="p">,</code><code class="mi">6</code><code class="p">))</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">hist</code><code class="p">(</code><code class="n">simulated_data</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">xticks</code><code class="p">([</code><code class="mi">0</code><code class="p">,</code><code class="mi">1</code><code class="p">])</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s1">'Predicted outcomes'</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s1">'Count'</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="s1">'Simulated outcomes of ZYX beating earnings expectations'</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code>&#13;
&#13;
<code class="c1">#Count the number of data points for each outcome</code>&#13;
<code class="n">y_0</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">simulated_data</code> <code class="o">==</code> <code class="mi">0</code><code class="p">)</code>&#13;
<code class="n">y_1</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">simulated_data</code> <code class="o">==</code> <code class="mi">1</code><code class="p">)</code>&#13;
&#13;
<code class="c1">#Compute the posterior predictive distribution</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Probability that ZYX will not beat earnings expectations (y=0) is:</code><code class="w"/>&#13;
<code class="p">{</code><code class="n">y_0</code><code class="o">/</code><code class="p">(</code><code class="n">y_0</code><code class="o">+</code><code class="n">y_1</code><code class="p">):</code><code class="mf">.3</code><code class="n">f</code><code class="p">}</code><code class="s2">")</code><code class="w"/>&#13;
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Probability that ZYX will beat earnings expectations (y=1) is:</code><code class="w"/>&#13;
<code class="p">{</code><code class="n">y_1</code><code class="o">/</code><code class="p">(</code><code class="n">y_0</code><code class="o">+</code><code class="n">y_1</code><code class="p">):</code><code class="mf">.3</code><code class="n">f</code><code class="p">}</code><code class="s2">")</code><code class="w"/>&#13;
</pre>&#13;
&#13;
<figure class="informal"><div class="figure"><img alt="Image" src="assets/pmlf_08in04.png"/>&#13;
</div></figure>&#13;
&#13;
<p>Notice that the probabilities of the outcome variables based on our posterior predictive distribution are almost equal to the theoretical probabilities for y = 0 and y = 1. This validates our claim that the sample size is large enough for asymptotic convergence and the LLN is working as expected. Now we continue to calculate our profits and losses based on a sequence of 20,000 possible outcomes generated by our model to compute the terminal wealth of all investors:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="c1">#Percentage losses when y=0 and earnings don't beat expectations</code>&#13;
<code class="n">loss</code> <code class="o">=</code> <code class="o">-</code><code class="mf">0.15</code>&#13;
<code class="c1">#Percentage profits when y=1 and earnings beat expectations</code>&#13;
<code class="n">profit</code> <code class="o">=</code> <code class="mf">0.05</code>&#13;
&#13;
<code class="c1">#Set the starting capital</code>&#13;
<code class="n">start_capital</code> <code class="o">=</code> <code class="mi">100000</code>&#13;
&#13;
<code class="c1">#Create a list of values for position_size or percentage of total capital </code>&#13;
<code class="c1">#invested in ZYX by an investor</code>&#13;
<code class="n">position_size</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">arange</code><code class="p">(</code><code class="mf">0.00</code><code class="p">,</code> <code class="mf">1.00</code><code class="p">,</code> <code class="mf">0.01</code><code class="p">)</code>&#13;
&#13;
<code class="c1">#Create an empty list to store the final capital values for </code>&#13;
<code class="c1">#each position_size of an investor</code>&#13;
<code class="n">final_capital_values</code> <code class="o">=</code> <code class="p">[]</code>&#13;
&#13;
<code class="c1">#Loop over each value of position_size f to calculate </code>&#13;
<code class="c1">#terminal wealth for each investor</code>&#13;
<code class="k">for</code> <code class="n">f</code> <code class="ow">in</code> <code class="n">position_size</code><code class="p">:</code>&#13;
   <code class="c1">#Set the initial capital for this simulation</code>&#13;
   <code class="n">capital</code> <code class="o">=</code> <code class="n">start_capital</code>&#13;
  &#13;
   <code class="c1">#Loop over each simulated data point and calculate the P&amp;L based on y=0 or y=1</code>&#13;
    <code class="k">for</code> <code class="n">y</code> <code class="ow">in</code> <code class="n">simulated_data</code><code class="p">:</code>&#13;
        <code class="k">if</code> <code class="n">y</code> <code class="o">==</code> <code class="mi">0</code><code class="p">:</code>&#13;
           <code class="n">capital</code> <code class="o">+=</code> <code class="n">capital</code> <code class="o">*</code> <code class="n">loss</code> <code class="o">*</code> <code class="n">f</code>&#13;
       <code class="k">else</code><code class="p">:</code>&#13;
           <code class="n">capital</code> <code class="o">+=</code> <code class="n">capital</code> <code class="o">*</code> <code class="n">profit</code> <code class="o">*</code> <code class="n">f</code>&#13;
      &#13;
   <code class="c1"># Append the final capital value to the list</code>&#13;
   <code class="n">final_capital_values</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">capital</code><code class="p">)</code>&#13;
  &#13;
<code class="c1">#Find the value of f that maximizes the final capital of each investor</code>&#13;
<code class="n">optimal_index</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">argmax</code><code class="p">(</code><code class="n">final_capital_values</code><code class="p">)</code>&#13;
<code class="n">optimal_f</code> <code class="o">=</code> <code class="n">f_values</code><code class="p">[</code><code class="n">optimal_index</code><code class="p">]</code>&#13;
<code class="n">max_capital</code> <code class="o">=</code> <code class="n">final_capital_values</code><code class="p">[</code><code class="n">optimal_index</code><code class="p">]</code>&#13;
&#13;
<code class="c1">#Plot the final capital values as a function of position size, f</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">figure</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">8</code><code class="p">,</code><code class="mi">6</code><code class="p">))</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">position_size</code><code class="p">,</code> <code class="n">final_capital_values</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s1">'Position size as a fraction of total capital'</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s1">'Final capital values'</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="s1">'Growth of total capital as a function of position size in ZYX'</code><code class="p">)</code>&#13;
<code class="c1"># Plot a vertical line at the optimal value of f</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">axvline</code><code class="p">(</code><code class="n">x</code><code class="o">=</code><code class="n">optimal_f</code><code class="p">,</code> <code class="n">color</code><code class="o">=</code><code class="s1">'red'</code><code class="p">,</code> <code class="n">linestyle</code><code class="o">=</code><code class="s1">'--'</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code>&#13;
&#13;
<code class="c1">#Print the optimal value of f and the corresponding final capital</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"The optimal fraction of total capital is </code><code class="si">{</code><code class="n">optimal_f</code><code class="si">:</code><code class="s2">.2f</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Initial capital of $</code><code class="si">{</code><code class="n">start_capital</code><code class="si">:</code><code class="s2">.0f</code><code class="si">}</code><code class="s2"> grows to a </code><code class="w"/>&#13;
<code class="n">final</code> <code class="n">capital</code> <code class="n">of</code> <code class="err">$</code><code class="p">{</code><code class="n">max_capital</code><code class="p">:</code><code class="mf">.0</code><code class="n">f</code><code class="p">}</code><code class="s2">")</code><code class="w"/>&#13;
</pre>&#13;
&#13;
<figure class="informal"><div class="figure"><img alt="Image" src="assets/pmlf_08in05.png"/>&#13;
</div></figure>&#13;
&#13;
<p>We can make a few obvious observations based on our simulation:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>Investors experience different wealth trajectories based on the fraction of the initial capital they invested in this series of hypothetical positive expectation bets (total of 20,000 i.i.d. bets).</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Investors start losing money if they invested more than 26% of their capital.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>All investors who invested more than 40% of their capital are broke.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>All investors who invested between 1% to 26% of their capital increased their wealth.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>The investor who invested only 13% of their capital had the greatest amount of terminal wealth. In this investment scenario, 13% of total capital is the Kelly optimal position size for growing one’s wealth.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>It is important to note that an investor’s risk of ruin is closely related to the position size of their initial capital and not to the volatility of returns of the stochastic process, which is the same for every investor.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Most importantly, even if you are willing to accept the related volatility of your investment, there is a limit to how much capital you should allocate to an investment. This is the fatal flaw of MPT/CAPM and reveals the foolishness of using volatility as a measure of risk.</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>As shown in our simulation, assuming that investing is an ergodic process and optimizing expected value leads to financial ruin for the majority of the investors applying MPT/CAPM principles of using volatility as a proxy for risk and disregarding position size. This is how LTCM justified leveraging its positions arbitrarily highly and disregarding the possibility of financial ruin.<a contenteditable="false" data-primary="" data-startref="ch08-ruin" data-type="indexterm" id="id1433"/><a contenteditable="false" data-primary="" data-startref="ch08-ruin2" data-type="indexterm" id="id1434"/><a contenteditable="false" data-primary="" data-startref="ch08-ruin3" data-type="indexterm" id="id1435"/><a contenteditable="false" data-primary="" data-startref="ch08-ruin4" data-type="indexterm" id="id1436"/><a contenteditable="false" data-primary="" data-startref="ch08-ruin5" data-type="indexterm" id="id1437"/></p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Kelly Criterion" data-type="sect2"><div class="sect2" id="kelly_criterion">&#13;
<h2>Kelly Criterion</h2>&#13;
&#13;
<p>In 1956, John Kelly, a physicist<a contenteditable="false" data-primary="probabilistic decision making with generative ensembles" data-secondary="capital allocation" data-tertiary="Kelly criterion" data-type="indexterm" id="ch08-kelc"/><a contenteditable="false" data-primary="decision making" data-secondary="probabilistic with generative ensembles" data-tertiary="Kelly criterion" data-type="indexterm" id="ch08-kelc2"/><a contenteditable="false" data-primary="Kelly criterion" data-type="indexterm" id="ch08-kelc3"/><a contenteditable="false" data-primary="Kelly, John" data-type="indexterm" id="id1438"/> working at Bell Labs, came up with the groundbreaking solution to the vexing problem of how much is optimal to invest in positive expectation opportunities following a non-ergodic stochastic process. His solution, commonly referred to as the Kelly criterion, is to maximize the expected compound growth rate of capital, or the expected logarithm of wealth.<sup><a data-type="noteref" href="ch08.html#ch08fn12" id="ch08fn12-marker">12</a></sup> The Kelly position size is the optimal amount of capital allocated to a sequence of positive expectation bets or investments that results in the maximum terminal wealth in the shortest amount of time without risking financial ruin.</p>&#13;
&#13;
<p>Say your wealthy adversary gives you another weighted coin that has a 55% probability of turning up heads. He offers you an infinite series of trades with even odds:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>On heads, you get two times your stake. On tails, you lose your entire stake.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>How much capital do you allocate to maximize your capital in the long term?</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>Let’s run a simulation in Python of a simple series of binary bets with fixed odds to illustrate the power of the Kelly criterion for maximizing your wealth:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>&#13;
<code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="k">as</code> <code class="nn">plt</code>&#13;
&#13;
<code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="mi">101</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># Weighted coin in your favor</code>&#13;
<code class="n">p</code> <code class="o">=</code> <code class="mf">0.55</code>&#13;
&#13;
<code class="c1"># The Kelly position size (edge/odds) for odds 1:1</code>&#13;
<code class="n">f_star</code> <code class="o">=</code> <code class="n">p</code> <code class="o">-</code> <code class="p">(</code><code class="mi">1</code> <code class="o">-</code> <code class="n">p</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># Number of series in Monte Carlo simulation</code>&#13;
<code class="n">n_series</code> <code class="o">=</code> <code class="mi">50</code>&#13;
&#13;
<code class="c1"># Number of trials per series</code>&#13;
<code class="n">n_trials</code> <code class="o">=</code> <code class="mi">500</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">run_simulation</code><code class="p">(</code><code class="n">f</code><code class="p">):</code>&#13;
<code class="c1">#Runs a Monte Carlo simulation of a betting strategy with </code>&#13;
<code class="c1">#the given Kelly fraction.</code>&#13;
<code class="c1">#Takes f, The Kelly fraction, as the argument and returns a NumPy array </code>&#13;
<code class="c1">#of the terminal wealths of the simulation.</code>&#13;
&#13;
    <code class="c1"># Array for storing results</code>&#13;
    <code class="n">c</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">zeros</code><code class="p">((</code><code class="n">n_trials</code><code class="p">,</code> <code class="n">n_series</code><code class="p">))</code>&#13;
&#13;
    <code class="c1"># Initial capital of $100</code>&#13;
    <code class="n">c</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code> <code class="o">=</code> <code class="mi">100</code>&#13;
&#13;
    <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">n_series</code><code class="p">):</code>&#13;
        <code class="k">for</code> <code class="n">t</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="n">n_trials</code><code class="p">):</code>&#13;
            <code class="c1"># Use binomial random variable because we are tossing </code>&#13;
            <code class="c1"># a weighted coin</code>&#13;
            <code class="n">outcome</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">binomial</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="n">p</code><code class="p">)</code>&#13;
&#13;
            <code class="c1"># If we win, we add the Kelly fraction to our accumulated capital</code>&#13;
            <code class="k">if</code> <code class="n">outcome</code> <code class="o">&gt;</code> <code class="mi">0</code><code class="p">:</code>&#13;
                <code class="n">c</code><code class="p">[</code><code class="n">t</code><code class="p">,</code> <code class="n">i</code><code class="p">]</code> <code class="o">=</code> <code class="p">(</code><code class="mi">1</code> <code class="o">+</code> <code class="n">f</code><code class="p">)</code> <code class="o">*</code> <code class="n">c</code><code class="p">[</code><code class="n">t</code> <code class="o">-</code> <code class="mi">1</code><code class="p">,</code> <code class="n">i</code><code class="p">]</code>&#13;
&#13;
            <code class="c1"># If we lose, we subtract the Kelly fraction from </code>&#13;
            <code class="c1"># our accumulated capital</code>&#13;
            <code class="k">else</code><code class="p">:</code>&#13;
                <code class="n">c</code><code class="p">[</code><code class="n">t</code><code class="p">,</code> <code class="n">i</code><code class="p">]</code> <code class="o">=</code> <code class="p">(</code><code class="mi">1</code> <code class="o">-</code> <code class="n">f</code><code class="p">)</code> <code class="o">*</code> <code class="n">c</code><code class="p">[</code><code class="n">t</code> <code class="o">-</code> <code class="mi">1</code><code class="p">,</code> <code class="n">i</code><code class="p">]</code>&#13;
&#13;
    <code class="k">return</code> <code class="n">c</code>&#13;
&#13;
<code class="c1"># Run simulations for different position sizes</code>&#13;
<code class="c1"># The Kelly position size is our optimal betting size</code>&#13;
<code class="n">c_kelly</code> <code class="o">=</code> <code class="n">run_simulation</code><code class="p">(</code><code class="n">f_star</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># Half Kelly size reduces the volatility while keeping the gains</code>&#13;
<code class="n">c_half_kelly</code> <code class="o">=</code> <code class="n">run_simulation</code><code class="p">(</code><code class="n">f_star</code> <code class="o">/</code> <code class="mi">2</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># Anything more than twice Kelly leads to ruin in the long run</code>&#13;
<code class="n">c_3_kelly</code> <code class="o">=</code> <code class="n">run_simulation</code><code class="p">(</code><code class="n">f_star</code> <code class="o">*</code> <code class="mi">3</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># Betting all your capital leads to ruin very quickly</code>&#13;
<code class="n">c_all_in</code> <code class="o">=</code> <code class="n">run_simulation</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># Plot the expected value/arithmetic mean of terminal wealth </code>&#13;
<code class="c1"># over all the iterations of 500 trials each</code>&#13;
<code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="mi">6</code><code class="p">))</code>&#13;
&#13;
<code class="c1"># Overlay multiple plots with different line styles and markers</code>&#13;
<code class="n">ax</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">c_kelly</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">),</code> <code class="s1">'b-'</code><code class="p">,</code> <code class="n">lw</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s1">'Kelly'</code><code class="p">)</code>&#13;
<code class="n">ax</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">c_half_kelly</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">),</code> <code class="s1">'g--'</code><code class="p">,</code> <code class="n">lw</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s1">'Half Kelly'</code><code class="p">)</code>&#13;
<code class="n">ax</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">c_3_kelly</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">),</code> <code class="s1">'m:'</code><code class="p">,</code> <code class="n">lw</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s1">'Three Kelly'</code><code class="p">)</code>&#13;
<code class="n">ax</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">c_all_in</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">),</code> <code class="s1">'r-.'</code><code class="p">,</code> <code class="n">lw</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s1">'All In'</code><code class="p">)</code>&#13;
&#13;
<code class="n">ax</code><code class="o">.</code><code class="n">legend</code><code class="p">(</code><code class="n">loc</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>&#13;
<code class="n">ax</code><code class="o">.</code><code class="n">set_title</code><code class="p">(</code><code class="s1">'Expected Wealth of Bettor With Different Position Sizes'</code><code class="p">)</code>&#13;
<code class="n">ax</code><code class="o">.</code><code class="n">set_ylabel</code><code class="p">(</code><code class="s1">'Terminal wealth'</code><code class="p">)</code>&#13;
<code class="n">ax</code><code class="o">.</code><code class="n">set_xlabel</code><code class="p">(</code><code class="s1">'Number of Bets'</code><code class="p">)</code>&#13;
&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>&#13;
&#13;
<figure class="informal"><div class="figure"><img alt="Image" src="assets/pmlf_08in06.png"/>&#13;
</div></figure>&#13;
&#13;
<p>For binary outcomes, an investor can compute the percentage of capital, F, to be allocated to an opportunity with positive expectation in the real world of non-ergodic investing processes. However, the popular literature on the Kelly criterion doesn’t provide the general Kelly position sizing formula that you can apply to investments or bets in which you lose a percentage of your stake and not your entire stake. The optimal fraction, F′, is:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>F′ = (W × p – L × q) / (W × L) where</p>&#13;
&#13;
	<ul>&#13;
		<li>&#13;
		<p>p is the probability of gain and q = 1 – p is the probability of loss.</p>&#13;
		</li>&#13;
		<li>&#13;
		<p>W is percentage gain and L is the percentage loss.</p>&#13;
		</li>&#13;
	</ul>&#13;
	</li>&#13;
	<li>&#13;
	<p>Note when L = 1, you lose your entire stake and you get the popular formula:</p>&#13;
&#13;
	<ul>&#13;
		<li>&#13;
		<p>F′ = (W × p – q) / W, or as it is popularly known, edge over odds.</p>&#13;
		</li>&#13;
		<li>&#13;
		<p>This formula is used in sports betting, where you can lose your entire stake.</p>&#13;
		</li>&#13;
	</ul>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>It is important to note that the Kelly formula relates the ensemble average to the time average of a single trajectory. The expected value, or edge, of the investment is in its numerator. But the denominator modifies the position size implied by the ensemble average by including the multiplicative losses and profits that will be incurred sequentially in the time average. This is the volatility drag we discussed in the ergodicity subsection earlier.</p>&#13;
&#13;
<p>The Kelly formula solves the gambler’s ruin problem for multiplicative dynamics quite elegantly. Recall that the gambler’s ruin problem involves a series of additive bets. In contrast, the Kelly criterion is used for a series of multiplicative bets. When the expected value or edge of an opportunity is zero, the Kelly formula gives it a zero position size. Furthermore, when the expectation is negative, the position size is also negative. This implies you should take the other side of the bet. In gambling, this would mean betting against gamblers and with the casino dealer. In markets, it means betting that markets will fall and taking a short position in an investment.</p>&#13;
&#13;
<p>The Kelly criterion has many desirable properties for investing in positive expectation investment opportunities:<sup><a data-type="noteref" href="ch08.html#ch08fn13" id="ch08fn13-marker">13</a></sup></p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>It is mathematically indisputable that the Kelly position maximizes the terminal wealth in the shortest amount of time without the risk of going broke.</p>&#13;
&#13;
	<ul>&#13;
		<li>&#13;
		<p>It generates exponential growth since profits are reinvested.</p>&#13;
		</li>&#13;
		<li>&#13;
		<p>It involves a multiperiod, myopic trading strategy where you can focus on the present opportunities without a need for a long-term plan.</p>&#13;
		</li>&#13;
	</ul>&#13;
	</li>&#13;
	<li>&#13;
	<p>It has risk management built into the formula:</p>&#13;
&#13;
	<ul>&#13;
		<li>&#13;
		<p>Kelly position size is a fraction of your capital.</p>&#13;
		</li>&#13;
		<li>&#13;
		<p>Position size becomes smaller as losses accumulate.</p>&#13;
		</li>&#13;
	</ul>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p class="pagebreak-before">The Kelly criterion expressed mathematically that evaluating expected values of investment opportunities is necessary but not sufficient. Sizing our investment position to account for the non-ergodic process of investing is of paramount importance and the sufficient condition we need. Unfortunately, capital allocation in financial markets is not that simple, and applying the Kelly criterion is challenging because markets are not stationary.<a contenteditable="false" data-primary="" data-startref="ch08-kelc" data-type="indexterm" id="id1439"/><a contenteditable="false" data-primary="" data-startref="ch08-kelc2" data-type="indexterm" id="id1440"/><a contenteditable="false" data-primary="" data-startref="ch08-kelc3" data-type="indexterm" id="id1441"/></p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Kelly Investor’s Ruin" data-type="sect2"><div class="sect2" id="kelly_investorapostrophes_ruin">&#13;
<h2>Kelly Investor’s Ruin</h2>&#13;
&#13;
<p>As we have mentioned in <a data-type="xref" href="ch01.html#the_need_for_probabilistic_machine_lear">Chapter 1</a>, financial markets<a contenteditable="false" data-primary="decision making" data-secondary="probabilistic with generative ensembles" data-tertiary="Kelly investor’s ruin" data-type="indexterm" id="id1442"/><a contenteditable="false" data-primary="probabilistic decision making with generative ensembles" data-secondary="capital allocation" data-tertiary="Kelly investor’s ruin" data-type="indexterm" id="id1443"/><a contenteditable="false" data-primary="Kelly criterion" data-secondary="Kelly investor’s ruin" data-type="indexterm" id="id1444"/><a contenteditable="false" data-primary="financial theory" data-secondary="financial markets non-ergodic" data-type="indexterm" id="id1445"/><a contenteditable="false" data-primary="financial theory" data-secondary="financial markets non-ergodic" data-tertiary="nonstationary also" data-type="indexterm" id="id1446"/> are not only non-ergodic, but they are also nonstationary. The underlying data-generating stochastic processes vary over time. This makes estimating the continually changing statistical properties of these processes hazardous, especially when the underlying structure of the market changes abruptly.</p>&#13;
&#13;
<p>Note that the posterior predictive distribution of the outcome variable Y″, the event that ZYX will beat earnings expectations, has a probability distribution due to the uncertainty of its parameter and data sampling. The 76% probability is just the mean value of its posterior predictive distribution. The expected value of our objective function turns negative if our estimate is less than 75%. There is not much margin for error here, and that should raise red flags. Based on the analytical formula and theoretical probability of an earnings beat of 75%, the Kelly position size should be zero. However, we have overshot the Kelly position, and our capital growth is suboptimal but positive for this simulation. It is quite possible that for another simulation with more samples, the position size changes and the current position size of 13% would lead to ruin.</p>&#13;
&#13;
<p>The Kelly position sizing formula is very sensitive to estimates of both the expected value (the “edge”) and the probabilities of gains and losses (the “odds”). The cardinal sin in applying the Kelly formula is to overbet or to have a position size larger than the Kelly size. As an investor’s position size goes past the Kelly optimum, the growth rate of their wealth decreases and inexorably moves them toward financial ruin.<sup><a data-type="noteref" href="ch08.html#ch08fn14" id="ch08fn14-marker">14</a></sup></p>&#13;
&#13;
<p class="pagebreak-before">Practitioners of the Kelly criterion use a fraction of the Kelly optimal size, such as half Kelly, to avoid overbetting, as it hedges against:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>Overestimating one’s edge</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Misestimating event odds</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Changing edge and odds</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>Fractional Kelly allocation strategies also reduce the volatility of returns that can accompany full Kelly position sizes. The full application of the Kelly criterion to a portfolio of investments involves nonlinear programming and is beyond the scope of this primer.<sup><a data-type="noteref" href="ch08.html#ch08fn15" id="ch08fn15-marker">15</a></sup><a contenteditable="false" data-primary="" data-startref="ch08-allo" data-type="indexterm" id="id1447"/></p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="summary-id00028">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>Complex decision making in the real world is an art and a science. Our probabilistic decision framework gives us the perspective and the tools to integrate our prior knowledge and subjective reasons with the objectivity of observed data and the unrelenting rigors of probability calculus. It enables us to make the best decisions that optimize our objectives in the face of uncertainty and incomplete information. To make such systematic decisions that we can entrust to machines, we need to specify an objective function and evaluate the function based on all possible outcomes generated from the posterior predictive distribution of our generative ensembles.</p>&#13;
&#13;
<p>Expected value, also known as an ensemble average, needs to be applied with great caution because finance and investing processes are non-ergodic. Since each investor’s wealth trajectory is unique and different from the ensemble average computed across all possible trajectories of all market participants, their time average is not the same as the expected value of the ensemble. Also, different sequences of events lead to different decisions and outcomes even though the time average may be equivalent. In risk management, it is prudent to use value at risk instead of volatility, as VaR is estimated from the distribution of possible returns. Furthermore, it is better to use a tail-risk value instead of expected shortfall as that, too, is an ensemble average. My generative versions of these risk measures are produced seamlessly by probabilistic ensembles as I have demonstrated in this chapter.</p>&#13;
&#13;
<p class="pagebreak-before">The expected value of investments is a useful evaluation tool because it separates gambling from investing. However, while positive expectation is a necessary condition for investing, it is not a sufficient condition in a non-ergodic world. You need a capital allocation algorithm to appropriately size your investment so that you don’t go broke on any wealth trajectory your investment takes and you have a realistic chance of growing your wealth.</p>&#13;
&#13;
<p>If you are looking to maximize the growth of your capital, you might want to consider the Kelly investment strategy, which outperforms any other capital allocation strategy, especially the suboptimal Markowitz’s mean-variance investment strategy. The Kelly criterion investment strategy has been used by the most successful investors of all time, including Warren Buffet, Ed Thorp, and James Simons. It is a travesty that the Kelly criterion is not taught in academia or professional programs. However, the Kelly formula is not a silver bullet and is hard to implement in a nonstationary world. That’s why most practitioners like me use fractional Kelly trading strategies to avoid overbetting and the risk of ruin.</p>&#13;
&#13;
<p>Unfortunately, there are no easy formulas or algorithms for success in trading, investing, and finance that can be encoded in any AI system, because markets are not stationary ergodic. Symbolic AI and probabilistic machine learning systems require human common sense and expertise to separate correlation from causation to successfully navigate the aleatory, epistemic, and ontological uncertainties produced by creative, emotional, and free-willed market participants. Now that is a generative ensemble that can almost surely put the odds in your favor.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="References" data-type="sect1"><div class="sect1" id="references-id00018">&#13;
<h1>References</h1>&#13;
&#13;
<p>Bhide, A. V. “Compulsive Gambling in Ancient Indian Texts.” <em>Indian Journal of Psychiatry</em> 49, no. 4 (2007): 294–95.</p>&#13;
&#13;
<p>Bower, Bruce “Simple Heresy: Rules of Thumb Challenge Complex Financial Analyses.” <em>Science News</em> 179, no. 12 (2011): 26–29.</p>&#13;
&#13;
<p>DeMiguel, Victor, Lorenzo Garlappi, and Raman Uppal. “Optimal Versus Naïve Diversification: How Inefficient is the 1/N Portfolio Strategy?” <em>Review of Financial Studies</em> 22, no. 5 (2009): 1915–53.</p>&#13;
&#13;
<p>Karasan, Abdullah. <em>Machine Learning for Financial Risk Management with Python</em>. O’Reilly Media, 2021.</p>&#13;
&#13;
<p>Kelly Jr., J. L. “A New Interpretation of Information Rate.” <em>The Bell System Technical Journal</em> 35, no. 4 (1956): 917–26.</p>&#13;
&#13;
<p>MacLean, L. C., E. O. Thorp, and W. T. Ziemba. “Long-Term Capital Growth: Good and Bad Properties of the Kelly Criterion.” <em>Quantitative Finance</em> 10, no. 7 (2010): <span class="keep-together">681–687.</span></p>&#13;
&#13;
<p>MacLean, L. C., E. O. Thorp, and W. T. Ziemba. “Medium Term Simulations of the Full Kelly and Fractional Kelly Investment Strategies.” In <em>The Kelly Capital Growth Investment Criterion: Theory and Practice</em>, edited by L. C. MacLean, E. O. Thorp, and W. T. Ziemba, 543–61 (Hackensack, NJ: World Scientific Publishing Company, 2011).</p>&#13;
&#13;
<p>Peters, Ole. “The Ergodicity Problem in Economics.” <em>Nature Physics</em> 15 (2019): <span class="keep-together">1216–21.</span></p>&#13;
&#13;
<p>Peters, Ole, and A. Adamou. “The Time Interpretation of Expected Utility Theory.” <a href="http://arXiv.org">arXiv.org</a>, February 28, 2021. <a href="https://arxiv.org/abs/1801.03680"><em class="hyperlink">https://arxiv.org/abs/1801.03680</em></a>.</p>&#13;
&#13;
<p>Peters, Ole, and Murray Gell-Mann. “Evaluating Gambles Using Dynamics,” <a href="http://arXiv.org">arXiv.org</a>, June 5, 2015. <a href="https://arxiv.org/abs/1405.0585"><em class="hyperlink">https://arxiv.org/abs/1405.0585</em></a>.</p>&#13;
&#13;
<p>Poundstone, William. <em>Fortune’s Formula: The Untold Story of the Scientific Betting System That Beat the Casinos and Wall Street</em>. New York: Hill and Wang, 2006.</p>&#13;
&#13;
<p>Robert, Christian P. <em>The Bayesian Choice: From Decision-Theoretic Foundations to Computational Implementation</em>. New York: Springer Science+Business Media, 2007.</p>&#13;
&#13;
<p>Thorp, Edward O. <em>A Man for All Markets: From Las Vegas to Wall Street, How I Beat the Dealer and the Market</em>. New York: Random House, 2017.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Further Reading" data-type="sect1"><div class="sect1" id="further_reading-id00010">&#13;
<h1>Further Reading</h1>&#13;
&#13;
<p>MacLean, L.C., E. O. Thorp, and W. T. Ziemba (eds.). <em>The Kelly Capital Growth Investment Criterion: Theory and Practice</em>. World Scientific Publishing Company, 2011.</p>&#13;
</div></section>&#13;
<div data-type="footnotes"><p data-type="footnote" id="ch08fn1"><sup><a href="ch08.html#ch08fn1-marker">1</a></sup> Christian P. Robert, <em>The Bayesian Choice: From Decision-Theoretic Foundations to Computational Implementation</em> (New York: Springer Science+Business Media, 2007).</p><p data-type="footnote" id="ch08fn2"><sup><a href="ch08.html#ch08fn2-marker">2</a></sup> Abdullah Karasan, <em>Machine Learning for Financial Risk Management with Python</em> (O’Reilly Media, 2021).</p><p data-type="footnote" id="ch08fn3"><sup><a href="ch08.html#ch08fn3-marker">3</a></sup> Ole Peters, “The Ergodicity Problem in Economics,” <em>Nature Physics</em> 15 (2019): 1216–1221; Ole Peters and A. Adamou, “The Time Interpretation of Expected Utility Theory,” <a href="http://arXiv.org">arXiv.org</a>, February 28, 2021, <a href="https://arxiv.org/abs/1801.03680"><em class="hyperlink">https://arxiv.org/abs/1801.03680</em></a>.</p><p data-type="footnote" id="ch08fn4"><sup><a href="ch08.html#ch08fn4-marker">4</a></sup> Adapted from an image on Wikimedia Commons.</p><p data-type="footnote" id="ch08fn5"><sup><a href="ch08.html#ch08fn5-marker">5</a></sup> A. V. Bhide, “Compulsive Gambling in Ancient Indian Texts,” <em>Indian Journal of Psychiatry</em> 49, no. 4 (2007): 294–95.</p><p data-type="footnote" id="ch08fn6"><sup><a href="ch08.html#ch08fn6-marker">6</a></sup> Ole Peters and Murray Gell-Mann, “Evaluating Gambles Using Dynamics,” <a href="http://arXiv.org">arXiv.org</a>, June 5, 2015, <a href="https://arxiv.org/abs/1405.0585"><em class="hyperlink">https://arxiv.org/abs/1405.0585</em></a>.</p><p data-type="footnote" id="ch08fn7"><sup><a href="ch08.html#ch08fn7-marker">7</a></sup> Edward O. Thorp, <em>A Man for All Markets: From Las Vegas to Wall Street, How I Beat the Dealer and the Market</em> (New York: Random House, 2017).</p><p data-type="footnote" id="ch08fn8"><sup><a href="ch08.html#ch08fn8-marker">8</a></sup> William Poundstone, <em>Fortune’s Formula: The Untold Story of the Scientific Betting System That Beat the Casinos and Wall Street</em> (New York: Hill and Wang, 2006).</p><p data-type="footnote" id="ch08fn9"><sup><a href="ch08.html#ch08fn9-marker">9</a></sup> Bruce Bower, “Simple Heresy: Rules of Thumb Challenge Complex Financial Analyses,” <em>Science News</em> 179, no. 12 (2011): 26–29.</p><p data-type="footnote" id="ch08fn10"><sup><a href="ch08.html#ch08fn10-marker">10</a></sup> Victor DeMiguel, Lorenzo Garlappi, and Raman Uppal, “Optimal Versus Naïve Diversification: How Inefficient is the 1/N Portfolio Strategy?” <em>Review of Financial Studies</em> 22, no. 5 (2009): 1915–53.</p><p data-type="footnote" id="ch08fn11"><sup><a href="ch08.html#ch08fn11-marker">11</a></sup> Adapted from an image on Wikimedia Commons.</p><p data-type="footnote" id="ch08fn12"><sup><a href="ch08.html#ch08fn12-marker">12</a></sup> J. L. Kelly Jr., “A New Interpretation of Information Rate,” <em>The Bell System Technical Journal</em> 35, no. 4 (1956): 917–26.</p><p data-type="footnote" id="ch08fn13"><sup><a href="ch08.html#ch08fn13-marker">13</a></sup> L. C. MacLean, E. O. Thorp, and W. T. Ziemba, “Long-Term Capital Growth: Good and Bad Properties of the Kelly Criterion,” <em>Quantitative Finance</em> 10, no. 7 (2010): 681–687.</p><p data-type="footnote" id="ch08fn14"><sup><a href="ch08.html#ch08fn14-marker">14</a></sup> L. C. MacLean, E. O. Thorp, Yonggan Zhao, and W. T. Ziemba, “Medium Term Simulations of the Full Kelly and Fractional Kelly Investment Strategies,” in <em>The Kelly Capital Growth Investment Criterion: Theory and Practice</em>, eds. L. C. MacLean, E. O. Thorp, and W. T. Ziemba (Hackensack, NJ: World Scientific Publishing Company, 2011), 543–61.</p><p data-type="footnote" id="ch08fn15"><sup><a href="ch08.html#ch08fn15-marker">15</a></sup> <em>The Kelly Capital Growth Investment Criterion: Theory and Practice</em>, eds. L.C. MacLean, E.O. Thorp, and W.T. Ziemba (Hackensack, NJ: World Scientific Publishing Company, 2011).</p></div></div></section></body></html>