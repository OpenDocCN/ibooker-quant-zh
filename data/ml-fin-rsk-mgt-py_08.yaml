- en: Chapter 6\. Credit Risk Estimation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章 信用风险估算
- en: Although market risk is much better researched, the larger part of banks’ economic
    capital is generally used for credit risk. The sophistication of traditional standard
    methods of measurement, analysis, and management of credit risk might, therefore,
    not be in line with its significance.
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 尽管市场风险得到了更好的研究，但银行经济资本的较大部分通常用于信用风险。因此，传统标准方法的信用风险测量、分析和管理的复杂性可能不符合其重要性。
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Uwe Wehrspohn (2002)
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Uwe Wehrspohn (2002)
- en: The primary role of financial institutions is to create a channel by which funds
    move from entities with surplus into ones with deficit. Thereby, financial institutions
    ensure the capital allocation in the financial system as well as gain profit in
    exchange for these transactions.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 金融机构的主要角色是建立一个渠道，使资金从盈余实体流向赤字实体。因此，金融机构确保了金融系统中的资本配置，并在这些交易中获取利润。
- en: However, there is an important risk for financial institutions to handle, which
    is credit risk. This is such a big risk that without it capital allocation might
    be less costly and more efficient. *Credit risk* is the risk that arises when
    a borrower is not able to honor their debt. In other words, when a borrower defaults,
    they fail to pay back their debt, which causes losses for financial institutions.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，金融机构处理的一个重要风险是信用风险。这是一个如此巨大的风险，以至于如果没有它，资本配置可能会更少成本更高效。*信用风险*是指借款人无法履行其债务时出现的风险。换句话说，当借款人违约时，他们未能偿还其债务，这导致金融机构遭受损失。
- en: 'Credit risk and its goal can be defined in a more formal way (BCBS and BIS
    2000):'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 信用风险及其目标可以用更正式的方式定义（BCBS和BIS 2000年）：
- en: Credit risk is most simply defined as the potential that a bank borrower or
    counterparty will fail to meet its obligations in accordance with agreed terms.
    The goal of credit risk management is to maximise a bank’s risk-adjusted rate
    of return by maintaining credit risk exposure within acceptable parameters.
  id: totrans-7
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 信用风险最简单的定义是银行借款人或交易对手可能无法按照约定的条件履行其义务的潜力。信用风险管理的目标是通过在可接受的参数内保持信用风险敞口，从而最大化银行的风险调整后的回报率。
- en: Estimating credit risk is so formidable a task that a regulatory body, Basel,
    closely monitors recent developments in the financial markets and sets regulations
    to strengthen bank capital requirements. The importance of having strong capital
    requirements for a bank rests on the idea that banks should have a capital buffer
    in turbulent times.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 估算信用风险是一项如此艰巨的任务，以至于一个监管机构巴塞尔密切监视金融市场的最新发展，并制定规定以加强银行的资本要求。对于银行拥有强大的资本要求的重要性在于，银行应在动荡时期拥有资本缓冲。
- en: There is a consensus among policy makers that financial institutions should
    have a minimum capital requirement to ensure the stability of the financial system
    because a series of defaults may result in a collapse in financial markets, as
    financial institutions provide collateral to one another. Those looking for a
    workaround for this capital requirement learned their lessons the hard way during
    the [2007—2008 mortgage crisis](https://oreil.ly/OjDw9).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 政策制定者普遍认为，为确保金融体系的稳定，金融机构应设定最低资本要求，因为一连串的违约可能导致金融市场崩溃，而金融机构向彼此提供抵押品。那些试图规避这种资本要求的人在[2007—2008年抵押贷款危机](https://oreil.ly/OjDw9)期间吃了苦头。
- en: Of course, ensuring at least a minimum capital requirement is a burden for financial
    institutions in the sense that capital is an asset they cannot channel to deficit
    entities to make a profit. Consequently, managing credit risk amounts to profitable
    and efficient transactions.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，至少确保最低资本要求对金融机构来说是一种负担，因为资本是它们无法向赤字实体输送以盈利的资产。因此，管理信用风险等于进行盈利和高效的交易。
- en: In this respect, this chapter shows how credit risk can be estimated using cutting-edge
    ML models. We start our discussion with a theoretical background of credit risk.
    Needless to say, there are many topics in credit risk analysis, but we confine
    our focus on probability of default and how we can introduce ML approaches for
    estimating it. For this purpose, customers are segmented via a clustering method
    so that models can be separately fitted to this data. This provides a better fit
    in the sense that the distribution of credit risk data changes across different
    customer segments. Given the clusters obtained, ML and deep learning models, including
    the Bayesian approach, are introduced to model the credit risk.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这方面，本章展示了如何利用前沿的机器学习模型估计信用风险。我们从信用风险的理论背景开始讨论。毫无疑问，在信用风险分析中有许多主题，但我们专注于违约概率以及如何引入机器学习方法来估计它。为此，通过聚类方法对客户进行分段，以便可以分别为这些数据拟合模型。这样做可以更好地适应不同客户段的信用风险数据分布变化。根据得到的聚类，引入了包括贝叶斯方法在内的机器学习和深度学习模型来建模信用风险。
- en: Estimating the Credit Risk
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 估计信用风险
- en: 'Aside from the probability of default (which is the likelihood that a borrower
    fails to cover their debt), credit risk has three defining characteristics:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 除了违约概率（即借款人未能偿还债务的可能性）之外，信用风险还具有三个定义特征：
- en: Exposure
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 暴露
- en: This refers to a party that may possibly default or suffer an adverse change
    in its ability to perform.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这指的是可能违约或其履行能力发生不利变化的方当事人。
- en: Likelihood
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 可能性
- en: The likelihood that this party will default on its obligations.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 此方当事人可能会违约其义务的可能性。
- en: Recovery rate
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 恢复率
- en: How much can be retrieved if a default takes place.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果违约发生，可以收回多少。
- en: The BCBS put forth the global financial credit management standards, which are
    known as the *Basel Accord*. There are currently three Basel Accords. The most
    distinctive rule set by Basel I in 1988 was the requirement to hold capital equating
    to at least 8% of risk-weighted assets.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: BIS提出了全球金融信用管理标准，称为*巴塞尔协议*。目前有三个巴塞尔协议。1988年的巴塞尔I协议规定，要求持有资本至少相当于风险加权资产的8%。
- en: 'Basel I includes the very first capital measurement system, which was created
    following the onset of the [Latin American debt crisis](https://oreil.ly/KI5vs).
    In Basel I, assets are classified as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 巴塞尔I协议包括第一个资本测量系统，这是在[拉美债务危机](https://oreil.ly/KI5vs)爆发后创建的。在巴塞尔I中，资产分类如下：
- en: 0% for risk-free assets
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0% 用于无风险资产。
- en: 20% for loans to other banks
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 20% 用于向其他银行提供贷款。
- en: 50% for residential mortgages
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 50% 用于住宅抵押贷款
- en: 100% for corporate debt
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 100% 用于企业债务
- en: 'In 1999, Basel II issued a revision to Basel I based on three main pillars:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 1999年，巴塞尔II根据三大支柱对巴塞尔I进行了修订。
- en: Minimum capital requirements, which sought to develop and expand the standardized
    rules set out in the 1988 Accord
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最低资本要求，旨在发展和扩展1988年协议中制定的标准化规则
- en: Supervisory review of an institution’s capital adequacy and internal assessment
    process
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对机构资本充足性和内部评估过程的监督审查
- en: Effective use of disclosure as a lever to strengthen market discipline and encourage
    sound banking practices
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用信息披露作为加强市场纪律和鼓励良好银行业务实践的杠杆
- en: The last accord, Basel III in 2010, was inevitable. as the 2007–2008 mortgage
    crisis heightened. It introduced a new set of measures to further strengthened
    liquidity and poor governance practices. For instance, equity requirements were
    introduced to prevent a serial failure in the financial system, known as *domino
    effect*, during times of financial turbulence and crises. Accordingly, Basel III
    requires the financial ratios for banks listed in [Table 6-1](#table6-1).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一项协议，2010年的巴塞尔III，是不可避免的。在2007-2008年的抵押贷款危机加剧之际。它引入了一套新的措施，进一步增强了流动性和恶劣治理实践。例如，引入了股本要求，以防止在金融系统中发生连续的失败，即所谓的*多米诺效应*，在金融动荡和危机时期。因此，巴塞尔III要求在[表6-1](#table6-1)中列出的银行金融比率。
- en: Table 6-1\. Financial ratios required by Basel III
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 表6-1\. 巴塞尔III要求的金融比率
- en: '| Financial ratio | Formula |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 金融比率 | 公式 |'
- en: '| --- | --- |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Tier 1 capital ratio | <math alttext="StartFraction Equity capital Over Risk
    weighted assets EndFraction greater-than equals 4.5 percent-sign"><mrow><mfrac><mrow><mtext>Equity</mtext><mtext>capital</mtext></mrow>
    <mrow><mtext>Risk</mtext><mtext>weighted</mtext><mtext>assets</mtext></mrow></mfrac>
    <mo>></mo> <mo>=</mo> <mn>4</mn> <mo>.</mo> <mn>5</mn> <mo>%</mo></mrow></math>
    |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 一级资本比率 | <math alttext="StartFraction Equity capital Over Risk weighted assets
    EndFraction greater-than equals 4.5 percent-sign"><mrow><mfrac><mrow><mtext>权益资本</mtext></mrow>
    <mrow><mtext>风险加权资产</mtext></mrow></mfrac> <mo>></mo> <mo>=</mo> <mn>4</mn> <mo>.</mo>
    <mn>5</mn> <mo>%</mo></mrow></math> |'
- en: '| Leverage ratio | <math alttext="StartFraction Tier 1 capital Over Average
    total assets EndFraction greater-than equals 3 percent-sign"><mrow><mfrac><mrow><mtext>Tier</mtext><mtext>1</mtext><mtext>capital</mtext></mrow>
    <mrow><mtext>Average</mtext><mtext>total</mtext><mtext>assets</mtext></mrow></mfrac>
    <mo>></mo> <mo>=</mo> <mn>3</mn> <mo>%</mo></mrow></math> |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 杠杆比率 | <math alttext="StartFraction Tier 1 capital Over Average total assets
    EndFraction greater-than equals 3 percent-sign"><mrow><mfrac><mrow><mtext>一级资本</mtext></mrow>
    <mrow><mtext>平均总资产</mtext></mrow></mfrac> <mo>></mo> <mo>=</mo> <mn>3</mn> <mo>%</mo></mrow></math>
    |'
- en: '| Liquidity coverage ratio | <math alttext="StartFraction Stock of high quality
    liquid assets Over Total net cash outflows over the next 30 calendar days EndFraction
    greater-than equals 100 percent-sign"><mrow><mfrac><mrow><mtext>Stock</mtext><mtext>of</mtext><mtext>high</mtext><mtext>quality</mtext><mtext>liquid</mtext><mtext>assets</mtext></mrow>
    <mrow><mtext>Total</mtext><mtext>net</mtext><mtext>cash</mtext><mtext>outflows</mtext><mtext>over</mtext><mtext>the</mtext><mtext>next</mtext><mtext>30</mtext><mtext>calendar</mtext><mtext>days</mtext></mrow></mfrac>
    <mo>></mo> <mo>=</mo> <mn>100</mn> <mo>%</mo></mrow></math> |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 流动性覆盖率 | <math alttext="StartFraction Stock of high quality liquid assets
    Over Total net cash outflows over the next 30 calendar days EndFraction greater-than
    equals 100 percent-sign"><mrow><mfrac><mrow><mtext>优质流动性资产存量</mtext></mrow> <mrow><mtext>未来30个日历日的总净现金流出</mtext></mrow></mfrac>
    <mo>></mo> <mo>=</mo> <mn>100</mn> <mo>%</mo></mrow></math> |'
- en: Basel II suggests banks implement either a standardized approach or an internal
    ratings–based (IRB) approach to estimate the credit risk. The standardized approach
    is out of the scope of this book, but interested readers can refer to the “Standardized
    Approach to Credit Risk” [consultative document from the BIS](https://oreil.ly/0Mj7J).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 巴塞尔协议II建议银行实施标准化方法或基于内部评级(IRB)的方法来估计信用风险。标准化方法超出了本书的范围，但感兴趣的读者可以参考《信用风险标准化方法》[BIS的咨询文件](https://oreil.ly/0Mj7J)。
- en: 'Let’s now focus on the IRB approach; the key parameters of this internal assessment
    are:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们专注于内部评级方法(IRB)；这种内部评估的关键参数包括：
- en: <math alttext="Expected loss equals EAD times LGD times PD" display="block"><mrow><mtext>Expected</mtext>
    <mtext>loss</mtext> <mo>=</mo> <mtext>EAD</mtext> <mo>×</mo> <mtext>LGD</mtext>
    <mo>×</mo> <mtext>PD</mtext></mrow></math>
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="Expected loss equals EAD times LGD times PD" display="block"><mrow><mtext>Expected</mtext>
    <mtext>loss</mtext> <mo>=</mo> <mtext>EAD</mtext> <mo>×</mo> <mtext>LGD</mtext>
    <mo>×</mo> <mtext>PD</mtext></mrow></math>
- en: where *PD* is the probability of default, *LGD* is the expected loss given default
    taking a value between 0 and 1, and *EAD* is the exposure at default.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*PD*是违约概率，*LGD*是违约时的预期损失（取值范围在0到1之间），*EAD*是违约时的暴露。
- en: 'The most important and challenging part of estimating credit risk is to model
    the probability of default, and the aim of this chapter is mainly to come up with
    an ML model to address this issue. Before moving forward, there is one more important
    issue in estimating credit risk that is sometimes neglected or overlooked: *risk*
    *bucketing*.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 估计信用风险最重要和具有挑战性的部分是建模违约概率，本章的目标主要是提出一个机器学习模型来解决这个问题。在继续之前，还有一个在估计信用风险中有时被忽视或忽略的重要问题：*风险分桶*。
- en: Risk Bucketing
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 风险分桶
- en: Risk bucketing is nothing but grouping borrowers with similar creditworthiness.
    The behind-the-scenes story of risk bucketing is to obtain homogenous groups or
    clusters so that we can better estimate the credit risk. Treating different risky
    borrowers equally may result in poor predictions because the model cannot capture
    entirely different characteristics of the data at once. Thus, by dividing the
    borrowers into different groups based on riskiness, risk bucketing enables us
    to make accurate predictions.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 风险分桶只是将信用价值相似的借款人分组。风险分桶背后的故事是获得同质性的组或簇，以便我们能更好地估计信用风险。将不同风险借款人一视同仁可能导致预测不佳，因为模型无法同时捕捉数据的完全不同特征。因此，通过根据风险性将借款人分成不同的组别，风险分桶使我们能够做出准确的预测。
- en: Risk bucketing can be accomplished via different statistical methods, but we
    will apply a clustering technique to end up with homogeneous clusters using K-means.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 风险分桶可以通过不同的统计方法来实现，但我们将应用聚类技术，最终得到使用K-means算法形成的同质性聚类。
- en: We live in the age of data, but that does not necessarily mean that we always
    find the data we are searching for. Rather, it is rare to find it without applying
    data-wrangling and cleaning techniques.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们生活在数据时代，但这并不意味着我们总能找到正在寻找的数据。相反，若不应用数据整理和清理技术，很少能找到它。
- en: Data with dependent variables is, of course, easy to work with and also helps
    us get more accurate results. However, sometimes we need to unveil the hidden
    characteristics of the data—that is, if the riskiness of the borrowers is not
    known, we are supposed to come up with a solution for grouping them based on their
    riskiness.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据具有依赖变量时，当然更易处理，同时还能帮助我们获得更准确的结果。然而，有时我们需要揭示数据的隐藏特征——也就是说，如果借款人的风险性未知，我们应该提出一种基于他们风险性分组的解决方案。
- en: 'Clustering is the method proposed to create these groups or *clusters*. Optimal
    clustering has clusters located far away from one another spatially:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是提出的方法，用于创建这些组或*簇*。最佳聚类应该使得空间上的簇之间距离较远：
- en: Clustering groups data instances into subsets in such a manner that similar
    instances are grouped together, while different instances belong to different
    groups. The instances are thereby organized into an efficient representation that
    characterizes the population being sampled.
  id: totrans-48
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 聚类将数据实例分组到子集中，使得相似实例归于同一组，而不同实例则属于不同组。这样一来，实例被组织成一个有效的表示，以描述被抽样的总体。
- en: ''
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Rokach and Maimon (2005)
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Rokach 和 Maimon（2005）
- en: 'Different clustering methods are available, but the K-means algorithm serves
    our purpose, which is to create risk bucketing for credit risk analysis. In K-means,
    the distance of observations within the cluster is calculated based on the cluster
    center, the *centroid*. Depending on the distance to the centroid, observations
    are clustered. This distance can be measured via different methods. Of them, the
    following are the most well-known metrics:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 存在不同的聚类方法，但K均值算法适合我们的目的，即为信用风险分析创建风险分组。在K均值中，观测值在簇内的距离是基于簇中心，即*质心*来计算的。根据到质心的距离，将观测值聚类。这种距离可以通过不同的方法来衡量。其中，以下是最为知名的度量方法：
- en: Euclidean
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 欧几里得
- en: <math alttext="StartRoot sigma-summation Underscript i equals 1 Overscript n
    Endscripts left-parenthesis p Subscript i Baseline minus q Subscript i Baseline
    right-parenthesis squared EndRoot"><msqrt><mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></msubsup> <msup><mrow><mo>(</mo><msub><mi>p</mi> <mi>i</mi></msub>
    <mo>-</mo><msub><mi>q</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mn>2</mn></msup></mrow></msqrt></math>
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartRoot sigma-summation Underscript i equals 1 Overscript n
    Endscripts left-parenthesis p Subscript i Baseline minus q Subscript i Baseline
    right-parenthesis squared EndRoot"><msqrt><mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></msubsup> <msup><mrow><mo>(</mo><msub><mi>p</mi> <mi>i</mi></msub>
    <mo>-</mo><msub><mi>q</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mn>2</mn></msup></mrow></msqrt></math>
- en: Minkowski
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 闵可夫斯基
- en: <math alttext="left-parenthesis sigma-summation Underscript i equals 1 Overscript
    n Endscripts StartAbsoluteValue p Subscript i Baseline minus q Subscript i Baseline
    EndAbsoluteValue Superscript p Baseline right-parenthesis Superscript 1 slash
    p"><mrow><msup><mrow><mo>(</mo> <msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></msubsup> <msup><mrow><mo>|</mo> <msub><mi>p</mi> <mi>i</mi></msub>
    <mo>-</mo> <msub><mi>q</mi> <mi>i</mi></msub> <mo>|</mo></mrow> <mi>p</mi></msup>
    <mo>)</mo></mrow> <mrow><mn>1</mn><mo>/</mo><mi>p</mi></mrow></msup></mrow></math>
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="left-parenthesis sigma-summation Underscript i equals 1 Overscript
    n Endscripts StartAbsoluteValue p Subscript i Baseline minus q Subscript i Baseline
    EndAbsoluteValue Superscript p Baseline right-parenthesis Superscript 1 slash
    p"><mrow><msup><mrow><mo>(</mo> <msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></msubsup> <msup><mrow><mo>|</mo> <msub><mi>p</mi> <mi>i</mi></msub>
    <mo>-</mo> <msub><mi>q</mi> <mi>i</mi></msub> <mo>|</mo></mrow> <mi>p</mi></msup>
    <mo>)</mo></mrow> <mrow><mn>1</mn><mo>/</mo><mi>p</mi></mrow></msup></mrow></math>
- en: Manhattan
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 曼哈顿
- en: <math alttext="StartRoot sigma-summation Underscript i equals 1 Overscript n
    Endscripts StartAbsoluteValue p Subscript i Baseline minus q Subscript i Baseline
    EndAbsoluteValue EndRoot"><msqrt><mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></msubsup> <mrow><mo>|</mo> <msub><mi>p</mi> <mi>i</mi></msub> <mo>-</mo>
    <msub><mi>q</mi> <mi>i</mi></msub> <mo>|</mo></mrow></mrow></msqrt></math>
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartRoot sigma-summation Underscript i equals 1 Overscript n
    Endscripts StartAbsoluteValue p Subscript i Baseline minus q Subscript i Baseline
    EndAbsoluteValue EndRoot"><msqrt><mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></msubsup> <mrow><mo>|</mo> <msub><mi>p</mi> <mi>i</mi></msub> <mo>-</mo>
    <msub><mi>q</mi> <mi>i</mi></msub> <mo>|</mo></mrow></mrow></msqrt></math>
- en: 'The aim in clustering is to minimize the distance between the centroid and
    observations so that similar observations will be on the same cluster. This logic
    rests on the intuition that the more similar observations are, the smaller the
    distance between them. So we are seeking to minimize the distance between observations
    and the centroid, which is another way of saying that we are minimizing the sum
    of the squared error between the centroid and the observations:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类的目标是最小化质心与观测值之间的距离，以便将相似的观测值放置在同一簇中。这种逻辑基于这样的直觉：观测值越相似，它们之间的距离就越小。因此，我们寻求最小化观测值与质心之间的距离，这另一种说法是我们在最小化质心和观测值之间的平方误差和：
- en: <math alttext="sigma-summation Underscript i equals 1 Overscript upper K Endscripts
    sigma-summation Underscript x element-of upper C Subscript i Endscripts left-parenthesis
    upper C Subscript i Baseline minus x right-parenthesis squared" display="block"><mrow><munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>K</mi></munderover> <munder><mo>∑</mo>
    <mrow><mi>x</mi><mo>∈</mo><msub><mi>C</mi> <mi>i</mi></msub></mrow></munder> <msup><mrow><mo>(</mo><msub><mi>C</mi>
    <mi>i</mi></msub> <mo>-</mo><mi>x</mi><mo>)</mo></mrow> <mn>2</mn></msup></mrow></math>
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="sigma-summation Underscript i equals 1 Overscript upper K Endscripts
    sigma-summation Underscript x element-of upper C Subscript i Endscripts left-parenthesis
    upper C Subscript i Baseline minus x right-parenthesis squared" display="block"><mrow><munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>K</mi></munderover> <munder><mo>∑</mo>
    <mrow><mi>x</mi><mo>∈</mo><msub><mi>C</mi> <mi>i</mi></msub></mrow></munder> <msup><mrow><mo>(</mo><msub><mi>C</mi>
    <mi>i</mi></msub> <mo>-</mo><mi>x</mi><mo>)</mo></mrow> <mn>2</mn></msup></mrow></math>
- en: 'where *x* is observation and <math alttext="upper C Subscript i"><msub><mi>C</mi>
    <mi>i</mi></msub></math> is the centroid of <math alttext="i Superscript t h"><msup><mi>i</mi>
    <mrow><mi>t</mi><mi>h</mi></mrow></msup></math> cluster. However, considering
    the number of observations and the combinations of clusters, the search area might
    be too big to handle. It may sound intimidating, but don’t worry: we have the
    *expectation-maximization* *(E-M)* algorithm behind our clustering. As K-means
    does not have a closed-form solution, we are searching for an approximate one,
    and E-M provides this solution. In the E-M algorithm, *E* refers to assigning
    observations to the nearest centroid, and *M* denotes completion of the data generation
    process by updating the parameters.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*x*是观测值，<math alttext="upper C Subscript i"><msub><mi>C</mi> <mi>i</mi></msub></math>
    是第<i>t</i>个簇的质心。然而，考虑到观测值的数量和簇的组合，搜索区域可能过大难以处理。这可能听起来有点吓人，但别担心：我们有期望-最大化（*E-M*）算法来支持我们的聚类。由于K均值没有闭式解，我们正在寻找一个近似解，而E-M算法提供了这个解。在期望-最大化（*E-M*）算法中，*E*表示将观测值分配给最近的质心，*M*表示通过更新参数完成数据生成过程。
- en: 'In the E-M algorithm, the distances between observations and the centroid is
    iteratively minimized. The algorithm works as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在期望-最大化（*E-M*）算法中，观测值与质心之间的距离被迭代地最小化。算法的工作步骤如下：
- en: Pick *k* random points to be centroids.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择*k*个随机点作为质心。
- en: Based on the distance metric chosen, calculate the distances between observations
    and *n* centroids. Based on these distances, assign each observation to the closest
    cluster.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据选择的距离度量计算观察和* n *质心之间的距离。基于这些距离，将每个观察分配到最近的集群。
- en: Update cluster centers based on the assignment.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据分配更新集群中心。
- en: Repeat the process from step 2 until the centroid does not change.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从步骤 2 开始重复该过程，直到质心不再改变。
- en: Now, we apply risk bucketing using K-means clustering. To decide the optimal
    number of clusters, different techniques will be employed. First, we use the *elbow
    method*, which is based on the *inertia*.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们使用 K-means 聚类进行风险分桶。要确定最优的簇数，将采用不同的技术。首先，我们使用*肘部法则*，这基于*惯性*。
- en: 'Inertia is computed as the sum of the squared distances of observations to
    their closest centroid. Second, the *Silhouette score* is introduced as a tool
    to decide the optimal number of clusters. This takes a value between 1 and -1\.
    A value of 1 indicates that an observation is close to the correct centroid and
    correctly classified. However, -1 shows that an observation is not correctly clustered.
    The strength of the Silhouette score rests on taking into account both the intracluster
    distance and the intercluster distance. The formula for Silhouette score is as
    follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 惯性被计算为观察到它们最近的质心的平方距离之和。其次，引入* Silhouette 分数* 作为工具来确定最优簇的数量。这个值在1到-1之间。值为1表示观察接近正确的质心并且正确分类。然而，-1显示观察未正确分组。Silhouette
    分数的强度在于考虑到簇内距离和簇间距离。Silhouette 分数的公式如下：
- en: <math alttext="Silhouette score equals StartFraction x minus y Over max left-parenthesis
    x comma y right-parenthesis EndFraction" display="block"><mrow><mtext>Silhouette</mtext>
    <mtext>score</mtext> <mo>=</mo> <mfrac><mrow><mi>x</mi><mo>-</mo><mi>y</mi></mrow>
    <mrow><mtext>max</mtext><mo>(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>)</mo></mrow></mfrac></mrow></math>
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="Silhouette score equals StartFraction x minus y Over max left-parenthesis
    x comma y right-parenthesis EndFraction" display="block"><mrow><mtext>Silhouette</mtext>
    <mtext>score</mtext> <mo>=</mo> <mfrac><mrow><mi>x</mi><mo>-</mo><mi>y</mi></mrow>
    <mrow><mtext>max</mtext><mo>(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>)</mo></mrow></mfrac></mrow></math>
- en: where *x* is the average intercluster distance between clusters, and *y* is
    the mean intracluster distance.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 * x * 是簇之间的平均距离，* y * 是簇内的平均距离。
- en: 'The third method is *Calinski-Harabasz* *(CH)*, which is known as the *variance
    ratio criterion*. The formula for the CH method is as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 第三种方法是*Calinski-Harabasz* *(CH)*，也称为*方差比准则*。CH 方法的公式如下：
- en: <math alttext="CH equals StartFraction upper S upper S Subscript upper B Baseline
    Over upper S upper S Subscript upper W Baseline EndFraction times StartFraction
    upper N minus k Over k minus 1 EndFraction" display="block"><mrow><mtext>CH</mtext>
    <mo>=</mo> <mfrac><mrow><mi>S</mi><msub><mi>S</mi> <mi>B</mi></msub></mrow> <mrow><mi>S</mi><msub><mi>S</mi>
    <mi>W</mi></msub></mrow></mfrac> <mo>×</mo> <mfrac><mrow><mi>N</mi><mo>-</mo><mi>k</mi></mrow>
    <mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></mfrac></mrow></math>
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="CH equals StartFraction upper S upper S Subscript upper B Baseline
    Over upper S upper S Subscript upper W Baseline EndFraction times StartFraction
    upper N minus k Over k minus 1 EndFraction" display="block"><mrow><mtext>CH</mtext>
    <mo>=</mo> <mfrac><mrow><mi>S</mi><msub><mi>S</mi> <mi>B</mi></msub></mrow> <mrow><mi>S</mi><msub><mi>S</mi>
    <mi>W</mi></msub></mrow></mfrac> <mo>×</mo> <mfrac><mrow><mi>N</mi><mo>-</mo><mi>k</mi></mrow>
    <mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></mfrac></mrow></math>
- en: where <math alttext="upper S upper S Subscript upper B"><mrow><mi>S</mi> <msub><mi>S</mi>
    <mi>B</mi></msub></mrow></math> denotes between-cluster variance, <math alttext="upper
    S upper S Subscript upper W"><mrow><mi>S</mi> <msub><mi>S</mi> <mi>W</mi></msub></mrow></math>
    represents within cluster variance, *N* is number of observations, and *k* is
    the number of clusters. Given this information, we are seeking a high CH score,
    as the larger (lower) the between-cluster variance (within cluster variance),
    the better it is for finding the optimal number of clusters.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 <math alttext="upper S upper S Subscript upper B"><mrow><mi>S</mi> <msub><mi>S</mi>
    <mi>B</mi></msub></mrow></math> 表示簇间方差， <math alttext="upper S upper S Subscript
    upper W"><mrow><mi>S</mi> <msub><mi>S</mi> <mi>W</mi></msub></mrow></math> 表示簇内方差，*
    N * 是观测数，* k * 是簇的数目。根据这些信息，我们寻找高 CH 分数，因为簇间方差（簇内方差越小）越大，越有利于找到最优簇的数量。
- en: 'The final approach is *gap analysis*. Tibshirani et al. (2001) came up with
    a unique idea by which we are able to find the optimal number of clusters based
    on reference distribution. Following the similar notations of Tibshirani et al.,
    let <math alttext="d Subscript i i Sub Superscript e"><msub><mi>d</mi> <mrow><mi>i</mi><msup><mi>i</mi>
    <mi>e</mi></msup></mrow></msub></math> be a Euclidean distance between <math alttext="x
    Subscript i j"><msub><mi>x</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub></math>
    and <math alttext="x Subscript i Sub Superscript e j"><msub><mi>x</mi> <msup><mi>i</mi>
    <mrow><mi>e</mi><mi>j</mi></mrow></msup></msub></math> and let <math alttext="upper
    C Subscript r"><msub><mi>C</mi> <mi>r</mi></msub></math> be the <math alttext="i
    Subscript t h"><msub><mi>i</mi> <mrow><mi>t</mi><mi>h</mi></mrow></msub></math>
    cluster denoting the number of observations in cluster *r*:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '最后的方法是*间隙分析*。Tibshirani 等人（2001年）提出了一种独特的方法，通过这种方法，我们能够基于参考分布找到最优的簇数。根据 Tibshirani
    等人的类似符号，设 <math alttext="d Subscript i i Sub Superscript e"><msub><mi>d</mi> <mrow><mi>i</mi><msup><mi>i</mi>
    <mi>e</mi></msup></mrow></msub></math> 是 <math alttext="x Subscript i j"><msub><mi>x</mi>
    <mrow><mi>i</mi><mi>j</mi></mrow></msub></math> 与 <math alttext="x Subscript i
    Sub Superscript e j"><msub><mi>x</mi> <msup><mi>i</mi> <mrow><mi>e</mi><mi>j</mi></mrow></msup></msub></math>
    之间的欧几里德距离，让 <math alttext="upper C Subscript r"><msub><mi>C</mi> <mi>r</mi></msub></math>
    是 <math alttext="i Subscript t h"><msub><mi>i</mi> <mrow><mi>t</mi><mi>h</mi></mrow></msub></math>
    簇，表示* r *数的观测:'
- en: <math alttext="sigma-summation Underscript j Endscripts left-parenthesis x Subscript
    i j Baseline minus x Subscript i Sub Superscript e j Subscript Baseline right-parenthesis
    squared" display="block"><mrow><munder><mo>∑</mo> <mi>j</mi></munder> <msup><mrow><mo>(</mo><msub><mi>x</mi>
    <mrow><mi>i</mi><mi>j</mi></mrow></msub> <mo>-</mo><msub><mi>x</mi> <msup><mi>i</mi>
    <mrow><mi>e</mi><mi>j</mi></mrow></msup></msub> <mo>)</mo></mrow> <mn>2</mn></msup></mrow></math>
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="sigma-summation Underscript j Endscripts left-parenthesis x Subscript
    i j Baseline minus x Subscript i Sub Superscript e j Subscript Baseline right-parenthesis
    squared" display="block"><mrow><munder><mo>∑</mo> <mi>j</mi></munder> <msup><mrow><mo>(</mo><msub><mi>x</mi>
    <mrow><mi>i</mi><mi>j</mi></mrow></msub> <mo>-</mo><msub><mi>x</mi> <msup><mi>i</mi>
    <mrow><mi>e</mi><mi>j</mi></mrow></msup></msub> <mo>)</mo></mrow> <mn>2</mn></msup></mrow></math>
- en: 'The sum of pairwise distances for all observations in cluster *r* is:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 所有集群中观察到的成对距离之和为：
- en: <math alttext="upper D Subscript r Baseline equals sigma-summation Underscript
    i comma i Superscript e Baseline element-of upper C Subscript r Baseline Endscripts
    d Subscript i comma i Sub Superscript e" display="block"><mrow><msub><mi>D</mi>
    <mi>r</mi></msub> <mo>=</mo> <munder><mo>∑</mo> <mrow><mi>i</mi><mo>,</mo><msup><mi>i</mi>
    <mi>e</mi></msup> <mo>∈</mo><msub><mi>C</mi> <mi>r</mi></msub></mrow></munder>
    <msub><mi>d</mi> <mrow><mi>i</mi><mo>,</mo><msup><mi>i</mi> <mi>e</mi></msup></mrow></msub></mrow></math>
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper D Subscript r Baseline equals sigma-summation Underscript
    i comma i Superscript e Baseline element-of upper C Subscript r Baseline Endscripts
    d Subscript i comma i Sub Superscript e" display="block"><mrow><msub><mi>D</mi>
    <mi>r</mi></msub> <mo>=</mo> <munder><mo>∑</mo> <mrow><mi>i</mi><mo>,</mo><msup><mi>i</mi>
    <mi>e</mi></msup> <mo>∈</mo><msub><mi>C</mi> <mi>r</mi></msub></mrow></munder>
    <msub><mi>d</mi> <mrow><mi>i</mi><mo>,</mo><msup><mi>i</mi> <mi>e</mi></msup></mrow></msub></mrow></math>
- en: 'The within-cluster sum of squares, <math alttext="upper W Subscript k"><msub><mi>W</mi>
    <mi>k</mi></msub></math> , is:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 簇内平方和， <math alttext="upper W Subscript k"><msub><mi>W</mi> <mi>k</mi></msub></math>
    ，为：
- en: <math alttext="upper W Subscript k Baseline equals sigma-summation Underscript
    r equals 1 Overscript k Endscripts StartFraction 1 Over 2 Subscript n Sub Subscript
    r Subscript Baseline EndFraction upper D Subscript r" display="block"><mrow><msub><mi>W</mi>
    <mi>k</mi></msub> <mo>=</mo> <munderover><mo>∑</mo> <mrow><mi>r</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>k</mi></munderover> <mfrac><mn>1</mn> <msub><mn>2</mn> <msub><mi>n</mi> <mi>r</mi></msub></msub></mfrac>
    <msub><mi>D</mi> <mi>r</mi></msub></mrow></math>
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper W Subscript k Baseline equals sigma-summation Underscript
    r equals 1 Overscript k Endscripts StartFraction 1 Over 2 Subscript n Sub Subscript
    r Subscript Baseline EndFraction upper D Subscript r" display="block"><mrow><msub><mi>W</mi>
    <mi>k</mi></msub> <mo>=</mo> <munderover><mo>∑</mo> <mrow><mi>r</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>k</mi></munderover> <mfrac><mn>1</mn> <msub><mn>2</mn> <msub><mi>n</mi> <mi>r</mi></msub></msub></mfrac>
    <msub><mi>D</mi> <mi>r</mi></msub></mrow></math>
- en: 'where *n* is the sample size and expectation of <math alttext="upper W Subscript
    k"><msub><mi>W</mi> <mi>k</mi></msub></math> is:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *n* 是样本大小，<math alttext="upper W Subscript k"><msub><mi>W</mi> <mi>k</mi></msub></math>
    的期望为：
- en: <math alttext="upper W Subscript k Baseline equals l o g left-parenthesis p
    n slash 12 right-parenthesis minus left-parenthesis 2 slash p right-parenthesis
    l o g left-parenthesis k right-parenthesis plus c o n s t a n t" display="block"><mrow><msub><mi>W</mi>
    <mi>k</mi></msub> <mo>=</mo> <mi>l</mi> <mi>o</mi> <mi>g</mi> <mrow><mo>(</mo>
    <mi>p</mi> <mi>n</mi> <mo>/</mo> <mn>12</mn> <mo>)</mo></mrow> <mo>-</mo> <mrow><mo>(</mo>
    <mn>2</mn> <mo>/</mo> <mi>p</mi> <mo>)</mo></mrow> <mi>l</mi> <mi>o</mi> <mi>g</mi>
    <mrow><mo>(</mo> <mi>k</mi> <mo>)</mo></mrow> <mo>+</mo> <mi>c</mi> <mi>o</mi>
    <mi>n</mi> <mi>s</mi> <mi>t</mi> <mi>a</mi> <mi>n</mi> <mi>t</mi></mrow></math>
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper W Subscript k Baseline equals l o g left-parenthesis p
    n slash 12 right-parenthesis minus left-parenthesis 2 slash p right-parenthesis
    l o g left-parenthesis k right-parenthesis plus c o n s t a n t" display="block"><mrow><msub><mi>W</mi>
    <mi>k</mi></msub> <mo>=</mo> <mi>l</mi> <mi>o</mi> <mi>g</mi> <mrow><mo>(</mo>
    <mi>p</mi> <mi>n</mi> <mo>/</mo> <mn>12</mn> <mo>)</mo></mrow> <mo>-</mo> <mrow><mo>(</mo>
    <mn>2</mn> <mo>/</mo> <mi>p</mi> <mo>)</mo></mrow> <mi>l</mi> <mi>o</mi> <mi>g</mi>
    <mrow><mo>(</mo> <mi>k</mi> <mo>)</mo></mrow> <mo>+</mo> <mi>c</mi> <mi>o</mi>
    <mi>n</mi> <mi>s</mi> <mi>t</mi> <mi>a</mi> <mi>n</mi> <mi>t</mi></mrow></math>
- en: 'where *p* and *k* are dimension and centroids, respectively. Let’s create a
    practice exercise using German credit risk data. The data is gathered from the
    [Kaggle platform](https://oreil.ly/4NgIy), and the explanations of the variables
    are shown here:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *p* 和 *k* 分别为维度和质心。让我们使用德国信用风险数据创建一个实践练习。该数据是从 [Kaggle 平台](https://oreil.ly/4NgIy)
    收集的，变量的解释如下：
- en: 'Age: Numerical'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 年龄：数值型
- en: 'Sex: Male, female'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性别：男性、女性
- en: 'Job: 0—unskilled and non-resident, 1—unskilled and resident, 2—skilled, 3—highly
    skilled'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作：0—无技能和非居民，1—无技能和居民，2—技术工人，3—高技能工人
- en: 'Housing: Own, rent, free'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 住房：自有、租赁、免费
- en: 'Saving accounts: Little, moderate, quite rich, rich'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 储蓄账户：少、中等、相当丰富、丰富
- en: 'Checking account: Numerical'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支票账户：数值型
- en: 'Credit amount: Numerical'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信用额度：数值型
- en: 'Duration: Numerical'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 期限：数值型
- en: 'Purpose: Car, furniture/equipment, radio/TV, domestic appliances, repairs,
    education, business, vacation/others'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目的：汽车、家具/设备、收音机/电视、家用电器、维修、教育、商业、度假/其他
- en: The estimate of the optimal clusters will be the value that maximizes the gap
    statistic, as the gap statistic is the difference between the total within-intracluster
    variation for different values of *k* and their expected values under null reference
    distribution of the respective data. The decision is made when we get the highest
    gap value.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 估计最佳聚类的值将是最大化间隙统计量的值，因为间隙统计量是不同 *k* 值的总内部聚类变化和它们在相应数据的空值参考分布下的期望值之间的差异。当我们获得最高的间隙值时，决策就做出了。
- en: 'In the following code block, we import the German credit dataset and drop the
    unnecessary columns. The dataset includes both categorical and numerical values,
    which need to be treated differently, and we will do this soon:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码块中，我们导入德国信用数据集并且删除了不必要的列。数据集包含分类和数值两种值，需要分别处理，我们很快就会做到这一点：
- en: '[PRE0]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[![1](assets/1.png)](#co_credit_risk_estimation_CO1-1)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_credit_risk_estimation_CO1-1)'
- en: 'Dropping unnecessary column named `Unnamed: 0`'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '删除不必要的列名为 `Unnamed: 0`'
- en: 'The summary statistics are given in the following code. Accordingly, the average
    age of the customers is roughly 35, average job type is skilled, average credit
    amount and duration are nearly 3,271 and 21, respectively. Additionally, the summary
    statistics tell us that the `credit amount` variable shows a relatively high standard
    deviation as expected. The `duration` and `age` variables have a very similar
    standard deviation, but the duration moves within a narrower interval as its minimum
    and maximum values are 4 and 72, respectively. As `job` is a discrete variable,
    it is natural to expect low dispersion and we have it:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要统计数据如下代码所示。根据统计数据，客户的平均年龄约为 35 岁，平均工作类型为技术工人，平均信用额度和期限分别为 3,271 和 21。此外，摘要统计数据告诉我们
    `信用额度` 变量显示出相对较高的标准差，正如预期的那样。`期限` 和 `年龄` 变量具有非常相似的标准差，但期限的变化范围在一个较窄的区间内，其最小值和最大值分别为
    4 和 72。由于 `工作` 是一个离散变量，因此预期低离散度，并且我们确实得到了：
- en: '[PRE1]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In what follows, the distribution of the numerical variables in the dataset
    are examined via histogram and it turns out none of the variables follow a normal
    distribution. The `age`, `credit amount`, and `duration` variables are positively
    skewed as we can see in [Figure 6-1](#credit_risk_hist), generated by the following:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，通过直方图检查数据集中数值变量的分布，结果显示没有一个变量遵循正态分布。正如我们在 [图 6-1](#credit_risk_hist) 中看到的那样，`年龄`、`信用额度`
    和 `期限` 变量呈正偏态分布。
- en: '[PRE2]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[![1](assets/1.png)](#co_credit_risk_estimation_CO2-1)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_credit_risk_estimation_CO2-1)'
- en: Setting a fix figure size
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 设定一个固定的图像尺寸
- en: '[![2](assets/2.png)](#co_credit_risk_estimation_CO2-2)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_credit_risk_estimation_CO2-2)'
- en: Dropping the object type variables to obtain all numerical variables
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 删除对象类型变量以获取所有数值变量
- en: '![credit_risk](assets/mlfr_0601.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![credit_risk](assets/mlfr_0601.png)'
- en: Figure 6-1\. Credit risk data histogram
  id: totrans-105
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-1\. 信用风险数据直方图
- en: '[Figure 6-1](#credit_risk_hist) shows the distribution of age, job, credit
    amount, and duration variables. Aside from the `job` variable, which is a discrete
    variable, all other variables have skewed distributions.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 6-1](#credit_risk_hist) 显示了年龄、工作、信用额度和期限变量的分布。除了 `工作` 变量是离散变量外，所有其他变量都呈偏态分布。'
- en: 'The elbow method, as a first method, is introduced in the following code snippet
    and the resulting [Figure 6-2](#elbow_kmeans). To find the optimal number of clusters,
    we observe the slope of the curve and decide the cut-off point at which the curve
    gets flatter—that is, the slope of the curve gets lower. As it gets flatter, the
    inertia, telling us how far away the points within a cluster are located, decreases,
    which is nice for the purpose of clustering. On the other hand, as we allow inertia
    to decrease, the number of clusters increases, which makes the analysis more complicated.
    Given that trade-off, the stopping criteria is the point where the curve gets
    flatter. In code:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 首先介绍肘部法，在下面的代码片段中，并得到结果 [图 6-2](#elbow_kmeans)。为了找到最佳的聚类数，我们观察曲线的斜率，并决定曲线变平的截止点——也就是说，曲线的斜率变小。随着曲线变平，所述聚类内点的惯性减少，这对于聚类是有益的。另一方面，随着我们允许惯性减少，聚类数量增加，这使得分析更加复杂。考虑到这种权衡，停止标准是曲线变平的点。在代码中：
- en: '[PRE3]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[![1](assets/1.png)](#co_credit_risk_estimation_CO3-1)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_credit_risk_estimation_CO3-1)'
- en: Applying standardization for scaling purpose
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 应用标准化以进行缩放
- en: '[![2](assets/2.png)](#co_credit_risk_estimation_CO3-2)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_credit_risk_estimation_CO3-2)'
- en: Running K-means algorithm
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 运行 K-means 算法
- en: '[![3](assets/3.png)](#co_credit_risk_estimation_CO3-3)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_credit_risk_estimation_CO3-3)'
- en: Calculating `inertia` and storing into a list named `distance`
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 计算 `inertia` 并存储到名为 `distance` 的列表中
- en: '[Figure 6-2](#elbow_kmeans) shows that the curve gets flatter after four clusters.
    Thus, the elbow method suggests that we stop at four clusters.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 6-2](#elbow_kmeans) 显示在四个聚类后曲线变得平缓。因此，肘部法建议我们在四个聚类停止。'
- en: '![elbow](assets/mlfr_0602.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![elbow](assets/mlfr_0602.png)'
- en: Figure 6-2\. Elbow method
  id: totrans-117
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-2\. 肘部法
- en: 'The following code, resulting in [Figure 6-3](#silhouette_kmeans), presents
    Silhouette scores on the x-axis for clusters 2 to 10\. Given the average Silhouette
    score represented by the dashed line, the optimal number of clusters can be two:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码给出了从聚类 2 到 10 的 Silhouette 分数，结果为 [图 6-3](#silhouette_kmeans)，x 轴上的 Silhouette
    分数。给定由虚线表示的平均 Silhouette 分数，最佳聚类数为两个：
- en: '[PRE4]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[![1](assets/1.png)](#co_credit_risk_estimation_CO4-1)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_credit_risk_estimation_CO4-1)'
- en: Importing the `silhouette_score` module to calculate Silhouette score
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 导入 `silhouette_score` 模块以计算 Silhouette 分数
- en: '[![2](assets/2.png)](#co_credit_risk_estimation_CO4-2)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_credit_risk_estimation_CO4-2)'
- en: Importing the `SilhouetteVisualizer` module to draw Silhouette plots
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 导入 `SilhouetteVisualizer` 模块以绘制 Silhouette 图
- en: '[![3](assets/3.png)](#co_credit_risk_estimation_CO4-3)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_credit_risk_estimation_CO4-3)'
- en: Using `divmod` for configuring labels, as it returns the quotient (`q`) and
    remainder (`r`)
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `divmod` 配置标签，因为它返回商 (`q`) 和余数 (`r`)
- en: '[![4](assets/4.png)](#co_credit_risk_estimation_CO4-4)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_credit_risk_estimation_CO4-4)'
- en: Plotting the Silhouette scores
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制 Silhouette 分数
- en: '![silhouette](assets/mlfr_0603.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![silhouette](assets/mlfr_0603.png)'
- en: Figure 6-3\. Silhouette score
  id: totrans-129
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-3\. Silhouette 分数
- en: 'As mentioned, the CH method is a convenient tool for finding optimal clustering,
    and the following code shows how we can use this method in Python, resulting in
    [Figure 6-4](#CH_analysis). We are looking for the highest CH score, and we’ll
    see that it is obtained at cluster 2:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，CH 方法是一种寻找最佳聚类的方便工具，以下代码显示了如何在 Python 中使用该方法，结果为 [图 6-4](#CH_analysis)。我们寻找最高的
    CH 分数，并且我们将看到它在聚类 2 处获得：
- en: '[PRE5]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[![1](assets/1.png)](#co_credit_risk_estimation_CO5-1)'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_credit_risk_estimation_CO5-1)'
- en: Importing `KElbowVisualizer` to draw the CH score
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 导入 `KElbowVisualizer` 以绘制 CH 分数
- en: '[![2](assets/2.png)](#co_credit_risk_estimation_CO5-2)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_credit_risk_estimation_CO5-2)'
- en: Visualizing the CH metric
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化 CH 指标
- en: '![CH_analysis](assets/mlfr_0604.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![CH_analysis](assets/mlfr_0604.png)'
- en: Figure 6-4\. The CH method
  id: totrans-137
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-4\. CH 方法
- en: '[Figure 6-4](#CH_analysis) shows that the elbow occurs at the second cluster,
    indicating that stopping at two clusters is the optimum decision.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 6-4](#CH_analysis) 显示肘部出现在第二个聚类，表明停在两个聚类是最佳决定。'
- en: 'The last step for finding the optimal number of clusters is gap analysis, resulting
    in [Figure 6-5](#gap_cluster):'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找最佳聚类数的最后一步是间隙分析，结果为 [图 6-5](#gap_cluster)：
- en: '[PRE6]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[![1](assets/1.png)](#co_credit_risk_estimation_CO6-1)'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_credit_risk_estimation_CO6-1)'
- en: Importing the `OptimalK` module for calculating the gap statistic
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 导入 `OptimalK` 模块以计算间隙统计量
- en: '[![2](assets/2.png)](#co_credit_risk_estimation_CO6-2)'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_credit_risk_estimation_CO6-2)'
- en: Running gap statistic using parallelization
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 运行间隙统计量并使用并行化
- en: '[![3](assets/3.png)](#co_credit_risk_estimation_CO6-3)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_credit_risk_estimation_CO6-3)'
- en: Identifying the number of clusters based on the gap statistic
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 根据间隙统计确定聚类数量
- en: '[![4](assets/4.png)](#co_credit_risk_estimation_CO6-4)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_credit_risk_estimation_CO6-4)'
- en: Storing the result of gap analysis
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 存储间隙分析结果
- en: '![gap_cluster](assets/mlfr_0605.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![gap_cluster](assets/mlfr_0605.png)'
- en: Figure 6-5\. Gap analysis
  id: totrans-150
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-5\. 差距分析
- en: What we observe in [Figure 6-5](#gap_cluster) is a sharp increase to the point
    at which the gap value reaches its peak, and the analysis suggests stopping at
    the maximum value at which we find the optimal number for clustering. In this
    case, we find the value at cluster 5, so this is the cut-off point.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[图 6-5](#gap_cluster)中观察到一个急剧增加的情况，直到间隙值达到其峰值，并且分析建议在找到最佳聚类数量的最大值时停止。在本例中，我们发现在第5个聚类处的值，因此这是截断点。
- en: 'In light of these discussions, two clusters are chosen to be the optimal number
    of clusters, and the K-means clustering analysis is conducted accordingly. To
    illustrate, given the clustering analysis, let us visualize 2-D clusters with
    the following, resulting in [Figure 6-6](#all_clusters):'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 综合这些讨论，选择了两个聚类作为最佳聚类数量，并相应进行了K-means聚类分析。为了说明，考虑到聚类分析，让我们用以下方式可视化2-D聚类结果，导致[图 6-6](#all_clusters)：
- en: '[PRE7]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[Figure 6-6](#all_clusters) presents the behavior of the observations and cross
    sign `x` indicates the cluster center, i.e., the centroid. Age represents the
    more dispersed data, and the centroid of the `age` variable is located above the
    `credit` variable. Two continuous variables, namely `credit` and `duration`, are
    shown in the second subplot of [Figure 6-6](#all_clusters), where we observe clearly
    separated clusters. This figure suggests that the duration variable is more volatile
    compared to the credit variable. In the last subplot, the relationship between
    `age` and `duration` is examined via scatter analysis. It turns out that there
    are many overlapping observations across these two variables.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 6-6](#all_clusters)展示了观察行为，交叉符号`x`表示聚类中心，即质心。年龄表示更分散的数据，而`age`变量的质心位于`credit`变量的上方。在[图 6-6](#all_clusters)的第二个子图中展示了两个连续变量，即`credit`和`duration`，我们观察到清晰分离的聚类。该图表明，与`credit`变量相比，持续时间变量更为波动。在最后一个子图中，通过散点分析检验了`age`和`duration`之间的关系。结果表明，在这两个变量之间存在许多重叠的观察结果。'
- en: '![clusters](assets/mlfr_0606.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![clusters](assets/mlfr_0606.png)'
- en: Figure 6-6\. K-means clusters
  id: totrans-156
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-6\. K-means聚类
- en: Probability of Default Estimation with Logistic Regression
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用逻辑回归进行违约概率估计
- en: Having obtained the clusters, we are able to treat customers with similar characteristics
    the same way—that is, the model learns in an easier and more stable way if data
    with similar distributions is provided. Conversely, using all the customers for
    the entire sample might result in poor and unstable predictions.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 获得聚类后，我们能够以同样方式处理具有类似特征的客户，即如果提供具有相似分布的数据，则模型可以更轻松和更稳定地学习。相反，对整个样本使用所有客户可能导致预测效果差且不稳定。
- en: This section is ultimately about calculating the probability of default with
    Bayesian estimation, but let’s first look at logistic regression for the sake
    of comparison.^([1](ch06.html#idm45737223570832))
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 本节最终涉及使用贝叶斯估计计算违约概率，但首先让我们看看逻辑回归，以便进行比较。^([1](ch06.html#idm45737223570832))
- en: Logistic regression is a classification algorithm, widely applicable in the
    finance industry. In other words, it proposes a regression approach to the classification
    problem. Logistic regression seeks to predict discrete output, taking into account
    some independent variables.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归是一种分类算法，在金融行业广泛应用。换句话说，它提出了解决分类问题的回归方法。逻辑回归旨在预测离散输出，并考虑了一些独立变量。
- en: 'Let *X* be the set of independent variables and *Y* be a binary (or multinomial)
    output. Then, the conditional probability becomes:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 让*X*为自变量集合，*Y*为二进制（或多项式）输出。然后，条件概率为：
- en: <math alttext="probability left-parenthesis upper Y equals 1 vertical-bar upper
    X equals x right-parenthesis" display="block"><mrow><mo form="prefix">Pr</mo>
    <mo>(</mo> <mi>Y</mi> <mo>=</mo> <mn>1</mn> <mo>|</mo> <mi>X</mi> <mo>=</mo> <mi>x</mi>
    <mo>)</mo></mrow></math>
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="probability left-parenthesis upper Y equals 1 vertical-bar upper
    X equals x right-parenthesis" display="block"><mrow><mo form="prefix">Pr</mo>
    <mo>(</mo> <mi>Y</mi> <mo>=</mo> <mn>1</mn> <mo>|</mo> <mi>X</mi> <mo>=</mo> <mi>x</mi>
    <mo>)</mo></mrow></math>
- en: 'This can be read as: given the values of *X*, what is the probability of having
    *Y* as 1? As the dependent variable of logistic regression is of the probabilistic
    type, we need to make sure the dependent variable cannot take on values other
    than between 0 and 1.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 可以理解为：给定*X*的值，*Y*为1的概率是多少？由于逻辑回归的因变量是概率型的，我们需要确保因变量的取值只能在0到1之间。
- en: 'To this aim, a modification is applied known as *logistic (logit) transformation*,
    which is simply the log of the odds ratio (*p* / 1 - *p*):'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，应用一种称为*logistic（logit）转换*的修改，它简单地是概率比的对数（*p* / 1 - *p*）：
- en: <math alttext="l o g left-parenthesis StartFraction p Over 1 minus p EndFraction
    right-parenthesis" display="block"><mrow><mi>l</mi> <mi>o</mi> <mi>g</mi> <mo>(</mo>
    <mfrac><mi>p</mi> <mrow><mn>1</mn><mo>-</mo><mi>p</mi></mrow></mfrac> <mo>)</mo></mrow></math>
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="l o g left-parenthesis StartFraction p Over 1 minus p EndFraction
    right-parenthesis" display="block"><mrow><mi>l</mi> <mi>o</mi> <mi>g</mi> <mo>(</mo>
    <mfrac><mi>p</mi> <mrow><mn>1</mn><mo>-</mo><mi>p</mi></mrow></mfrac> <mo>)</mo></mrow></math>
- en: 'And the logistic regression model takes the following form:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归模型的形式如下所示：
- en: <math alttext="l o g left-parenthesis StartFraction p Over 1 minus p EndFraction
    right-parenthesis equals beta 0 plus beta 1 x" display="block"><mrow><mi>l</mi>
    <mi>o</mi> <mi>g</mi> <mrow><mo>(</mo> <mfrac><mi>p</mi> <mrow><mn>1</mn><mo>-</mo><mi>p</mi></mrow></mfrac>
    <mo>)</mo></mrow> <mo>=</mo> <msub><mi>β</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>β</mi>
    <mn>1</mn></msub> <mi>x</mi></mrow></math>
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="l o g left-parenthesis StartFraction p Over 1 minus p EndFraction
    right-parenthesis equals beta 0 plus beta 1 x" display="block"><mrow><mi>l</mi>
    <mi>o</mi> <mi>g</mi> <mrow><mo>(</mo> <mfrac><mi>p</mi> <mrow><mn>1</mn><mo>-</mo><mi>p</mi></mrow></mfrac>
    <mo>)</mo></mrow> <mo>=</mo> <msub><mi>β</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>β</mi>
    <mn>1</mn></msub> <mi>x</mi></mrow></math>
- en: 'Solving *p* results in:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 解决*p*的结果如下：
- en: <math alttext="p equals StartFraction e Superscript beta 0 plus beta 1 x Baseline
    Over 1 plus e Superscript beta 0 plus beta 1 x Baseline EndFraction" display="block"><mrow><mi>p</mi>
    <mo>=</mo> <mfrac><msup><mi>e</mi> <mrow><msub><mi>β</mi> <mn>0</mn></msub> <mo>+</mo><msub><mi>β</mi>
    <mn>1</mn></msub> <mi>x</mi></mrow></msup> <mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi>
    <mrow><msub><mi>β</mi> <mn>0</mn></msub> <mo>+</mo><msub><mi>β</mi> <mn>1</mn></msub>
    <mi>x</mi></mrow></msup></mrow></mfrac></mrow></math>
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="p equals StartFraction e Superscript beta 0 plus beta 1 x Baseline
    Over 1 plus e Superscript beta 0 plus beta 1 x Baseline EndFraction" display="block"><mrow><mi>p</mi>
    <mo>=</mo> <mfrac><msup><mi>e</mi> <mrow><msub><mi>β</mi> <mn>0</mn></msub> <mo>+</mo><msub><mi>β</mi>
    <mn>1</mn></msub> <mi>x</mi></mrow></msup> <mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi>
    <mrow><msub><mi>β</mi> <mn>0</mn></msub> <mo>+</mo><msub><mi>β</mi> <mn>1</mn></msub>
    <mi>x</mi></mrow></msup></mrow></mfrac></mrow></math>
- en: 'Let’s start off our application by preparing the data. First, we distinguish
    the clusters as 0 and 1\. The credit data has a column named `risk`, suggesting
    the risk level of the customers. Next, the number of observations per risk in
    cluster 0 and cluster 1 are examined; it turns out we have 571 and 129 good customers
    in the cluster 0 and 1, respectively. In code:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先准备数据应用。首先，我们将集群区分为0和1。信用数据有一列名为`risk`，表示客户的风险水平。接下来，检查集群0和集群1中每种风险的观察次数；结果显示，集群0和1中有571名好客户和129名好客户。在代码中：
- en: '[PRE8]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[![1](assets/1.png)](#co_credit_risk_estimation_CO7-1)'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_credit_risk_estimation_CO7-1)'
- en: Obtaining cluster numbers
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 获取集群编号
- en: '[![2](assets/2.png)](#co_credit_risk_estimation_CO7-2)'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_credit_risk_estimation_CO7-2)'
- en: Based on the cluster numbers, differentiating the clusters and storing them
    in a dictionary called `cluster_dict`
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 基于集群编号，区分集群并将它们存储在名为`cluster_dict`的字典中
- en: '[![3](assets/3.png)](#co_credit_risk_estimation_CO7-3)'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_credit_risk_estimation_CO7-3)'
- en: Creating a `clusters` column using K-means labels
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 使用K-means标签创建`clusters`列
- en: '[![4](assets/4.png)](#co_credit_risk_estimation_CO7-4)'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_credit_risk_estimation_CO7-4)'
- en: Observing the number of observations of categories within a cluster
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 观察集群内类别的观察次数
- en: '[![5](assets/5.png)](#co_credit_risk_estimation_CO7-5)'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_credit_risk_estimation_CO7-5)'
- en: Finding number of observations per category
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 查找每个类别的观察次数
- en: 'Next, we draw a couple of bar plots to show the difference of the number of
    observations per risk level category (Figures [6-7](#risk_level1) and [6-8](#risk_level2)):'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们绘制了几个条形图，以显示每个风险级别类别的观察次数差异（图 [6-7](#risk_level1) 和 [6-8](#risk_level2)）：
- en: '[PRE9]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![cluster_2](assets/mlfr_0607.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![cluster_2](assets/mlfr_0607.png)'
- en: Figure 6-7\. Frequency of risk level of the first cluster
  id: totrans-185
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-7\. 第一个集群风险水平的频率
- en: '![cluster_2](assets/mlfr_0608.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![cluster_2](assets/mlfr_0608.png)'
- en: Figure 6-8\. Frequency of risk level of the second cluster
  id: totrans-187
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-8\. 第二个集群风险水平的频率
- en: Based on the clusters we defined previously, we can analyze the frequency of
    risk level by histogram. [Figure 6-7](#risk_level1) shows that there is an imbalance
    distribution across risk level in the first cluster, whereas the frequency of
    good and bad risk levels are more balanced, if not perfectly balanced, in [Figure 6-8](#risk_level2).
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 基于我们之前定义的集群，我们可以通过直方图分析风险水平的频率。[图 6-7](#risk_level1) 显示，在第一个集群中，风险水平的分布是不平衡的，而在
    [图 6-8](#risk_level2) 中，好和坏风险水平的频率更加平衡，如果不是完全平衡的话。
- en: 'At this point, let’s take a step back and focus on an entirely different problem:
    *class imbalance*. In credit risk analysis, it is not uncommon to have a class
    imbalance problem. Class imbalance arises when one class dominates over another.
    To illustrate, in our case, given the data obtained from the first cluster, we
    have 571 customers with a good credit record and 193 customers with a bad one.
    As can be readily observed, customers with good credit records dominate over customers
    with bad records; that is basically what we refer to as a class imbalance.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，让我们退后一步，专注于一个完全不同的问题：*类不平衡*。在信用风险分析中，类不平衡问题并不少见。当一个类占主导地位时，就会出现类不平衡。例如，在我们的案例中，根据从第一个集群获得的数据，我们有571名信用记录良好的客户和193名信用记录不良的客户。可以清楚地看到，信用记录良好的客户占据了上风；这基本上就是我们所说的类不平衡。
- en: 'There are numerous ways to handle this issue: up-sampling, down-sampling, the
    synthetic minority oversampling technique (SMOTE), and the edited nearest neighbor
    (ENN) rule. To take advantage of a hybrid approach, we’ll incorporate a combination
    of SMOTE and ENN so we can clean the unwanted overlapping observations between
    classes, which will help us detect the optimal balancing ratio and, in turn, boost
    the predictive performance (Tuong et al. 2018). Converting imbalanced data into
    balanced data will be our first step in predicting the probability of default,
    but please note that we will merely apply this technique to the data obtained
    from the first cluster.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 处理此问题有多种方法：上采样、下采样、合成少数类过采样技术（SMOTE）和编辑最近邻规则（ENN）。为了利用混合方法，我们将结合SMOTE和ENN，以清理类之间不必要的重叠观测，这将帮助我们检测到最佳的平衡比率，并进而提高预测性能（Tuong等，2018）。将不平衡数据转换为平衡数据将是预测违约概率的第一步，但请注意，我们仅将此技术应用于从第一个聚类中获取的数据。
- en: 'Now, we next apply a train-test split. To do that, we need to convert the categorical
    variable `Risk` into a discrete variable. The category `good` takes a value of
    1, and `bad` takes a value of 0\. In a train-test split, 80% of the data is devoted
    to training samples and 20% of is allocated to the test sample:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们接下来进行训练-测试拆分。为此，我们需要将分类变量`Risk`转换为离散变量。类别`good`取值为1，`bad`取值为0。在训练-测试拆分中，将80%的数据用于训练样本，20%用于测试样本：
- en: '[PRE10]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[![1](assets/1.png)](#co_credit_risk_estimation_CO8-1)'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_credit_risk_estimation_CO8-1)'
- en: Discretization of the variable
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 变量的离散化
- en: '[![2](assets/2.png)](#co_credit_risk_estimation_CO8-2)'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_credit_risk_estimation_CO8-2)'
- en: Creating data based on the first cluster and dropping last column from `X_train`
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 根据第一个聚类创建数据，并从`X_train`中删除最后一列
- en: '[![3](assets/3.png)](#co_credit_risk_estimation_CO8-3)'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_credit_risk_estimation_CO8-3)'
- en: Creating data based on the second cluster and dropping last column from `X_train`
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 根据第二个聚类创建数据，并从`X_train`中删除最后一列
- en: 'After these preparations, we are ready to move ahead and run the logistic regression
    to predict the probability of default. The library that we’ll make use of is called
    `statsmodels`, and it is allowed to have a summary table. The following result
    is based on the first cluster data. According to the result, the `age`, `credit
    amount`, and `job` variables are positively related with the creditworthiness
    of customer, while a negative association emerges between the `dependent` and
    `duration` variables. This finding suggests that all the estimated coefficients
    reveal statistically significant results at a 1% significance level. A general
    interpretation would be that a slide in duration and a surge in credit amount,
    age, and job imply a high probability of default:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些准备工作完成后，我们准备继续运行逻辑回归以预测违约概率。我们将使用的库名为`statsmodels`，它允许有一个汇总表。以下结果基于第一个聚类数据。根据结果，`age`、`credit
    amount`和`job`变量与客户的信用价值呈正相关，而`dependent`和`duration`变量之间存在负相关。这一发现表明，在1%的显著性水平上，所有估计系数都显示出统计上显著的结果。一般的解释是，持续时间的下降和信用额度、年龄和工作的增加都意味着违约的高概率：
- en: '[PRE11]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[![1](assets/1.png)](#co_credit_risk_estimation_CO9-1)'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_credit_risk_estimation_CO9-1)'
- en: Importing `SMOTEENN` to deal with the class imbalance problem
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 导入`SMOTEENN`以处理类别不平衡问题
- en: '[![2](assets/2.png)](#co_credit_risk_estimation_CO9-2)'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_credit_risk_estimation_CO9-2)'
- en: Creating `y_train` based on cluster 0 and risk level
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 根据聚类 0 和风险水平创建`y_train`
- en: '[![3](assets/3.png)](#co_credit_risk_estimation_CO9-3)'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_credit_risk_estimation_CO9-3)'
- en: Running the `SMOTEENN` method with a random state of 2
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 运行带有随机状态为2的`SMOTEENN`方法
- en: '[![4](assets/4.png)](#co_credit_risk_estimation_CO9-4)'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_credit_risk_estimation_CO9-4)'
- en: Turning the imbalanced data into balanced data
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 将不平衡数据转换为平衡数据
- en: '[![5](assets/5.png)](#co_credit_risk_estimation_CO9-5)'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_credit_risk_estimation_CO9-5)'
- en: Configuring the logistic regression model
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 配置逻辑回归模型
- en: '[![6](assets/6.png)](#co_credit_risk_estimation_CO9-6)'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](assets/6.png)](#co_credit_risk_estimation_CO9-6)'
- en: Running the logistic regression model
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 运行逻辑回归模型
- en: 'In what follows, prediction analysis is conducted by creating different datasets
    based on clusters. For the sake of testing, the following analysis is done with
    test data, and results in [Figure 6-9](#roc_auc_curve1_first):'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，通过基于聚类创建不同的数据集进行预测分析。为了测试的目的，以下分析是在测试数据上进行的，并导致[图表 6-9](#roc_auc_curve1_first)中的结果：
- en: '[PRE12]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[![1](assets/1.png)](#co_credit_risk_estimation_CO10-1)'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_credit_risk_estimation_CO10-1)'
- en: Creating first test data based on cluster 0
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 基于第0类别的聚类创建第一组测试数据
- en: '[![2](assets/2.png)](#co_credit_risk_estimation_CO10-2)'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_credit_risk_estimation_CO10-2)'
- en: Creating second test data based on cluster 1
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 基于第一类别的聚类创建第二组测试数据
- en: '[![3](assets/3.png)](#co_credit_risk_estimation_CO10-3)'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_credit_risk_estimation_CO10-3)'
- en: Running prediction using `X_test1`
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`X_test1`进行预测
- en: '[![4](assets/4.png)](#co_credit_risk_estimation_CO10-4)'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_credit_risk_estimation_CO10-4)'
- en: Obtaining false and true positives using `roc_curve` function
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`roc_curve`函数获取假阳性和真阳性
- en: '[![5](assets/5.png)](#co_credit_risk_estimation_CO10-5)'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_credit_risk_estimation_CO10-5)'
- en: Compute the `roc-auc` score
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 计算`roc-auc`分数
- en: '![roc_auc1](assets/mlfr_0609.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![roc_auc1](assets/mlfr_0609.png)'
- en: Figure 6-9\. ROC-AUC curve of the first cluster
  id: totrans-226
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-9\. 第一个聚类的ROC-AUC曲线
- en: The ROC-AUC curve is a convenient tool in the presence of imbalanced data. The
    ROC-AUC curve in [Figure 6-9](#roc_auc_curve1_first) suggests that the performance
    of the model is not very good, because it moves just above the 45-degree line.
    Generally speaking, given the test results, a good ROC-AUC curve should be close
    to 1, implying that there is a close-to-perfect separation.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: ROC-AUC曲线是在存在不平衡数据时的一个方便工具。[图 6-9](#roc_auc_curve1_first)中的ROC-AUC曲线表明模型的性能并不是很好，因为它仅略高于45度线。一般来说，根据测试结果，一个良好的ROC-AUC曲线应该接近1，意味着有接近完美的分离。
- en: Moving on to the second set of training samples obtained from the second cluster,
    the signs of the estimated coefficients of `job`, `duration`, and `age` are positive,
    suggesting that customers with `job` type of `1` and having larger duration tend
    to default, and the `credit amount` variable shows a negative relation with dependent
    variable. However, all the estimated coefficients are statistically insignificant
    at 95% confidence interval; therefore, it makes no sense to further interpret
    the findings.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 继续使用从第二个聚类获得的第二组训练样本，`job`、`duration`和`age`的估计系数表现出正相关，表明`job`类型为`1`且持续时间较长的客户倾向于违约，而`credit
    amount`变量与因变量呈负相关。然而，所有估计系数在95%置信区间下都不显著；因此，进一步解释这些发现是毫无意义的。
- en: 'Similar to what we did with the first set of test data, we create a second
    set of test data to run the prediction to draw the ROC-AUC curve, resulting in
    [Figure 6-10](#roc_auc_curve1_second):'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于我们对第一组测试数据所做的操作，我们创建第二组测试数据来运行预测以绘制ROC-AUC曲线，结果显示为[图 6-10](#roc_auc_curve1_second)：
- en: '[PRE13]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Given the test data, the result shown in [Figure 6-10](#roc_auc_curve1_second)
    is worse than the previous application, as can be confirmed by the AUC score of
    0.4064\. Considering this data, we are far from saying that logistic regression
    is doing a good job of modeling probability of default using the German credit
    risk dataset.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 根据测试数据，[图 6-10](#roc_auc_curve1_second)中显示的结果比先前的应用要差，这可以通过0.4064的AUC分数确认。考虑到这些数据，我们远不能说逻辑回归在使用德国信用风险数据集建模违约概率方面做得很好。
- en: We will now use different models to see how good the logistic regression is
    in modeling this type of problem relative to other methods. Thus, in the following
    part, we will take a look at Bayesian estimation with maximum a posteriori (MAP)
    probability and Markov Chain Monte Carlo (MCMC) approaches. We will then explore
    those approaches using a few well-known ML models—SVM, random forest, and neural
    networks using `MLPRegressor`—and we will test the deep learning model with TensorFlow.
    This application will show us which model works better in modeling the probability
    of default.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将使用不同的模型来看看逻辑回归在相对于其他方法建模此类问题中的表现如何。因此，在接下来的部分，我们将研究最大后验概率（MAP）贝叶斯估计和马尔可夫链蒙特卡洛（MCMC）方法。然后，我们将使用几个知名的ML模型——如SVM、随机森林和使用`MLPRegressor`的神经网络——来探索这些方法，并将使用TensorFlow测试深度学习模型。这个应用将展示哪个模型在建模违约概率方面表现更好。
- en: '![roc_auc2](assets/mlfr_0610.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![roc_auc2](assets/mlfr_0610.png)'
- en: Figure 6-10\. ROC-AUC curve of the second cluster
  id: totrans-234
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-10\. 第二个聚类的ROC-AUC曲线
- en: Probability of Default Estimation with the Bayesian Model
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用贝叶斯模型进行违约概率估计
- en: In this part, we’ll use the `PYMC3` package, which is a Python package for Bayesian
    estimation, to predict the probability of default. However, there are several
    approaches for running Bayesian analysis using `PYMC3`, and for the first application,
    we’ll use the MAP distribution discussed in [Chapter 4](ch04.html#chapter_4).
    As a quick reminder, given the representative posterior distribution, MAP becomes
    an efficient model in this case. Moreover, we select the Bayesian model with a
    deterministic variable (*p*) that is entirely determined by its parents—that is,
    `age`, `job`, `credit amount`, and `duration`.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在这部分中，我们将使用Python包`PYMC3`进行贝叶斯估计，预测违约的概率。但是，使用`PYMC3`运行贝叶斯分析有几种方法，对于第一个应用程序，我们将使用[第4章](ch04.html#chapter_4)中讨论的MAP分布。作为一个快速提醒，鉴于代表性后验分布，MAP在这种情况下成为一个有效的模型。此外，我们选择具有确定性变量(*p*)的贝叶斯模型，该变量完全由其父变量决定，即`age`、`job`、`credit
    amount`和`duration`。
- en: 'Let’s compare the results obtained from Bayesian analysis with that of logistic
    regression:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将贝叶斯分析的结果与逻辑回归进行比较：
- en: '[PRE14]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[![1](assets/1.png)](#co_credit_risk_estimation_CO11-1)'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_credit_risk_estimation_CO11-1)'
- en: Importing `PYMC3`
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 导入`PYMC3`
- en: '[![2](assets/2.png)](#co_credit_risk_estimation_CO11-2)'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_credit_risk_estimation_CO11-2)'
- en: Importing `arviz` for exploratory analysis of Bayesian models
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 导入`arviz`用于贝叶斯模型的探索性分析
- en: '[![3](assets/3.png)](#co_credit_risk_estimation_CO11-3)'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_credit_risk_estimation_CO11-3)'
- en: Identifying Bayesian model as `logistic_model1`
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 将贝叶斯模型标识为`logistic_model1`
- en: '[![4](assets/4.png)](#co_credit_risk_estimation_CO11-4)'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_credit_risk_estimation_CO11-4)'
- en: Identifying the assumed distributions of the variables as normal with defined
    `mu` and `sigma` parameters
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 确定变量的假设分布为正态分布，具有定义的`mu`和`sigma`参数
- en: '[![5](assets/5.png)](#co_credit_risk_estimation_CO11-5)'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_credit_risk_estimation_CO11-5)'
- en: Running a deterministic model using the first sample
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 使用第一个样本运行确定性模型
- en: '[![6](assets/6.png)](#co_credit_risk_estimation_CO11-6)'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](assets/6.png)](#co_credit_risk_estimation_CO11-6)'
- en: Running a Bernoulli distribution to model the dependent variable
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 运行伯努利分布来建模因变量
- en: '[![7](assets/7.png)](#co_credit_risk_estimation_CO11-7)'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '[![7](assets/7.png)](#co_credit_risk_estimation_CO11-7)'
- en: Fitting the MAP model to data
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 将MAP模型拟合到数据中
- en: '[![8](assets/8.png)](#co_credit_risk_estimation_CO11-8)'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '[![8](assets/8.png)](#co_credit_risk_estimation_CO11-8)'
- en: Storing all the results of the estimated coefficients into `param`s with six
    decimals
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有估计系数的结果存储在`param`s中，保留六位小数
- en: The most striking observation is that the differences between estimated coefficients
    are so small that they can be ignored. The difference occurs in the decimals.
    Taking the estimated coefficient of the credit amount variable as an example,
    we have estimated the coefficient to be 1.3290 in logistic regression and 1.3272
    in Bayesian analysis.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 最引人注目的观察是估计系数之间的差异非常小，可以忽略不计。差异出现在小数部分。以信用金额变量的估计系数为例，我们在逻辑回归中估计系数为1.3290，在贝叶斯分析中为1.3272。
- en: 'The story is more or less the same when it comes to comparing the analysis
    result based on the second cluster data:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 当比较基于第二簇数据的分析结果时，故事基本相同：
- en: '[PRE15]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The most remarkable difference occurs in the `duration` variable. The estimated
    coefficients of this variable are 0.1046 and 0.1045 in logistic regression and
    Bayesian estimation, respectively.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 最显著的差异出现在`duration`变量中。这个变量的估计系数在逻辑回归和贝叶斯估计中分别为0.1046和0.1045。
- en: Instead of finding the local maximum, which is sometimes difficult to get, we
    look for an approximate expectation based on the sampling procedure. This is referred
    to as MCMC in the Bayesian setting. As we discussed in [Chapter 4](ch04.html#chapter_4),
    one of the most well known methods is the Metropolis-Hastings (M-H) algorithm.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不是寻找局部最大值，有时很难得到，而是基于抽样过程寻找近似期望。在贝叶斯设置中，这称为MCMC。正如我们在[第4章](ch04.html#chapter_4)中讨论的，其中最著名的方法之一是Metropolis-Hastings（M-H）算法。
- en: 'The Python code that applies Bayesian estimation based on the M-H algorithm
    is shown in the following and results in [Figure 6-11](#MCMC_risk_cluster1). Accordingly,
    we draw 10,000 posterior samples to simulate the posterior distribution for two
    independent Markov chains. The summary table for the estimated coefficients is
    provided in the code as well:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 基于M-H算法的贝叶斯估计的Python代码如下所示，并导致[图 6-11](#MCMC_risk_cluster1)的结果。因此，我们抽取10,000个后验样本，模拟两个独立的马尔可夫链的后验分布。代码中还提供了估计系数的摘要表：
- en: '[PRE16]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[![1](assets/1.png)](#co_credit_risk_estimation_CO12-1)'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_credit_risk_estimation_CO12-1)'
- en: Importing the `logging` package to suppress the warning messages
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 导入`logging`包以抑制警告消息
- en: '[![2](assets/2.png)](#co_credit_risk_estimation_CO12-2)'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_credit_risk_estimation_CO12-2)'
- en: Naming the package for logging
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 为日志记录命名包
- en: '[![3](assets/3.png)](#co_credit_risk_estimation_CO12-3)'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_credit_risk_estimation_CO12-3)'
- en: Suppressing errors without raising exceptions
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 抑制错误而不引发异常
- en: '[![4](assets/4.png)](#co_credit_risk_estimation_CO12-4)'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_credit_risk_estimation_CO12-4)'
- en: Initiating the M-H model
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 启动M-H模型
- en: '[![5](assets/5.png)](#co_credit_risk_estimation_CO12-5)'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_credit_risk_estimation_CO12-5)'
- en: Running the model with 10,000 samples and ignoring the progress bar
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 使用10,000个样本运行模型并忽略进度条
- en: '[![6](assets/6.png)](#co_credit_risk_estimation_CO12-6)'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](assets/6.png)](#co_credit_risk_estimation_CO12-6)'
- en: Creating a simple posterior plot using `plot_trace`
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 创建简单的后验概率图使用`plot_trace`
- en: '[![7](assets/7.png)](#co_credit_risk_estimation_CO12-7)'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '[![7](assets/7.png)](#co_credit_risk_estimation_CO12-7)'
- en: Printing the first four rows of the summary result
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 打印摘要结果的前四行
- en: '![MCMC_risk_cluster1](assets/mlfr_0611.png)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![MCMC_risk_cluster1](assets/mlfr_0611.png)'
- en: Figure 6-11\. Bayesian estimation with M—H with first cluster
  id: totrans-277
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-11\. 使用M—H进行贝叶斯估计的第一个聚类
- en: The result suggests that the predictive performances are supposed be very close
    to that of logistic regression, as the estimated coefficients of these two models
    are quite similar.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，预测性能应该非常接近逻辑回归的性能，因为这两个模型的估计系数非常相似。
- en: In [Figure 6-11](#MCMC_risk_cluster1), we see the dashed and solid lines. Given
    the first cluster data, the plot located on the lefthand side of [Figure 6-11](#MCMC_risk_cluster1)
    shows the sample values of the related parameters. Though it is not our present
    focus, we can observe the deterministic variable, *p*, located in the last plot.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在[Figure 6-11](#MCMC_risk_cluster1)中，我们看到了虚线和实线。给定第一个聚类数据，位于[Figure 6-11](#MCMC_risk_cluster1)左侧的图表显示了相关参数的样本值。虽然这不是我们目前的重点，但我们可以观察到最后一个图中的确定性变量*p*。
- en: In a similar vein, the result of Bayesian estimation with M-H based on the second
    cluster performs very closely to the logistic regression. However, the results
    obtained from MAP application are better, which is expected primarily because
    M-H works with random sampling. It is not, however, the only potential reason
    for this small deviation that we’ll discuss.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，基于第二个聚类的M-H贝叶斯估计结果与逻辑回归非常接近。然而，从MAP应用程序获得的结果更好，这主要是因为M-H使用随机抽样。然而，这种小偏差的另一个潜在原因我们将讨论。
- en: 'As for the data that we obtained from the second cluster, the result of Bayesian
    estimation with M-H can be seen in the following code, which also creates the
    plot shown in [Figure 6-12](#MCMC_risk_cluster2):'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 至于我们从第二个聚类获得的数据，可以在下面的代码中看到M-H贝叶斯估计的结果，该代码还创建了[Figure 6-12](#MCMC_risk_cluster2)中显示的图表：
- en: '[PRE17]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![MCMC_risk_cluster2](assets/mlfr_0612.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![MCMC_risk_cluster2](assets/mlfr_0612.png)'
- en: Figure 6-12\. Bayesian estimation with M—H with second cluster
  id: totrans-284
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-12\. 使用M—H进行贝叶斯估计的第二个聚类
- en: Let’s now discuss the limitations of the M-H model, which may shed some light
    on the discrepancies across the model results. One disadvantage of the M-H algorithm
    is its sensitivity to step size. Small steps hinder the convergence process. Conversely,
    big steps may cause a high rejection rate. Besides, M-H may suffer from rare events—as
    the probability of these events are low, requiring a large sample to obtain a
    reliable estimation—and that is our focus in this case.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们讨论M-H模型的局限性，这可能会揭示模型结果之间的差异。M-H算法的一个缺点是对步长的敏感性。小步骤阻碍了收敛过程。相反，大步骤可能导致高拒绝率。此外，M-H可能会受到罕见事件的影响——由于这些事件的概率很低，需要大样本才能获得可靠的估计——这在本例中是我们关注的焦点。
- en: Now, let’s consider what happens if we use SVM to predict probability of default
    and compare its performance with logistic regression.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们考虑如果我们使用SVM预测违约概率会发生什么，并将其性能与逻辑回归进行比较。
- en: Probability of Default Estimation with Support Vector Machines
  id: totrans-287
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用支持向量机进行违约概率估计
- en: SVM is thought to be a parametric model, and it works well with high-dimensional
    data. The probability of default case in a multivariate setting may provide fertile
    ground for running SVM. Before proceeding, it would be a good idea to briefly
    discuss a new approach that we will use to run hyperparameter tuning, namely `HalvingRandomSearchCV`.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: SVM被认为是一个参数化模型，并且在高维数据上表现良好。在多变量设置中的违约案例可能为运行SVM提供了丰富的数据。在继续之前，简要讨论我们将用来运行超参数调整的新方法是`HalvingRandomSearchCV`。
- en: '`HalvingRandomSearchCV` works with iterative selection so that it uses fewer
    resources, thereby boosting performance and getting you some time back. `HalvingRandomSearchCV`
    tries to find the optimal parameters using successive halving to identify candidate
    parameters. The logic behind this process is as follows:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '`HalvingRandomSearchCV`使用迭代选择，以使用更少的资源，从而提高性能并节省时间。`HalvingRandomSearchCV`尝试使用连续减半来识别候选参数的最佳值。这个过程的逻辑如下：'
- en: Evaluate all parameter combinations, exploiting a certain number of training
    samples at first iteration.
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估所有参数组合，在第一次迭代中利用一定数量的训练样本。
- en: Use some of the selected parameters in the second iteration with a large number
    of training samples.
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第二次迭代中，使用选定的一些参数和大量的训练样本。
- en: Only include the top-scoring candidates in the model until the last iteration.
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 直到最后一次迭代只包括得分最高的候选模型。
- en: 'Using the credit dataset, we predict the probability of default with support
    vector classification (SVC). Again, we use two different datasets based on the
    clustering we performed at the very first part of this chapter. The results are
    provided in the following:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 使用信用数据集，我们使用支持向量分类（SVC）预测违约的概率。同样，根据我们在本章最初部分执行的聚类，我们使用两个不同的数据集。结果如下所示：
- en: '[PRE18]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[![1](assets/1.png)](#co_credit_risk_estimation_CO13-1)'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_credit_risk_estimation_CO13-1)'
- en: Importing the library to enable successive halving search
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 导入库以启用连续减半搜索。
- en: '[![2](assets/2.png)](#co_credit_risk_estimation_CO13-2)'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_credit_risk_estimation_CO13-2)'
- en: Importing the library to run the halving search
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 导入库以运行减半搜索。
- en: '[![3](assets/3.png)](#co_credit_risk_estimation_CO13-3)'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_credit_risk_estimation_CO13-3)'
- en: Running the halving search using parallel processing
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 使用并行处理运行减半搜索。
- en: '[![4](assets/4.png)](#co_credit_risk_estimation_CO13-4)'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_credit_risk_estimation_CO13-4)'
- en: Running a prediction analysis
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 运行预测分析。
- en: An important step to take in SVM is hyperparameter tuning. Using a halving search
    approach, we try to find out the best combination of `kernel`, `gamma`, and `C`.
    It turns out that the only difference across the two different samples occurs
    in the `gamma` and `C` hyperparameters. In the first cluster, the optimal `C`
    score is 1, whereas it is 0.001 in the second one. The higher `C` value indicates
    that we should choose a smaller margin to make a better classification. As for
    the `gamma` hyperparameter, both clusters take the same value. Having a lower
    `gamma` amounts to a larger influence of the support vector on the decision. The
    optimal kernel is Gaussian, and the `gamma` value is 0.01 for both clusters.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: SVM中的一个重要步骤是超参数调整。使用减半搜索方法，我们试图找到最佳的`kernel`、`gamma`和`C`的组合。结果表明，两个不同样本之间唯一的区别在于`gamma`和`C`超参数上。在第一个聚类中，最优的`C`得分为1，而在第二个聚类中为0.001。更高的`C`值表示我们应选择更小的边距以获得更好的分类效果。至于`gamma`超参数，两个聚类都采用相同的值。较低的`gamma`意味着支持向量对决策的影响更大。最佳的核函数是高斯核，两个聚类的`gamma`值均为0.01。
- en: The AUC performance criteria indicates that the predictive performance of SVC
    is slightly below that of logistic regression. More precisely, AUC of the SVC
    is 0.5179, and that implies that SVC performs worse than logistic regression for
    the first cluster.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: AUC性能指标表明，SVC的预测性能略低于逻辑回归。具体来说，SVC的AUC为0.5179，这意味着SVC在第一个聚类中表现不如逻辑回归。
- en: 'The second cluster shows that the performance of SVC is even slightly worse
    than that of the first cluster, and this indicates the SVC does not perform well
    on this data, as it is not clearly separable data, this implies that SVC does
    not work well with low-dimensional spaces:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个聚类显示，SVC的性能甚至比第一个聚类稍差，这表明SVC在这些数据上表现不佳，因为这些数据不太可能清晰地分离，这暗示SVC在低维空间中表现不佳。
- en: '[PRE19]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Well, maybe we’ve had enough of parametric methods—let’s move on to nonparametric
    methods. Now, the word *nonparametric* may sound confusing, but it is nothing
    but a model with an infinite number of parameters, and one that becomes more complex
    as the number of observations increases. Random forest is one of the most applicable
    nonparametric models in ML, and we’ll discuss that next.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，也许我们已经足够使用参数化方法了，让我们转向非参数化方法。现在，*非参数化* 这个词可能听起来令人困惑，但它只是一个具有无限参数的模型，随着观察数量的增加而变得更加复杂。随机森林是机器学习中最适用的非参数模型之一，我们接下来会讨论这个模型。
- en: Probability of Default Estimation with Random Forest
  id: totrans-308
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用随机森林进行违约概率估计
- en: The random forest classifier is another model we can employ to model the probability
    of default. Although random forest fails in high-dimensional cases, our data is
    not that complex, and the beauty of random forest lies in its good predictive
    performance in the presence of a large number of samples, so it’s plausible to
    think that the random forest model might outperform the SVC model.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林分类器是我们可以用来建模违约概率的另一个模型。虽然随机森林在高维情况下失败，但我们的数据并不那么复杂，而随机森林的优势在于在大量样本存在时具有良好的预测性能，因此可以认为随机森林模型可能会胜过SVC模型。
- en: Using halving search approach, we try to find out the best combination of `n_estimators`,
    `criterion`, `max_features`, `max_depth`, `min_samples_split`. The result suggests
    that we use `n_estimators` of 300, `min_samples_split` of 10, `max_depth` of 6
    with a gini criterion, and `sqrt` `max_features` for the first cluster. As for
    the second cluster, we have two different optimal hyperparameters as can be seen
    in the following. Having larger depth in a tree-based model amounts to having
    a more complex model. With that said, the model proposed for the second cluster
    is a bit more complex. The `max_features` hyperparameter seems to be different
    across samples; in the first cluster, the maximum number of features is picked
    via <math alttext="StartRoot number of features EndRoot"><msqrt><mrow><mtext>number</mtext>
    <mtext>of</mtext> <mtext>features</mtext></mrow></msqrt></math> .
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 使用减半搜索方法，我们试图找出最佳的 `n_estimators`、`criterion`、`max_features`、`max_depth`、`min_samples_split`
    组合。结果表明，对于第一个集群，我们使用 `n_estimators` 为300，`min_samples_split` 为10，`max_depth` 为6，采用基尼系数的方法，以及
    `sqrt` 作为 `max_features`。至于第二个集群，我们有两种不同的最优超参数，如下所示。在基于树的模型中，较大的深度意味着更复杂的模型。话虽如此，提议用于第二个集群的模型稍微复杂一些。`max_features`
    超参数在样本间似乎有所不同；在第一个集群中，通过 <math alttext="StartRoot number of features EndRoot"><msqrt><mrow><mtext>number</mtext>
    <mtext>of</mtext> <mtext>features</mtext></mrow></msqrt></math> 选择了最大特征数。
- en: 'Given the first cluster data, the AUC score of 0.5387 indicates that random
    forest has a better performance compared to the other models:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于第一个集群的数据，AUC得分为0.5387表明随机森林相比其他模型有更好的性能：
- en: '[PRE20]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The following code shows a random forest run based on the second cluster:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码展示了基于第二个集群的随机森林运行：
- en: '[PRE21]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Random forest has a much better predictive performance in the second cluster,
    with an AUC score of 0.5906\. Given the predictive performance of random forest,
    we can conclude that random forest does a better job of fitting the data. This
    is partly because of the low-dimensional characteristics of the data, as random
    forest turns out to be a good choice when data has low dimensionality and a large
    number of observations.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林在第二个集群中具有更好的预测性能，AUC得分为0.5906。鉴于随机森林的预测性能，我们可以得出结论，随机森林更擅长拟合数据。这部分是由于数据的低维特性，因为当数据维度低且观察数目大时，随机森林证明是一个不错的选择。
- en: Probability of Default Estimation with Neural Network
  id: totrans-316
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用神经网络进行违约概率估计
- en: Given the complexity of the probability of default estimation, unveiling the
    hidden structure of the data is a tough task, but the NN structure does a good
    job handling this, so it would be an ideal candidate model for such tasks. In
    setting up the NN model, `GridSearchCV` is used to optimize the number of hidden
    layers, optimization technique, and learning rate.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于违约概率估计的复杂性，揭示数据的隐藏结构是一项艰巨的任务，但神经网络结构在处理此问题时表现出色，因此它是这类任务的理想候选模型。在设置神经网络模型时，使用
    `GridSearchCV` 来优化隐藏层的数量、优化技术和学习率。
- en: In running the model, we first employ the `MLP` library, which allows us to
    control for many parameters, including hidden layer size, optimization technique
    (solver), and learning rate. Comparing the optimized hyperparameters of the two
    clusters indicates that the only difference is in the number of neurons in the
    hidden layer. Accordingly, we have larger number of neurons in the first hidden
    layer in cluster one. However, the neuron number is larger in the second hidden
    layer in the second cluster.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行模型时，我们首先使用`MLP`库，它允许我们控制许多参数，包括隐藏层大小、优化技术（求解器）和学习率。比较两个簇的优化超参数表明，唯一的区别在于隐藏层中神经元的数量。因此，在第一个簇中，第一个隐藏层中的神经元数量较大。但是，在第二个簇中，第二个隐藏层中的神经元数量较大。
- en: 'The following code suggests that data based on the first cluster is only a
    marginal improvement. In other words, the AUC moves to 0.5263, only slightly worse
    than random forest:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码表明基于第一簇数据的性能只有轻微改善。换句话说，AUC变为0.5263，仅比随机森林略差：
- en: '[PRE22]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The ROC-AUC score obtained from the second cluster is 0.6155, with two hidden
    layers endowed with 10 and 100 neurons, respectively. Moreover, the best optimization
    technique is `adam`, and optimum initial learning rate is 0.05\. This is the highest
    AUC score we’ve obtained, implying that the NN is able to capture the dynamics
    of the complex and nonlinear data, as shown here:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 从第二簇中获得的ROC-AUC得分为0.6155，具有10个和100个神经元的两个隐藏层。此外，最佳优化技术为`adam`，最佳初始学习率为0.05。这是我们获得的最高AUC得分，表明神经网络能够捕捉复杂和非线性数据的动态，如下所示：
- en: '[PRE23]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Probability of Default Estimation with Deep Learning
  id: totrans-323
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用深度学习进行违约概率估计
- en: Let’s now take a look at the performance of a deep learning model using TensorFlow
    via `KerasClassifier`, which enables us to control for the hyperparameters.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看看使用TensorFlow通过`KerasClassifier`控制超参数的深度学习模型的性能。
- en: The hyperparameters that we tune in this model are batch size, epoch, and dropout
    rate. As probability of default is a classification problem, the sigmoid activation
    function appears to be the optimal function to use. Deep learning is based on
    the structure of NNs, but provides a more complex structure, so it is expected
    to better capture the dynamics of data in a way that enables us to have better
    predictive performance.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这个模型中调整的超参数是批量大小、epoch和dropout率。由于违约概率是一个分类问题，Sigmoid激活函数似乎是最优的函数选择。深度学习基于神经网络的结构，但提供了更复杂的结构，因此预计能更好地捕捉数据的动态，从而使我们具有更好的预测性能。
- en: 'As we can see in the following code, the predictive performance of the second
    sample stumbles, however, with an AUC score of 0.5628:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 如下所示，第二个样本的预测性能出现了问题，然而，AUC得分为0.5628：
- en: '[PRE24]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[![1](assets/1.png)](#co_credit_risk_estimation_CO14-1)'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_credit_risk_estimation_CO14-1)'
- en: Importing `KerasClassifier` to run grid search
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 导入`KerasClassifier`以运行网格搜索
- en: '[![2](assets/2.png)](#co_credit_risk_estimation_CO14-2)'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_credit_risk_estimation_CO14-2)'
- en: Importing `logging` to suppress the warning messages
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 导入`logging`以抑制警告消息
- en: '[![3](assets/3.png)](#co_credit_risk_estimation_CO14-3)'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_credit_risk_estimation_CO14-3)'
- en: Naming TensorFlow for logging
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 为TensorFlow命名以记录日志
- en: Given the optimized hyperparameters of dropout, batch size, and epoch, the deep
    learning model produces the best performance among the models we have employed
    so far, with an AUC score of 0.5614\. The difference between MLPClassifier and
    deep learning models used in this chapter is the number of neurons in the hidden
    layer. Technically, these two models are deep learning models with different structures.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 给定了dropout、批量大小和epoch的优化超参数，深度学习模型在我们迄今使用的模型中产生了最佳性能，AUC得分为0.5614。MLPClassifier和本章中使用的深度学习模型之间的区别在于隐藏层中的神经元数量。从技术上讲，这两个模型都是具有不同结构的深度学习模型。
- en: '[PRE25]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[![1](assets/1.png)](#co_credit_risk_estimation_CO15-1)'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_credit_risk_estimation_CO15-1)'
- en: Calling a predefined function named `DL_risk` to run with optimized hyperparameters
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 调用一个名为`DL_risk`的预定义函数以优化超参数运行。
- en: '[![2](assets/2.png)](#co_credit_risk_estimation_CO15-2)'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_credit_risk_estimation_CO15-2)'
- en: Applying the grid search
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 应用网格搜索
- en: '[PRE26]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[![1](assets/1.png)](#co_credit_risk_estimation_CO16-1)'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_credit_risk_estimation_CO16-1)'
- en: Running deep learning algorithm with optimum hyperparameter of dropout rate
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 使用最优的dropout率超参数运行深度学习算法
- en: '[![2](assets/2.png)](#co_credit_risk_estimation_CO16-2)'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_credit_risk_estimation_CO16-2)'
- en: Running deep learning algorithm with optimum hyperparameter of batch size
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 运行具有最佳批大小超参数的深度学习算法
- en: '[![3](assets/3.png)](#co_credit_risk_estimation_CO16-3)'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_credit_risk_estimation_CO16-3)'
- en: Running deep learning algorithm with optimum hyperparameter of epoch number
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 运行具有最佳超参数（epoch number）的深度学习算法
- en: '[![4](assets/4.png)](#co_credit_risk_estimation_CO16-4)'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_credit_risk_estimation_CO16-4)'
- en: Computing the ROC-AUC score after flattening the prediction
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 在展平预测后计算 ROC-AUC 分数
- en: '[PRE27]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This finding confirms that DL models have become increasingly popular in financial
    modeling. In the industry, however, due to the opaque nature of network structure,
    this method is suggested for use in conjunction with traditional models.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 这一发现证实了 DL 模型在金融建模中变得越来越受欢迎。然而，在工业界，由于网络结构的不透明性，建议将该方法与传统模型结合使用。
- en: Conclusion
  id: totrans-351
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Credit risk analysis has a long tradition but is also still a challenging task
    to accomplish. This chapter attempted to present a brand new ML-based approach
    to tackling this problem and to getting better predictive performance. In the
    first part of the chapter, the main concepts related to credit risk were provided.
    Then, we applied a well-known parametric model, logistic regression, to German
    credit risk data. The performance of logistic regression was then compared with
    Bayesian estimation based on MAP and M-H. Finally, core machine learning models—namely
    SVC, random forest, and NNs with deep learning—were employed, and the performance
    of all models was compared.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 信用风险分析有着悠久的传统，但仍然是一项具有挑战性的任务。本章尝试提出一种全新的基于机器学习的方法来解决这个问题，并获得更好的预测性能。在本章的第一部分，提供了与信用风险相关的主要概念。然后，我们将一个众所周知的参数模型，即
    logistic 回归，应用到了德国信用风险数据中。然后，比较了 logistic 回归与基于 MAP 和 M-H 的贝叶斯估计的性能。最后，应用了核心机器学习模型—即
    SVC、随机森林和带有深度学习的 NNs，并比较了所有模型的性能。
- en: 'In the next chapter, a neglected dimension risk will be introduced: liquidity
    risk. The appreciation of liquidity risk has grown considerably since the 2007–2008
    financial crisis and has turned out to be an important part of risk management.'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，将介绍一个被忽视的维度风险：流动性风险。自2007-2008年金融危机以来，人们对流动性风险的重视程度大幅增长，并已成为风险管理的重要组成部分。
- en: References
  id: totrans-354
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Articles cited in this chapter:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 本章引用的文章：
- en: Basel Committee on Banking Supervision, and Bank for International Settlements.
    2000\. “Principles for the Management of Credit Risk.” Bank for International
    Settlements.
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 银行监督管理委员会（Basel Committee on Banking Supervision）和国际清算银行（Bank for International
    Settlements）。2000年。“信用风险管理原则。”国际清算银行。
- en: 'Le, Tuong, Mi Young Lee, Jun Ryeol Park, and Sung Wook Baik. 2018\. “Oversampling
    Techniques for Bankruptcy Prediction: Novel Features from a Transaction Dataset.”
    *Symmetry* 10 (4): 79.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Le, Tuong, Mi Young Lee, Jun Ryeol Park, 和 Sung Wook Baik. 2018\. “破产预测的过采样技术：来自交易数据集的新特征。”
    *对称性* 10 (4): 79。'
- en: 'Tibshirani, Robert, Guenther Walther, and Trevor Hastie. 2001\. “Estimating
    the Number of Clusters in a Data Set via the Gap Statistic.” *Journal of the Royal
    Statistical Society: Series B (Statistical Methodology)* 63 (2): 411-423.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tibshirani, Robert, Guenther Walther, 和 Trevor Hastie. 2001\. “通过间隙统计量估计数据集中的簇数。”
    *英国皇家统计学会杂志：B系列（统计方法）* 63 (2): 411-423。'
- en: 'Books and PhD theses cited in this chapter:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 本章引用的书籍和博士论文：
- en: 'Rokach, Lior, and Oded Maimon. 2005\. “Clustering methods.” In *Data Mining
    and Knowledge Discovery Handbook*, 321-352\. Boston: Springer.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rokach, Lior, 和 Oded Maimon. 2005\. “聚类方法。” 在 *数据挖掘与知识发现手册*，321-352\. 波士顿：斯普林格出版社。
- en: 'Wehrspohn, Uwe. 2002\. “Credit Risk Evaluation: Modeling-Analysis-Management.”
    PhD dissertation. Harvard.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wehrspohn, Uwe. 2002\. “信用风险评估：建模-分析-管理。”哈佛大学博士论文。
- en: ^([1](ch06.html#idm45737223570832-marker)) It is useful to run logistic regression
    to initialize results for priors in Bayesian estimation.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch06.html#idm45737223570832-marker)) 运行 logistic 回归以初始化贝叶斯估计中的先验结果是有用的。
