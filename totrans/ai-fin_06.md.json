["```py\nIn [1]: import numpy as np\n\nIn [2]: def f(x):\n            return 2 + 1 / 2 * x\n\nIn [3]: x = np.arange(-4, 5)\n        x\nOut[3]: array([-4, -3, -2, -1,  0,  1,  2,  3,  4])\n\nIn [4]: y = f(x)\n        y\nOut[4]: array([0. , 0.5, 1. , 1.5, 2. , 2.5, 3. , 3.5, 4. ])\n```", "```py\nIn [5]: x\nOut[5]: array([-4, -3, -2, -1,  0,  1,  2,  3,  4])\n\nIn [6]: y\nOut[6]: array([0. , 0.5, 1. , 1.5, 2. , 2.5, 3. , 3.5, 4. ])\n\nIn [7]: beta = np.cov(x, y, ddof=0)[0, 1] / x.var()  ![1](Images/1.png)\n        beta  ![1](Images/1.png)\nOut[7]: 0.49999999999999994\n\nIn [8]: alpha = y.mean() - beta * x.mean()  ![2](Images/2.png)\n        alpha  ![2](Images/2.png)\nOut[8]: 2.0\n\nIn [9]: y_ = alpha + beta * x  ![3](Images/3.png)\n\nIn [10]: np.allclose(y_, y)  ![4](Images/4.png)\nOut[10]: True\n```", "```py\nIn [11]: import eikon as ek\n         import configparser\n\nIn [12]: c = configparser.ConfigParser()\n         c.read('../aiif.cfg')\n         ek.set_app_key(c['eikon']['app_id'])\n         2020-08-04 10:30:18,059 P[14938] [MainThread 4521459136] Error on handshake\n          port 9000 : ReadTimeout(ReadTimeout())\n```", "```py\nIn [14]: symbols = ['AAPL.O', 'MSFT.O', 'NFLX.O', 'AMZN.O']  ![1](Images/1.png)\n\nIn [15]: data = ek.get_timeseries(symbols,\n                                  fields='CLOSE',\n                                  start_date='2019-07-01',\n                                  end_date='2020-07-01')  ![2](Images/2.png)\n\nIn [16]: data.info()  ![3](Images/3.png)\n         <class 'pandas.core.frame.DataFrame'>\n         DatetimeIndex: 254 entries, 2019-07-01 to 2020-07-01\n         Data columns (total 4 columns):\n          #   Column  Non-Null Count  Dtype\n         ---  ------  --------------  -----\n          0   AAPL.O  254 non-null    float64\n          1   MSFT.O  254 non-null    float64\n          2   NFLX.O  254 non-null    float64\n          3   AMZN.O  254 non-null    float64\n         dtypes: float64(4)\n         memory usage: 9.9 KB\n\nIn [17]: data.tail()  ![4](Images/4.png)\nOut[17]: CLOSE       AAPL.O  MSFT.O  NFLX.O   AMZN.O\n         Date\n         2020-06-25  364.84  200.34  465.91  2754.58\n         2020-06-26  353.63  196.33  443.40  2692.87\n         2020-06-29  361.78  198.44  447.24  2680.38\n         2020-06-30  364.80  203.51  455.04  2758.82\n         2020-07-01  364.11  204.70  485.64  2878.70\n```", "```py\nIn [18]: data = ek.get_timeseries('AMZN.O',\n                                  fields='*',\n                                  start_date='2020-08-03',\n                                  end_date='2020-08-04',\n                                  interval='minute')  ![1](Images/1.png)\n\nIn [19]: data.info()\n         <class 'pandas.core.frame.DataFrame'>\n         DatetimeIndex: 911 entries, 2020-08-03 08:01:00 to 2020-08-04 00:00:00\n         Data columns (total 6 columns):\n          #   Column  Non-Null Count  Dtype\n         ---  ------  --------------  -----\n          0   HIGH    911 non-null    float64\n          1   LOW     911 non-null    float64\n          2   OPEN    911 non-null    float64\n          3   CLOSE   911 non-null    float64\n          4   COUNT   911 non-null    float64\n          5   VOLUME  911 non-null    float64\n         dtypes: float64(6)\n         memory usage: 49.8 KB\n\nIn [20]: data.head()\nOut[20]: AMZN.O                  HIGH      LOW     OPEN    CLOSE  COUNT  VOLUME\n         Date\n         2020-08-03 08:01:00  3190.00  3176.03  3176.03  3178.17   18.0   383.0\n         2020-08-03 08:02:00  3183.02  3176.03  3180.00  3177.01   15.0   513.0\n         2020-08-03 08:03:00  3179.91  3177.05  3179.91  3177.05    5.0    14.0\n         2020-08-03 08:04:00  3184.00  3179.91  3179.91  3184.00    8.0   102.0\n         2020-08-03 08:05:00  3184.91  3182.91  3183.30  3184.00   12.0   403.0\n```", "```py\nIn [21]: data_grid, err = ek.get_data(['AAPL.O', 'IBM', 'GOOG.O', 'AMZN.O'],\n                                      ['TR.TotalReturnYTD', 'TR.WACCBeta',\n                                       'YRHIGH', 'YRLOW',\n                                       'TR.Ebitda', 'TR.GrossProfit'])  ![1](Images/1.png)\n\nIn [22]: data_grid\nOut[22]:   Instrument  YTD Total Return      Beta   YRHIGH      YRLOW        EBITDA  \\\n         0     AAPL.O         49.141271  1.221249   425.66   192.5800  7.647700e+10\n         1        IBM         -5.019570  1.208156   158.75    90.5600  1.898600e+10\n         2     GOOG.O         10.278829  1.067084  1586.99  1013.5361  4.757900e+10\n         3     AMZN.O         68.406897  1.338106  3344.29  1626.0318  3.025600e+10\n\n            Gross Profit\n         0   98392000000\n         1   36488000000\n         2   89961000000\n         3  114986000000\n```", "```py\nIn [23]: import tpqoa\n\nIn [24]: oa = tpqoa.tpqoa('../aiif.cfg')  ![1](Images/1.png)\n\nIn [25]: oa.stream_data('BTC_USD', stop=5)  ![2](Images/2.png)\n         2020-08-04T08:30:38.621075583Z 11298.8 11334.8\n         2020-08-04T08:30:50.485678488Z 11298.3 11334.3\n         2020-08-04T08:30:50.801666847Z 11297.3 11333.3\n         2020-08-04T08:30:51.326269990Z 11296.0 11332.0\n         2020-08-04T08:30:54.423973431Z 11296.6 11332.6\n```", "```py\nIn [26]: data = ek.get_timeseries('AAPL.O',\n                                  fields='*',\n                                  start_date='2020-08-03 15:00:00',\n                                  end_date='2020-08-03 16:00:00',\n                                  interval='tick')  ![1](Images/1.png)\n\nIn [27]: data.info()\n         <class 'pandas.core.frame.DataFrame'>\n         DatetimeIndex: 50000 entries, 2020-08-03 15:26:24.889000 to 2020-08-03\n          15:59:59.762000\n         Data columns (total 2 columns):\n          #   Column  Non-Null Count  Dtype\n         ---  ------  --------------  -----\n          0   VALUE   49953 non-null  float64\n          1   VOLUME  50000 non-null  float64\n         dtypes: float64(2)\n         memory usage: 1.1 MB\n\nIn [28]: data.head()\nOut[28]: AAPL.O                    VALUE  VOLUME\n         Date\n         2020-08-03 15:26:24.889  439.06   175.0\n         2020-08-03 15:26:24.889  439.08     3.0\n         2020-08-03 15:26:24.890  439.08   100.0\n         2020-08-03 15:26:24.890  439.08     5.0\n         2020-08-03 15:26:24.899  439.10    35.0\n```", "```py\nIn [29]: news = ek.get_news_headlines('R:TSLA.O PRODUCTION',\n                                  date_from='2020-06-01',\n                                  date_to='2020-08-01',\n                                  count=7\n                                 )  ![1](Images/1.png)\n\nIn [30]: news\nOut[30]:                                           versionCreated  \\\n         2020-07-29 11:02:31.276 2020-07-29 11:02:31.276000+00:00\n         2020-07-28 00:59:48.000        2020-07-28 00:59:48+00:00\n         2020-07-23 21:20:36.090 2020-07-23 21:20:36.090000+00:00\n         2020-07-23 08:22:17.000        2020-07-23 08:22:17+00:00\n         2020-07-23 07:08:48.000        2020-07-23 07:46:56+00:00\n         2020-07-23 00:55:54.000        2020-07-23 00:55:54+00:00\n         2020-07-22 21:35:42.640 2020-07-22 22:13:26.597000+00:00\n\n                                                                          text  \\\n         2020-07-29 11:02:31.276  Tesla Launches Hiring Spree in China as It Pre...\n         2020-07-28 00:59:48.000    Tesla hiring in Shanghai as production ramps up\n         2020-07-23 21:20:36.090     Tesla speeds up Model 3 production in Shanghai\n         2020-07-23 08:22:17.000  UPDATE 1-'Please mine more nickel,' Musk urges...\n         2020-07-23 07:08:48.000  'Please mine more nickel,' Musk urges as Tesla...\n         2020-07-23 00:55:54.000  USA-Tesla choisit le Texas pour la production ...\n         2020-07-22 21:35:42.640  TESLA INC - THE REAL LIMITATION ON TESLA GROWT...\n\n                                                                       storyId  \\\n         2020-07-29 11:02:31.276  urn:newsml:reuters.com:20200729:nCXG3W8s9X:1\n         2020-07-28 00:59:48.000  urn:newsml:reuters.com:20200728:nL3N2EY3PG:8\n         2020-07-23 21:20:36.090  urn:newsml:reuters.com:20200723:nNRAcf1v8f:1\n         2020-07-23 08:22:17.000  urn:newsml:reuters.com:20200723:nL3N2EU1P9:1\n         2020-07-23 07:08:48.000  urn:newsml:reuters.com:20200723:nL3N2EU0HH:1\n         2020-07-23 00:55:54.000  urn:newsml:reuters.com:20200723:nL5N2EU03M:1\n         2020-07-22 21:35:42.640  urn:newsml:reuters.com:20200722:nFWN2ET120:2\n\n                                 sourceCode\n         2020-07-29 11:02:31.276  NS:CAIXIN\n         2020-07-28 00:59:48.000    NS:RTRS\n         2020-07-23 21:20:36.090  NS:SOUTHC\n         2020-07-23 08:22:17.000    NS:RTRS\n         2020-07-23 07:08:48.000    NS:RTRS\n         2020-07-23 00:55:54.000    NS:RTRS\n         2020-07-22 21:35:42.640    NS:RTRS\n\nIn [31]: storyId = news['storyId'][1]  ![2](Images/2.png)\n\nIn [32]: from IPython.display import HTML\n\nIn [33]: HTML(ek.get_news_story(storyId)[:1148])  ![3](Images/3.png)\nOut[33]: <IPython.core.display.HTML object>\n```", "```py\nJan 06, 2020\n\nTesla, Inc.TSLA registered record production and deliveries of 104,891 and\n112,000 vehicles, respectively, in the fourth quarter of 2019.\n\nNotably, the company's Model S/X and Model 3 reported record production and\ndeliveries in the fourth quarter. The Model S/X division recorded production\nand delivery volume of 17,933 and 19,450 vehicles, respectively. The Model 3\ndivision registered production of 86,958 vehicles, while 92,550 vehicles were\ndelivered.\n\nIn 2019, Tesla delivered 367,500 vehicles, reflecting an increase of 50%, year\nover year, and nearly in line with the company's full-year guidance of 360,000\nvehicles.\n```", "```py\nIn [34]: import nlp  ![1](Images/1.png)\n         import requests\n\nIn [35]: sources = [\n             'https://nr.apple.com/dE0b1T5G3u',  # iPad Pro\n             'https://nr.apple.com/dE4c7T6g1K',  # MacBook Air\n             'https://nr.apple.com/dE4q4r8A2A',  # Mac Mini\n         ]  ![2](Images/2.png)\n\nIn [36]: html = [requests.get(url).text for url in sources]  ![3](Images/3.png)\n\nIn [37]: data = [nlp.clean_up_text(t) for t in html]  ![4](Images/4.png)\n\nIn [38]: data[0][536:1001]  ![5](Images/5.png)\nOut[38]: ' display, powerful a12x bionic chip and face id introducing the new ipad pro\n          with all-screen design and next-generation performance. new york apple today\n          introduced the new ipad pro with all-screen design and next-generation\n          performance, marking the biggest change to ipad ever. the all-new design\n          pushes 11-inch and 12.9-inch liquid retina displays to the edges of ipad pro\n          and integrates face id to securely unlock ipad with just a glance.1 the a12x\n          bionic chip w'\n```", "```py\nIn [39]: from twitter import Twitter, OAuth\n\nIn [40]: t = Twitter(auth=OAuth(c['twitter']['access_token'],\n                                c['twitter']['access_secret_token'],\n                                c['twitter']['api_key'],\n                                c['twitter']['api_secret_key']),\n                     retry=True)  ![1](Images/1.png)\n\nIn [41]: l = t.statuses.home_timeline(count=5)  ![2](Images/2.png)\n\nIn [42]: for e in l:\n             print(e['text'])  ![2](Images/2.png)\n         The Bank of England is effectively subsidizing polluting industries in its\n          pandemic rescue program, a think tank sa… https://t.co/Fq5jl2CIcp\n         Cool shared task: mining scientific contributions (by @SeeTedTalk @SoerenAuer\n          and Jennifer D'Souza)\n         https://t.co/dm56DMUrWm\n         Twelve people were hospitalized in Wyoming on Monday after a hot air balloon\n          crash, officials said.\n\n         Three hot air… https://t.co/EaNBBRXVar\n         President Trump directed controversial Pentagon pick into new role with\n          similar duties after nomination failed https://t.co/ZyXpPcJkcQ\n         Company announcement: Revolut launches Open Banking for its 400,000 Italian...\n          https://t.co/OfvbgwbeJW #fintech\n\nIn [43]: l = t.statuses.user_timeline(screen_name='dyjh', count=5)  ![3](Images/3.png)\n\nIn [44]: for e in l:\n             print(e['text'])  ![3](Images/3.png)\n         #Python for #AlgoTrading (focus on the process) &amp; #AI in #Finance (focus\n          on prediction methods) will complement eac… https://t.co/P1s8fXCp42\n         Currently putting finishing touches on #AI in #Finance (@OReillyMedia). Book\n          going into production shortly. https://t.co/JsOSA3sfBL\n         Chinatown Is Coming Back, One Noodle at a Time https://t.co/In5kXNeVc5\n         Alt data industry balloons as hedge funds strive for Covid edge via @FT |\n         \"We remain of the view that alternative d… https://t.co/9HtUOjoEdz\n         @Wolf_Of_BTC Just follow me on Twitter (or LinkedIn). Then you will notice for\n          sure when it is out.\n```", "```py\nIn [45]: d = t.search.tweets(q='#Python', count=7)  ![1](Images/1.png)\n\nIn [46]: for e in d['statuses']:\n             print(e['text'])  ![1](Images/1.png)\n         RT @KirkDBorne: #AI is Reshaping Programming — Tips on How to Stay on Top:\n          https://t.co/CFNu1i352C\n         ——\n         Courses:\n         1: #MachineLearning — Jupyte…\n         RT @reuvenmlerner: Today, a #Python student's code didn't print:\n\n         x = 5\n         if x == 5:\n             print: ('yes!')\n\n         There was a typo, namely : after pr…\n         RT @GavLaaaaaaaa: Javascript Does Not Need a StringBuilder\n          https://t.co/aS7NzHLO65 #programming #softwareengineering #bigdata\n          #datascience…\n         RT @CodeFlawCo: It is necessary to publish regular updates on Twitter\n          #programmer #coder #developer #technology RT @pak_aims: Learning to C…\n         RT @GavLaaaaaaaa: Javascript Does Not Need a StringBuilder\n          https://t.co/aS7NzHLO65 #programming #softwareengineering #bigdata\n          #datascience…\n```", "```py\nIn [47]: l = t.statuses.user_timeline(screen_name='elonmusk', count=50)  ![1](Images/1.png)\n\nIn [48]: tl = [e['text'] for e in l]  ![2](Images/2.png)\n\nIn [49]: tl[:5]  ![3](Images/3.png)\nOut[49]: ['@flcnhvy @Lindw0rm @cleantechnica True',\n          '@Lindw0rm @cleantechnica Highly likely down the road',\n          '@cleantechnica True fact',\n         '@NASASpaceflight Scrubbed for the day. A Raptor turbopump spin start valve\n          didn’t open, triggering an automatic abo… https://t.co/QDdlNXFgJg',\n          '@Erdayastronaut I’m in the Boca control room. Hop attempt in ~33 minutes.']\n\nIn [50]: wc = nlp.generate_word_cloud(' '.join(tl), 35,\n                     name='../../images/ch04/musk_twitter_wc.png'\n                     )  ![4](Images/4.png)\n```", "```py\nIn [51]: import numpy as np\n         import pandas as pd\n         from pylab import plt, mpl\n         from scipy.optimize import minimize\n         plt.style.use('seaborn')\n         mpl.rcParams['savefig.dpi'] = 300\n         mpl.rcParams['font.family'] = 'serif'\n         np.set_printoptions(precision=5, suppress=True,\n                            formatter={'float': lambda x: f'{x:6.3f}'})\n\nIn [52]: url = 'http://hilpisch.com/aiif_eikon_eod_data.csv'  ![1](Images/1.png)\n\nIn [53]: raw = pd.read_csv(url, index_col=0, parse_dates=True).dropna()  ![1](Images/1.png)\n\nIn [54]: raw.info()  ![1](Images/1.png)\n         <class 'pandas.core.frame.DataFrame'>\n         DatetimeIndex: 2516 entries, 2010-01-04 to 2019-12-31\n         Data columns (total 12 columns):\n          #   Column  Non-Null Count  Dtype\n         ---  ------  --------------  -----\n          0   AAPL.O  2516 non-null   float64\n          1   MSFT.O  2516 non-null   float64\n          2   INTC.O  2516 non-null   float64\n          3   AMZN.O  2516 non-null   float64\n          4   GS.N    2516 non-null   float64\n          5   SPY     2516 non-null   float64\n          6   .SPX    2516 non-null   float64\n          7   .VIX    2516 non-null   float64\n          8   EUR=    2516 non-null   float64\n          9   XAU=    2516 non-null   float64\n          10  GDX     2516 non-null   float64\n          11  GLD     2516 non-null   float64\n         dtypes: float64(12)\n         memory usage: 255.5 KB\n\nIn [55]: symbols = ['AAPL.O', 'MSFT.O', 'INTC.O', 'AMZN.O', 'GLD']  ![2](Images/2.png)\n\nIn [56]: rets = np.log(raw[symbols] / raw[symbols].shift(1)).dropna()  ![3](Images/3.png)\n\nIn [57]: (raw[symbols] / raw[symbols].iloc[0]).plot(figsize=(10, 6));  ![4](Images/4.png)\n```", "```py\nIn [58]: weights = len(rets.columns) * [1 / len(rets.columns)]  ![1](Images/1.png)\n\nIn [59]: def port_return(rets, weights):\n             return np.dot(rets.mean(), weights) * 252  ![2](Images/2.png)\n\nIn [60]: port_return(rets, weights)  ![2](Images/2.png)\nOut[60]: 0.15694764653018106\n\nIn [61]: def port_volatility(rets, weights):\n             return np.dot(weights, np.dot(rets.cov() * 252 , weights)) ** 0.5  ![3](Images/3.png)\n\nIn [62]: port_volatility(rets, weights)  ![3](Images/3.png)\nOut[62]: 0.16106507848480675\n\nIn [63]: def port_sharpe(rets, weights):\n             return port_return(rets, weights) / port_volatility(rets, weights)  ![4](Images/4.png)\n\nIn [64]: port_sharpe(rets, weights)  ![4](Images/4.png)\nOut[64]: 0.97443622172255\n```", "```py\nIn [65]: w = np.random.random((1000, len(symbols)))  ![1](Images/1.png)\n         w = (w.T / w.sum(axis=1)).T  ![1](Images/1.png)\n\nIn [66]: w[:5]  ![1](Images/1.png)\nOut[66]: array([[ 0.184,  0.157,  0.227,  0.353,  0.079],\n                [ 0.207,  0.282,  0.258,  0.023,  0.230],\n                [ 0.313,  0.284,  0.051,  0.340,  0.012],\n                [ 0.238,  0.181,  0.145,  0.191,  0.245],\n                [ 0.246,  0.256,  0.315,  0.181,  0.002]])\n\nIn [67]: pvr = [(port_volatility(rets[symbols], weights),\n                 port_return(rets[symbols], weights))\n                for weights in w]  ![2](Images/2.png)\n         pvr = np.array(pvr)  ![2](Images/2.png)\n\nIn [68]: psr = pvr[:, 1] / pvr[:, 0]  ![3](Images/3.png)\n\nIn [69]: plt.figure(figsize=(10, 6))\n         fig = plt.scatter(pvr[:, 0], pvr[:, 1],\n                           c=psr, cmap='coolwarm')\n         cb = plt.colorbar(fig)\n         cb.set_label('Sharpe ratio')\n         plt.xlabel('expected volatility')\n         plt.ylabel('expected return')\n         plt.title(' | '.join(symbols));\n```", "```py\nIn [70]: bnds = len(symbols) * [(0, 1),]  ![1](Images/1.png)\n         bnds  ![1](Images/1.png)\nOut[70]: [(0, 1), (0, 1), (0, 1), (0, 1), (0, 1)]\n\nIn [71]: cons = {'type': 'eq', 'fun': lambda weights: weights.sum() - 1}  ![2](Images/2.png)\n\nIn [72]: opt_weights = {}\n         for year in range(2010, 2019):\n             rets_ = rets[symbols].loc[f'{year}-01-01':f'{year}-12-31']  ![3](Images/3.png)\n             ow = minimize(lambda weights: -port_sharpe(rets_, weights),\n                           len(symbols) * [1 / len(symbols)],\n                           bounds=bnds,\n                           constraints=cons)['x']  ![4](Images/4.png)\n             opt_weights[year] = ow  ![5](Images/5.png)\n\nIn [73]: opt_weights  ![5](Images/5.png)\nOut[73]: {2010: array([ 0.366,  0.000,  0.000,  0.056,  0.578]),\n          2011: array([ 0.543,  0.000,  0.077,  0.000,  0.380]),\n          2012: array([ 0.324,  0.000,  0.000,  0.471,  0.205]),\n          2013: array([ 0.012,  0.305,  0.219,  0.464,  0.000]),\n          2014: array([ 0.452,  0.115,  0.419,  0.000,  0.015]),\n          2015: array([ 0.000,  0.000,  0.000,  1.000,  0.000]),\n          2016: array([ 0.150,  0.260,  0.000,  0.058,  0.533]),\n          2017: array([ 0.231,  0.203,  0.031,  0.109,  0.426]),\n          2018: array([ 0.000,  0.295,  0.000,  0.705,  0.000])}\n```", "```py\nIn [74]: res = pd.DataFrame()\n         for year in range(2010, 2019):\n             rets_ = rets[symbols].loc[f'{year}-01-01':f'{year}-12-31']\n             epv = port_volatility(rets_, opt_weights[year])  ![1](Images/1.png)\n             epr = port_return(rets_, opt_weights[year])  ![1](Images/1.png)\n             esr = epr / epv  ![1](Images/1.png)\n             rets_ = rets[symbols].loc[f'{year + 1}-01-01':f'{year + 1}-12-31']\n             rpv = port_volatility(rets_, opt_weights[year]) ![2](Images/2.png)\n             rpr = port_return(rets_, opt_weights[year])  ![2](Images/2.png)\n             rsr = rpr / rpv  ![2](Images/2.png)\n             res = res.append(pd.DataFrame({'epv': epv, 'epr': epr, 'esr': esr,\n                                            'rpv': rpv, 'rpr': rpr, 'rsr': rsr},\n                                           index=[year + 1]))\n\nIn [75]: res\nOut[75]:            epv       epr       esr       rpv       rpr       rsr\n         2011  0.157440  0.303003  1.924564  0.160622  0.133836  0.833235\n         2012  0.173279  0.169321  0.977156  0.182292  0.161375  0.885256\n         2013  0.202460  0.278459  1.375378  0.168714  0.166897  0.989228\n         2014  0.181544  0.368961  2.032353  0.197798  0.026830  0.135645\n         2015  0.160340  0.309486  1.930190  0.211368 -0.024560 -0.116194\n         2016  0.326730  0.778330  2.382179  0.296565  0.103870  0.350242\n         2017  0.106148  0.090933  0.856663  0.079521  0.230630  2.900235\n         2018  0.086548  0.260702  3.012226  0.157337  0.038234  0.243004\n         2019  0.323796  0.228008  0.704174  0.207672  0.275819  1.328147\n\nIn [76]: res.mean()\nOut[76]: epv    0.190920\n         epr    0.309689\n         esr    1.688320\n         rpv    0.184654\n         rpr    0.123659\n         rsr    0.838755\n         dtype: float64\n```", "```py\nIn [77]: res[['epv', 'rpv']].corr()\nOut[77]:           epv       rpv\n         epv  1.000000  0.765733\n         rpv  0.765733  1.000000\n\nIn [78]: res[['epv', 'rpv']].plot(kind='bar', figsize=(10, 6),\n                 title='Expected vs. Realized Portfolio Volatility');\n```", "```py\nIn [79]: res[['epr', 'rpr']].corr()\nOut[79]:           epr       rpr\n         epr  1.000000 -0.350437\n         rpr -0.350437  1.000000\n\nIn [80]: res[['epr', 'rpr']].plot(kind='bar', figsize=(10, 6),\n                 title='Expected vs. Realized Portfolio Return');\n```", "```py\nIn [81]: res[['esr', 'rsr']].corr()\nOut[81]:           esr       rsr\n         esr  1.000000 -0.698607\n         rsr -0.698607  1.000000\n\nIn [82]: res[['esr', 'rsr']].plot(kind='bar', figsize=(10, 6),\n                 title='Expected vs. Realized Sharpe Ratio');\n```", "```py\nIn [83]: r = 0.005  ![1](Images/1.png)\n\nIn [84]: market = '.SPX'  ![2](Images/2.png)\n\nIn [85]: rets = np.log(raw / raw.shift(1)).dropna()\n\nIn [86]: res = pd.DataFrame()\n\nIn [87]: for sym in rets.columns[:4]:\n             print('\\n' + sym)\n             print(54 * '=')\n             for year in range(2010, 2019):\n                 rets_ = rets.loc[f'{year}-01-01':f'{year}-12-31']\n                 muM = rets_[market].mean() * 252\n                 cov = rets_.cov().loc[sym, market]  ![3](Images/3.png)\n                 var = rets_[market].var()  ![3](Images/3.png)\n                 beta = cov / var  ![3](Images/3.png)\n                 rets_ = rets.loc[f'{year + 1}-01-01':f'{year + 1}-12-31']\n                 muM = rets_[market].mean() * 252\n                 mu_capm = r + beta * (muM - r)  ![4](Images/4.png)\n                 mu_real = rets_[sym].mean() * 252  ![5](Images/5.png)\n                 res = res.append(pd.DataFrame({'symbol': sym,\n                                                'mu_capm': mu_capm,\n                                                'mu_real': mu_real},\n                                               index=[year + 1]),\n                                 sort=True)  ![6](Images/6.png)\n                 print('{} | beta: {:.3f} | mu_capm: {:6.3f} | mu_real: {:6.3f}'\n                       .format(year + 1, beta, mu_capm, mu_real))  ![6](Images/6.png)\n```", "```py\n         AAPL.O\n         ======================================================\n         2011 | beta: 1.052 | mu_capm: -0.000 | mu_real:  0.228\n         2012 | beta: 0.764 | mu_capm:  0.098 | mu_real:  0.275\n         2013 | beta: 1.266 | mu_capm:  0.327 | mu_real:  0.053\n         2014 | beta: 0.630 | mu_capm:  0.070 | mu_real:  0.320\n         2015 | beta: 0.833 | mu_capm: -0.005 | mu_real: -0.047\n         2016 | beta: 1.144 | mu_capm:  0.103 | mu_real:  0.096\n         2017 | beta: 1.009 | mu_capm:  0.180 | mu_real:  0.381\n         2018 | beta: 1.379 | mu_capm: -0.091 | mu_real: -0.071\n         2019 | beta: 1.252 | mu_capm:  0.316 | mu_real:  0.621\n\n         MSFT.O\n         ======================================================\n         2011 | beta: 0.890 | mu_capm:  0.001 | mu_real: -0.072\n         2012 | beta: 0.816 | mu_capm:  0.104 | mu_real:  0.029\n         2013 | beta: 1.109 | mu_capm:  0.287 | mu_real:  0.337\n         2014 | beta: 0.876 | mu_capm:  0.095 | mu_real:  0.216\n         2015 | beta: 0.955 | mu_capm: -0.007 | mu_real:  0.178\n         2016 | beta: 1.249 | mu_capm:  0.113 | mu_real:  0.113\n         2017 | beta: 1.224 | mu_capm:  0.217 | mu_real:  0.321\n         2018 | beta: 1.303 | mu_capm: -0.086 | mu_real:  0.172\n         2019 | beta: 1.442 | mu_capm:  0.364 | mu_real:  0.440\n\n         INTC.O\n         ======================================================\n         2011 | beta: 1.081 | mu_capm: -0.000 | mu_real:  0.142\n         2012 | beta: 0.842 | mu_capm:  0.108 | mu_real: -0.163\n         2013 | beta: 1.081 | mu_capm:  0.280 | mu_real:  0.230\n         2014 | beta: 0.883 | mu_capm:  0.096 | mu_real:  0.335\n         2015 | beta: 1.055 | mu_capm: -0.008 | mu_real: -0.052\n         2016 | beta: 1.009 | mu_capm:  0.092 | mu_real:  0.051\n         2017 | beta: 1.261 | mu_capm:  0.223 | mu_real:  0.242\n         2018 | beta: 1.163 | mu_capm: -0.076 | mu_real:  0.017\n         2019 | beta: 1.376 | mu_capm:  0.347 | mu_real:  0.243\n\n         AMZN.O\n         ======================================================\n         2011 | beta: 1.102 | mu_capm: -0.001 | mu_real: -0.039\n         2012 | beta: 0.958 | mu_capm:  0.122 | mu_real:  0.374\n         2013 | beta: 1.116 | mu_capm:  0.289 | mu_real:  0.464\n         2014 | beta: 1.262 | mu_capm:  0.135 | mu_real: -0.251\n         2015 | beta: 1.473 | mu_capm: -0.013 | mu_real:  0.778\n         2016 | beta: 1.122 | mu_capm:  0.102 | mu_real:  0.104\n         2017 | beta: 1.118 | mu_capm:  0.199 | mu_real:  0.446\n         2018 | beta: 1.300 | mu_capm: -0.086 | mu_real:  0.251\n         2019 | beta: 1.619 | mu_capm:  0.408 | mu_real:  0.207\n```", "```py\nIn [88]: sym = 'AMZN.O'\n\nIn [89]: res[res['symbol'] == sym].corr()\nOut[89]:           mu_capm   mu_real\n         mu_capm  1.000000 -0.004826\n         mu_real -0.004826  1.000000\n\nIn [90]: res[res['symbol'] == sym].plot(kind='bar',\n                         figsize=(10, 6), title=sym);\n```", "```py\nIn [91]: grouped = res.groupby('symbol').mean()\n         grouped\nOut[91]:          mu_capm   mu_real\n         symbol\n         AAPL.O  0.110855  0.206158\n         AMZN.O  0.128223  0.259395\n         INTC.O  0.117929  0.116180\n         MSFT.O  0.120844  0.192655\n\nIn [92]: grouped.plot(kind='bar', figsize=(10, 6), title='Average Values');\n```", "```py\nIn [93]: factors = ['.SPX', '.VIX', 'EUR=', 'XAU=']  ![1](Images/1.png)\n\nIn [94]: res = pd.DataFrame()\n\nIn [95]: np.set_printoptions(formatter={'float': lambda x: f'{x:5.2f}'})\n\nIn [96]: for sym in rets.columns[:4]:\n             print('\\n' + sym)\n             print(71 * '=')\n             for year in range(2010, 2019):\n                 rets_ = rets.loc[f'{year}-01-01':f'{year}-12-31']\n                 reg = np.linalg.lstsq(rets_[factors],\n                                       rets_[sym], rcond=-1)[0]  ![2](Images/2.png)\n                 rets_ = rets.loc[f'{year + 1}-01-01':f'{year + 1}-12-31']\n                 mu_apt = np.dot(rets_[factors].mean() * 252, reg)  ![3](Images/3.png)\n                 mu_real =  rets_[sym].mean() * 252  ![4](Images/4.png)\n                 res = res.append(pd.DataFrame({'symbol': sym,\n                                 'mu_apt': mu_apt, 'mu_real': mu_real},\n                                  index=[year + 1]))\n                 print('{} | fl: {} | mu_apt: {:6.3f} | mu_real: {:6.3f}'\n                       .format(year + 1, reg.round(2), mu_apt, mu_real))\n```", "```py\n         AAPL.O\n         =======================================================================\n         2011 | fl: [ 0.91 -0.04 -0.35  0.12] | mu_apt:  0.011 | mu_real:  0.228\n         2012 | fl: [ 0.76 -0.02 -0.24  0.05] | mu_apt:  0.099 | mu_real:  0.275\n         2013 | fl: [ 1.67  0.04 -0.56  0.10] | mu_apt:  0.366 | mu_real:  0.053\n         2014 | fl: [ 0.53 -0.00  0.02  0.16] | mu_apt:  0.050 | mu_real:  0.320\n         2015 | fl: [ 1.07  0.02  0.25  0.01] | mu_apt: -0.038 | mu_real: -0.047\n         2016 | fl: [ 1.21  0.01 -0.14 -0.02] | mu_apt:  0.110 | mu_real:  0.096\n         2017 | fl: [ 1.10  0.01 -0.15 -0.02] | mu_apt:  0.170 | mu_real:  0.381\n         2018 | fl: [ 1.06 -0.03 -0.15  0.12] | mu_apt: -0.088 | mu_real: -0.071\n         2019 | fl: [ 1.37  0.01 -0.20  0.13] | mu_apt:  0.364 | mu_real:  0.621\n\n         MSFT.O\n         =======================================================================\n         2011 | fl: [ 0.98  0.01  0.02 -0.11] | mu_apt: -0.008 | mu_real: -0.072\n         2012 | fl: [ 0.82  0.00 -0.03 -0.01] | mu_apt:  0.103 | mu_real:  0.029\n         2013 | fl: [ 1.14  0.00 -0.07 -0.01] | mu_apt:  0.294 | mu_real:  0.337\n         2014 | fl: [ 1.28  0.05  0.04  0.07] | mu_apt:  0.149 | mu_real:  0.216\n         2015 | fl: [ 1.20  0.03  0.05  0.01] | mu_apt: -0.016 | mu_real:  0.178\n         2016 | fl: [ 1.44  0.03 -0.17 -0.02] | mu_apt:  0.127 | mu_real:  0.113\n         2017 | fl: [ 1.33  0.01 -0.14  0.00] | mu_apt:  0.216 | mu_real:  0.321\n         2018 | fl: [ 1.10 -0.02 -0.14  0.22] | mu_apt: -0.087 | mu_real:  0.172\n         2019 | fl: [ 1.51  0.01 -0.16 -0.02] | mu_apt:  0.378 | mu_real:  0.440\n\n         INTC.O\n         =======================================================================\n         2011 | fl: [ 1.17  0.01  0.05 -0.13] | mu_apt: -0.010 | mu_real:  0.142\n         2012 | fl: [ 1.03  0.04  0.01  0.03] | mu_apt:  0.122 | mu_real: -0.163\n         2013 | fl: [ 1.06 -0.01 -0.10  0.01] | mu_apt:  0.267 | mu_real:  0.230\n         2014 | fl: [ 0.96  0.02  0.36 -0.02] | mu_apt:  0.063 | mu_real:  0.335\n         2015 | fl: [ 0.93 -0.01 -0.09  0.02] | mu_apt:  0.001 | mu_real: -0.052\n         2016 | fl: [ 1.02  0.00 -0.05  0.06] | mu_apt:  0.099 | mu_real:  0.051\n         2017 | fl: [ 1.41  0.02 -0.18  0.03] | mu_apt:  0.226 | mu_real:  0.242\n         2018 | fl: [ 1.12 -0.01 -0.11  0.17] | mu_apt: -0.076 | mu_real:  0.017\n         2019 | fl: [ 1.50  0.01 -0.34  0.30] | mu_apt:  0.431 | mu_real:  0.243\n\n         AMZN.O\n         =======================================================================\n         2011 | fl: [ 1.02 -0.03 -0.18 -0.14] | mu_apt: -0.016 | mu_real: -0.039\n         2012 | fl: [ 0.98 -0.01 -0.17 -0.09] | mu_apt:  0.117 | mu_real:  0.374\n         2013 | fl: [ 1.07 -0.00  0.09  0.00] | mu_apt:  0.282 | mu_real:  0.464\n         2014 | fl: [ 1.54  0.03  0.01 -0.08] | mu_apt:  0.176 | mu_real: -0.251\n         2015 | fl: [ 1.26 -0.02  0.45 -0.11] | mu_apt: -0.044 | mu_real:  0.778\n         2016 | fl: [ 1.06 -0.00 -0.15 -0.04] | mu_apt:  0.099 | mu_real:  0.104\n         2017 | fl: [ 0.94 -0.02  0.12 -0.03] | mu_apt:  0.185 | mu_real:  0.446\n         2018 | fl: [ 0.90 -0.04 -0.25  0.28] | mu_apt: -0.085 | mu_real:  0.251\n         2019 | fl: [ 1.99  0.05 -0.37  0.12] | mu_apt:  0.506 | mu_real:  0.207\n```", "```py\nIn [97]: sym = 'AMZN.O'\n\nIn [98]: res[res['symbol'] == sym].corr()\nOut[98]:            mu_apt   mu_real\n         mu_apt   1.000000 -0.098281\n         mu_real -0.098281  1.000000\n\nIn [99]: res[res['symbol'] == sym].plot(kind='bar',\n                         figsize=(10, 6), title=sym);\n```", "```py\nIn [100]: grouped = res.groupby('symbol').mean()\n          grouped\nOut[100]:           mu_apt   mu_real\n          symbol\n          AAPL.O  0.116116  0.206158\n          AMZN.O  0.135528  0.259395\n          INTC.O  0.124811  0.116180\n          MSFT.O  0.128441  0.192655\n\nIn [101]: grouped.plot(kind='bar', figsize=(10, 6), title='Average Values');\n```", "```py\nIn [102]: factors = pd.read_csv('http://hilpisch.com/aiif_eikon_eod_factors.csv',\n                                index_col=0, parse_dates=True) ![1](Images/1.png)\n\nIn [103]: (factors / factors.iloc[0]).plot(figsize=(10, 6));  ![2](Images/2.png)\n```", "```py\nIn [104]: start = '2017-01-01'  ![1](Images/1.png)\n          end = '2020-01-01'  ![1](Images/1.png)\n\nIn [105]: retsd = rets.loc[start:end].copy()  ![2](Images/2.png)\n          retsd.dropna(inplace=True)  ![2](Images/2.png)\n\nIn [106]: retsf = np.log(factors / factors.shift(1))  ![3](Images/3.png)\n          retsf = retsf.loc[start:end]  ![3](Images/3.png)\n          retsf.dropna(inplace=True)  ![3](Images/3.png)\n          retsf = retsf.loc[retsd.index].dropna()  ![3](Images/3.png)\n\nIn [107]: retsf.corr()  ![4](Images/4.png)\nOut[107]:               market      size  volatility     value      risk    growth  \\\n          market      1.000000  0.935867    0.845010  0.964124  0.947150  0.959038\n          size        0.935867  1.000000    0.791767  0.965739  0.983238  0.835477\n          volatility  0.845010  0.791767    1.000000  0.778294  0.865467  0.818280\n          value       0.964124  0.965739    0.778294  1.000000  0.958359  0.864222\n          risk        0.947150  0.983238    0.865467  0.958359  1.000000  0.858546\n          growth      0.959038  0.835477    0.818280  0.864222  0.858546  1.000000\n          momentum    0.928705  0.796420    0.819585  0.818796  0.825563  0.952956\n\n                      momentum\n          market      0.928705\n          size        0.796420\n          volatility  0.819585\n          value       0.818796\n          risk        0.825563\n          growth      0.952956\n          momentum    1.000000\n```", "```py\nIn [108]: res = pd.DataFrame()\n\nIn [109]: np.set_printoptions(formatter={'float': lambda x: f'{x:5.2f}'})\n\nIn [110]: split = int(len(retsf) * 0.5)\n          for sym in rets.columns[:4]:\n              print('\\n' + sym)\n              print(74 * '=')\n              retsf_, retsd_ = retsf.iloc[:split], retsd.iloc[:split]\n              reg = np.linalg.lstsq(retsf_, retsd_[sym], rcond=-1)[0]\n              retsf_, retsd_ = retsf.iloc[split:], retsd.iloc[split:]\n              mu_apt = np.dot(retsf_.mean() * 252, reg)\n              mu_real =  retsd_[sym].mean() * 252\n              res = res.append(pd.DataFrame({'mu_apt': mu_apt,\n                              'mu_real': mu_real}, index=[sym,]),\n                              sort=True)\n              print('fl: {} | apt: {:.3f} | real: {:.3f}'\n                    .format(reg.round(1), mu_apt, mu_real))\n\n          AAPL.O\n          ==========================================================================\n          fl: [ 2.30  2.80 -0.70 -1.40 -4.20  2.00 -0.20] | apt: 0.115 | real: 0.301\n\n          MSFT.O\n          ==========================================================================\n          fl: [ 1.50  0.00  0.10 -1.30 -1.40  0.80  1.00] | apt: 0.181 | real: 0.304\n\n          INTC.O\n          ==========================================================================\n          fl: [-3.10  1.60  0.40  1.30 -2.60  2.50  1.10] | apt: 0.186 | real: 0.118\n\n          AMZN.O\n          ==========================================================================\n          fl: [ 9.10  3.30 -1.00 -7.10 -3.10 -1.80  1.20] | apt: 0.019 | real: 0.050\n\nIn [111]: res.plot(kind='bar', figsize=(10, 6));\n```", "```py\nIn [112]: sym\nOut[112]: 'AMZN.O'\n\nIn [113]: rets_sym = np.dot(retsf_, reg)  ![1](Images/1.png)\n\nIn [114]: rets_sym = pd.DataFrame(rets_sym,\n                                  columns=[sym + '_apt'],\n                                  index=retsf_.index)  ![2](Images/2.png)\n\nIn [115]: rets_sym[sym + '_real'] = retsd_[sym]  ![3](Images/3.png)\n\nIn [116]: rets_sym.mean() * 252  ![4](Images/4.png)\nOut[116]: AMZN.O_apt     0.019401\n          AMZN.O_real    0.050344\n          dtype: float64\n\nIn [117]: rets_sym.std() * 252 ** 0.5  ![5](Images/5.png)\nOut[117]: AMZN.O_apt     0.270995\n          AMZN.O_real    0.307653\n          dtype: float64\n\nIn [118]: rets_sym.corr()  ![6](Images/6.png)\nOut[118]:              AMZN.O_apt  AMZN.O_real\n          AMZN.O_apt     1.000000     0.832218\n          AMZN.O_real    0.832218     1.000000\n\nIn [119]: rets_sym.cumsum().apply(np.exp).plot(figsize=(10, 6));\n```", "```py\nIn [120]: rets_sym['same'] = (np.sign(rets_sym[sym + '_apt']) ==\n                              np.sign(rets_sym[sym + '_real']))\n\nIn [121]: rets_sym['same'].value_counts()\nOut[121]: True     288\n          False     89\n          Name: same, dtype: int64\n\nIn [122]: rets_sym['same'].value_counts()[True] / len(rets_sym)\nOut[122]: 0.7639257294429708\n```", "```py\nIn [1]: import numpy as np\n        import pandas as pd\n        from pylab import plt, mpl\n        np.random.seed(100)\n        plt.style.use('seaborn')\n        mpl.rcParams['savefig.dpi'] = 300\n        mpl.rcParams['font.family'] = 'serif'\n\nIn [2]: N = 10000\n\nIn [3]: snrn = np.random.standard_normal(N)  ![1](Images/1.png)\n        snrn -= snrn.mean()  ![2](Images/2.png)\n        snrn /= snrn.std()  ![3](Images/3.png)\n\nIn [4]: round(snrn.mean(), 4)  ![2](Images/2.png)\nOut[4]: -0.0\n\nIn [5]: round(snrn.std(), 4)  ![3](Images/3.png)\nOut[5]: 1.0\n\nIn [6]: plt.figure(figsize=(10, 6))\n        plt.hist(snrn, bins=35);\n```", "```py\nIn [7]: numbers = np.ones(N) * 1.5  ![1](Images/1.png)\n        split = int(0.25 * N)  ![1](Images/1.png)\n        numbers[split:3 * split] = -1  ![1](Images/1.png)\n        numbers[3 * split:4 * split] = 0  ![1](Images/1.png)\n\nIn [8]: numbers -= numbers.mean()  ![2](Images/2.png)\n        numbers /= numbers.std()  ![3](Images/3.png)\n\nIn [9]: round(numbers.mean(), 4)  ![2](Images/2.png)\nOut[9]: 0.0\n\nIn [10]: round(numbers.std(), 4)  ![3](Images/3.png)\nOut[10]: 1.0\n\nIn [11]: plt.figure(figsize=(10, 6))\n         plt.hist(numbers, bins=35);\n```", "```py\nIn [12]: import math\n         import scipy.stats as scs\n         import statsmodels.api as sm\n\nIn [13]: def dN(x, mu, sigma):\n             ''' Probability density function of a normal random variable x.\n             '''\n             z = (x - mu) / sigma\n             pdf = np.exp(-0.5 * z ** 2) / math.sqrt(2 * math.pi * sigma ** 2)\n             return pdf\n\nIn [14]: def return_histogram(rets, title=''):\n             ''' Plots a histogram of the returns.\n             '''\n             plt.figure(figsize=(10, 6))\n             x = np.linspace(min(rets), max(rets), 100)\n             plt.hist(np.array(rets), bins=50,\n                      density=True, label='frequency')  ![1](Images/1.png)\n             y = dN(x, np.mean(rets), np.std(rets))  ![2](Images/2.png)\n             plt.plot(x, y, linewidth=2, label='PDF')  ![2](Images/2.png)\n             plt.xlabel('log returns')\n             plt.ylabel('frequency/probability')\n             plt.title(title)\n             plt.legend()\n```", "```py\nIn [15]: return_histogram(snrn)\n```", "```py\nIn [16]: return_histogram(numbers)\n```", "```py\nIn [17]: def return_qqplot(rets, title=''):\n             ''' Generates a Q-Q plot of the returns.\n '''\n             fig = sm.qqplot(rets, line='s', alpha=0.5)\n             fig.set_size_inches(10, 6)\n             plt.title(title)\n             plt.xlabel('theoretical quantiles')\n             plt.ylabel('sample quantiles')\n\nIn [18]: return_qqplot(snrn)\n```", "```py\nIn [19]: return_qqplot(numbers)\n```", "```py\nIn [20]: def print_statistics(rets):\n             print('RETURN SAMPLE STATISTICS')\n             print('---------------------------------------------')\n             print('Skew of Sample Log Returns {:9.6f}'.format(\n                         scs.skew(rets)))\n             print('Skew Normal Test p-value   {:9.6f}'.format(\n                         scs.skewtest(rets)[1]))\n             print('---------------------------------------------')\n             print('Kurt of Sample Log Returns {:9.6f}'.format(\n                         scs.kurtosis(rets)))\n             print('Kurt Normal Test p-value   {:9.6f}'.format(\n                         scs.kurtosistest(rets)[1]))\n             print('---------------------------------------------')\n             print('Normal Test p-value        {:9.6f}'.format(\n                         scs.normaltest(rets)[1]))\n             print('---------------------------------------------')\n\nIn [21]: print_statistics(snrn)\n         RETURN SAMPLE STATISTICS\n         ---------------------------------------------\n         Skew of Sample Log Returns  0.016793\n         Skew Normal Test p-value    0.492685\n         ---------------------------------------------\n         Kurt of Sample Log Returns -0.024540\n         Kurt Normal Test p-value    0.637637\n         ---------------------------------------------\n         Normal Test p-value         0.707334\n         ---------------------------------------------\n\nIn [22]: print_statistics(numbers)\n         RETURN SAMPLE STATISTICS\n         ---------------------------------------------\n         Skew of Sample Log Returns  0.689254\n         Skew Normal Test p-value    0.000000\n         ---------------------------------------------\n         Kurt of Sample Log Returns -1.141902\n         Kurt Normal Test p-value    0.000000\n         ---------------------------------------------\n         Normal Test p-value         0.000000\n         ---------------------------------------------\n```", "```py\nIn [23]: raw = pd.read_csv('http://hilpisch.com/aiif_eikon_eod_data.csv',\n                           index_col=0, parse_dates=True).dropna()\n\nIn [24]: rets = np.log(raw / raw.shift(1)).dropna()\n\nIn [25]: symbol = '.SPX'\n\nIn [26]: return_histogram(rets[symbol].values, symbol)\n```", "```py\nIn [27]: return_qqplot(rets[symbol].values, symbol)\n```", "```py\nIn [28]: symbols = ['.SPX', 'AMZN.O', 'EUR=', 'GLD']\n\nIn [29]: for sym in symbols:\n             print('\\n{}'.format(sym))\n             print(45 * '=')\n             print_statistics(rets[sym].values)\n\n         .SPX\n         =============================================\n         RETURN SAMPLE STATISTICS\n         ---------------------------------------------\n         Skew of Sample Log Returns -0.497160\n         Skew Normal Test p-value    0.000000\n         ---------------------------------------------\n         Kurt of Sample Log Returns  4.598167\n         Kurt Normal Test p-value    0.000000\n         ---------------------------------------------\n         Normal Test p-value         0.000000\n         ---------------------------------------------\n\n         AMZN.O\n         =============================================\n         RETURN SAMPLE STATISTICS\n         ---------------------------------------------\n         Skew of Sample Log Returns  0.135268\n         Skew Normal Test p-value    0.005689\n         ---------------------------------------------\n         Kurt of Sample Log Returns  7.344837\n         Kurt Normal Test p-value    0.000000\n         ---------------------------------------------\n         Normal Test p-value         0.000000\n         ---------------------------------------------\n\n         EUR=\n         =============================================\n         RETURN SAMPLE STATISTICS\n         ---------------------------------------------\n         Skew of Sample Log Returns -0.053959\n         Skew Normal Test p-value    0.268203\n         ---------------------------------------------\n         Kurt of Sample Log Returns  1.780899\n         Kurt Normal Test p-value    0.000000\n         ---------------------------------------------\n         Normal Test p-value         0.000000\n         ---------------------------------------------\n\n         GLD\n         =============================================\n         RETURN SAMPLE STATISTICS\n         ---------------------------------------------\n         Skew of Sample Log Returns -0.581025\n         Skew Normal Test p-value    0.000000\n         ---------------------------------------------\n         Kurt of Sample Log Returns  5.899701\n         Kurt Normal Test p-value    0.000000\n         ---------------------------------------------\n         Normal Test p-value         0.000000\n         ---------------------------------------------\n```", "```py\nIn [30]: r = 0.005\n\nIn [31]: market = '.SPX'\n\nIn [32]: res = pd.DataFrame()\n\nIn [33]: for sym in rets.columns[:4]:\n             for year in range(2010, 2019):\n                 rets_ = rets.loc[f'{year}-01-01':f'{year}-12-31']\n                 muM = rets_[market].mean() * 252\n                 cov = rets_.cov().loc[sym, market]\n                 var = rets_[market].var()\n                 beta = cov / var\n                 rets_ = rets.loc[f'{year + 1}-01-01':f'{year + 1}-12-31']\n                 muM = rets_[market].mean() * 252\n                 mu_capm = r + beta * (muM - r)\n                 mu_real = rets_[sym].mean() * 252\n                 res = res.append(pd.DataFrame({'symbol': sym,\n                                                'beta': beta,\n                                                'mu_capm': mu_capm,\n                                                'mu_real': mu_real},\n                                               index=[year + 1]),\n                                 sort=True)\n```", "```py\nIn [34]: from sklearn.metrics import r2_score\n\nIn [35]: reg = np.polyfit(res['beta'], res['mu_capm'], deg=1)\n         res['mu_capm_ols'] = np.polyval(reg, res['beta'])\n\nIn [36]: r2_score(res['mu_capm'], res['mu_capm_ols'])\nOut[36]: 0.09272355783573516\n\nIn [37]: res.plot(kind='scatter', x='beta', y='mu_capm', figsize=(10, 6))\n         x = np.linspace(res['beta'].min(), res['beta'].max())\n         plt.plot(x, np.polyval(reg, x), 'g--', label='regression')\n         plt.legend();\n```", "```py\nIn [38]: reg = np.polyfit(res['beta'], res['mu_real'], deg=1)\n         res['mu_real_ols'] = np.polyval(reg, res['beta'])\n\nIn [39]: r2_score(res['mu_real'], res['mu_real_ols'])\nOut[39]: 0.04466919444752959\n\nIn [40]: res.plot(kind='scatter', x='beta', y='mu_real', figsize=(10, 6))\n         x = np.linspace(res['beta'].min(), res['beta'].max())\n         plt.plot(x, np.polyval(reg, x), 'g--', label='regression')\n         plt.legend();\n```", "```py\n#\n# NLP Helper Functions\n#\n# Artificial Intelligence in Finance\n# (c) Dr Yves J Hilpisch\n# The Python Quants GmbH\n#\nimport re\nimport nltk\nimport string\nimport pandas as pd\nfrom pylab import plt\nfrom wordcloud import WordCloud\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import wordnet as wn\nfrom lxml.html.clean import Cleaner\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nplt.style.use('seaborn')\n\ncleaner = Cleaner(style=True, links=True, allow_tags=[''],\n                  remove_unknown_tags=False)\n\nstop_words = stopwords.words('english')\nstop_words.extend(['new', 'old', 'pro', 'open', 'menu', 'close'])\n\ndef remove_non_ascii(s):\n    ''' Removes all non-ascii characters.\n '''\n    return ''.join(i for i in s if ord(i) < 128)\n\ndef clean_up_html(t):\n    t = cleaner.clean_html(t)\n    t = re.sub('[\\n\\t\\r]', ' ', t)\n    t = re.sub(' +', ' ', t)\n    t = re.sub('<.*?>', '', t)\n    t = remove_non_ascii(t)\n    return t\n\ndef clean_up_text(t, numbers=False, punctuation=False):\n    ''' Cleans up a text, e.g. HTML document,\n from HTML tags and also cleans up the\n text body.\n '''\n    try:\n        t = clean_up_html(t)\n    except:\n        pass\n    t = t.lower()\n    t = re.sub(r\"what's\", \"what is \", t)\n    t = t.replace('(ap)', '')\n    t = re.sub(r\"\\'ve\", \" have \", t)\n    t = re.sub(r\"can't\", \"cannot \", t)\n    t = re.sub(r\"n't\", \" not \", t)\n    t = re.sub(r\"i'm\", \"i am \", t)\n    t = re.sub(r\"\\'s\", \"\", t)\n    t = re.sub(r\"\\'re\", \" are \", t)\n    t = re.sub(r\"\\'d\", \" would \", t)\n    t = re.sub(r\"\\'ll\", \" will \", t)\n    t = re.sub(r'\\s+', ' ', t)\n    t = re.sub(r\"\\\\\", \"\", t)\n    t = re.sub(r\"\\'\", \"\", t)\n    t = re.sub(r\"\\\"\", \"\", t)\n    if numbers:\n        t = re.sub('[^a-zA-Z ?!]+', '', t)\n    if punctuation:\n        t = re.sub(r'\\W+', ' ', t)\n    t = remove_non_ascii(t)\n    t = t.strip()\n    return t\n\ndef nltk_lemma(word):\n    ''' If one exists, returns the lemma of a word.\n I.e. the base or dictionary version of it.\n '''\n    lemma = wn.morphy(word)\n    if lemma is None:\n        return word\n    else:\n        return lemma\n\ndef tokenize(text, min_char=3, lemma=True, stop=True,\n             numbers=False):\n    ''' Tokenizes a text and implements some\n transformations.\n '''\n    tokens = nltk.word_tokenize(text)\n    tokens = [t for t in tokens if len(t) >= min_char]\n    if numbers:\n        tokens = [t for t in tokens if t[0].lower()\n                  in string.ascii_lowercase]\n    if stop:\n        tokens = [t for t in tokens if t not in stop_words]\n    if lemma:\n        tokens = [nltk_lemma(t) for t in tokens]\n    return tokens\n\ndef generate_word_cloud(text, no, name=None, show=True):\n    ''' Generates a word cloud bitmap given a\n text document (string).\n It uses the Term Frequency (TF) and\n Inverse Document Frequency (IDF)\n vectorization approach to derive the\n importance of a word -- represented\n by the size of the word in the word cloud.\n\n Parameters\n ==========\n text: str\n text as the basis\n no: int\n number of words to be included\n name: str\n path to save the image\n show: bool\n whether to show the generated image or not\n '''\n    tokens = tokenize(text)\n    vec = TfidfVectorizer(min_df=2,\n                      analyzer='word',\n                      ngram_range=(1, 2),\n                      stop_words='english'\n                     )\n    vec.fit_transform(tokens)\n    wc = pd.DataFrame({'words': vec.get_feature_names(),\n                       'tfidf': vec.idf_})\n    words = ' '.join(wc.sort_values('tfidf', ascending=True)['words'].head(no))\n    wordcloud = WordCloud(max_font_size=110,\n                      background_color='white',\n                      width=1024, height=768,\n                      margin=10, max_words=150).generate(words)\n    if show:\n        plt.figure(figsize=(10, 10))\n        plt.imshow(wordcloud, interpolation='bilinear')\n        plt.axis('off')\n        plt.show()\n    if name is not None:\n        wordcloud.to_file(name)\n\ndef generate_key_words(text, no):\n    try:\n        tokens = tokenize(text)\n        vec = TfidfVectorizer(min_df=2,\n                      analyzer='word',\n                      ngram_range=(1, 2),\n                      stop_words='english'\n                     )\n\n        vec.fit_transform(tokens)\n        wc = pd.DataFrame({'words': vec.get_feature_names(),\n                       'tfidf': vec.idf_})\n        words = wc.sort_values('tfidf', ascending=False)['words'].values\n        words = [ a for a in words if not a.isnumeric()][:no]\n    except:\n        words = list()\n    return words\n```"]