["```py\nIn [1]: import os\n        import math\n        import random\n        import numpy as np\n        import pandas as pd\n        from pylab import plt, mpl\n        plt.style.use('seaborn')\n        mpl.rcParams['savefig.dpi'] = 300\n        mpl.rcParams['font.family'] = 'serif'\n        np.set_printoptions(precision=4, suppress=True)\n        os.environ['PYTHONHASHSEED'] = '0'\nIn [2]: import gym\n\nIn [3]: env = gym.make('CartPole-v0')  ![1](Images/1.png)\n\nIn [4]: env.seed(100)  ![1](Images/1.png)\n        env.action_space.seed(100)  ![1](Images/1.png)\nOut[4]: [100]\n\nIn [5]: env.observation_space  ![2](Images/2.png)\nOut[5]: Box(4,)\n\nIn [6]: env.observation_space.low.astype(np.float16)  ![2](Images/2.png)\nOut[6]: array([-4.8  ,   -inf, -0.419,   -inf], dtype=float16)\n\nIn [7]: env.observation_space.high.astype(np.float16)  ![2](Images/2.png)\nOut[7]: array([4.8  ,   inf, 0.419,   inf], dtype=float16)\n\nIn [8]: state = env.reset()  ![3](Images/3.png)\n\nIn [9]: state  ![4](Images/4.png)\nOut[9]: array([-0.0163,  0.0238, -0.0392, -0.0148])\n```", "```py\nIn [10]: env.action_space  ![1](Images/1.png)\nOut[10]: Discrete(2)\n\nIn [11]: env.action_space.n  ![1](Images/1.png)\nOut[11]: 2\n\nIn [12]: env.action_space.sample()  ![2](Images/2.png)\nOut[12]: 1\n\nIn [13]: env.action_space.sample()   ![2](Images/2.png)\nOut[13]: 0\n\nIn [14]: a = env.action_space.sample()  ![2](Images/2.png)\n         a  ![2](Images/2.png)\nOut[14]: 1\n\nIn [15]: state, reward, done, info = env.step(a)  ![3](Images/3.png)\n         state, reward, done, info  ![4](Images/4.png)\nOut[15]: (array([-0.0158,  0.2195, -0.0395, -0.3196]), 1.0, False, {})\n```", "```py\nIn [16]: env.reset()\n         for e in range(1, 200):\n             a = env.action_space.sample()  ![1](Images/1.png)\n             state, reward, done, info = env.step(a) ![2](Images/2.png)\n             print(f'step={e:2d} | state={state} | action={a} | reward={reward}')\n             if done and (e + 1) < 200:  ![3](Images/3.png)\n                 print('*** FAILED ***')  ![3](Images/3.png)\n                 break\n         step= 1 | state=[-0.0423  0.1982  0.0256 -0.2476] | action=1 | reward=1.0\n         step= 2 | state=[-0.0383  0.0028  0.0206  0.0531] | action=0 | reward=1.0\n         step= 3 | state=[-0.0383  0.1976  0.0217 -0.2331] | action=1 | reward=1.0\n         step= 4 | state=[-0.0343  0.0022  0.017   0.0664] | action=0 | reward=1.0\n         step= 5 | state=[-0.0343  0.197   0.0184 -0.2209] | action=1 | reward=1.0\n         step= 6 | state=[-0.0304  0.0016  0.0139  0.0775] | action=0 | reward=1.0\n         step= 7 | state=[-0.0303  0.1966  0.0155 -0.2107] | action=1 | reward=1.0\n         step= 8 | state=[-0.0264  0.0012  0.0113  0.0868] | action=0 | reward=1.0\n         step= 9 | state=[-0.0264  0.1962  0.013  -0.2023] | action=1 | reward=1.0\n         step=10 | state=[-0.0224  0.3911  0.009  -0.4908] | action=1 | reward=1.0\n         step=11 | state=[-0.0146  0.5861 -0.0009 -0.7807] | action=1 | reward=1.0\n         step=12 | state=[-0.0029  0.7812 -0.0165 -1.0736] | action=1 | reward=1.0\n         step=13 | state=[ 0.0127  0.9766 -0.0379 -1.3714] | action=1 | reward=1.0\n         step=14 | state=[ 0.0323  1.1722 -0.0654 -1.6758] | action=1 | reward=1.0\n         step=15 | state=[ 0.0557  0.9779 -0.0989 -1.4041] | action=0 | reward=1.0\n         step=16 | state=[ 0.0753  0.7841 -0.127  -1.1439] | action=0 | reward=1.0\n         step=17 | state=[ 0.0909  0.5908 -0.1498 -0.8936] | action=0 | reward=1.0\n         step=18 | state=[ 0.1028  0.7876 -0.1677 -1.2294] | action=1 | reward=1.0\n         step=19 | state=[ 0.1185  0.9845 -0.1923 -1.5696] | action=1 | reward=1.0\n         step=20 | state=[ 0.1382  0.7921 -0.2237 -1.3425] | action=0 | reward=1.0\n         *** FAILED ***\n\nIn [17]: done\nOut[17]: True\n```", "```py\nIn [18]: np.random.seed(100)  ![1](Images/1.png)\n\nIn [19]: weights = np.random.random(4) * 2 - 1  ![1](Images/1.png)\n\nIn [20]: weights  ![1](Images/1.png)\nOut[20]: array([ 0.0868, -0.4433, -0.151 ,  0.6896])\n\nIn [21]: state = env.reset()  ![2](Images/2.png)\n\nIn [22]: state  ![2](Images/2.png)\nOut[22]: array([-0.0347, -0.0103,  0.047 , -0.0315])\n\nIn [23]: s = np.dot(state, weights)  ![3](Images/3.png)\n         s  ![3](Images/3.png)\nOut[23]: -0.02725361929630797\n```", "```py\nIn [24]: if s < 0:\n             a = 0\n         else:\n             a = 1\n\nIn [25]: a\nOut[25]: 0\n```", "```py\nIn [26]: def run_episode(env, weights):\n             state = env.reset()\n             treward = 0\n             for _ in range(200):\n                 s = np.dot(state, weights)\n                 a = 0 if s < 0 else 1\n                 state, reward, done, info = env.step(a)\n                 treward += reward\n                 if done:\n                     break\n             return treward\n\nIn [27]: run_episode(env, weights)\nOut[27]: 41.0\n```", "```py\nIn [28]: def set_seeds(seed=100):\n             random.seed(seed)\n             np.random.seed(seed)\n             env.seed(seed)\n\nIn [29]: set_seeds()\n         num_episodes = 1000\n\nIn [30]: besttreward = 0\n         for e in range(1, num_episodes + 1):\n             weights = np.random.rand(4) * 2 - 1  ![1](Images/1.png)\n             treward = run_episode(env, weights)  ![2](Images/2.png)\n             if treward > besttreward:  ![3](Images/3.png)\n                 besttreward = treward  ![4](Images/4.png)\n                 bestweights = weights  ![5](Images/5.png)\n                 if treward == 200:\n                     print(f'SUCCESS | episode={e}')\n                     break\n                 print(f'UPDATE  | episode={e}')\n         UPDATE  | episode=1\n         UPDATE  | episode=2\n         SUCCESS | episode=13\n\nIn [31]: weights\nOut[31]: array([-0.4282,  0.7048,  0.95  ,  0.7697])\n```", "```py\nIn [32]: res = []\n         for _ in range(100):\n             treward = run_episode(env, weights)\n             res.append(treward)\n         res[:10]\nOut[32]: [200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0]\n\nIn [33]: sum(res) / len(res)\nOut[33]: 200.0\n```", "```py\nIn [34]: import tensorflow as tf\n         from keras.layers import Dense, Dropout\n         from keras.models import Sequential\n         from keras.optimizers import Adam, RMSprop\n         from sklearn.metrics import accuracy_score\n         Using TensorFlow backend.\n\nIn [35]: def set_seeds(seed=100):\n             random.seed(seed)\n             np.random.seed(seed)\n             tf.random.set_seed(seed)\n             env.seed(seed)\n             env.action_space.seed(100)\n```", "```py\nIn [36]: class NNAgent:\n             def __init__(self):\n                 self.max = 0  ![1](Images/1.png)\n                 self.scores = list()\n                 self.memory = list()\n                 self.model = self._build_model()\n\n             def _build_model(self):  ![2](Images/2.png)\n                 model = Sequential()\n                 model.add(Dense(24, input_dim=4,\n                                 activation='relu'))\n                 model.add(Dense(1, activation='sigmoid'))\n                 model.compile(loss='binary_crossentropy',\n                               optimizer=RMSprop(lr=0.001))\n                 return model\n\n             def act(self, state):  ![3](Images/3.png)\n                 if random.random() <= 0.5:\n                     return env.action_space.sample()\n                 action = np.where(self.model.predict(\n                     state, batch_size=None)[0, 0] > 0.5, 1, 0)\n                 return action\n\n             def train_model(self, state, action):  ![4](Images/4.png)\n                 self.model.fit(state, np.array([action,]),\n                                epochs=1, verbose=False)\n\n             def learn(self, episodes):  ![5](Images/5.png)\n                 for e in range(1, episodes + 1):\n                     state = env.reset()\n                     for _ in range(201):\n                         state = np.reshape(state, [1, 4])\n                         action = self.act(state)\n                         next_state, reward, done, info = env.step(action)\n                         if done:\n                             score = _ + 1\n                             self.scores.append(score)\n                             self.max = max(score, self.max)  ![1](Images/1.png)\n                             print('episode: {:4d}/{} | score: {:3d} | max: {:3d}'\n                                   .format(e, episodes, score, self.max), end='\\r')\n                             break\n                         self.memory.append((state, action))\n                         self.train_model(state, action)  ![4](Images/4.png)\n                         state = next_state\n```", "```py\nIn [37]: set_seeds(100)\n         agent = NNAgent()\n\nIn [38]: episodes = 500\n\nIn [39]: agent.learn(episodes)\n         episode:  500/500 | score:  11 | max:  44\nIn [40]: sum(agent.scores) / len(agent.scores)  ![1](Images/1.png)\nOut[40]: 13.682\n```", "```py\nIn [41]: f = np.array([m[0][0] for m in agent.memory])  ![1](Images/1.png)\n         f  ![1](Images/1.png)\nOut[41]: array([[-0.0163,  0.0238, -0.0392, -0.0148],\n                [-0.0158,  0.2195, -0.0395, -0.3196],\n                [-0.0114,  0.0249, -0.0459, -0.0396],\n                ...,\n                [ 0.0603,  0.9682, -0.0852, -1.4595],\n                [ 0.0797,  1.1642, -0.1144, -1.7776],\n                [ 0.103 ,  1.3604, -0.15  , -2.1035]])\n\nIn [42]: l = np.array([m[1] for m in agent.memory])  ![2](Images/2.png)\n         l  ![2](Images/2.png)\nOut[42]: array([1, 0, 1, ..., 1, 1, 1])\n\nIn [43]: accuracy_score(np.where(agent.model.predict(f) > 0.5, 1, 0), l)\nOut[43]: 0.7525626872733008\n```", "```py\nIn [44]: from collections import deque\n         from keras.optimizers import Adam, RMSprop\n\nIn [45]: class DQLAgent:\n             def __init__(self, gamma=0.95, hu=24, opt=Adam,\n                    lr=0.001, finish=False):\n                 self.finish = finish\n                 self.epsilon = 1.0  ![1](Images/1.png)\n                 self.epsilon_min = 0.01  ![2](Images/2.png)\n                 self.epsilon_decay = 0.995  ![3](Images/3.png)\n                 self.gamma = gamma  ![4](Images/4.png)\n                 self.batch_size = 32  ![5](Images/5.png)\n                 self.max_treward = 0\n                 self.averages = list()\n                 self.memory = deque(maxlen=2000)  ![6](Images/6.png)\n                 self.osn = env.observation_space.shape[0]\n                 self.model = self._build_model(hu, opt, lr)\n\n             def _build_model(self, hu, opt, lr):\n                 model = Sequential()\n                 model.add(Dense(hu, input_dim=self.osn,\n                                 activation='relu'))\n                 model.add(Dense(hu, activation='relu'))\n                 model.add(Dense(env.action_space.n, activation='linear'))\n                 model.compile(loss='mse', optimizer=opt(lr=lr))\n                 return model\n\n             def act(self, state):\n                 if random.random() <= self.epsilon:\n                     return env.action_space.sample()\n                 action = self.model.predict(state)[0]\n                 return np.argmax(action)\n\n             def replay(self):\n                 batch = random.sample(self.memory, self.batch_size)  ![7](Images/7.png)\n                 for state, action, reward, next_state, done in batch:\n                     if not done:\n                         reward += self.gamma * np.amax(\n                             self.model.predict(next_state)[0])  ![8](Images/8.png)\n                     target = self.model.predict(state)\n                     target[0, action] = reward\n                     self.model.fit(state, target, epochs=1,\n                                    verbose=False)  ![9](Images/9.png)\n                 if self.epsilon > self.epsilon_min:\n                     self.epsilon *= self.epsilon_decay  ![10](Images/10.png)\n\n             def learn(self, episodes):\n                 trewards = []\n                 for e in range(1, episodes + 1):\n                     state = env.reset()\n                     state = np.reshape(state, [1, self.osn])\n                     for _ in range(5000):\n                         action = self.act(state)\n                         next_state, reward, done, info = env.step(action)\n                         next_state = np.reshape(next_state,\n                                                 [1, self.osn])\n                         self.memory.append([state, action, reward,\n                                              next_state, done])  ![11](Images/11.png)\n                         state = next_state\n                         if done:\n                             treward = _ + 1\n                             trewards.append(treward)\n                             av = sum(trewards[-25:]) / 25\n                             self.averages.append(av)\n                             self.max_treward = max(self.max_treward, treward)\n                             templ = 'episode: {:4d}/{} | treward: {:4d} | '\n                             templ += 'av: {:6.1f} | max: {:4d}'\n                             print(templ.format(e, episodes, treward, av,\n                                                self.max_treward), end='\\r')\n                             break\n                     if av > 195 and self.finish:\n                         break\n                     if len(self.memory) > self.batch_size:\n                         self.replay()  ![12](Images/12.png)\n             def test(self, episodes):\n                 trewards = []\n                 for e in range(1, episodes + 1):\n                     state = env.reset()\n                     for _ in range(5001):\n                         state = np.reshape(state, [1, self.osn])\n                         action = np.argmax(self.model.predict(state)[0])\n                         next_state, reward, done, info = env.step(action)\n                         state = next_state\n                         if done:\n                             treward = _ + 1\n                             trewards.append(treward)\n                             print('episode: {:4d}/{} | treward: {:4d}'\n                                   .format(e, episodes, treward), end='\\r')\n                             break\n                 return trewards\n```", "```py\nIn [46]: episodes = 1000\n\nIn [47]: set_seeds(100)\n         agent = DQLAgent(finish=True)\n\nIn [48]: agent.learn(episodes)\n         episode:  400/1000 | treward:  200 | av:  195.4 | max:  200\nIn [49]: plt.figure(figsize=(10, 6))\n         x = range(len(agent.averages))\n         y = np.polyval(np.polyfit(x, agent.averages, deg=3), x)\n         plt.plot(agent.averages, label='moving average')\n         plt.plot(x, y, 'r--', label='trend')\n         plt.xlabel('episodes')\n         plt.ylabel('total reward')\n         plt.legend();\n```", "```py\nIn [50]: trewards = agent.test(100)\n         episode:  100/100 | treward:  200\nIn [51]: sum(trewards) / len(trewards)\nOut[51]: 200.0\n```", "```py\nIn [52]: class observation_space:\n             def __init__(self, n):\n                 self.shape = (n,)\n\nIn [53]: class action_space:\n             def __init__(self, n):\n                 self.n = n\n             def seed(self, seed):\n                 pass\n             def sample(self):\n                 return random.randint(0, self.n - 1)\n```", "```py\nIn [54]: class Finance:\n             url = 'http://hilpisch.com/aiif_eikon_eod_data.csv'\n             def __init__(self, symbol, features):\n                 self.symbol = symbol\n                 self.features = features\n                 self.observation_space = observation_space(4)\n                 self.osn = self.observation_space.shape[0]\n                 self.action_space = action_space(2)\n                 self.min_accuracy = 0.475  ![1](Images/1.png)\n                 self._get_data()\n                 self._prepare_data()\n             def _get_data(self):\n                 self.raw = pd.read_csv(self.url, index_col=0,\n                                        parse_dates=True).dropna()\n             def _prepare_data(self):\n                 self.data = pd.DataFrame(self.raw[self.symbol])\n                 self.data['r'] = np.log(self.data / self.data.shift(1))\n                 self.data.dropna(inplace=True)\n                 self.data = (self.data - self.data.mean()) / self.data.std()\n                 self.data['d'] = np.where(self.data['r'] > 0, 1, 0)\n             def _get_state(self):\n                 return self.data[self.features].iloc[\n                     self.bar - self.osn:self.bar].values  ![2](Images/2.png)\n             def seed(self, seed=None):\n                 pass\n             def reset(self):  ![3](Images/3.png)\n                 self.treward = 0\n                 self.accuracy = 0\n                 self.bar = self.osn\n                 state = self.data[self.features].iloc[\n                     self.bar - self.osn:self.bar]\n                 return state.values\n             def step(self, action):\n                 correct = action == self.data['d'].iloc[self.bar]  ![4](Images/4.png)\n                 reward = 1 if correct else 0  ![5](Images/5.png)\n                 self.treward += reward  ![6](Images/6.png)\n                 self.bar += 1  ![7](Images/7.png)\n                 self.accuracy = self.treward / (self.bar - self.osn)  ![8](Images/8.png)\n                 if self.bar >= len(self.data):  ![9](Images/9.png)\n                     done = True\n                 elif reward == 1:  ![10](Images/10.png)\n                     done = False\n                 elif (self.accuracy < self.min_accuracy and\n                       self.bar > self.osn + 10):  ![11](Images/11.png)\n                     done = True\n                 else:  ![12](Images/12.png)\n                     done = False\n                 state = self._get_state()\n                 info = {}\n                 return state, reward, done, info\n```", "```py\nIn [55]: env = Finance('EUR=', 'EUR=')  ![1](Images/1.png)\n\nIn [56]: env.reset()\nOut[56]: array([1.819 , 1.8579, 1.7749, 1.8579])\n\nIn [57]: a = env.action_space.sample()\n         a\nOut[57]: 0\n\nIn [58]: env.step(a)\nOut[58]: (array([1.8579, 1.7749, 1.8579, 1.947 ]), 0, False, {})\n```", "```py\nIn [59]: set_seeds(100)\n         agent = DQLAgent(gamma=0.5, opt=RMSprop)\n\nIn [60]: episodes = 1000\n\nIn [61]: agent.learn(episodes)\n         episode: 1000/1000 | treward: 2511 | av: 1012.7 | max: 2511\nIn [62]: agent.test(3)\n         episode:    3/3 | treward: 2511\nOut[62]: [2511, 2511, 2511]\n\nIn [63]: plt.figure(figsize=(10, 6))\n         x = range(len(agent.averages))\n         y = np.polyval(np.polyfit(x, agent.averages, deg=3), x)\n         plt.plot(agent.averages, label='moving average')\n         plt.plot(x, y, 'r--', label='regression')\n         plt.xlabel('episodes')\n         plt.ylabel('total reward')\n         plt.legend();\n```", "```py\nIn [64]: class Finance:\n             url = 'http://hilpisch.com/aiif_eikon_eod_data.csv'\n             def __init__(self, symbol, features, window, lags,\n                          leverage=1, min_performance=0.85,\n                          start=0, end=None, mu=None, std=None):\n                 self.symbol = symbol\n                 self.features = features  ![1](Images/1.png)\n                 self.n_features = len(features)\n                 self.window = window\n                 self.lags = lags  ![1](Images/1.png)\n                 self.leverage = leverage  ![2](Images/2.png)\n                 self.min_performance = min_performance  ![3](Images/3.png)\n                 self.start = start\n                 self.end = end\n                 self.mu = mu\n                 self.std = std\n                 self.observation_space = observation_space(self.lags)\n                 self.action_space = action_space(2)\n                 self._get_data()\n                 self._prepare_data()\n             def _get_data(self):\n                 self.raw = pd.read_csv(self.url, index_col=0,\n                                        parse_dates=True).dropna()\n             def _prepare_data(self):\n                 self.data = pd.DataFrame(self.raw[self.symbol])\n                 self.data = self.data.iloc[self.start:]\n                 self.data['r'] = np.log(self.data / self.data.shift(1))\n                 self.data.dropna(inplace=True)\n                 self.data['s'] = self.data[self.symbol].rolling(\n                                                       self.window).mean()   ![4](Images/4.png)\n                 self.data['m'] = self.data['r'].rolling(self.window).mean()  ![4](Images/4.png)\n                 self.data['v'] = self.data['r'].rolling(self.window).std()  ![4](Images/4.png)\n                 self.data.dropna(inplace=True)\n                 if self.mu is None:\n                     self.mu = self.data.mean()  ![5](Images/5.png)\n                     self.std = self.data.std()  ![5](Images/5.png)\n                 self.data_ = (self.data - self.mu) / self.std  ![5](Images/5.png)\n                 self.data_['d'] = np.where(self.data['r'] > 0, 1, 0)\n                 self.data_['d'] = self.data_['d'].astype(int)\n                 if self.end is not None:\n                     self.data = self.data.iloc[:self.end - self.start]\n                     self.data_ = self.data_.iloc[:self.end - self.start]\n             def _get_state(self):\n                 return self.data_[self.features].iloc[self.bar -\n                                         self.lags:self.bar]\n             def seed(self, seed):\n                 random.seed(seed)\n                 np.random.seed(seed)\n             def reset(self):\n                 self.treward = 0\n                 self.accuracy = 0\n                 self.performance = 1\n                 self.bar = self.lags\n                 state = self.data_[self.features].iloc[self.bar-\n                                 self.lags:self.bar]\n                 return state.values\n             def step(self, action):\n                 correct = action == self.data_['d'].iloc[self.bar]\n                 ret = self.data['r'].iloc[self.bar] * self.leverage  ![6](Images/6.png)\n                 reward_1 = 1 if correct else 0\n                 reward_2 = abs(ret) if correct else -abs(ret)  ![7](Images/7.png)\n                 factor = 1 if correct else -1\n                 self.treward += reward_1\n                 self.bar += 1\n                 self.accuracy = self.treward / (self.bar - self.lags)\n                 self.performance *= math.exp(reward_2)  ![8](Images/8.png)\n                 if self.bar >= len(self.data):\n                     done = True\n                 elif reward_1 == 1:\n                     done = False\n                 elif (self.performance < self.min_performance and\n                       self.bar > self.lags + 5):\n                     done = True\n                 else:\n                     done = False\n                 state = self._get_state()\n                 info = {}\n                 return state.values, reward_1 + reward_2 * 5, done, info\n```", "```py\nIn [65]: env = Finance('EUR=', ['EUR=', 'r'], 10, 5)\n\nIn [66]: a = env.action_space.sample()\n         a\nOut[66]: 0\n\nIn [67]: env.reset()\nOut[67]: array([[ 1.7721, -1.0214],\n                [ 1.5973, -2.4432],\n                [ 1.5876, -0.1208],\n                [ 1.6292,  0.6083],\n                [ 1.6408,  0.1807]])\n\nIn [68]: env.step(a)\nOut[68]: (array([[ 1.5973, -2.4432],\n                 [ 1.5876, -0.1208],\n                 [ 1.6292,  0.6083],\n                 [ 1.6408,  0.1807],\n                 [ 1.5725, -0.9502]]),\n          1.0272827803740798,\n          False,\n          {})\n```", "```py\nIn [69]: class FQLAgent:\n             def __init__(self, hidden_units, learning_rate, learn_env, valid_env):\n                 self.learn_env = learn_env\n                 self.valid_env = valid_env\n                 self.epsilon = 1.0\n                 self.epsilon_min = 0.1\n                 self.epsilon_decay = 0.98\n                 self.learning_rate = learning_rate\n                 self.gamma = 0.95\n                 self.batch_size = 128\n                 self.max_treward = 0\n                 self.trewards = list()\n                 self.averages = list()\n                 self.performances = list()\n                 self.aperformances = list()\n                 self.vperformances = list()\n                 self.memory = deque(maxlen=2000)\n                 self.model = self._build_model(hidden_units, learning_rate)\n\n             def _build_model(self, hu, lr):\n                 model = Sequential()\n                 model.add(Dense(hu, input_shape=(\n                     self.learn_env.lags, self.learn_env.n_features),\n                                 activation='relu'))\n                 model.add(Dropout(0.3, seed=100))\n                 model.add(Dense(hu, activation='relu'))\n                 model.add(Dropout(0.3, seed=100))\n                 model.add(Dense(2, activation='linear'))\n                 model.compile(\n                     loss='mse',\n                     optimizer=RMSprop(lr=lr)\n                 )\n                 return model\n\n             def act(self, state):\n                 if random.random() <= self.epsilon:\n                     return self.learn_env.action_space.sample()\n                 action = self.model.predict(state)[0, 0]\n                 return np.argmax(action)\n\n             def replay(self):\n                 batch = random.sample(self.memory, self.batch_size)\n                 for state, action, reward, next_state, done in batch:\n                     if not done:\n                         reward += self.gamma * np.amax(\n                             self.model.predict(next_state)[0, 0])\n                     target = self.model.predict(state)\n                     target[0, 0, action] = reward\n                     self.model.fit(state, target, epochs=1,\n                                    verbose=False)\n                 if self.epsilon > self.epsilon_min:\n                     self.epsilon *= self.epsilon_decay\n\n             def learn(self, episodes):\n                 for e in range(1, episodes + 1):\n                     state = self.learn_env.reset()\n                     state = np.reshape(state, [1, self.learn_env.lags,\n                                                self.learn_env.n_features])\n                     for _ in range(10000):\n                         action = self.act(state)\n                         next_state, reward, done, info = \\\n                                         self.learn_env.step(action)\n                         next_state = np.reshape(next_state,\n                                         [1, self.learn_env.lags,\n                                          self.learn_env.n_features])\n                         self.memory.append([state, action, reward,\n                                              next_state, done])\n                         state = next_state\n                         if done:\n                             treward = _ + 1\n                             self.trewards.append(treward)\n                             av = sum(self.trewards[-25:]) / 25\n                             perf = self.learn_env.performance\n                             self.averages.append(av)\n                             self.performances.append(perf)\n                             self.aperformances.append(\n                                 sum(self.performances[-25:]) / 25)\n                             self.max_treward = max(self.max_treward, treward)\n                             templ = 'episode: {:2d}/{} | treward: {:4d} | '\n                             templ += 'perf: {:5.3f} | av: {:5.1f} | max: {:4d}'\n                             print(templ.format(e, episodes, treward, perf,\n                                           av, self.max_treward), end='\\r')\n                             break\n                     self.validate(e, episodes)\n                     if len(self.memory) > self.batch_size:\n                         self.replay()\n             def validate(self, e, episodes):\n                 state = self.valid_env.reset()\n                 state = np.reshape(state, [1, self.valid_env.lags,\n                                            self.valid_env.n_features])\n                 for _ in range(10000):\n                     action = np.argmax(self.model.predict(state)[0, 0])\n                     next_state, reward, done, info = self.valid_env.step(action)\n                     state = np.reshape(next_state, [1, self.valid_env.lags,\n                                            self.valid_env.n_features])\n                     if done:\n                         treward = _ + 1\n                         perf = self.valid_env.performance\n                         self.vperformances.append(perf)\n                         if e % 20 == 0:\n                             templ = 71 * '='\n                             templ += '\\nepisode: {:2d}/{} | VALIDATION | '\n                             templ += 'treward: {:4d} | perf: {:5.3f} | '\n                             templ += 'eps: {:.2f}\\n'\n                             templ += 71 * '='\n                             print(templ.format(e, episodes, treward,\n                                                perf, self.epsilon))\n                         break\n```", "```py\nIn [70]: symbol = 'EUR='\n         features = [symbol, 'r', 's', 'm', 'v']\n\nIn [71]: a = 0\n         b = 2000\n         c = 500\n\nIn [72]: learn_env = Finance(symbol, features, window=10, lags=6,\n                          leverage=1, min_performance=0.85,\n                          start=a, end=a + b, mu=None, std=None)\n\nIn [73]: learn_env.data.info()\n         <class 'pandas.core.frame.DataFrame'>\n         DatetimeIndex: 2000 entries, 2010-01-19 to 2017-12-26\n         Data columns (total 5 columns):\n          #   Column  Non-Null Count  Dtype\n         ---  ------  --------------  -----\n          0   EUR=    2000 non-null   float64\n          1   r       2000 non-null   float64\n          2   s       2000 non-null   float64\n          3   m       2000 non-null   float64\n          4   v       2000 non-null   float64\n         dtypes: float64(5)\n         memory usage: 93.8 KB\n\nIn [74]: valid_env = Finance(symbol, features, window=learn_env.window,\n                          lags=learn_env.lags, leverage=learn_env.leverage,\n                          min_performance=learn_env.min_performance,\n                          start=a + b, end=a + b + c,\n                          mu=learn_env.mu, std=learn_env.std)\n\nIn [75]: valid_env.data.info()\n         <class 'pandas.core.frame.DataFrame'>\n         DatetimeIndex: 500 entries, 2017-12-27 to 2019-12-20\n         Data columns (total 5 columns):\n          #   Column  Non-Null Count  Dtype\n         ---  ------  --------------  -----\n          0   EUR=    500 non-null    float64\n          1   r       500 non-null    float64\n          2   s       500 non-null    float64\n          3   m       500 non-null    float64\n          4   v       500 non-null    float64\n         dtypes: float64(5)\n         memory usage: 23.4 KB\n\nIn [76]: set_seeds(100)\n         agent = FQLAgent(24, 0.0001, learn_env, valid_env)\n\nIn [77]: episodes = 61\n\nIn [78]: agent.learn(episodes)\n         =======================================================================\n         episode: 20/61 | VALIDATION | treward:  494 | perf: 1.169 | eps: 0.68\n         =======================================================================\n         =======================================================================\n         episode: 40/61 | VALIDATION | treward:  494 | perf: 1.111 | eps: 0.45\n         =======================================================================\n         =======================================================================\n         episode: 60/61 | VALIDATION | treward:  494 | perf: 1.089 | eps: 0.30\n         =======================================================================\n         episode: 61/61 | treward: 1994 | perf: 1.268 | av: 1615.1 | max: 1994\nIn [79]: agent.epsilon\nOut[79]: 0.291602079838278\n\nIn [80]: plt.figure(figsize=(10, 6))\n         x = range(1, len(agent.averages) + 1)\n         y = np.polyval(np.polyfit(x, agent.averages, deg=3), x)\n         plt.plot(agent.averages, label='moving average')\n         plt.plot(x, y, 'r--', label='regression')\n         plt.xlabel('episodes')\n         plt.ylabel('total reward')\n         plt.legend();\n```", "```py\nIn [81]: plt.figure(figsize=(10, 6))\n         x = range(1, len(agent.performances) + 1)\n         y = np.polyval(np.polyfit(x, agent.performances, deg=3), x)\n         y_ = np.polyval(np.polyfit(x, agent.vperformances, deg=3), x)\n         plt.plot(agent.performances[:], label='training')\n         plt.plot(agent.vperformances[:], label='validation')\n         plt.plot(x, y, 'r--', label='regression (train)')\n         plt.plot(x, y_, 'r-.', label='regression (valid)')\n         plt.xlabel('episodes')\n         plt.ylabel('gross performance')\n         plt.legend();\n```"]