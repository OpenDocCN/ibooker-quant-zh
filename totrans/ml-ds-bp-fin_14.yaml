- en: Chapter 10\. Natural Language Processing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章 自然语言处理
- en: Natural language processing (NLP) is a subfield of artificial intelligence used
    to aid computers in understanding natural human language. Most NLP techniques
    rely on machine learning to derive meaning from human languages. When text has
    been provided, the computer utilizes algorithms to extract meaning associated
    with every sentence and collect essential data from them. NLP manifests itself
    in different forms across many disciplines under various aliases, including (but
    not limited to) textual analysis, text mining, computational linguistics, and
    content analysis.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）是人工智能的一个子领域，用于帮助计算机理解自然人类语言。大多数NLP技术依赖于机器学习来从人类语言中提取含义。当提供文本后，计算机利用算法从每个句子中提取相关的含义，并收集其中的关键数据。NLP在许多领域以不同形式表现，有许多别名，包括（但不限于）文本分析、文本挖掘、计算语言学和内容分析。
- en: In the financial landscape, one of the earliest applications of NLP was implemented
    by the US Securities and Exchange Commission (SEC). The group used text mining
    and natural language processing to detect accounting fraud. The ability of NLP
    algorithms to scan and analyze legal and other documents at a high speed provides
    banks and other financial institutions with enormous efficiency gains to help
    them meet compliance regulations and combat fraud.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在金融领域，NLP最早的应用之一是由美国证券交易委员会（SEC）实施的。该组织使用文本挖掘和自然语言处理来检测会计欺诈。NLP算法扫描和分析法律和其他文件的能力提供了银行和其他金融机构巨大的效率增益，帮助它们符合合规法规并打击欺诈行为。
- en: In the investment process, uncovering investment insights requires not only
    domain knowledge of finance but also a strong grasp of data science and machine
    learning principles. NLP tools may help detect, measure, predict, and anticipate
    important market characteristics and indicators, such as market volatility, liquidity
    risks, financial stress, housing prices, and unemployment.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在投资过程中，揭示投资见解不仅需要金融领域的专业知识，还需要对数据科学和机器学习原理有深厚的掌握。自然语言处理工具可以帮助检测、衡量、预测和预见重要的市场特征和指标，如市场波动性、流动性风险、金融压力、房价和失业率。
- en: News has always been a key factor in investment decisions. It is well established
    that company-specific, macroeconomic, and political news strongly influence the
    financial markets. As technology advances, and market participants become more
    connected, the volume and frequency of news will continue to grow rapidly. Even
    today, the volume of daily text data being produced presents an untenable task
    for even a large team of fundamental researchers to navigate. Fundamental analysis
    assisted by NLP techniques is now critical to unlock the complete picture of how
    experts and the masses feel about the market.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 新闻一直是投资决策的关键因素。众所周知，公司特定的、宏观经济的和政治新闻强烈影响金融市场。随着技术的进步和市场参与者的日益联结，每日产生的文本数据量和频率将继续迅速增长。即使在今天，每天产生的文本数据量也使得即使是一个庞大的基础研究团队也难以应对。通过NLP技术辅助的基础分析现在对解锁专家和大众对市场感受的完整图片至关重要。
- en: In banks and other organizations, teams of analysts are dedicated to poring
    over, analyzing, and attempting to quantify qualitative data from news and SEC-mandated
    reporting. Automation using NLP is well suited in this context. NLP can provide
    in-depth support in the analysis and interpretation of various reports and documents.
    This reduces the strain that repetitive, low-value tasks put on human employees.
    It also provides a level of objectivity and consistency to otherwise subjective
    interpretations; mistakes from human error are lessened. NLP can also allow a
    company to garner insights that can be used to assess a creditor’s risk or gauge
    brand-related sentiment from content across the web.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在银行和其他组织中，团队的分析师专注于浏览、分析和试图量化从新闻和SEC规定的报告中提取的定性数据。在这种背景下，利用NLP进行自动化是非常合适的。NLP可以在分析和解释各种报告和文件时提供深入支持。这减轻了重复的、低价值任务给人类员工带来的压力。它还为本来主观解释提供了客观性和一致性；减少了人为错误带来的错误。NLP还可以使公司获取见解，用于评估债权人风险或从网络内容中评估与品牌相关的情绪。
- en: With the rise in popularity of live chat software in banking and finance businesses,
    NLP-based chatbots are a natural evolution. The combination of robo-advisors with
    chatbots is expected to automate the entire process of wealth and portfolio management.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 随着银行业和金融业中实时聊天软件的普及，基于自然语言处理的聊天机器人是其自然演变。预计将机器人顾问与聊天机器人结合起来，自动化整个财富和投资组合管理过程。
- en: In this chapter, we present three NLP-based case studies that cover applications
    of NLP in algorithmic trading, chatbot creation, and document interpretation and
    automation. The case studies follow a standardized seven-step model development
    process presented in [Chapter 2](ch02.xhtml#Chapter2). Key model steps for NLP-based
    problems are data preprocessing, feature representation, and inference. As such,
    these areas, along with the related concepts and Python-based examples, are outlined
    in this chapter.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了三个基于自然语言处理的案例研究，涵盖了算法交易、聊天机器人创建以及文档解释与自动化等应用。这些案例研究遵循了在[第二章](ch02.xhtml#Chapter2)中呈现的标准化七步模型开发过程。解决基于自然语言处理的问题的关键模型步骤包括数据预处理、特征表示和推理。因此，在本章中概述了这些领域及其相关概念和基于
    Python 的示例。
- en: '[“Case Study 1: NLP and Sentiment Analysis–Based Trading Strategies”](#CaseStudy1NLP)
    demonstrates the usage of sentiment analysis and word embedding for a trading
    strategy. This case study highlights key focus areas for implementing an NLP-based
    trading strategy.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[“案例研究 1：基于情感分析的交易策略”](#CaseStudy1NLP)展示了情感分析和词嵌入在交易策略中的应用。该案例研究突出了实施基于自然语言处理的交易策略的关键重点。'
- en: 'In [“Case Study 2: Chatbot Digital Assistant”](#CaseStudy2NLP), we create a
    chatbot and demonstrate how NLP enables chatbots to understand messages and respond
    appropriately. We leverage Python-based packages and modules to develop a chatbot
    in a few lines of code.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在[“案例研究 2：聊天机器人数字助理”](#CaseStudy2NLP)中，我们创建了一个聊天机器人，并展示了自然语言处理如何使聊天机器人理解消息并恰当地回应。我们利用基于
    Python 的包和模块，在几行代码中开发了一个聊天机器人。
- en: '[“Case Study 3: Document Summarization”](#CaseStudy3NLP) illustrates the use
    of an NLP-based *topic modeling* technique to discover hidden topics or themes
    across documents. The purpose of this case study is to demonstrate the usage of
    NLP to automatically summarize large collections of documents to facilitate organization
    and management, as well as search and recommendations.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[“案例研究 3：文档摘要”](#CaseStudy3NLP)展示了使用基于自然语言处理的*主题建模*技术来发现文档间的隐藏主题或主题。这个案例研究的目的是演示利用自然语言处理自动总结大量文档以便于组织管理、搜索和推荐。'
- en: This Chapter’s Code Repository
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本章的代码库
- en: The Python code for this chapter is included under the [Chapter 10 - Natural
    Language Processing](https://oreil.ly/J2FFn) folder of the online GitHub repository
    for this chapter. For any new NLP-based case study, use the common template from
    the code repository and modify the elements specific to the case study. The templates
    are designed to run on the cloud (i.e., Kaggle, Google Colab, and AWS).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的 Python 代码包含在在线 GitHub 代码库的[第 10 章 - 自然语言处理](https://oreil.ly/J2FFn)文件夹中。对于任何新的基于自然语言处理的案例研究，使用代码库中的通用模板，并修改特定于案例研究的元素。这些模板设计为在云端运行（例如
    Kaggle、Google Colab 和 AWS）。
- en: 'Natural Language Processing: Python Packages'
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理：Python 包
- en: Python is one of the best options to build an NLP-based expert system, and a
    large variety of open source NLP libraries are available for Python programmers.
    These libraries and packages contain ready-to-use modules and functions to incorporate
    complex NLP steps and algorithms, making implementation fast, easy, and efficient.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Python 是构建基于自然语言处理专家系统的最佳选择之一，为 Python 程序员提供了大量开源自然语言处理库。这些库和包包含了可直接使用的模块和函数，用于集成复杂的自然语言处理步骤和算法，使得实施快速、简单且高效。
- en: In this section, we will describe three Python-based NLP libraries we’ve found
    to be the most useful and that we will be using in this chapter.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将描述三个我们认为最有用的基于 Python 的自然语言处理库，并将在本章中使用它们。
- en: NLTK
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NLTK
- en: '[NLTK](https://www.nltk.org) is the most famous Python NLP library, and it
    has led to incredible breakthroughs across several areas. Its modularized structure
    makes it excellent for learning and exploring NLP concepts. However, it has heavy
    functionality with a steep learning curve.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[NLTK](https://www.nltk.org) 是最著名的 Python 自然语言处理库，已在多个领域取得了令人惊叹的突破。其模块化结构使其非常适合学习和探索自然语言处理的概念。然而，它的功能强大，学习曲线陡峭。'
- en: 'NLTK can be installed using the typical installation procedure. After installing
    NLTK, NLTK Data needs to be downloaded. The NLTK Data package includes a pretrained
    tokenizer `punkt` for English, which can be downloaded as well:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK 可以使用常规安装程序安装。安装 NLTK 后，还需要下载 NLTK Data。NLTK Data 包含了一个用于英语的预训练分词器 `punkt`，也可以下载：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: TextBlob
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TextBlob
- en: '[TextBlob](https://oreil.ly/tABh4) is built on top of NLTK. This is one of
    the best libraries for fast prototyping or building applications with minimal
    performance requirements. TextBlob makes text processing simple by providing an
    intuitive interface to NLTK. TextBlob can be imported using the following command:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[TextBlob](https://oreil.ly/tABh4) 建立在 NLTK 之上。这是一个用于快速原型设计或构建应用程序的最佳库之一，其性能要求最低。TextBlob通过为
    NLTK 提供直观的接口，简化了文本处理。可以使用以下命令导入 TextBlob：'
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: spaCy
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: spaCy
- en: '[spaCy](https://spacy.io) is an NLP library designed to be fast, streamlined,
    and production-ready. Its philosophy is to present only one algorithm (the best
    one) for each purpose. We don’t have to make choices and can focus on being productive.
    spaCy uses its own pipeline to perform multiple preprocessing steps at the same
    time. We will demonstrate it in a subsequent section.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[spaCy](https://spacy.io) 是一个专为快速、简洁和即用型设计的 NLP 库。其理念是为每个目的只提供一种算法（最佳算法）。我们不必做出选择，可以专注于提高工作效率。spaCy
    使用自己的管道同时执行多个预处理步骤。我们将在随后的部分进行演示。'
- en: 'spaCy’s models can be installed as Python packages, just like any other module.
    To load a model, use `spacy.load` with the model’s shortcut link or package name
    or a path to the data directory:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy 的模型可以安装为 Python 包，就像任何其他模块一样。要加载模型，请使用 `spacy.load` 与模型的快捷链接或包名称或数据目录的路径：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In addition to these, there are a few other libraries, such as gensim, that
    we will explore for some of the examples in this chapter.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些之外，还有一些其他库，比如 gensim，我们将在本章的一些示例中探索它们。
- en: 'Natural Language Processing: Theory and Concepts'
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理：理论与概念
- en: As we have already established, NLP is a subfield of artificial intelligence
    concerned with programming computers to process textual data in order to gain
    useful insights. All NLP applications go through common sequential steps, which
    include some combination of preprocessing textual data and representing the text
    as predictive features before feeding them into a statistical inference algorithm.
    [Figure 10-1](#StepsNLP) outlines the major steps in an NLP-based application.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们已经确定的，NLP 是人工智能的一个子领域，涉及编程使计算机处理文本数据以获取有用的洞见。所有 NLP 应用程序都经历常见的顺序步骤，包括某种形式的文本数据预处理，并在将其输入统计推断算法之前将文本表示为预测特征。[图 10-1](#StepsNLP)
    概述了基于 NLP 的应用程序中的主要步骤。
- en: '![mlbf 1001](Images/mlbf_1001.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![mlbf 1001](Images/mlbf_1001.png)'
- en: Figure 10-1\. Natural language processing pipeline
  id: totrans-31
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-1\. 自然语言处理流水线
- en: The next section reviews these steps. For a thorough coverage of the topic,
    the reader is referred to [*Natural Language Processing with Python*](https://www.oreilly.com/library/view/natural-language-processing/9780596803346)
    by Steven Bird, Ewan Klein, and Edward Loper (O’Reilly).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将回顾这些步骤。如需全面了解该主题，可参考 Steven Bird、Ewan Klein 和 Edward Loper（O’Reilly）的[*Python
    自然语言处理*](https://www.oreilly.com/library/view/natural-language-processing/9780596803346)。
- en: 1\. Preprocessing
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 预处理
- en: There are usually multiple steps involved in preprocessing textual data for
    NLP. [Figure 10-1](#StepsNLP) shows the key components of the preprocessing steps
    for NLP. These are tokenization, stop words removal, stemming, lemmatization,
    PoS (part-of-speech) tagging, and NER (Name Entity Recognition).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在为 NLP 预处理文本数据时通常涉及多个步骤。[图 10-1](#StepsNLP) 显示了用于 NLP 预处理步骤的关键组件。这些步骤包括分词、去停用词、词干提取、词形还原、词性标注和命名实体识别。
- en: 1.1\. Tokenization
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1\. 分词
- en: '*Tokenization* is the task of splitting a text into meaningful segments, called
    tokens. These segments could be words, punctuation, numbers, or other special
    characters that are the building blocks of a sentence. A set of predetermined
    rules allows us to effectively convert a sentence into a list of tokens. The following
    code snippets show sample word tokenization using the NLTK and TextBlob packages:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*分词*是将文本分割成有意义的片段（称为标记）的任务。这些片段可以是单词、标点符号、数字或其他构成句子的特殊字符。一组预定的规则使我们能够有效地将句子转换为标记列表。以下代码片段展示了使用
    NLTK 和 TextBlob 包进行示例词分词的样本：'
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The NLTK data package includes a pretrained `Punkt` tokenizer for English,
    which was previously loaded:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK 数据包中包含了一个预训练的英文 `Punkt` 分词器，之前已加载：
- en: '[PRE4]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`Output`'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出`'
- en: '[PRE5]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let’s look at tokenization using TextBlob:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看使用 TextBlob 进行标记化：
- en: '[PRE6]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '`Output`'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出`'
- en: '[PRE7]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 1.2\. Stop words removal
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2\. 停用词移除
- en: 'At times, extremely common words that offer little value in modeling are excluded
    from the vocabulary. These words are called stop words. The code for removing
    stop words using the NLTK library is shown below:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，在建模中排除那些提供很少价值的极为常见的单词。这些单词被称为停用词。使用 NLTK 库去除停用词的代码如下所示：
- en: '[PRE8]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '`Output`'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出`'
- en: '[PRE9]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We first load the language model and store it in the stop words variable. The
    `stopwords.words('english')` is a set of default stop words for the English language
    model in NLTK. Next, we simply iterate through each word in the input text, and
    if the word exists in the stop word set of the NLTK language model, the word is
    removed. As we can see, stop words, such as *are* and *most*, are removed from
    the sentence.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 首先加载语言模型并将其存储在停用词变量中。`stopwords.words('english')` 是 NLTK 语言模型中默认的英语停用词集合。接下来，我们只需迭代输入文本中的每个单词，如果该单词存在于
    NLTK 语言模型的停用词集合中，则将其移除。正如我们所见，像 *are* 和 *most* 这样的停用词已从句子中移除。
- en: 1.3\. Stemming
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3\. 词干提取
- en: '*Stemming* is the process of reducing inflected (or sometimes derived) words
    to their stem, base, or root form (generally a written word form). For example,
    if we were to stem the words *Stems*, *Stemming*, *Stemmed*, and *Stemitization*,
    the result would be a single word: *Stem*. The code for stemming using the NLTK
    library is shown here:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '*词干提取* 是将屈折（或有时是派生）的单词减少为它们的词干、基本形式或根形式（通常是书面单词形式）的过程。例如，如果我们对 *Stems*, *Stemming*,
    *Stemmed*, 和 *Stemitization* 进行词干提取，结果将是一个单词：*Stem*。使用 NLTK 库进行词干提取的代码如下：'
- en: '[PRE10]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '`Output`'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出`'
- en: '[PRE11]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 1.4\. Lemmatization
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4\. 词形还原
- en: 'A slight variant of stemming is *lemmatization*. The major difference between
    the two processes is that stemming can often create nonexistent words, whereas
    lemmas are actual words. An example of lemmatization is *run* as a base form for
    words like *running* and *ran*, or that the words *better* and *good* are considered
    the same lemma. The code for lemmatization using the TextBlob library is shown
    below:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*词形归并* 是词干提取的一个轻微变体。两个过程的主要区别在于，词干提取通常会创建不存在的单词，而词形归并产生的是实际的单词形式。词形归并的一个例子是将
    *run* 作为 *running* 和 *ran* 等词的基本形式，或者将 *better* 和 *good* 视为相同的词形。使用 TextBlob 库进行词形归并的代码如下所示：'
- en: '[PRE12]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '`Output`'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出`'
- en: '[PRE13]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 1.5\. PoS tagging
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.5\. 词性标注
- en: '*Part-of-speech (PoS) tagging* is the process of assigning a token to its grammatical
    category (e.g., verb, noun, etc.) in order to understand its role within a sentence.
    PoS tags have been used for a variety of NLP tasks and are extremely useful since
    they provide a linguistic signal of how a word is being used within the scope
    of a phrase, sentence, or document.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*词性标注* 是将一个标记分配给它的语法类别（例如动词、名词等）的过程，以便理解它在句子中的角色。词性标记已被用于各种自然语言处理任务，并且非常有用，因为它们提供了一个关于单词在短语、句子或文档中使用方式的语言信号。'
- en: 'After a sentence is split into tokens, a tagger, or PoS tagger, is used to
    assign each token to a part-of-speech category. Historically, [hidden Markov models
    (HMM)](https://oreil.ly/OpuRm) were used to create such taggers. More recently,
    artificial neural networks have been leveraged. The code for PoS tagging using
    the TextBlob library is shown here:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在将句子分割为标记后，使用一个标记器或词性标注器将每个标记分配到一个词性类别中。在历史上，使用[隐马尔可夫模型（HMM）](https://oreil.ly/OpuRm)来创建这样的标注器。近年来，也开始使用人工神经网络。使用
    TextBlob 库进行词性标注的代码如下所示：
- en: '[PRE14]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '`Output`'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出`'
- en: '[PRE15]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 1.6\. Named entity recognition
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.6\. 命名实体识别
- en: '*Named entity recognition* (NER) is an optional next step in data preprocessing
    that seeks to locate and classify named entities in text into predefined categories.
    These categories can include names of persons, organizations, locations, expressions
    of times, quantities, monetary values, or percentages. The NER performed using
    spaCy is shown below:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*命名实体识别*（NER）是数据预处理中的可选下一步，旨在将文本中的命名实体定位并分类到预定义的类别中。这些类别可以包括人名、组织名、地点名、时间表达、数量、货币值或百分比。使用
    spaCy 进行的命名实体识别如下所示：'
- en: '[PRE16]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '`Output`'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出`'
- en: '[PRE17]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Visualizing named entities in text using the `displacy` module, as shown in
    [Figure 10-2](#NER), can also be incredibly helpful in speeding up development
    and debugging the code and training process:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在文本中使用 `displacy` 模块来可视化命名实体，如 [图 10-2](#NER) 所示，可以极大地帮助加快开发和调试代码以及训练过程：
- en: '[PRE18]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![mlbf 1002](Images/mlbf_1002.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![mlbf 1002](Images/mlbf_1002.png)'
- en: Figure 10-2\. NER output
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-2\. 命名实体识别输出
- en: '1.7\. spaCy: All of the above steps in one go'
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.7\. spaCy：一步到位地执行上述所有步骤
- en: All the preprocessing steps shown above can be performed in one step using spaCy.
    When we call *nlp* on a text, spaCy first tokenizes the text to produce a *Doc*
    object. The *Doc* is then processed in several different steps. This is also referred
    to as the *processing pipeline*. The pipeline used by the default models consists
    of a *tagger*, a *parser*, and an *entity recognizer*. Each pipeline component
    returns the processed *Doc*, which is then passed on to the next component, as
    demonstrated in [Figure 10-3](#PreprocessingPipeline).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 所有上述预处理步骤可以在spaCy中一步完成。当我们在文本上调用*nlp*时，spaCy首先对文本进行标记化以生成*Doc*对象。然后，*Doc*在多个不同的步骤中进行处理。这也称为*处理管道*。默认模型使用的管道由*标记器*、*解析器*和*实体识别器*组成。每个管道组件返回处理后的*Doc*，然后传递给下一个组件，如[图10-3](#PreprocessingPipeline)所示。
- en: '![mlbf 1003](Images/mlbf_1003.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![mlbf 1003](Images/mlbf_1003.png)'
- en: Figure 10-3\. spaCy pipeline (based on an image from [the spaCy website](https://oreil.ly/ZhMlp).
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-3\. spaCy处理管道（基于[spaCy网站](https://oreil.ly/ZhMlp)上的一幅图像）。
- en: '[PRE19]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '`Output`'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出`'
- en: '|  | Token | is_stop_word | lemma | POS |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '|  | 标记 | 是否停止词 | 词形 | 词性 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0 | Google | False | Google | PROPN |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 0 | Google | False | Google | PROPN |'
- en: '| 1 | is | True | be | VERB |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 是 | True | be | VERB |'
- en: '| 2 | looking | False | look | VERB |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 看 | False | look | VERB |'
- en: '| 3 | at | True | at | ADP |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 在 | True | at | ADP |'
- en: '| 4 | buying | False | buy | VERB |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 购买 | False | buy | VERB |'
- en: '| 5 | U.K. | False | U.K. | PROPN |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 英国 | False | U.K. | PROPN |'
- en: '| 6 | startup | False | startup | NOUN |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 6 | startup | False | startup | NOUN |'
- en: '| 7 | for | True | for | ADP |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 对于 | True | for | ADP |'
- en: '| 8 | $ | False | $ | SYM |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 8 | $ | False | $ | SYM |'
- en: '| 9 | 1 | False | 1 | NUM |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 1 | False | 1 | NUM |'
- en: '| 10 | billion | False | billion | NUM |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 十亿 | False | billion | NUM |'
- en: The output for each of the preprocessing steps is shown in the preceding table.
    Given that spaCy performs a wide range of NLP-related tasks in a single step,
    it is a highly recommended package. As such, we will be using spaCy extensively
    in our case studies.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 每个预处理步骤的输出如上表所示。考虑到spaCy在单一步骤中执行广泛的自然语言处理任务，它是一个强烈推荐的包。因此，在我们的案例研究中，我们将广泛使用spaCy。
- en: In addition to the above preprocessing steps, there are other frequently used
    preprocessing steps, such as *lower casing* or *nonalphanumeric data removing*,
    that we can perform depending on the type of data. For example, data scraped from
    a website has to be cleansed further, including the removal of HTML tags. Data
    from a PDF report must be converted into a text format.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述预处理步骤外，还有其他经常使用的预处理步骤，例如*小写处理*或*非字母数字数据去除*，这些步骤取决于数据类型可以执行。例如，从网站上爬取的数据必须进一步清洗，包括去除HTML标签。从PDF报告中提取的数据必须转换为文本格式。
- en: 'Other optional preprocessing steps include dependency parsing, coreference
    resolution, triplet extraction, and relation extraction:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 其他可选的预处理步骤包括依赖分析、核心指代消解、三元组提取和关系提取：
- en: Dependency parsing
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 依赖分析
- en: Assigns a syntactic structure to sentences to make sense of how the words in
    the sentence relate to each other.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 为句子分配句法结构，以理解句子中单词之间的关系。
- en: Coreference resolution
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 核心指代消解
- en: The process of connecting tokens that represent the same entity. It is common
    in languages to introduce a subject with their name in one sentence and then refer
    to them as him/her/it in subsequent sentences.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 连接代表同一实体的标记的过程。在语言中，通常在一句话中引入主语并在随后的句子中用他/她/它代指他们。
- en: Triplet extraction
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 三元组提取
- en: The process of recording subject, verb, and object triplets when available in
    the sentence structure.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在句子结构中记录主语、动词和宾语三元组的过程（可用时）。
- en: Relation extraction
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 关系提取
- en: A broader form of triplet extraction in which entities can have multiple interactions.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个更广泛的三元组提取形式，其中实体可以有多种交互。
- en: These additional steps should be performed only if they will help with the task
    at hand. We will demonstrate examples of these preprocessing steps in the case
    studies in this chapter.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 只有在有助于手头任务时才执行这些额外的步骤。我们将在本章的案例研究中展示这些预处理步骤的示例。
- en: 2\. Feature Representation
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 特征表示
- en: The vast majority of NLP-related data, such as news feed articles, PDF reports,
    social media posts, and audio files, is created for human consumption. As such,
    it is often stored in an unstructured format, which cannot be readily processed
    by computers. In order for the preprocessed information to be conveyed to the
    statistical inference algorithm, the tokens need to be translated into predictive
    features. A model is used to embed raw text into a *vector space*.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数与自然语言处理相关的数据，如新闻稿、PDF报告、社交媒体帖子和音频文件，都是为人类消费而创建的。因此，它们通常以非结构化格式存储，计算机无法直接处理。为了将预处理信息传递给统计推断算法，需要将标记转换为预测特征。模型用于将原始文本嵌入到*向量空间*中。
- en: 'Feature representation involves two things:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 特征表示涉及两个方面：
- en: A vocabulary of known words.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已知单词的词汇表。
- en: A measure of the presence of known words.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已知单词存在的度量。
- en: 'Some of the feature representation methods are:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 一些特征表示方法包括：
- en: Bag of words
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词袋模型
- en: TF-IDF
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TF-IDF
- en: Word embedding
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词嵌入
- en: Pretrained models (e.g., word2vec, [GloVe](https://oreil.ly/u9SZG), spaCy’s
    word embedding model)
  id: totrans-117
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预训练模型（例如word2vec，[GloVe](https://oreil.ly/u9SZG)，spaCy的词嵌入模型）
- en: Customized deep learning–based feature representation^([1](ch10.xhtml#idm45174899480680))
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自定义深度学习的特征表示^([1](ch10.xhtml#idm45174899480680))
- en: Let’s learn more about each of these methods.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更多地了解每种方法。
- en: 2.1\. Bag of words—word count
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. 词袋模型—词频统计
- en: In natural language processing, a common technique for extracting features from
    text is to place all words that occur in the text in a bucket. This approach is
    called a *bag of words* model. It’s referred to as a bag of words because any
    information about the structure of the sentence is lost. In this technique, we
    build a single matrix from a collection of texts, as shown in [Figure 10-4](#BagOfWords),
    in which each row represents a token and each column represents a document or
    sentence in our corpus. The values of the matrix represent the count of the number
    of instances of the token appearing.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理中，从文本中提取特征的常见技术是将文本中出现的所有单词放入一个桶中。这种方法称为*词袋*模型。它被称为词袋模型，因为它丢失了关于句子结构的任何信息。在这种技术中，我们从一组文本中构建一个单一的矩阵，如[图
    10-4](#BagOfWords)所示，其中每一行表示一个标记，每一列表示我们语料库中的一个文档或句子。矩阵的值表示标记出现的次数。
- en: '![mlbf 1004](Images/mlbf_1004.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![mlbf 1004](Images/mlbf_1004.png)'
- en: Figure 10-4\. Bag of words
  id: totrans-123
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-4\. 词袋模型
- en: 'The `CountVectorizer` from sklearn provides a simple way to both tokenize a
    collection of text documents and encode new documents using that vocabulary. The
    `fit_transform` function learns the vocabulary from one or more documents and
    encodes each document in the word as a vector:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '`CountVectorizer`来自sklearn，提供了一种简单的方法来对文本文档集合进行标记化，并使用该词汇表对新文档进行编码。`fit_transform`函数从一个或多个文档中学习词汇，并将每个文档编码为一个词向量：'
- en: '[PRE20]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '`Output`'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出`'
- en: '[PRE21]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We can see an array version of the encoded vector showing a count of one occurrence
    for each word except *the* (index 10), which has an occurrence of two. Word counts
    are a good starting point, but they are very basic. One issue with simple counts
    is that some words like *the* will appear many times, and their large counts will
    not be very meaningful in the encoded vectors. These bag of words representations
    are sparse because the vocabularies are vast, and a given word or document would
    be represented by a large vector comprised mostly of zero values.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到编码向量的数组版本显示每个单词出现一次，除了*the*（索引 10），它出现了两次。词频是一个很好的起点，但它们非常基础。简单计数的一个问题是，像*the*这样的一些词会出现很多次，它们的大量计数在编码向量中意义不大。这些词袋表示是稀疏的，因为词汇量庞大，给定的单词或文档将由大部分零值组成。
- en: 2.2\. TF-IDF
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. TF-IDF
- en: 'An alternative is to calculate word frequencies, and by far the most popular
    method for that is *TF-IDF*, which stands for *Term Frequency–Inverse Document
    Frequency*:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种选择是计算词频，迄今为止最流行的方法是*TF-IDF*，即*词频-逆文档频率*：
- en: Term Frequency
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 词频
- en: This summarizes how often a given word appears within a document.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这总结了特定单词在文档中出现的频率。
- en: Inverse Document Frequency
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 逆文档频率
- en: This downscales words that appear a lot across documents.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这降低了跨文档频繁出现的词的权重。
- en: 'Put simply, TF-IDF is a word frequency score that tries to highlight words
    that are more interesting (i.e., frequent *within* a document, but not *across*
    documents). The *TfidfVectorizer* will tokenize documents, learn the vocabulary
    and the inverse document frequency weightings, and allow you to encode new documents:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 简单地说，TF-IDF 是一个单词频率分数，试图突出显示更有趣的单词（即在文档内频繁但在文档间不频繁）。*TfidfVectorizer* 将标记文档、学习词汇表和反文档频率加权，并允许您对新文档进行编码：
- en: '[PRE22]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '`Output`'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出`'
- en: '[PRE23]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: In the provided code snippet, a vocabulary of nine words is learned from the
    documents. Each word is assigned a unique integer index in the output vector.
    The sentences are encoded as a nine-element sparse array, and we can review the
    final scorings of each word with different values from the other words in the
    vocabulary.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在提供的代码片段中，从文档中学习了一个包含九个单词的词汇表。每个单词在输出向量中被分配了一个唯一的整数索引。句子被编码为一个九元稀疏数组，我们可以通过不同于词汇表中其他单词的值来审查每个单词的最终得分。
- en: 2.3\. Word embedding
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3\. 单词嵌入
- en: A *word embedding* represents words and documents using a dense vector representation.
    In an embedding, words are represented by dense vectors in which a vector represents
    the projection of the word into a continuous vector space. The position of a word
    within the vector space is learned from text and is based on the words that surround
    the word when it is used. The position of a word in the learned vector space is
    referred to as its *embedding*.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '*单词嵌入* 使用稠密向量表示单词和文档。在嵌入中，单词通过稠密向量表示，其中向量表示单词投射到连续向量空间中。单词在向量空间中的位置是从文本中学习的，基于在使用单词时周围的单词。单词在学习的向量空间中的位置称为其*嵌入*。'
- en: Some of the models of learning word embeddings from text include word2Vec, spaCy’s
    pretrained word embedding model, and GloVe. In addition to these carefully designed
    methods, a word embedding can be learned as part of a deep learning model. This
    can be a slower approach, but it tailors the model to a specific training dataset.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 从文本学习单词嵌入的一些模型包括 word2Vec、spaCy 的预训练单词嵌入模型和 GloVe。除了这些精心设计的方法外，单词嵌入还可以作为深度学习模型的一部分进行学习。这可能是一种较慢的方法，但它会根据特定的训练数据集调整模型。
- en: '2.3.1\. Pretrained model: Via spaCy'
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.1\. 预训练模型：通过 spaCy
- en: spaCy comes with built-in representation of text as vectors at different levels
    of word, sentence, and document. The underlying vector representations come from
    a word embedding model, which generally produces a dense, multidimensional semantic
    representation of words (as shown in the following example). The word embedding
    model includes 20,000 unique vectors with 300 dimensions. Using this vector representation,
    we can calculate similarities and dissimilarities between tokens, named entities,
    noun phrases, sentences, and documents.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy 自带文本的向量表示，包括单词、句子和文档的不同级别。底层的向量表示来自于单词嵌入模型，通常生成单词的稠密、多维语义表示（如下例所示）。单词嵌入模型包括
    20,000 个唯一的 300 维向量。利用这种向量表示，我们可以计算标记、命名实体、名词短语、句子和文档之间的相似性和不相似性。
- en: 'The word embedding in spaCy is performed by first loading the model and then
    processing text. The vectors can be accessed directly using the `.vector` attribute
    of each processed token (i.e., word). The mean vector for the entire sentence
    is also calculated simply by using the vector, providing a very convenient input
    for machine learning models based on sentences:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在spaCy中，单词嵌入是通过首先加载模型然后处理文本来执行的。可以直接使用每个处理过的标记（即单词）的`.vector`属性访问向量。还可以通过使用向量来简单计算整个句子的平均向量，为基于句子的机器学习模型提供非常便捷的输入：
- en: '[PRE24]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '`Output:\`'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE25]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The vector representation of the sentence for the first 10 features of the pretrained
    model is shown in the output.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出中显示了预训练模型的前十个特征的句子的向量表示。
- en: '2.3.2\. Pretrained model: Word2Vec using gensim package'
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.2\. 预训练模型：使用 gensim 包的 Word2Vec
- en: 'The Python-based implementation of the word2vec model using the [gensim package](https://oreil.ly/p9hOJ)
    is demonstrated here:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这里演示了使用[gensim包](https://oreil.ly/p9hOJ)的基于Python的word2vec模型的实现：
- en: '[PRE26]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '`Output`'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出`'
- en: '[PRE27]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The vector representation of the sentence for the first five features of the
    pretrained word2vec model is shown above.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 上面显示了预训练的word2vec模型的前五个特征的句子的向量表示。
- en: 3\. Inference
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 推理
- en: As with other artificial intelligence tasks, an inference generated by an NLP
    application usually needs to be translated into a decision in order to be actionable.
    Inference falls under three machine learning categories covered in the previous
    chapters (i.e., supervised, unsupervised, and reinforcement learning). While the
    type of inference required depends on the business problem and the type of training
    data, the most commonly used algorithms are supervised and unsupervised.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他人工智能任务一样，由自然语言处理应用程序生成的推理通常需要被翻译成决策以便可执行。推理属于前面章节涵盖的三种机器学习类别之一（即，监督、无监督和强化学习）。虽然所需的推理类型取决于业务问题和训练数据的类型，但最常用的算法是监督和无监督。
- en: One of the most frequently used supervised methodologies in NLP is the *Naive
    Bayes* model, as it can produce reasonable accuracy using simple assumptions.
    A more complex supervised methodology is using artificial neural network architectures.
    In past years, these architectures, such as recurrent neural networks (RNNs),
    have dominated NLP-based inference.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理中，最常用的监督方法之一是 *Naive Bayes* 模型，因为它可以使用简单的假设产生合理的准确性。更复杂的监督方法是使用人工神经网络架构。在过去的几年中，这些架构，如循环神经网络
    (RNNs)，已经主导了基于自然语言处理的推理。
- en: Most of the existing literature in NLP focuses on supervised learning. As such,
    unsupervised learning applications constitute a relatively less developed subdomain
    in which measuring *document similarity* is among the most common tasks. A popular
    unsupervised technique applied in NLP is *Latent Semantic Analysis* (LSA). LSA
    looks at relationships between a set of documents and the words they contain by
    producing a set of latent concepts related to the documents and terms. LSA has
    paved the way for a more sophisticated approach called *Latent Dirichlet Allocation*
    (LDA), under which documents are modeled as a finite mixture of topics. These
    topics in turn are modeled as a finite mixture over words in the vocabulary. LDA
    has been extensively used for *topic modeling*—a growing area of research in which
    NLP practitioners build probabilistic generative models to reveal likely topic
    attributions for words.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理中的大部分现有文献都集中在监督学习上。因此，无监督学习应用构成了一个相对不太发达的子领域，其中衡量 *文档相似性* 是最常见的任务之一。在自然语言处理中应用的一种流行的无监督技术是
    *潜在语义分析* (LSA)。LSA 通过生成与文档和词相关的一组潜在概念来查看一组文档和它们包含的单词之间的关系。LSA 为一种更复杂的方法铺平了道路，这种方法称为
    *潜在狄利克雷分配* (LDA)，在其中，文档被建模为主题的有限混合。这些主题又被建模为词汇表中的单词的有限混合。LDA 已被广泛用于 *主题建模* ——这是一个研究日益增长的领域，在该领域中，自然语言处理从业者构建概率生成模型以揭示单词可能的主题归属。
- en: Since we have reviewed many supervised and unsupervised learning models in the
    previous chapters, we will provide details only on Naive Bayes and LDA models
    in the next sections. These are used extensively in NLP and were not covered in
    the previous chapters.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在前面的章节中已经审查了许多监督和无监督学习模型，所以我们将仅在接下来的章节中详细介绍 Naive Bayes 和 LDA 模型。这些模型在自然语言处理中被广泛使用，并且在前面的章节中没有涉及到。
- en: 3.1\. Supervised learning example—Naive Bayes
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 监督学习示例—Naive Bayes
- en: Naive Bayes is a family of algorithms based on applying [*Bayes’s theorem*](https://oreil.ly/bVeZK)
    with a strong (naive) assumption that every feature used to predict the category
    of a given sample is independent of the others. They are probabilistic classifiers
    and therefore will calculate the probability of each category using Bayes’s theorem.
    The category with the highest probability will be output.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: Naive Bayes 是一类基于应用 [*贝叶斯定理*](https://oreil.ly/bVeZK) 的算法族，其强（天真）假设是用于预测给定样本类别的每个特征都与其他特征无关。它们是概率分类器，因此将使用贝叶斯定理计算每个类别的概率。输出的将是具有最高概率的类别。
- en: In NLP, a Naive Bayes approach assumes that all word features are independent
    of each other given the class labels. Due to this simplifying assumption, Naive
    Bayes is very compatible with a bag-of-words word representation, and it has been
    demonstrated to be fast, reliable, and accurate in a number of NLP applications.
    Moreover, despite its simplifying assumptions, it is competitive with (and at
    times even outperforms) more complicated classifiers.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理中，Naive Bayes 方法假定所有单词特征在给定类标签的情况下彼此独立。由于这一简化假设，Naive Bayes 与词袋表示法非常兼容，并且已经证明在许多自然语言处理应用中快速、可靠和准确。此外，尽管有简化的假设，但它在某些情况下与更复杂的分类器相比具有竞争力甚至表现更好。
- en: 'Let us look at the usage of Naive Bayes for the inference in a sentiment analysis
    problem. We take a dataframe in which there are two sentences with sentiments
    assigned to each. In the next step, we convert the sentences into a feature representation
    using `CountVectorizer`. The features and sentiments are used to train and test
    the model using Naive Bayes:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看朴素贝叶斯在情感分析问题中的推理使用。我们拿一个包含两个带情感的句子的数据框架。在下一步中，我们使用`CountVectorizer`将这些句子转换为特征表示。这些特征和情感被用来训练和测试朴素贝叶斯模型：
- en: '[PRE28]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '`Output`'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '`Output`'
- en: '[PRE29]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: As we can see, the Naive Bayes trains the model fairly well from the two sentences.
    The model gives a sentiment of zero and one for the test sentences “Apple price
    plunge” and “Amazon price jumps,” respectively, given the sentences used for training
    also had the keywords “plunge” and “jumps,” with corresponding sentiment assignments.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，朴素贝叶斯从这两个句子中很好地训练了模型。该模型为测试句子“Apple price plunge”和“Amazon price jumps”分别给出了情感值零和一，因为训练时使用的句子也具有关键词“plunge”和“jumps”，并对应情感分配。
- en: '3.2\. Unsupervised learning example: LDA'
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 无监督学习示例：LDA
- en: 'LDA is extensively used for *topic modeling* because it tends to produce meaningful
    topics that humans can interpret, assigns topics to new documents, and is extensible.
    It works by first making a key assumption: documents are generated by first selecting
    *topics*, and then, for each topic, a set of *words*. The algorithm then reverse
    engineers this process to find the topics in a document.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: LDA广泛用于*主题建模*，因为它倾向于生成人类可以解释的有意义的主题，并为新文档分配主题，并且是可扩展的。它的工作方式首先是做出一个关键假设：文档是通过首先选择*主题*，然后对于每个主题选择一组*单词*来生成的。然后算法逆向工程这个过程来找出文档中的主题。
- en: 'In the following code snippet, we show an implementation of LDA for topic modeling.
    We take two sentences and convert the sentences into a feature representation
    using `CountVectorizer`. These features and the sentiments are used to train the
    model and produce two smaller matrices representing the topics:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码片段中，我们展示了一个用于主题建模的LDA实现。我们拿两个句子，并使用`CountVectorizer`将这些句子转换为特征表示。这些特征和情感被用来训练模型，并生成代表主题的两个较小的矩阵：
- en: '[PRE30]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '`Output`'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '`Output`'
- en: '[PRE31]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We will be using LDA for topic modeling in the third case study of this chapter
    and will discuss the concepts and interpretation in detail.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的第三个案例研究中，我们将使用LDA进行主题建模，并详细讨论概念和解释。
- en: To review, in order to approach any NLP-based problem, we need to follow the
    preprocessing, feature extraction, and inference steps. Now, let’s dive into the
    case studies.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下，为了解决任何基于NLP的问题，我们需要遵循预处理、特征提取和推理步骤。现在，让我们深入研究案例研究。
- en: 'Case Study 1: NLP and Sentiment Analysis–Based Trading Strategies'
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究1：基于NLP和情感分析的交易策略
- en: 'Natural language processing offers the ability to quantify text. One can begin
    to ask questions such as: How positive or negative is this news? and How can we
    quantify words?'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理提供了量化文本的能力。人们可以开始问这样的问题：这篇新闻有多正面或负面？我们如何量化这些词语？
- en: Perhaps the most notable application of NLP is its use in algorithmic trading.
    NLP provides an efficient means of monitoring market sentiments. By applying NLP-based
    sentiment analysis techniques to news articles, reports, social media, or other
    web content, one can effectively determine whether those sources have a positive
    or negative senitment score. Sentiment scores can be used as a directional signal
    to buy stocks with positive scores and sell stocks with negative ones.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理最显著的应用可能是在算法交易中的应用。NLP提供了一种有效的监控市场情绪的手段。通过将基于NLP的情感分析技术应用于新闻文章、报告、社交媒体或其他网络内容，可以有效地确定这些来源的情感积分是正面的还是负面的。情感分数可以用作买入具有正面分数的股票和卖出具有负面分数的股票的定向信号。
- en: Trading strategies based on text data are becoming more popular as the amount
    of unstructured data increases. In this case study we are going to look at how
    one can use NLP-based sentiments to build a trading strategy.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 基于文本数据的交易策略因非结构化数据量的增加而越来越受欢迎。在这个案例研究中，我们将看看如何使用基于NLP的情感来构建交易策略。
- en: This case study combines concepts presented in previous chapters. The overall
    model development steps of this case study are similar to the seven-step model
    development in prior case studies, with slight modifications.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 本案例研究结合了前几章介绍的概念。本案例研究的整体模型开发步骤与前几个案例研究中的七步模型开发类似，略有修改。
- en: '![](Images/bracket_top.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/bracket_top.png)'
- en: Blueprint for Building a Trading Strategy Based on Sentiment Analysis
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 建立基于情感分析的交易策略的蓝图
- en: 1\. Problem definition
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1\. 问题定义
- en: Our goal is to (1) use NLP to extract information from news headlines, (2) assign
    a sentiment to that information, and (3) use sentiment analysis to build a trading
    strategy.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是（1）使用NLP从新闻标题中提取信息，（2）为该信息分配情感，以及（3）使用情感分析构建交易策略。
- en: 'The data used for this case study will be from the following sources:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 本案例研究使用的数据将来自以下来源：
- en: News headlines data compiled from the RSS feeds of several news websites
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 从几家新闻网站的RSS源编译的新闻标题数据
- en: For the purpose of this study, we will look only at the headlines, not at the
    full text of the stories. Our dataset contains around 82,000 headlines from May
    2011 through December 2018.^([2](ch10.xhtml#idm45174898988232))
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 为了本研究的目的，我们只关注新闻标题，而不是整篇文章。我们的数据集包含从2011年5月至2018年12月约82,000个新闻标题。^([2](ch10.xhtml#idm45174898988232))
- en: Yahoo Finance website for stock data
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: Yahoo Finance网站上的股票数据
- en: The return data for stocks used in this case study is derived from Yahoo Finance
    price data.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 本案例研究中使用的股票回报数据来自Yahoo Finance的价格数据。
- en: '[Kaggle](https://www.kaggle.com)'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '[Kaggle](https://www.kaggle.com)'
- en: We will use the labeled data of news sentiments for a classification-based sentiment
    analysis model. Note that this data may not be fully applicable to the case at
    hand and is used here for demonstration purposes.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用带标签的新闻情感数据进行基于分类的情感分析模型。请注意，这些数据可能并不完全适用于本案例，仅用于演示目的。
- en: Stock market lexicon
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 股市词汇表
- en: '*Lexicon* refers to the component of an NLP system that contains information
    (semantic, grammatical) about individual words or word strings. This is created
    based on stock market conversations in microblogging services.^([3](ch10.xhtml#idm45174898982312))'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '*词汇表*指的是NLP系统中包含有关单词或词组的信息（语义、语法）的组件。这是根据微博服务中的股市交流创建的。^([3](ch10.xhtml#idm45174898982312))'
- en: The key steps of this case study are outlined in [Figure 10-5](#StepsSentimentTrading).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 本案例研究的关键步骤详见[图 10-5](#StepsSentimentTrading)。
- en: '![mlbf 1005](Images/mlbf_1005.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![mlbf 1005](Images/mlbf_1005.png)'
- en: Figure 10-5\. Steps in a sentiment analysis–based trading strategy
  id: totrans-197
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-5\. 基于情感分析的交易策略步骤
- en: Once we are done with preprocessing, we will look at the different sentiment
    analysis models. The results from the sentiment analysis step are used to develop
    the trading strategy.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们完成预处理，我们将研究不同的情感分析模型。情感分析步骤的结果用于开发交易策略。
- en: 2\. Getting started—loading the data and Python packages
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2\. 入门—加载数据和Python包
- en: 2.1\. Loading the Python packages
  id: totrans-200
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1\. 加载Python包
- en: The first set of libraries to be loaded are the NLP-specific libraries discussed
    above. Refer to the Jupyter notebook of this case study for details of the other
    libraries.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 首先加载的一组库是上述的NLP专用库。有关其他库的详细信息，请参阅本案例研究的Jupyter笔记本。
- en: '[PRE32]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 2.2\. Loading the data
  id: totrans-203
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2\. 加载数据
- en: 'In this step, we load the stock price data from Yahoo Finance. We select 10
    stocks for this case study. These stocks are some of the largest stocks in the
    S&P 500 by market share:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在此步骤中，我们从Yahoo Finance加载股票价格数据。我们选择了10支股票作为本案例研究的对象。这些股票是标准普尔500指数中市值最大的股票之一：
- en: '[PRE33]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '![mlbf 10in01](Images/mlbf_10in01.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![mlbf 10in01](Images/mlbf_10in01.png)'
- en: The data contains the price and volume data of the stocks along with their ticker
    name. In the next step, we look at the news data.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 数据包含股票的价格和成交量数据以及它们的代码名称。在下一步中，我们将研究新闻数据。
- en: 3\. Data preparation
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3\. 数据准备
- en: In this step, we load and preprocess the news data, followed by combining the
    news data with the stock return data. This combined dataset will be used for the
    model development.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在此步骤中，我们加载并预处理新闻数据，然后将新闻数据与股票回报数据合并。这个合并后的数据集将用于模型开发。
- en: 3.1\. Preprocessing news data
  id: totrans-211
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1\. 预处理新闻数据
- en: 'The news data is downloaded from the News RSS feed, and the file is available
    in JSON format. The JSON files for different dates are kept under a zipped folder.
    The data is downloaded using the standard web-scraping Python package Beautiful
    Soup, which is an open source framework. Let us look at the content of the downloaded
    JSON file:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 新闻数据从新闻RSS源下载，并以JSON格式保存。不同日期的JSON文件存储在一个压缩文件夹中。数据使用标准的网络抓取Python包Beautiful
    Soup下载。让我们来看看下载的JSON文件内容：
- en: '[PRE35]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '`Output`'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出`'
- en: '[PRE36]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We can see that the JSON format is not suitable for the algorithm. We need
    to get the news from the JSONs. Regex becomes the vital part of this step. Regex
    can find a pattern in the raw, messy text and perform actions accordingly. The
    following function parses HTML by using information encoded in the JSON file:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，JSON格式不适合该算法。我们需要从JSON中获取新闻。在此步骤中，正则表达式成为关键部分。正则表达式可以在原始、混乱的文本中找到模式并执行相应的操作。以下函数通过使用JSON文件中编码的信息解析HTML：
- en: '[PRE37]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Let us see how the output looks like after running the JSON parser:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看运行JSON解析器后的输出：
- en: '[PRE38]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '`Output`'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '`Output`'
- en: '[PRE39]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: As we can see, the output is converted into a more readable format after JSON
    parsing.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，JSON解析后的输出转换为更易读的格式。
- en: 'While evaluating the sentiment analysis models, we also analyze the relationship
    between the sentiments and subsequent stock performance. In order to understand
    the relationship, we use *event return*, which is the return that corresponds
    to the event. We do this because at times the news is reported late (i.e., after
    market participants are aware of the announcement) or after market close. Having
    a slightly wider window ensures that we capture the essence of the event. *Event
    return* is defined as:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估情感分析模型时，我们还分析情感与后续股票表现之间的关系。为了理解这种关系，我们使用*事件回报*，即与事件相对应的回报。我们这样做是因为有时新闻报道较晚（即市场参与者已经了解公告），或者在市场关闭后。略微扩大窗口可以确保捕捉事件的本质。*事件回报*的定义如下：
- en: <math><mrow><msub><mi>R</mi> <mrow><mi>t</mi><mo>–</mo><mn>1</mn></mrow></msub>
    <mo>+</mo> <msub><mi>R</mi> <mi>t</mi></msub> <mo>+</mo> <msub><mi>R</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></math>
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><msub><mi>R</mi> <mrow><mi>t</mi><mo>–</mo><mn>1</mn></mrow></msub>
    <mo>+</mo> <msub><mi>R</mi> <mi>t</mi></msub> <mo>+</mo> <msub><mi>R</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></math>
- en: where <math display="inline"><mrow><msub><mi>R</mi> <mrow><mi>t</mi><mo>–</mo><mn>1</mn></mrow></msub>
    <mo>,</mo> <msub><mi>R</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></math>
    are the returns before and after the news data, and <math alttext="upper R Subscript
    t"><msub><mi>R</mi> <mi>t</mi></msub></math> is the return on the day of the news
    (i.e., time *t*).
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 <math display="inline"><mrow><msub><mi>R</mi> <mrow><mi>t</mi><mo>–</mo><mn>1</mn></mrow></msub>
    <mo>,</mo> <msub><mi>R</mi> <mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></math>
    是新闻数据前后的回报，而 <math alttext="upper R Subscript t"><msub><mi>R</mi> <mi>t</mi></msub></math>
    是新闻当天的回报（即时间 *t*）。
- en: 'Let us extract the event return from the data:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从数据中提取事件回报：
- en: '[PRE40]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Now we have all the data in place. We will prepare a combined dataframe, which
    will have the news headlines mapped to the date, the returns (event return, current
    return, and next day’s return), and stock ticker. This dataframe will be used
    for building the sentiment analysis model and the trading strategy:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了所有数据。我们将准备一个合并的数据框架，其中将新闻标题映射到日期，回报（事件回报、当前回报和次日回报）和股票代码。此数据框将用于构建情感分析模型和交易策略：
- en: '[PRE41]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '`Output`'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '`Output`'
- en: '|  | ticker | headline | date | eventRet | Close |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '|  | ticker | headline | date | eventRet | Close |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 5 | AMZN | Whole Foods (WFMI) –5.2% following a downgrade… | 2011-05-02 |
    0.017650 | 201.19 |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| 5 | AMZN | Whole Foods (WFMI) –5.2% following a downgrade… | 2011-05-02 |
    0.017650 | 201.19 |'
- en: '| 11 | NFLX | Netflix (NFLX +1.1%) shares post early gains a… | 2011-05-02
    | –0.013003 | 33.88 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 11 | NFLX | Netflix (NFLX +1.1%) shares post early gains a… | 2011-05-02
    | –0.013003 | 33.88 |'
- en: 'Let us look at the overall shape of the data:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看数据的整体形状：
- en: '[PRE42]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '`Output`'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '`Output`'
- en: '[PRE43]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: In this step, we prepared a clean dataframe that has ticker, headline, event
    return, return for a given day, and future return for 10 stock tickers, totaling
    2,759 rows of data. Let us evaluate the models for sentiment analysis in the next
    step.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在此步骤中，我们准备了一个干净的数据框架，其中包含10个股票代码的标题、事件回报、给定日期的回报和未来10天的回报，总共有2759行数据。让我们在下一步中评估情感分析模型。
- en: 4\. Evaluate models for sentiment analysis
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4\. 评估情感分析模型
- en: 'In this section, we will go through the following three approaches of computing
    sentiments for the news:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论以下三种计算新闻情绪的方法：
- en: Predefined model—TextBlob package
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预定义模型—TextBlob包
- en: Tuned model—classification algorithms and LSTM
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整模型—分类算法和LSTM
- en: Model based on financial lexicon
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于金融词汇的模型
- en: Let us go through the steps.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐步进行。
- en: 4.1\. Predefined model—TextBlob package
  id: totrans-246
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1\. 预定义模型—TextBlob包
- en: 'The `TextBlob sentiment` function is a pretrained model based on the Naive
    Bayes classification algorithm. The function maps adjectives that are frequently
    found in movie reviews^([4](ch10.xhtml#idm45174898012728)) to sentiment polarity
    scores ranging from –1 to +1 (negative to positive), converting a sentence to
    a numerical value. We apply this on all headline articles. An example of getting
    the sentiment for a news text is shown below:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '`TextBlob情绪`函数是基于朴素贝叶斯分类算法的预训练模型。该函数将经常出现在电影评论中的形容词映射到从–1到+1（负面到正面）的情绪极性分数，将句子转换为数值。我们将其应用在所有的头条新闻上。以下是获取新闻文本情感的示例：'
- en: '[PRE44]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '`Output`'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出`'
- en: '[PRE45]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The sentiment for the statement is 0.5\. We apply this on all headlines we
    have in the data:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 该声明的情绪为0.5。我们将其应用在我们拥有的所有头条新闻上：
- en: '[PRE46]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Let us inspect the scatterplot of the sentiments and returns to examine the
    correlation between the two for all 10 stocks.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查散点图的情绪和回报，以检查所有10只股票之间的相关性。
- en: '![mlbf 1006](Images/mlbf_1006.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![mlbf 1006](Images/mlbf_1006.png)'
- en: 'A plot for a single stock (APPL) is also shown in the following chart (see
    the code in the Jupyter notebook in the GitHub repository for this book for more
    details on the code):'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 单个股票（APPL）的图表也显示在以下图表中（有关代码的详细信息，请参见GitHub存储库中的Jupyter笔记本）：
- en: '![mlbf 10in03](Images/mlbf_10in03.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![mlbf 10in03](Images/mlbf_10in03.png)'
- en: 'From the scatterplots, we can see that there is not a strong relationship between
    the news and the sentiments. The correlation between return and sentiments is
    positive (4.27%), which means that news with positive sentiments leads to positive
    return and is expected. However, the correlation is not very high. Even looking
    at the overall scatterplot, we see the majority of the sentiments concentrated
    around zero. This raises the question of whether a sentiment score trained on
    movie reviews is appropriate for stock prices. The `sentiment_assessments` attribute
    lists the underlying values for each token and can help us understand the reason
    for the overall sentiment of a sentence:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 从散点图中我们可以看出，新闻和情绪之间没有很强的关系。回报与情绪之间的相关性是正的（4.27%），这意味着情绪积极的新闻导致积极的回报，这是预期的。然而，相关性并不是很高。即使在整体的散点图上看，我们看到大多数情绪集中在零附近。这引发了一个问题，即电影评论训练的情感评分是否适用于股票价格。`sentiment_assessments`属性列出了每个标记的基础值，可以帮助我们理解句子整体情绪的原因：
- en: '[PRE47]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '`Output`'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出`'
- en: '[PRE48]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: We see that the statement has a positive sentiment of 0.5, but it appears the
    word “touching” gave rise to the positive sentiment. More intuitive words, such
    as “high,” do not. This example shows that the context of the training data is
    important for the sentiment score to be meaningful. There are many predefined
    packages and functions available for sentiment analysis, but it is important to
    be careful and have a thorough understanding of the problem’s context before using
    a function or an algorithm for sentiment analysis.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到这个声明的情绪是0.5，但似乎是“touching”这个词引起了积极情绪。更直观的词语，如“high”，却没有。这个例子显示了训练数据的上下文对于情感分数的意义是重要的。在进行情感分析之前，有许多预定义的包和函数可供使用，但在使用函数或算法进行情感分析之前，认真并全面了解问题的背景是很重要的。
- en: For this case study, we may need sentiments trained on the financial news. Let
    us take a look at that in the next step.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个案例研究，我们可能需要针对金融新闻进行情感训练。让我们在下一步中看看。
- en: 4.2\. Supervised learning—classification algorithms and LSTM
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 监督学习——分类算法和LSTM
- en: 'In this step, we develop a customized model for sentiment analysis based on
    available labeled data. The label data for this is obtained from the [Kaggle website](https://www.kaggle.com):'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步中，我们根据可用的标记数据开发了一个定制的情绪分析模型。这些标签数据是从[Kaggle网站](https://www.kaggle.com)获取的。
- en: '[PRE49]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '`Output`'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出`'
- en: '|  | datetime | headline | ticker | sentiment |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '|  | 日期时间 | 标题 | 股票 | 情绪 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0 | 1/16/2020 5:25 | $MMM fell on hard times but could be set to re… | MMM
    | 0 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 1/16/2020 5:25 | $MMM遭遇困境，但可能即将… | MMM | 0 |'
- en: '| 1 | 1/11/2020 6:43 | Wolfe Research Upgrades 3M $MMM to ¡§Peer Perf… | MMM
    | 1 |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1/11/2020 6:43 | Wolfe Research将3M $MMM升级为“同行表现…… | MMM | 1 |'
- en: The data has headlines for the news across 30 different stocks, totaling 9,470
    rows, and has sentiments labeled zero and one. We perform the classification steps
    using the classification model development template presented in [Chapter 6](ch06.xhtml#Chapter6).
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 数据包括30个不同股票的新闻标题，总计9,470行，并且有情感标签为零或一。我们使用第[6章](ch06.xhtml#Chapter6)中呈现的分类模型开发模板执行分类步骤。
- en: 'In order to run a supervised learning model, we first need to convert the news
    headlines into a feature representation. For this exercise, the underlying vector
    representations come from a *spaCy word embedding model*, which generally produces
    a dense, multidimensional semantic representation of words (as shown in the example
    below). The word embedding model includes 20,000 unique vectors with 300 dimensions.
    We apply this on all headlines in the data processed in the previous step:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 为了运行监督学习模型，我们首先需要将新闻标题转换为特征表示。在本练习中，底层的向量表示来自于一个*spaCy词嵌入模型*，通常会生成单词的密集的、多维的语义表示（如下例所示）。词嵌入模型包括20,000个唯一向量，每个向量有300维。我们在前一步骤处理的所有新闻标题上应用此模型：
- en: '[PRE50]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Now that we have prepared the independent variable, we train the classification
    model in a similar manner as discussed in [Chapter 6](ch06.xhtml#Chapter6). We
    have the sentiments label zero or one as the dependent variable. We first divide
    the data into training and test sets and run the key classification models (i.e.,
    logistic regression, CART, SVM, random forest, and artificial neural network).
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好独立变量，我们将以与第[6章](ch06.xhtml#Chapter6)讨论类似的方式训练分类模型。我们将情感标签为零或一作为因变量。首先我们将数据分成训练集和测试集，并运行关键的分类模型（即逻辑回归、CART、SVM、随机森林和人工神经网络）。
- en: We will also include LSTM, which is an RNN-based model,^([5](ch10.xhtml#idm45174897768808))
    in the list of models considered. An RNN-based model performs well for NLP, because
    it stores the information for current features as well neighboring ones for prediction.
    It maintains a memory based on past information, which enables the model to predict
    the current output conditioned on long distance features and looks at the words
    in the context of the entire sentence, rather than simply looking at the individual
    words.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将包括LSTM在内，这是一种基于RNN的模型，^([5](ch10.xhtml#idm45174897768808))列入考虑的模型列表中。基于RNN的模型在自然语言处理中表现良好，因为它存储当前特征以及相邻特征以进行预测。它根据过去的信息维持记忆，使得模型能够根据长距离特征预测当前输出，并查看整个句子上下文中的单词，而不仅仅是看个别单词。
- en: 'For us to be able to feed the data into our LSTM model, all input documents
    must have the same length. We use the Keras `tokenizer` function to tokenize the
    strings and then use `texts_to_sequences` to make sequences of words. More details
    can be found on the [Keras website](https://oreil.ly/2YS-P). We will limit the
    maximum review length to *max_words* by truncating longer reviews and pad shorter
    reviews with a null value (0). We can accomplish this using the `pad_sequences`
    function, also in Keras. The third parameter is the *input_length* (set to 50),
    which is the length of each comment sequence:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够将数据输入我们的LSTM模型，所有输入文档必须具有相同的长度。我们使用Keras `tokenizer`函数对字符串进行标记化，然后使用`texts_to_sequences`将单词序列化。更多细节可以在[Keras网站](https://oreil.ly/2YS-P)上找到。我们将通过截断较长的评论并使用空值（0）填充较短的评论，将最大评论长度限制为*max_words*。我们可以使用Keras中的`pad_sequences`函数来实现这一点。第三个参数是*input_length*（设置为50），即每个评论序列的长度：
- en: '[PRE51]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'In the following code snippet, we use the Keras library to build an artificial
    neural network classifier based on an underlying LSTM model. The network starts
    with an *embedding* layer. This layer lets the system expand each token to a larger
    vector, allowing the network to represent a word in a meaningful way. The layer
    takes 20,000 as the first argument (i.e., the size of our vocabulary) and 300
    as the second input parameter (i.e., the dimension of the embedding). Finally,
    given that this is a classification problem and the output needs to be labeled
    as zero or one, the `KerasClassifier` function is used as a wrapper over the LSTM
    model to produce a binary (zero or one) output:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码片段中，我们使用Keras库基于底层的LSTM模型构建了一个人工神经网络分类器。网络从一个*嵌入*层开始。该层允许系统将每个令牌扩展到一个较大的向量，使得网络能够以有意义的方式表示单词。该层将20,000作为第一个参数（即我们词汇的大小），300作为第二个输入参数（即嵌入的维度）。最后，考虑到这是一个分类问题，输出需要被标记为零或一，`KerasClassifier`函数被用作LSTM模型的包装器以生成二进制（零或一）输出：
- en: '[PRE52]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The comparison of all the machine learning models is as follows:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 所有机器学习模型的比较如下：
- en: '![mlbf 10in04](Images/mlbf_10in04.png)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![mlbf 10in04](Images/mlbf_10in04.png)'
- en: As expected, the LSTM model has the best performance in the test set (accuracy
    of 96.7%) as compared to all other models. The performance of the ANN, with a
    training set accuracy of 99% and a test set accuracy of 93.8%, is comparable to
    the LSTM-based model. The performances of random forest (RF), SVM, and logistic
    regression (LR) are reasonable as well. CART and KNN do not perform as well as
    other models. CART shows high overfitting. Let us use the LSTM model for the computation
    of the sentiments in the data in the following steps.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期，LSTM模型在测试集中表现最佳（准确率为96.7%），相比其他模型。ANN的性能，训练集准确率为99%，测试集准确率为93.8%，与基于LSTM的模型相媲美。随机森林（RF）、支持向量机（SVM）和逻辑回归（LR）的性能也很合理。CART和KNN的表现不如其他模型。CART显示出严重的过拟合。让我们使用LSTM模型来计算数据中的情感值。
- en: 4.3\. Unsupervised—model based on a financial lexicon
  id: totrans-283
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 无监督——基于金融词汇表的模型
- en: 'In this case study, we update the VADER lexicon with words and sentiments from
    a lexicon adapted to stock market conversations in microblogging services:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个案例研究中，我们将VADER词汇表与适用于股市微博服务的词汇和情感进行更新：
- en: Lexicons
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 词典
- en: 'Special dictionaries or vocabularies that have been created for analyzing sentiments.
    Most lexicons have a list of positive and negative *polar* words with some score
    associated with them. Using various techniques, such as the position of words,
    the surrounding words, context, parts of speech, and phrases, scores are assigned
    to the text documents for which we want to compute the sentiment. After aggregating
    these scores, we get the final sentiment:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 专门用于分析情感的特殊词典或词汇表。大多数词典都列有带有与之相关的分数的正面和负面 *极性* 词语。使用各种技术，如词语的位置、周围的词语、上下文、词类和短语，为我们想要计算情感的文档分配分数。在聚合这些分数之后，我们得到最终的情感：
- en: VADER (Valence Aware Dictionary for Sentiment Reasoning)
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: VADER（情感推理的价值感知词典）
- en: A prebuilt sentiment analysis model included in the NLTK package. It can give
    both positive and negative polarity scores as well as the strength of the emotion
    of a text sample. It is rule-based and relies heavily on human-rated texts. These
    are words or any textual form of communication labeled according to their semantic
    orientation as either positive or negative.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK包中包含的预建情感分析模型。它可以给出文本样本的正负极性分数以及情感强度。这是基于规则的，并且在很大程度上依赖于人工标注的文本。这些是根据它们的语义取向（正面或负面）标记的单词或任何文本形式的通信。
- en: 'This lexical resource was automatically created using diverse statistical measures
    and a large set of labeled messages from StockTwits, which is a social media platform
    designed for sharing ideas among investors, traders, and entrepreneurs.^([6](ch10.xhtml#idm45174897534248))
    The sentiments are between –1 and 1, similar to the sentiments from TextBlob.
    In the following code snippet, we train the model based on the financial sentiments:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 这个词汇资源是利用各种统计措施和大量来自StockTwits的标记消息自动创建的，StockTwits是一个专为投资者、交易员和企业家分享想法而设计的社交媒体平台。^([6](ch10.xhtml#idm45174897534248))
    这些情感分数介于-1和1之间，与TextBlob的情感分析类似。在以下代码片段中，我们基于金融情感来训练模型：
- en: '[PRE53]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Let us check the sentiment of a news item:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来检查一条新闻的情感：
- en: '[PRE54]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '`Output`'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '`Output`'
- en: '[PRE56]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'We get the sentiments for all the news headlines based in our dataset:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 我们根据数据集中的所有新闻标题获取情感值：
- en: '[PRE57]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Let us look at the relationship between the returns and sentiments, which is
    computed using the lexicon-based methodology for the entire dataset.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看基于词典的方法计算整个数据集的回报和情感之间的关系。
- en: '![mlbf 10in05](Images/mlbf_10in05.png)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
  zh: '![mlbf 10in05](Images/mlbf_10in05.png)'
- en: There are not many instances of high returns for lower sentiment scores, but
    the data may not be very clear. We will look deeper into the comparison of different
    types of sentiment analysis in the next section.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 针对低情感分数的高回报实例不多，但数据可能不太清晰。我们将在下一节更深入地比较不同类型的情感分析。
- en: 4.4\. Exploratory data analysis and comparison
  id: totrans-301
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4\. 探索性数据分析和比较
- en: 'In this section, we compare the sentiments computed using the different techniques
    presented above. Let us look at the sample headlines and the sentiments from three
    different methodologies, followed by a visual analysis:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们比较了使用上述不同技术计算的情感。让我们看一下样本标题和三种不同方法的情感分析，然后进行视觉分析：
- en: '|  | ticker | headline | sentiment_textblob | sentiment_LSTM | sentiment_lex
    |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '|  | 股票代码 | 头条 | 情感_textblob | 情感_LSTM | 情感_词汇 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 4620 | TSM | TSMC (TSM +1.8%) is trading higher after reporting its October
    sales rose 12.6% M/M. DigiTimes adds TSMC has seen a 20%+ jump in orders from
    QCOM, NVDA, SPRD, and Mediatek. The numbers suggest TSMC could beat its Q4 guidance
    (though December tends to be weak), and that chip demand could be stabilizing
    after getting hit hard by inventory corrections. (earlier) (UMC sales) | 0.036667
    | 1 | 0.5478 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| 4620 | 台积电 | 台积电（TSM +1.8%）在报告其10月销售额环比上升12.6%后交易更高。《DigiTimes》补充道，TSMC从高通、英伟达、联发科和联咏等公司那里看到订单增长超过20%。这些数字表明，尽管12月通常疲软，但TSMC可能会超过其第四季度的指导，而芯片需求可能正在通过库存调整后稳定下来。（此前）（联电销售）
    | 0.036667 | 1 | 0.5478 |'
- en: Looking at one of the headlines, the sentiment from this sentence is positive.
    However, the TextBlob sentiment result is smaller in magnitude, suggesting that
    the sentiment is more neutral. This points back to the previous assumption that
    the model trained on movie sentiments likely will not be accurate for stock sentiments.
    The classification-based model correctly suggests the sentiment is positive, but
    it is binary. `Sentiment_lex` has a more intuitive output with a significantly
    positive sentiment.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 查看其中一个标题，这个句子的情感是积极的。然而，TextBlob的情感结果较小，表明情感更为中性。这再次指向之前的假设，即基于电影情感训练的模型可能不适合股票情感。基于分类的模型正确指出情感是积极的，但是它是二进制的。`Sentiment_lex`给出了一个更直观的输出，情感显著为积极。
- en: 'Let us review the correlation of all the sentiments from different methodologies
    versus returns:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们审视来自不同方法的所有情感与回报的相关性：
- en: '![mlbf 10in06](Images/mlbf_10in06.png)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![mlbf 10in06](Images/mlbf_10in06.png)'
- en: All sentiments have positive relationships with the returns, which is intuitive
    and expected. The sentiments from the lexicon methodology are highest, which means
    the stock’s event return can be predicted the best using the lexicon methodology.
    Recall that this methodology leverages financial terms in the model. The LSTM-based
    method also performs better than the TextBlob approach, but the performance is
    slightly worse compared to the lexicon-based methodology.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 所有情感与回报都有正相关关系，这是直觉和预期的。从词汇学方法来看，所有股票的事件回报可以通过这种方法预测得最好。请记住，这种方法利用了金融术语来建模。基于LSTM的方法性能也优于TextBlob方法，但与基于词汇的方法相比稍逊一筹。
- en: 'Let us look at the performance of the methodology at the ticker level. We chose
    a few tickers with the highest market cap for the analysis:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看股票级别的方法论表现。我们选择了市值最高的几个股票进行分析：
- en: '![mlbf 10in07](Images/mlbf_10in07.png)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
  zh: '![mlbf 10in07](Images/mlbf_10in07.png)'
- en: Looking at the chart, the correlation from the lexicon methodology is highest
    across all stock tickers, which corroborates the conclusion from the previous
    analysis. It means the returns can be predicted the best using the lexicon methodology.
    The TextBlob-based sentiments show unintuitive results in some cases, such as
    with JPM.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 查看图表，从词汇学方法论来看，所有股票代码中的相关性最高，这与之前分析的结论一致。这意味着可以最好地使用词汇学方法预测回报。基于TextBlob的情感分析在某些情况下显示出不直观的结果，比如在JPM的情况下。
- en: 'Let us look at the scatterplot for lexicon versus TextBlob methodologies for
    AMZN and GOOG. We will set the LSTM-based method aside since the binary sentiments
    will not be meaningful in the scatterplot:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看AMZN和GOOG的词汇学与TextBlob方法的散点图。由于二进制情感在散点图中没有意义，我们将LSTM方法搁置一边：
- en: '![mlbf 10in08](Images/mlbf_10in08.png)![mlbf 10in09](Images/mlbf_10in09.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![mlbf 10in08](Images/mlbf_10in08.png)![mlbf 10in09](Images/mlbf_10in09.png)'
- en: The lexicon-based sentiments on the left show a positive relationship between
    the sentiments and returns. Some of the points with the highest returns are associated
    with the most positive news. Also, the scatterplot is more uniformly distributed
    in the case of lexicon as compared to TextBlob. The sentiments for TextBlob are
    concentrated around zero, probably because the model is not able to categorize
    financial sentiments well. For the trading strategy, we will be using the lexicon-based
    sentiments, as these are the most appropriate based on the analysis in this section.
    The LSTM-based sentiments are good as well, but they are labeled either zero or
    one. The more granular lexicon-based sentiments are preferred.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧基于词汇的情感显示出情感与收益之间的正相关关系。一些具有最高收益的点与最积极的新闻相关联。此外，与TextBlob相比，基于词汇的散点图更加均匀分布。TextBlob的情感集中在零附近，可能是因为该模型无法很好地分类金融情感。对于交易策略，我们将使用基于词汇的情感，因为根据本节的分析，这些是最合适的选择。基于LSTM的情感也不错，但它们被标记为零或一。更为细粒度的基于词汇的情感更受青睐。
- en: 5\. Models evaluation—building a trading strategy
  id: totrans-316
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5\. 模型评估—构建交易策略
- en: The sentiment data can be used in several ways for building a trading strategy.
    The sentiments can be used as a stand-alone signal to decide buy, sell, or hold
    actions. The sentiment score or the word vectors can also be used to predict the
    return or price of a stock. That prediction can be used to build a trading strategy.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 情感数据可以通过多种方式用于构建交易策略。情感可以作为独立信号用于决定买入、卖出或持有操作。情感评分或词向量还可以用于预测股票的收益或价格。该预测可以用于构建交易策略。
- en: 'In this section, we demonstrate a trading strategy in which we buy or sell
    a stock based on the following approach:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了一种交易策略，根据以下方法买入或卖出股票：
- en: Buy a stock when the change in sentiment score (current sentiment score/previous
    sentiment score) is greater than 0.5\. Sell a stock when the change in sentiment
    score is less than –0.5\. The sentiment score used here is based on the lexicon-based
    sentiments computed in the previous step.
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当情感评分变化（当前情感评分/前一情感评分）大于0.5时购买股票。当情感评分变化小于-0.5时卖出股票。此处使用的情感评分基于前一步骤中计算的基于词汇的情感。
- en: In addition to the sentiments, we use moving average (based on the last 15 days)
    while making a buy or sell decision.
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了情感之外，在做出买卖决策时我们还使用了移动平均（基于过去15天的数据）。
- en: Trades (i.e., buy or sell) are in 100 shares. The initial amount available for
    trading is set to $100,000.
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交易（即买入或卖出）以100股为单位。用于交易的初始金额设定为$100,000。
- en: The strategy threshold, the lot size, and the initial capital can be tweaked
    depending on the performance of the strategy.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 根据策略的表现，可以调整策略阈值、手数和初始资本。
- en: 5.1\. Setting up a strategy
  id: totrans-323
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1\. 设置策略
- en: To set up the trading strategy, we use *backtrader*, which is a convenient Python-based
    framework for implementing and backtesting trading strategies. Backtrader allows
    us to write reusable trading strategies, indicators, and analyzers instead of
    having to spend time building infrastructure. We use the [Quickstart code in the
    backtrader documentation](https://oreil.ly/lyYs4) as a base and adapt it to our
    sentiment-based trading strategy.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 为了设置交易策略，我们使用*backtrader*，这是一个便捷的基于Python的框架，用于实现和回测交易策略。Backtrader允许我们编写可重用的交易策略、指标和分析器，而无需花费时间建设基础设施。我们使用[backtrader文档中的快速入门代码](https://oreil.ly/lyYs4)作为基础，并将其调整为基于情感的交易策略。
- en: 'The following code snippet summarizes the buy and sell logic for the strategy.
    Refer to the Jupyter notebook of this case study for the detailed implementation:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段总结了策略的买入和卖出逻辑。详细的实现请参考本案例研究的Jupyter笔记本：
- en: '[PRE58]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 5.2\. Results for individual stocks
  id: totrans-327
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2\. 单个股票的结果
- en: 'First, we run our strategy on GOOG and look at the results:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们在GOOG上运行我们的策略并查看结果：
- en: '[PRE59]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The output shows the trading log for some of the days and the final return:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示了某些日子的交易日志和最终收益：
- en: '`Output`'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '`Output`'
- en: '[PRE60]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: We analyze the backtesting result in the following plot produced by the backtrader
    package. Refer to the Jupyter notebook of this case study for the detailed version
    of this chart.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 我们分析了由backtrader包生成的下图中的回测结果。详细的图表版本请参考本案例研究的Jupyter笔记本。
- en: '![mlbf 10in10](Images/mlbf_10in10.png)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
  zh: '![mlbf 10in10](Images/mlbf_10in10.png)'
- en: 'The results show an overall profit of $49,719\. The chart is a typical chart^([7](ch10.xhtml#idm45174897105944))
    produced by the backtrader package and is divided into four panels:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示总体利润为$49,719。图表是由 backtrader 包生成的典型图表^([7](ch10.xhtml#idm45174897105944))，分为四个面板：
- en: Top panel
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 顶部面板
- en: The top panel is the *cash value observer*. It keeps track of the cash and the
    total portolio value during the life of the backtesting run. In this run, we started
    with $100,000 and ended with $149,719.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 顶部面板是*现金价值观察者*。它在回测运行期间跟踪现金和总投资组合价值。在这次运行中，我们以$100,000起步，以$149,719结束。
- en: Second panel
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 第二面板
- en: This panel is the *trade observer*. It shows the realized profit/loss of each
    trade. A trade is defined as opening a position and taking the position back to
    zero (directly or crossing over from long to short or short to long). Looking
    at this panel, five out of eight trades are profitable for the strategy.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 此面板是*交易观察者*。它显示每笔交易的实现利润/损失。交易定义为开仓和将头寸归零（直接或从多头到空头或空头到多头）。从这个面板来看，对于策略来说，有八次交易中的五次是盈利的。
- en: Third panel
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 第三面板
- en: This panel is *buy sell observer*. It indicates where buy and sell operations
    have taken place. In general, we see that the buy action takes place when the
    stock price is increasing, and the sell action takes place when the stock price
    has started declining.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 此面板是*买卖观察者*。它指示了买入和卖出操作的发生位置。总的来说，我们看到买入行为发生在股价上涨时，而卖出行为发生在股价开始下跌时。
- en: Bottom panel
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 底部面板
- en: This panel shows the sentiment score, varying between –1 and 1.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 此面板显示情绪得分，介于-1和1之间。
- en: 'Now we choose one of the days (2015-07-17) when a buy action was triggered
    and analyze the news for Google on that and the previous day:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们选择了其中一天（2015-07-17），当买入行动被触发，并分析了该天和前一天的谷歌新闻：
- en: '[PRE61]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '`Output`'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '`Output`'
- en: '[PRE62]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Clearly, the news on the selected day mentions the upgrade of Google, a piece
    of positive news. The previous day mentions the revenue missing estimates, which
    is negative news. Hence, there was a significant change of the news sentiment
    on the selected day, resulting in a buy action triggered by the trading algorithm.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，选定日的新闻提到了谷歌的升级，这是一则积极的新闻。前一天提到了收入低于预期，这是一则负面新闻。因此，在选定的日子，新闻情绪发生了显著变化，导致交易算法触发了买入行动。
- en: 'Next, we run the strategy for FB:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们对 FB 运行策略：
- en: '[PRE63]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '`Output`'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '`Output`'
- en: '[PRE64]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '![mlbf 10in12](Images/mlbf_10in12.png)'
  id: totrans-353
  prefs: []
  type: TYPE_IMG
  zh: '![mlbf 10in12](Images/mlbf_10in12.png)'
- en: 'The details of the backtesting results of the strategy are as follows:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 策略的回测结果的详细信息如下：
- en: Top panel
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 顶部面板
- en: The cash value panel shows an overall profit of $8,041.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 现金价值面板显示总体利润为$8,041。
- en: Second panel
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 第二面板
- en: The trade observer panel shows that six out of seven actions were profitable.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 交易观察者面板显示，七次交易中有六次是盈利的。
- en: Third panel
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 第三面板
- en: The buy/sell observer shows that in general the buy (sell) action took place
    when the stock price was increasing (decreasing).
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 买卖观察者显示，总的来说，买入（卖出）行为发生在股价上涨（下跌）时。
- en: Bottom panel
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 底部面板
- en: It shows a high number of positive sentiments for FB around the 2013–2014 period.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 它显示了在2013年至2014年期间对于 FB 的积极情绪较高的数量。
- en: 5.3\. Results for multiple stocks
  id: totrans-363
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3\. 多只股票的结果
- en: 'In the previous step, we executed the trading strategy on individual stocks.
    Here, we run it on all 10 stocks for which we computed the sentiments:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一步中，我们对各个股票执行了交易策略。在这里，我们对我们计算了情绪的所有10支股票进行了运行：
- en: '[PRE65]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '`Output`'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '`Output`'
- en: '![mlbf 10in13](Images/mlbf_10in13.png)'
  id: totrans-367
  prefs: []
  type: TYPE_IMG
  zh: '![mlbf 10in13](Images/mlbf_10in13.png)'
- en: The strategy performs quite well and yields an overall profit for all the stocks.
    As mentioned before, the buy and sell actions are performed in a lot size of 100\.
    Hence, the dollar amount used is proportional to the stock price. We see the highest
    nominal profit from AMZN and GOOG, which is primarily attributed to the high dollar
    amounts invested for these stocks given their high stock price. Other than overall
    profit, several other metrics, such as Sharpe ratio and maximum drawdown, can
    be used to analyze the performance.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 该策略表现相当不错，并为所有股票带来了总体利润。如前所述，买入和卖出行为的执行是以100手为单位进行的。因此，使用的美元金额与股票价格成比例。我们看到
    AMZN 和 GOOG 的名义利润最高，这主要归因于对这些股票的高金额投资，考虑到它们的高股价。除了总体利润之外，还可以使用几个其他指标，如夏普比率和最大回撤，来分析绩效。
- en: 5.4\. Varying the strategy time period
  id: totrans-369
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4\. 变化策略时间段
- en: 'In the previous analysis, we used the time period from 2011 to 2018 for our
    backtesting. In this step, to further analyze the effectiveness of our strategy,
    we vary the time period of the backtesting and analyze the results. First, we
    run the strategy for all the stocks for the time period between 2012 and 2014:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的分析中，我们使用了从 2011 年到 2018 年的时间段进行了回测。在这一步骤中，为了进一步分析我们策略的有效性，我们变化了回测的时间段并分析了结果。首先，我们在
    2012 年到 2014 年之间为所有股票运行了该策略：
- en: '[PRE66]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '`Output`'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '`Output`'
- en: '![mlbf 10in14](Images/mlbf_10in14.png)'
  id: totrans-373
  prefs: []
  type: TYPE_IMG
  zh: '![mlbf 10in14](Images/mlbf_10in14.png)'
- en: 'The strategy yields an overall profit for all the stocks except AMZN and WMT.
    Now we run the strategy between 2016 and 2018:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 该策略使得除了 AMZN 和 WMT 之外的所有股票总体上获利。现在我们在 2016 年到 2018 年之间运行该策略：
- en: '[PRE67]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '`Output`'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '`Output`'
- en: '![mlbf 10in15](Images/mlbf_10in15.png)'
  id: totrans-377
  prefs: []
  type: TYPE_IMG
  zh: '![mlbf 10in15](Images/mlbf_10in15.png)'
- en: We see a good performance of the sentiment-based strategy across all the stocks
    except AAPL, and we can conclude that it performs quite well on different time
    periods. The strategy can be adjusted by modifying the trading rules or lot sizes.
    Additional metrics can also be used to understand the performance of the strategy.
    The sentiments can also be used along with the other features, such as correlated
    variables and technical indicators for prediction.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到情感驱动策略在所有股票中的表现良好，除了 AAPL 外，我们可以得出它在不同时间段表现相当不错的结论。该策略可以通过修改交易规则或手数大小进行调整。还可以使用其他指标来理解策略的表现。情感还可以与其他特征一起使用，如相关变量和技术指标用于预测。
- en: Conclusion
  id: totrans-379
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结论
- en: In this case study, we looked at various ways in which unstructured data can
    be converted to structured data and then used for analysis and prediction using
    tools for NLP. We have demonstrated three different approaches, including deep
    learning models to develop a model for computing the sentiments. We performed
    a comparison of the models and concluded that one of the most important steps
    in training the model for sentiment analysis is using a domain-specific vocabulary.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个案例研究中，我们探讨了将非结构化数据转换为结构化数据，并使用自然语言处理工具进行分析和预测的各种方法。我们展示了三种不同的方法，包括使用深度学习模型开发计算情绪的模型。我们对这些模型进行了比较，并得出结论：在训练情绪分析模型时，使用领域特定的词汇表是其中一个最重要的步骤。
- en: We also used a pretrained English model by spaCy to convert a sentence into
    sentiments and used the sentiments as signals to develop a trading strategy. The
    initial results suggested that the model trained on a financial lexicon–based
    sentiment could prove to be a viable model for a trading strategy. Additional
    improvements to this can be made by using more complex pretrained sentiment analysis
    models, such as BERT by Google, or different pretrained NLP models available in
    open source platforms. Existing NLP libraries fill in some of the preprocessing
    and encoding steps to allow us to focus on the inference step.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还使用了 spaCy 的预训练英语模型将句子转换为情感，并将情感用作开发交易策略的信号。初步结果表明，基于金融词汇的情感模型训练可能是一个可行的交易策略模型。可以通过使用更复杂的预训练情感分析模型（如
    Google 的 BERT）或开源平台上其他预训练的自然语言处理模型来进一步改进这一模型。现有的 NLP 库填补了一些预处理和编码步骤，使我们能够专注于推理步骤。
- en: We could build on the trading strategy by including more correlated variables,
    technical indicators, or even improved sentiment analysis by using more sophisticated
    preprocessing steps and models based on more relevant financial text data.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 通过包括更多相关变量、技术指标或使用更复杂的预处理步骤和基于更相关的金融文本数据的模型，我们可以进一步完善基于情感的交易策略。
- en: '![](Images/bracket_bottom.png)'
  id: totrans-383
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/bracket_bottom.png)'
- en: 'Case Study 2: Chatbot Digital Assistant'
  id: totrans-384
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究 2：聊天机器人数字助理
- en: '*Chatbots* are computer programs that maintain a conversation with a user in
    natural language. They can understand the user’s intent and send responses based
    on an organization’s business rules and data. These chatbots use deep learning
    and NLP to process language, enabling them to understand human speech.'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '*Chatbots* 是能够用自然语言与用户进行对话的计算机程序。它们能够理解用户的意图，并根据组织的业务规则和数据发送响应。这些聊天机器人使用深度学习和自然语言处理（NLP）来处理语言，从而能够理解人类的语音。'
- en: Chatbots are increasingly being implemented across many domains for financial
    services. Banking bots enable consumers to check their balance, transfer money,
    pay bills, and more. Brokering bots enable consumers to find investment options,
    make investments, and track balances. Customer support bots provide instant responses,
    dramatically increasing customer satisfaction. News bots deliver personalized
    current events information, while enterprise bots enable employees to check leave
    balance, file expenses, check their inventory balance, and approve transactions.
    In addition to automating the process of assisting customers and employees, chatbots
    can help financial institutions gain information about their customers. The bot
    phenomenon has the potential to cause broad disruption in many areas within the
    finance sector.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 越来越多的聊天机器人正在金融服务领域得到应用。银行业的机器人使消费者能够查询余额、转账、支付账单等。经纪业的机器人使消费者能够找到投资选项、进行投资并跟踪余额。客户支持机器人提供即时响应，显著提高客户满意度。新闻机器人提供个性化的当前事件信息，企业机器人使员工能够查询休假余额、提交费用、检查库存余额并批准交易。除了自动化协助客户和员工的过程外，聊天机器人还可以帮助金融机构获取有关客户的信息。这种机器人现象有潜力在金融部门的许多领域引发广泛的颠覆。
- en: 'Depending on the way bots are programmed, we can categorize chatbots into two
    variants:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 根据机器人的编程方式，我们可以将聊天机器人分为两种变体：
- en: Rule-based
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 基于规则
- en: This variety of chatbots is trained according to rules. These chatbots do not
    learn through interactions and may sometimes fail to answer complex queries outside
    of the defined rules.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的聊天机器人根据规则进行训练。这些聊天机器人不通过交互学习，并且有时无法回答超出定义规则的复杂查询。
- en: Self-learning
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 自学习
- en: 'This variety of bots relies on ML and AI technologies to converse with users.
    Self-learning chatbots are further divided into *retrieval-based* and *generative*:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的聊天机器人依赖于机器学习和人工智能技术与用户交谈。自学习聊天机器人进一步分为*检索型*和*生成型*：
- en: Retrieval-based
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 检索型
- en: These chatbot are trained to rank the best response from a finite set of predefined
    responses.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 这些聊天机器人被训练来从有限的预定义响应集中排名最佳响应。
- en: Generative
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 生成型
- en: These chatbots are not built with predefined responses. Instead, they are trained
    using a large number of previous conversations. They require a very large amount
    of conversational data to train.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 这些聊天机器人不是通过预定义的响应构建的。相反，它们是使用大量先前对话来训练的。它们需要大量的对话数据来进行训练。
- en: In this case study, we will prototype a self-learning chatbot that can answer
    financial questions.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个案例研究中，我们将原型化一个可以回答财务问题的自学习聊天机器人。
- en: '![](Images/bracket_top.png)'
  id: totrans-397
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/bracket_top.png)'
- en: Blueprint for Creating a Custom Chatbot Using NLP
  id: totrans-398
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用自然语言处理创建自定义聊天机器人的蓝图
- en: 1\. Problem definition
  id: totrans-399
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1\. 问题定义
- en: The goal of this case study is to build a basic prototype of a conversational
    chatbot powered by NLP. The primary purpose of this chatbot is to help a user
    retrieve a financial ratio about a particular company. Such chatbots are designed
    to quickly retrieve the details about a stock or an instrument that may help the
    user make a trading decision.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 这个案例研究的目标是建立一个基本的基于自然语言处理的对话式聊天机器人原型。这种聊天机器人的主要目的是帮助用户检索特定公司的财务比率。这些聊天机器人旨在快速获取有关股票或工具的详细信息，以帮助用户进行交易决策。
- en: In addition to retrieving a financial ratio, the chatbot could also engage in
    casual conversations with a user, perform basic mathematical calculations, and
    provide answers to questions from a list used to train it. We intend to use Python
    packages and functions for chatbot creation and to customize several components
    of the chatbot architecture to adapt to our requirements.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 除了检索财务比率，聊天机器人还可以与用户进行随意的对话，执行基本的数学计算，并为训练使用的问题提供答案。我们打算使用Python包和函数来创建聊天机器人，并定制聊天机器人架构的多个组件，以适应我们的需求。
- en: The chatbot prototype created in this case study is designed to understand user
    inputs and intention and retrieve the information they are seeking. It is a small
    prototype that could be enhanced for use as an information retrieval bot in banking,
    brokering, or customer support.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个案例研究中创建的聊天机器人原型旨在理解用户输入和意图，并检索他们正在寻找的信息。这是一个小型原型，可以改进为在银行业务、经纪业务或客户支持中用作信息检索机器人。
- en: 2\. Getting started—loading the libraries
  id: totrans-403
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2\. 入门—加载库
- en: 'For this case study, we will use two text-based libraries: spaCy and [ChatterBot](https://oreil.ly/_1DPE).
    spaCy has been previously introduced; ChatterBot is a Python library used to create
    simple chatbots with minimal programming required.'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个案例研究，我们将使用两个基于文本的库：spaCy 和 [ChatterBot](https://oreil.ly/_1DPE)。spaCy 已经被介绍过；ChatterBot
    是一个用于创建简单聊天机器人的 Python 库，只需很少的编程即可。
- en: An untrained instance of ChatterBot starts off with no knowledge of how to communicate.
    Each time a user enters a statement, the library saves the input and response
    text. As ChatterBot receives more inputs, the number of responses it can offer
    and the accuracy of those responses increase. The program selects the response
    by searching for the closest matching known statement to the input. It then returns
    the most likely response to that statement based on how frequently each response
    is issued by the people the bot communicates with.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 一个未经训练的 ChatterBot 实例开始时不具有沟通的知识。每次用户输入语句时，库都会保存输入和响应文本。随着 ChatterBot 收到更多的输入，它能够提供的响应数量和这些响应的准确性会增加。程序通过搜索与输入最接近的已知语句来选择响应。然后，根据每个响应被与机器人交流的人们发出的频率，返回对该语句的最有可能的响应。
- en: 2.1\. Load libraries
  id: totrans-406
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1\. 加载库
- en: 'We import spaCy using the following Python code:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下 Python 代码导入 spaCy：
- en: '[PRE68]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'The ChatterBot library has the modules `LogicAdapter`, `ChatterBotCorpusTrainer`,
    and `ListTrainer`. These modules are used by our bot in order to construct responses
    to user queries. We begin by importing the following:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: ChatterBot 库具有模块 `LogicAdapter`、`ChatterBotCorpusTrainer` 和 `ListTrainer`。我们的机器人使用这些模块构建响应用户查询的响应。我们从导入以下开始：
- en: '[PRE69]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Other libraries used in this exercise are as follows:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 此练习中使用的其他库如下：
- en: '[PRE70]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: Before we move to the customized chatbot, let us develop a chatbot using the
    default features of the ChatterBot package.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们转向定制的聊天机器人之前，让我们使用 ChatterBot 包的默认特性开发一个聊天机器人。
- en: 3\. Training a default chatbot
  id: totrans-414
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3\. 训练默认聊天机器人
- en: 'ChatterBot and many other chatbot packages come with a data utility module
    that can be used to train chatbots. Here are the ChatterBot components we will
    be using:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: ChatterBot 和许多其他聊天机器人包都带有一个数据实用程序模块，可用于训练聊天机器人。以下是我们将要使用的 ChatterBot 组件：
- en: Logic adapters
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑适配器
- en: 'Logic adapters determine the logic for how ChatterBot selects a response to
    a given input statement. It is possible to enter any number of logic adapters
    for your bot to use. In the example below, we are using two inbuilt adapters:
    *BestMatch*, which returns the best known responses, and *MathematicalEvaluation*,
    which performs mathematical operations.'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑适配器确定了 ChatterBot 如何选择响应给定输入语句的逻辑。您可以输入任意数量的逻辑适配器供您的机器人使用。在下面的示例中，我们使用了两个内置适配器：*BestMatch*，它返回最佳已知响应，以及
    *MathematicalEvaluation*，它执行数学运算。
- en: Preprocessors
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理器
- en: ChatterBot’s preprocessors are simple functions that modify the input statement
    a chatbot receives before the statement gets processed by the logic adapter. The
    preprocessors can be customized to perform different preprocessing steps, such
    as tokenization and lemmatization, in order to have clean and processed data.
    In the example below, the default preprocessor for cleaning white spaces, `clean_whitespace`,
    is used.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: ChatterBot 的预处理器是简单的函数，它们在逻辑适配器处理语句之前修改聊天机器人接收到的输入语句。预处理器可以定制以执行不同的预处理步骤，比如分词和词形还原，以便得到干净且处理过的数据。在下面的示例中，使用了清理空格的默认预处理器
    `clean_whitespace`。
- en: Corpus training
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 语料库训练
- en: ChatterBot comes with a corpus data and utility module that makes it easy to
    quickly train the bot to communicate. We use the already existing corpuses *english,
    english.greetings*, and *english.conversations* for training the chatbot.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: ChatterBot 自带一个语料库数据和实用程序模块，使得快速训练机器人进行通信变得容易。我们使用已有的语料库 *english, english.greetings*
    和 *english.conversations* 来训练聊天机器人。
- en: List training
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 列表训练
- en: Just like the corpus training, we train the chatbot with the conversations that
    can be used for training using *ListTrainer*. In the example below, we have trained
    the chatbot using some sample commands. The chatbot can be trained using a significant
    amount of conversation data.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 就像语料库训练一样，我们使用 *ListTrainer* 训练聊天机器人可以用于训练的对话。在下面的示例中，我们使用了一些示例命令来训练聊天机器人。可以使用大量的对话数据来训练聊天机器人。
- en: '[PRE71]'
  id: totrans-424
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Once the chatbot is trained, we can test the trained chatbot by having the
    following conversation:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦聊天机器人被训练好，我们可以通过以下对话来测试训练好的聊天机器人：
- en: '[PRE72]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: In this example, we see a chatbot that gives an intuitive reply in response
    to the input. The first two responses are due to the training on the English greetings
    and English conversation corpuses. Additionally, the responses to *Tell me a joke*
    and *what is a dollar* are due to the training on the English corpus. The computation
    in the fourth line is the result of the chatbot being trained on the `MathematicalEvaluation`
    logical adapter. The responses to *Help!* and *What is Bitcoin?* are the result
    of the customized list trainers. Additionally, we see two different replies to
    *What is Bitcoin?*, given that we trained it using the list trainers.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们看到一个聊天机器人对输入做出直观回复。前两个回复是由于对英语问候语和英语对话语料库的训练。此外，对*Tell me a joke*和*what
    is a dollar*的回复是由于对英语语料库的训练。第四行中的计算是聊天机器人在`MathematicalEvaluation`逻辑适配器上训练的结果。对*Help!*和*What
    is Bitcoin?*的回复是定制列表训练器的结果。此外，我们看到对*What is Bitcoin?*有两种不同的回复，这是因为我们使用列表训练器进行了训练。
- en: Next, we move on to creating a chatbot designed to use a customized logical
    adapter to give financial ratios.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一个设计用于使用定制逻辑适配器给出财务比率的聊天机器人。
- en: '4\. Data preparation: Customized chatbot'
  id: totrans-429
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4\. 数据准备：定制聊天机器人
- en: 'We want our chatbot to be able to recognize and group subtly different inquiries.
    For example, one might want to ask about the company *Apple Inc.* by simply referring
    to it as *Apple*, and we would want to map it to a ticker—*AAPL*, in this case.
    Constructing commonly used phrases in order to refer to firms can be built by
    using a dictionary as follows:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望我们的聊天机器人能够识别和分组微妙不同的查询。例如，有人可能想询问关于公司*苹果公司*，只是简单地称之为*苹果*，而我们希望将其映射到一个股票代码——在本例中为*AAPL*。通过以下方式使用字典构建通常用于引用公司的短语：
- en: '[PRE73]'
  id: totrans-431
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Similarly, we want to build a map for financial ratios:'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们希望为财务比率建立映射：
- en: '[PRE74]'
  id: totrans-433
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'The keys of this dictionary can be used to map to an internal system or API.
    Finally, we want the user to be able to request the phrase in multiple formats.
    Saying *Get me the [RATIO] for [COMPANY]* should be treated similarly to *What
    is the [RATIO] for [COMPANY]?* We build these sentence templates for our model
    to train on by building a list as follows:'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 这个字典的键可以用来映射到内部系统或API。最后，我们希望用户能够以多种格式请求短语。说*Get me the [RATIO] for [COMPANY]*应该与*What
    is the [RATIO] for [COMPANY]?*类似对待。我们通过以下方式构建这些句子模板供我们的模型训练：
- en: '[PRE75]'
  id: totrans-435
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 4.1\. Data construction
  id: totrans-436
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1\. 数据构造
- en: 'We begin constructing our model by creating *reverse* *dictionaries*:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过创建*反向* *字典*来开始构建我们的模型：
- en: '[PRE76]'
  id: totrans-438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Next, we create sample statements for our model. We build a function that gives
    us a random sentence structure, inquiring about a random financial ratio for a
    random company. We will be creating a custom named entity recognition_ model in
    the spaCy framework. This requires training the model to pick up the word or phrase
    in a sample sentence. To train the spaCy model, we need to provide it with an
    example, such as *(*Get me the ROE for Citi*, {"entities”: [(11, 14, *RATIO*),
    (19, 23, *COMPANY*) ]})*.'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们为我们的模型创建样本语句。我们构建一个函数，该函数给出一个随机的句子结构，询问一个随机公司的随机财务比率。我们将在spaCy框架中创建一个自定义命名实体识别模型。这需要训练模型以在样本句子中捕捉单词或短语。为了训练spaCy模型，我们需要提供一个示例，例如*（Get
    me the ROE for Citi，{"entities"：[(11, 14，*RATIO*），（19, 23，*COMPANY*）]}）*。
- en: 4.2\. Training data
  id: totrans-440
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2\. 训练数据
- en: 'The first part of the training example is the sentence. The second is a dictionary
    that consists of entities and the starting and ending index of the label:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 训练示例的第一部分是句子。第二部分是一个包含实体及其标签起始和结束索引的字典：
- en: '[PRE77]'
  id: totrans-442
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Let us define the training data:'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义训练数据：
- en: '[PRE78]'
  id: totrans-444
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 5\. Model creation and training
  id: totrans-445
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5\. 模型创建和训练
- en: Once we have the training data, we construct a *blank* model in spaCy. spaCy’s
    models are statistical, and every decision they make—for example, which part-of-speech
    tag to assign, or whether a word is a named entity—is a prediction. This prediction
    is based on the examples the model has seen during training. To train a model,
    you first need training data—examples of text and the labels you want the model
    to predict. This could be a part-of-speech tag, a named entity, or any other information.
    The model is then shown the unlabeled text and makes a prediction. Because we
    know the correct answer, we can give the model feedback on its prediction in the
    form of an *error gradient* of the loss function. This calculates the difference
    between the training example and the expected output, as shown in [Figure 10-6](#MLTraining).
    The greater the difference, the more significant the gradient, and the more updates
    we need to make to our model.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了训练数据，我们在 spaCy 中构建一个 *空白* 模型。spaCy 的模型是统计的，它们做出的每个决定 — 例如分配哪个词性标签，或者一个词是否是命名实体
    — 都是一个预测。这个预测基于模型在训练过程中看到的示例。要训练一个模型，首先需要训练数据 — 文本示例和您希望模型预测的标签。这可以是词性标签、命名实体或任何其他信息。然后，模型将展示未标记的文本并做出预测。因为我们知道正确答案，所以我们可以以
    *损失函数的误差梯度* 的形式给模型反馈其预测的差异。这计算出训练示例与期望输出之间的差异，如 [图 10-6](#MLTraining) 所示。差异越大，梯度越显著，我们就需要对模型进行更多更新。
- en: '![mlbf 1007](Images/mlbf_1007.png)'
  id: totrans-447
  prefs: []
  type: TYPE_IMG
  zh: '![mlbf 1007](Images/mlbf_1007.png)'
- en: Figure 10-6\. Machine learning–based training in spaCy
  id: totrans-448
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-6\. 基于机器学习的 spaCy 训练
- en: '[PRE79]'
  id: totrans-449
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Next, we create an NER pipeline to our model:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们为我们的模型创建一个 NER 流水线：
- en: '[PRE80]'
  id: totrans-451
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Then we add the training labels that we use:'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们添加我们使用的训练标签：
- en: '[PRE81]'
  id: totrans-453
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 5.1\. Model optimization function
  id: totrans-454
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1\. 模型优化函数
- en: 'Now we start optimization of our models:'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们开始优化我们的模型：
- en: '[PRE82]'
  id: totrans-456
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: Training the NER model is akin to updating the weights for each token. The most
    important step is to use a good optimizer. The more examples of our training data
    that we provide spaCy, the better it will be at recognizing generalized results.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 训练 NER 模型类似于更新每个标记的权重。使用良好的优化器是最重要的步骤。我们提供给 spaCy 的训练数据越多，它在识别广义结果方面的表现就会越好。
- en: 5.2\. Custom logic adapter
  id: totrans-458
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. 自定义逻辑适配器
- en: 'Next, we build our custom logic adapter:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们构建我们的自定义逻辑适配器：
- en: '[PRE83]'
  id: totrans-460
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: With this custom logic adapter, our chatbot will take each input statement and
    try to recognize a *RATIO* and/or *COMPANY* using our NER model. If the model
    finds exactly one *COMPANY* and exactly one *RATIO*, it constructs a URL to guide
    the user.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个自定义逻辑适配器，我们的聊天机器人将接受每个输入语句，并尝试使用我们的 NER 模型识别 *RATIO* 和/或 *COMPANY*。如果模型确切地找到一个
    *COMPANY* 和一个 *RATIO*，它将构建一个 URL 来指导用户。
- en: 5.3\. Model usage—training and testing
  id: totrans-462
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3\. 模型使用 — 训练和测试
- en: 'Now we begin using our chatbot by using the following import:'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们开始使用以下导入使用我们的聊天机器人：
- en: '[PRE84]'
  id: totrans-464
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: We construct our chatbot by adding the `FinancialRatioAdapter` logical adapter
    that we created above to the chatbot. Although the following code snippet only
    shows us adding the `FinancialRatioAdapter`, note that other logical adapters,
    lists, and corpuses used in the prior training of the chatbot are also included.
    Please refer to the Jupyter notebook of the case study for more details.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过将上述创建的 `FinancialRatioAdapter` 逻辑适配器添加到聊天机器人中构建我们的聊天机器人。虽然下面的代码片段仅显示我们添加了
    `FinancialRatioAdapter`，但请注意之前训练过程中使用的其他逻辑适配器、列表和语料库也都包含在内。有关更多详情，请参阅案例研究的 Jupyter
    笔记本。
- en: '[PRE85]'
  id: totrans-466
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Now we test our chatbot using the following statements:'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们使用以下语句测试我们的聊天机器人：
- en: '[PRE86]'
  id: totrans-468
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: As shown above, the custom logic adapter for our chatbot finds a RATIO and/or
    COMPANY in the sentence using our NLP model. If an exact pair is detected, the
    model constructs a URL to guide the user to the answer. Additionally, other logical
    adapters, such as mathematical evaluation, work as expected.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所示，我们聊天机器人的自定义逻辑适配器可以在句子中找到 *RATIO* 和/或 *COMPANY*，使用我们的 NLP 模型。如果检测到一个确切的配对，模型将构建一个
    URL 来引导用户获取答案。此外，其他逻辑适配器（如数学评估）也能如预期地工作。
- en: Conclusion
  id: totrans-470
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结论
- en: Overall, this case study provides an introduction to a number of aspects of
    chatbot development.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，这个案例研究介绍了聊天机器人开发的多个方面。
- en: Using the ChatterBot library in Python allows us to build a simple interface
    to resolve user inputs. To train a blank model, one must have a substantial training
    dataset. In this case study, we looked at patterns available to us and used them
    to generate training samples. Getting the right amount of training data is usually
    the hardest part of constructing a custom chatbot.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中使用ChatterBot库可以构建一个简单的接口来解决用户输入。要训练一个空模型，必须有大量的训练数据集。在这个案例研究中，我们查看了可用的模式，并使用它们生成训练样本。获得正确数量的训练数据通常是构建自定义聊天机器人的最困难的部分。
- en: This case study is a demo project, and significant enhancements can be made
    to each component to extend it to a wide variety of tasks. Additional preprocessing
    steps can be added to have cleaner data to work with. To generate a response from
    our bot for input questions, the logic can be refined further to incorporate better
    similarity measures and embeddings. The chatbot can be trained on a bigger dataset
    using more advanced ML techniques. A series of custom logic adapters can be used
    to construct a more sophisticated ChatterBot. This can be generalized to more
    interesting tasks, such as retrieving information from a database or asking for
    more input from the user.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 本案例研究是一个演示项目，每个组件都可以进行重大改进，以扩展到各种任务。可以添加额外的预处理步骤以获得更清洁的数据。为了从我们的机器人中生成输入问题的响应，逻辑可以进一步优化，以包含更好的相似度测量和嵌入。聊天机器人可以使用更先进的ML技术在更大的数据集上进行训练。一系列自定义逻辑适配器可以用于构建更复杂的ChatterBot。这可以推广到更有趣的任务，如从数据库检索信息或向用户请求更多输入。
- en: '![](Images/bracket_bottom.png)'
  id: totrans-474
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/bracket_bottom.png)'
- en: 'Case Study 3: Document Summarization'
  id: totrans-475
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究3：文档摘要
- en: Document summarization refers to the selection of the most important points
    and topics in a document and arranging them in a comprehensive manner. As discussed
    earlier, analysts at banks and other financial service organizations pore over,
    analyze, and attempt to quantify qualitative data from news, reports, and documents.
    Document summarization using NLP can provide in-depth support in this analyzing
    and interpretation. When tailored to financial documents, such as earning reports
    and financial news, it can help analysts quickly derive key topics and market
    signals from content. Document summarization can also be used to improve reporting
    efforts and can provide timely updates on key matters.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 文档摘要指的是在文档中选择最重要的观点和主题，并以全面的方式进行整理。如前所述，银行及其他金融服务机构的分析师们仔细研究、分析并试图量化来自新闻、报告和文件的定性数据。利用自然语言处理进行文档摘要可以在分析和解释过程中提供深入的支持。当应用于财务文件（如收益报告和财经新闻）时，文档摘要能够帮助分析师快速提取内容中的关键主题和市场信号。文档摘要还可用于改善报告工作，并能够及时更新关键事项。
- en: In NLP, *topic models* (such as LDA, introduced earlier in the chapter) are
    the most frequently used tools for the extraction of sophisticated, interpretable
    text features. These models can surface key topics, themes, or signals from large
    collections of documents and can be effectively used for document summarization.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理中，*主题模型*（如本章节早些时候介绍的LDA）是最常用的工具，用于提取复杂而可解释的文本特征。这些模型能够从大量文档中浮出关键的主题、主题或信号，并且可以有效用于文档摘要。
- en: '![](Images/bracket_top.png)'
  id: totrans-478
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/bracket_top.png)'
- en: Blueprint for Using NLP for Document Summarization
  id: totrans-479
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用自然语言处理进行文档摘要的蓝图
- en: 1\. Problem definition
  id: totrans-480
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1\. 问题定义
- en: The goal of this case study is to effectively discover common topics from earnings
    call transcripts of publicly traded companies using LDA. A core advantage of this
    technique compared to other approaches, is that no prior knowledge of the topics
    is needed.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 本案例研究的目标是利用LDA有效地从上市公司的收益电话会议记录中发现共同的主题。与其他方法相比，这种技术的核心优势在于不需要先验的主题知识。
- en: 2\. Getting started—loading the data and Python packages
  id: totrans-482
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2\. 入门 - 加载数据和Python包
- en: 2.1\. Loading the Python packages
  id: totrans-483
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1\. 加载Python包
- en: 'For this case study, we will extract the text from a PDF. Hence, the Python
    library *pdf-miner* is used for processing the PDF files into a text format. Libraries
    for feature extraction and topic modeling are also loaded. The libraries for the
    visualization will be loaded later in the case study:'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本案例研究，我们将从PDF中提取文本。因此，Python库*pdf-miner*用于将PDF文件处理为文本格式。还加载了用于特征提取和主题建模的库。可视化库将在案例研究的后续加载：
- en: '`Libraries for pdf conversion`'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: '`PDF转换库`'
- en: '[PRE87]'
  id: totrans-486
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: '`Libraries for feature extraction and topic modeling`'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: '`特征提取和主题建模库`'
- en: '[PRE88]'
  id: totrans-488
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '`Other libraries`'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: '`其他库`'
- en: '[PRE89]'
  id: totrans-490
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 3\. Data preparation
  id: totrans-491
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3\. 数据准备
- en: 'The `convert_pdf_to_txt` function defined below pulls out all characters from
    a PDF document except the images. The function simply takes in the PDF document,
    extracts all characters from the document, and outputs the extracted text as a
    Python list of strings:'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 下面定义的`convert_pdf_to_txt`函数从PDF文档中提取除图片外的所有字符。该函数简单地接收PDF文档，提取文档中的所有字符，并将提取的文本输出为Python字符串列表：
- en: '[PRE90]'
  id: totrans-493
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'In the next step, the PDF is converted to text using the above function and
    saved in a text file:'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，使用上述函数将PDF转换为文本，并保存在文本文件中：
- en: '[PRE91]'
  id: totrans-495
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'Let us look at the raw document:'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下原始文档：
- en: '[PRE92]'
  id: totrans-497
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: '`Output`'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出`'
- en: '[PRE93]'
  id: totrans-499
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'The text extracted from the PDF document contains uninformative characters
    that need to be removed. These characters reduce the effectiveness of our models
    as they provide unnecessary count ratios. The following function uses a series
    of regular expression (*regex*) searches as well as list comprehension to replace
    uninformative characters with a blank space:'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 从PDF文档提取的文本包含需要移除的无信息字符。这些字符会降低模型的效果，因为它们提供了不必要的计数比率。以下函数使用一系列正则表达式（*regex*）搜索以及列表推导来将无信息字符替换为空格：
- en: '[PRE94]'
  id: totrans-501
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 4\. Model construction and training
  id: totrans-502
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4\. 模型构建和训练
- en: 'The `CountVectorizer` function from the sklearn module is used with minimal
    parameter tuning to represent the clean document as a *document term matrix*.
    This is performed because our modeling requires that strings be represented as
    integers. The `CountVectorizer` shows the number of times a word occurs in the
    list after the removal of stop words. The document term matrix was formatted into
    a Pandas dataframe in order to inspect the dataset. This dataframe shows the word-occurrence
    count of each term in the document:'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 使用sklearn模块中的`CountVectorizer`函数进行最小参数调整，将干净的文档表示为*文档术语矩阵*。这是因为我们的建模需要将字符串表示为整数。`CountVectorizer`显示了在去除停用词后单词在列表中出现的次数。文档术语矩阵被格式化为Pandas数据框以便检查数据集。该数据框显示了文档中每个术语的词频统计：
- en: '[PRE95]'
  id: totrans-504
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'In the next step, the document term matrix will be used as the input data to
    the LDA algorithm for topic modeling. The algorithm was fitted to isolate five
    distinct topic contexts, as shown by the following code. This value can be adjusted
    depending on the level of granularity one intends to obtain from the modeling:'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，文档术语矩阵将作为输入数据用于LDA算法进行主题建模。该算法被拟合以隔离五个不同的主题上下文，如下代码所示。此值可以根据建模的粒度调整：
- en: '[PRE96]'
  id: totrans-506
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: '[PRE97]'
  id: totrans-507
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'The following code uses the *mglearn* library to display the top 10 words within
    each specific topic model:'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码使用*mglearn*库显示每个特定主题模型中的前10个词：
- en: '[PRE98]'
  id: totrans-509
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: '`Output`'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出`'
- en: '[PRE99]'
  id: totrans-511
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: Each topic in the table is expected to represent a broader theme. However, given
    that we trained the model on only a single document, the themes across the topics
    may not be very distinct from each other.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 预计表格中的每个主题都代表一个更广泛的主题。然而，由于我们仅对单一文档进行了模型训练，因此各主题间的主题可能并不十分明显。
- en: Looking at the broader theme, topic 2 discusses quarters, months, and currency
    units related to asset valuation. Topic 3 reveals information on income from real
    estate, mortgages, and related instrument. Topic 5 also has terms related to asset
    valuation. The first topic references balance sheet items and derivatives. Topic
    4 is slightly similar to topic 1 and has words related to an investment process.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 在更广泛的主题方面，主题2讨论了与资产估值相关的季度、月份和货币单位。主题3揭示了关于房地产收入、抵押贷款及相关工具的信息。主题5也涉及与资产估值相关的术语。第一个主题涉及资产负债表项目和衍生品。主题4与主题1略有相似，涉及投资过程中的词汇。
- en: In terms of overall theme, topics 2 and 5 are quite distinct from the others.
    There may also be some similarity between topics 1 and 4, based on the top words.
    In the next section, we will try to understand the separation between these topics
    using the Python library *pyLDAvis*.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 就整体主题而言，主题2和主题5与其他主题有很大的区别。基于前几个词，主题1和主题4可能也存在某种相似性。在下一节中，我们将尝试使用Python库*pyLDAvis*来理解这些主题之间的区分。
- en: 5\. Visualization of topics
  id: totrans-515
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5\. 主题可视化
- en: In this section, we visualize the topics using different techniques.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们使用不同的技术来可视化主题。
- en: 5.1\. Topic visualization
  id: totrans-517
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1\. 主题可视化
- en: '*Topic visualization* facilitates the evaluation of topic quality using human
    judgment. *pyLDAvis* is a library that displays the global relationships between
    topics while also facilitating their semantic evaluation by inspecting the terms
    most closely associated with each topic and, inversely, the topics associated
    with each term. It also addresses the challenge in which frequently used terms
    in a document tend to dominate the distribution over words that define a topic.'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: '*主题可视化*有助于通过人类判断评估主题质量。*pyLDAvis*是一个库，显示了主题之间的全局关系，同时通过检查与每个主题最相关的术语以及与术语相关的主题来促进其语义评估。它还解决了文档中频繁使用的术语倾向于主导定义主题的单词分布的挑战。'
- en: 'Below, the *pyLDAvis_* library is used to visualize the topic models:'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 下面使用*pyLDAvis_*库来展示主题模型：
- en: '[PRE100]'
  id: totrans-520
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: '`Output`'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出`'
- en: '![mlbf 10in16](Images/mlbf_10in16.png)'
  id: totrans-522
  prefs: []
  type: TYPE_IMG
  zh: '![mlbf 10in16](Images/mlbf_10in16.png)'
- en: We notice that topics 2 and 5 are quite distant from each other. This is what
    we observed in the section above from the overall theme and list of words under
    these topics. Topics 1 and 4 are quite close, which validates our observation
    above. Such close topics should be analyzed more intricately and might be combined
    if needed. The relevance of the terms under each topic, as shown in the right
    panel of the chart, can also be used to understand the differences. Topics 3 and
    4 are relatively close as well, although topic 3 is quite distant from the others.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到主题2和主题5相距甚远。这与上面章节中我们从整体主题和词汇列表中观察到的情况相符。主题1和主题4非常接近，这验证了我们上面的观察。这些相近的主题如果需要的话可以更详细地分析和合并。右侧图表中显示的每个主题下的术语的相关性也可以用来理解它们的差异。主题3和主题4也比较接近，尽管主题3与其他主题相距较远。
- en: 5.2\. Word cloud
  id: totrans-524
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2\. 词云
- en: 'In this step, a *word cloud* is generated for the entire document to note the
    most recurrent terms in the document:'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步骤中，生成了一个*词云*，用于记录文档中最频繁出现的术语：
- en: '[PRE101]'
  id: totrans-526
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: '`Output`'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出`'
- en: '![mlbf 10in17](Images/mlbf_10in17.png)'
  id: totrans-528
  prefs: []
  type: TYPE_IMG
  zh: '![mlbf 10in17](Images/mlbf_10in17.png)'
- en: The word cloud generally agrees with the results from the topic modeling, as
    recurrent words, such as *loan*, *real estate*, *third quarter*, and *fair value*,
    are larger and bolder.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 词云与主题建模的结果基本一致，如*贷款*、*房地产*、*第三季度*和*公平价值*等重复出现的词更大更粗。
- en: By integrating the information from the steps above, we may come up with the
    list of topics represented by the document. For the document in our case study,
    we see that words like *third quarter*, *first nine*, and *nine months* occur
    quite frequently. In the word list, there are several topics related to balance
    sheet items. So the document might be a third-quarter financial balance sheet
    with all credit and assets values in that quarter.
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 通过整合上述步骤中的信息，我们可以列出文档所代表的主题列表。在我们的案例研究文档中，我们发现像*第三季度*、*前九个月*和*九个月*这样的词频繁出现。在词汇列表中，有几个与资产负债表项目相关的主题。因此，该文档可能是一个第三季度的财务资产负债表，包含该季度的所有信用和资产价值。
- en: Conclusion
  id: totrans-531
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结论
- en: In this case study, we explored the use of topic modeling to gain insights into
    the content of a document. We demonstrated the use of the LDA model, which extracts
    plausible topics and allows us to gain a high-level understanding of large amounts
    of text in an automated way.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个案例研究中，我们探讨了主题建模在理解文档内容中的应用。我们展示了LDA模型的使用，该模型提取出合理的主题，并允许我们以自动化的方式对大量文本进行高层次理解。
- en: We performed extraction of the text from a document in PDF format and performed
    further data preprocessing. The results, alongside the visualizations, demonstrated
    that the topics are intuitive and meaningful.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从PDF格式的文档中提取了文本并进行了进一步的数据预处理。结果与可视化一起表明，这些主题直观且意义深远。
- en: Overall, the case study shows how machine learning and NLP can be applied across
    many domains—such as investment analysis, asset modeling, risk management, and
    regulatory compliance—to summarize documents, news, and reports in order to significantly
    reduce manual processing. Given this ability to quickly access and verify relevant,
    filtered information, analysts may be able to provide more comprehensive and informative
    reports on which management can base their decisions.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，案例研究展示了机器学习和自然语言处理如何在诸如投资分析、资产建模、风险管理和监管合规性等多个领域中应用，以总结文档、新闻和报告，从而显著减少手动处理。有了这种快速访问和验证相关信息的能力，分析师可以提供更全面和信息丰富的报告，供管理层基于其决策。
- en: '![](Images/bracket_bottom.png)'
  id: totrans-535
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/bracket_bottom.png)'
- en: Chapter Summary
  id: totrans-536
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 章节总结
- en: The field of NLP has made significant progress, resulting in technologies that
    have and will continue to revolutionize how financial institutions operate. In
    the near term, we are likely to see an increase in NLP-based technologies across
    different domains of finance, including asset management, risk management, and
    process automation. The adoption and understanding of NLP methodologies and related
    infrastructure are very important for financial institutions.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理领域取得了显著进展，导致了将继续改变金融机构运营方式的技术的出现。在近期，我们可能会看到基于自然语言处理的技术在金融的不同领域中的增加，包括资产管理、风险管理和流程自动化。金融机构采用和理解自然语言处理方法及相关基础设施非常重要。
- en: Overall, the concepts in Python, machine learning, and finance presented in
    this chapter through the case studies can be used as a blueprint for any other
    NLP-based problem in finance.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，本章通过案例研究中呈现的 Python、机器学习和金融概念可以作为金融领域中任何其他基于自然语言处理的问题的蓝图。
- en: Exercises
  id: totrans-539
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: Using the concepts from case study 1, use NLP-based techniques to develop a
    trading strategy using Twitter data.
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用案例研究 1 中的概念，使用基于自然语言处理的技术开发一个利用 Twitter 数据的交易策略。
- en: In case study 1, use the word2vec word embedding method to generate the word
    vectors and incorporate it into the trading strategy.
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在案例研究 1 中，使用 word2vec 词嵌入方法生成词向量，并将其纳入交易策略中。
- en: Using the concepts from case study 2, test a few more logical adapters to the
    chatbot.
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用案例研究 2 中的概念，测试一些更多的逻辑适配器到聊天机器人。
- en: Using the concepts from case study 3, perform topic modeling on a set of financial
    news articles for a given day and retrieve the key themes of the day.
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用案例研究 3 中的概念，对一组金融新闻文章进行主题建模，并提取当天的关键主题。
- en: ^([1](ch10.xhtml#idm45174899480680-marker)) A customized deep learning–based
    feature representation model is built in case study 1 of this chapter.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch10.xhtml#idm45174899480680-marker)) 本章案例研究 1 中构建了一个定制的基于深度学习的特征表示模型。
- en: ^([2](ch10.xhtml#idm45174898988232-marker)) The news can be downloaded by a
    simple web-scraping program in Python using packages such as Beautiful Soup. Readers
    should talk to the website or follow its terms of service in order to use the
    news for commercial purpose.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch10.xhtml#idm45174898988232-marker)) 这些新闻可以通过 Python 中的简单网页抓取程序下载，使用诸如
    Beautiful Soup 之类的包。读者应该与网站沟通或遵循其服务条款，以便将新闻用于商业目的。
- en: '^([3](ch10.xhtml#idm45174898982312-marker)) The source of this lexicon is Nuno
    Oliveira, Paulo Cortez, and Nelson Areal, “Stock Market Sentiment Lexicon Acquisition
    Using Microblogging Data and Statistical Measures,” *Decision Support Systems*
    85 (March 2016): 62–73.'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch10.xhtml#idm45174898982312-marker)) 这个词典的来源是 Nuno Oliveira、Paulo Cortez
    和 Nelson Areal 的文章，“利用微博数据和统计量获取股票市场情绪词典”，*决策支持系统* 85（2016 年 3 月）：62–73。
- en: ^([4](ch10.xhtml#idm45174898012728-marker)) We also train a sentiment analysis
    model on the financial data in the subsequent section and compare the results
    against the TextBlob model.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch10.xhtml#idm45174898012728-marker)) 我们还在随后的章节中对金融数据进行情感分析模型的训练，并将结果与
    TextBlob 模型进行比较。
- en: ^([5](ch10.xhtml#idm45174897768808-marker)) Refer to [Chapter 5](ch05.xhtml#Chapter5)
    for more details on RNN models.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch10.xhtml#idm45174897768808-marker)) 更多关于 RNN 模型的详细信息，请参考[第五章](ch05.xhtml#Chapter5)。
- en: '^([6](ch10.xhtml#idm45174897534248-marker)) The source of this lexicon is Nuno
    Oliveira, Paulo Cortez, and Nelson Areal, “Stock Market Sentiment Lexicon Acquisition
    Using Microblogging Data and Statistical Measures,” *Decision Support Systems*
    85 (March 2016): 62–73.'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch10.xhtml#idm45174897534248-marker)) 这个词典的来源是 Nuno Oliveira、Paulo Cortez
    和 Nelson Areal 的文章，“利用微博数据和统计量获取股票市场情绪词典”，*决策支持系统* 85（2016 年 3 月）：62–73。
- en: ^([7](ch10.xhtml#idm45174897105944-marker)) Refer to the plotting section of
    the [backtrader website](https://oreil.ly/j2pT0) for more details on the backtrader’s
    charts and the panels.
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch10.xhtml#idm45174897105944-marker)) 更多关于 backtrader 的图表和面板的绘制部分的详细信息，请参考[backtrader
    网站](https://oreil.ly/j2pT0)。
