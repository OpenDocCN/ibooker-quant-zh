<html><head></head><body><section data-pdf-bookmark="Chapter 4. The Dangers of Conventional Statistical Methodologies" data-type="chapter" epub:type="chapter"><div class="chapter" id="the_dangers_of_conventional_statistical">&#13;
<h1><span class="label">Chapter 4. </span>The Dangers of Conventional <span class="keep-together">Statistical Methodologies</span></h1>&#13;
&#13;
<blockquote data-type="epigraph" epub:type="epigraph">&#13;
<p>Worse than useless.</p>&#13;
&#13;
<p>—Jerzy Neyman, eminent mathematical statistician, referring to the statistical inference methodology of R. A. Fisher, the chief architect of conventional statistics</p>&#13;
</blockquote>&#13;
&#13;
<p>Recall from <a data-type="xref" href="ch01.html#the_need_for_probabilistic_machine_lear">Chapter 1</a> that all <a contenteditable="false" data-primary="Neyman, Jerzy" data-type="indexterm" id="id823"/><a contenteditable="false" data-primary="Fisher, Ronald A." data-type="indexterm" id="id824"/>financial models are at the mercy of the trifecta of errors, namely: errors in model specifications; errors in model parameter estimates; and errors resulting from the failure of a model to adapt to structural changes in its environment. Because of these errors, we need dynamic models that quantify the uncertainty inherent in our financial inferences and predictions.<a contenteditable="false" data-primary="models" data-secondary="wrong, even dangerous" data-seealso="dangers of conventional statistical methods" data-tertiary="dangers of conventional statistics" data-type="indexterm" id="id825"/><a contenteditable="false" data-primary="models" data-secondary="uncertainty must be quantified" data-type="indexterm" id="id826"/><a contenteditable="false" data-primary="dangers of conventional statistical methods" data-secondary="about" data-type="indexterm" id="id827"/></p>&#13;
&#13;
<p>A statistical inference methodology known<a contenteditable="false" data-primary="null hypothesis significance testing (NHST)" data-secondary="about" data-type="indexterm" id="id828"/><a contenteditable="false" data-primary="financial theory" data-secondary="flaws of" data-seealso="dangers of conventional statistical methods" data-tertiary="NHST" data-type="indexterm" id="id829"/><a contenteditable="false" data-primary="dangers of conventional statistical methods" data-secondary="frequentist view worse than useless" data-tertiary="null hypothesis significance testing" data-type="indexterm" id="id830"/><a contenteditable="false" data-primary="frequentist view of probability" data-secondary="worse than useless" data-tertiary="null hypothesis significance testing" data-type="indexterm" id="id831"/><a contenteditable="false" data-primary="probability" data-secondary="frequentist conventional view" data-tertiary="null hypothesis significance testing" data-type="indexterm" id="id832"/><a contenteditable="false" data-primary="null hypothesis significance testing (NHST)" data-secondary="must be rejected" data-type="indexterm" id="id833"/> as null hypothesis significance testing (NHST) almost completely dominates the research and practice of social and economic sciences. In this chapter, we examine how NHST and its p-value statistic is used for testing hypotheses and quantifying uncertainty of model parameters. The deep logical flaws of NHST methodology are primarily responsible for the reproducibility crisis in all the social and economic sciences, where the majority of published research findings are false.<sup><a data-type="noteref" href="ch04.html#ch04fn1" id="ch04fn1-marker">1</a></sup> In the next couple of sections, we expose the statistical skullduggery of NHST and its p-value statistic and show you how it is guilty<a contenteditable="false" data-primary="prosecutor’s fallacy" data-secondary="about" data-type="indexterm" id="id834"/> of the prosecutor’s fallacy. This fallacy is another version of the inverse fallacy, where a conditional statement is falsely equated with its inverse, thereby violating the inverse probability rule.</p>&#13;
&#13;
<p>Given the deep flaws and abuses<a contenteditable="false" data-primary="confidence intervals (CIs)" data-secondary="about" data-type="indexterm" id="id835"/><a contenteditable="false" data-primary="p-values" data-secondary="flaws and abuses" data-type="indexterm" id="id836"/> of p-values for quantifying parameter uncertainty,<sup><a data-type="noteref" href="ch04.html#ch04fn2" id="ch04fn2-marker">2</a></sup> another methodology known as confidence intervals (CIs) is touted by orthodox statisticians as its mathematically rigorous replacement. Unfortunately, CIs are also the wrong tool for data analysis, since they were not designed to make statistical inferences from a single experiment.<sup><a data-type="noteref" href="ch04.html#ch04fn3" id="ch04fn3-marker">3</a></sup> <a contenteditable="false" data-primary="confidence intervals (CIs)" data-secondary="central limit theorem assumptions violated" data-type="indexterm" id="id837"/><a contenteditable="false" data-primary="dangers of conventional statistical methods" data-secondary="confidence intervals" data-tertiary="central limit theorem assumptions violated" data-type="indexterm" id="id838"/><a contenteditable="false" data-primary="central limit theorem (CLT)" data-secondary="confidence intervals violating assumptions" data-type="indexterm" id="id839"/>Most importantly, the application of CIs in finance often violates the assumptions of the central limit theorem (CLT), making CIs invalid. In this chapter, we explore the trio of errors in applying CIs that are common in financial research and practice. We develop an ordinary least squares (OLS) linear regression model of equity returns using Statsmodels, a Python statistical package, to illustrate these three error types. We use the diagnostic test results of our regression model to support our reasons why CIs should not be used in data analyses in general and finance in particular.</p>&#13;
&#13;
<section data-pdf-bookmark="The Inverse Fallacy" data-type="sect1"><div class="sect1" id="the_inverse_fallacy">&#13;
<h1>The Inverse Fallacy</h1>&#13;
&#13;
<p>Recall the proof of the inverse probability rule,<a contenteditable="false" data-primary="dangers of conventional statistical methods" data-secondary="inverse fallacy" data-type="indexterm" id="ch04-inv"/><a contenteditable="false" data-primary="inverse probability rule" data-secondary="dangers of conventional statistical methods" data-type="indexterm" id="ch04-inv2"/><a contenteditable="false" data-primary="inverse fallacy" data-type="indexterm" id="ch04-inv3"/><a contenteditable="false" data-primary="prosecutor’s fallacy" data-type="indexterm" id="ch04-inv4"/><a contenteditable="false" data-primary="product rule for conditional probabilities" data-secondary="inverse probability rule derived from" data-type="indexterm" id="id840"/><a contenteditable="false" data-primary="inverse probability rule" data-secondary="product rule reformulation" data-type="indexterm" id="id841"/><a contenteditable="false" data-primary="transposed conditional fallacy" data-see="prosecutor’s fallacy" data-type="indexterm" id="id842"/> a trivial reformulation of the product rule. For any nonzero probability event H and D:</p>&#13;
&#13;
<ul class="list_style_type_none">&#13;
	<li>&#13;
	<p><em>P(H and D) = P(D and H) (product of probabilities commute)</em></p>&#13;
	</li>&#13;
	<li>&#13;
	<p><em>P(H|D) × P(D) = P(D|H) × P(H) (applying product rule to both sides)</em></p>&#13;
	</li>&#13;
	<li>&#13;
	<p><em>P(H|D) = P(D|H) × P(H) / P(D) (the inverse probability rule)</em></p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>Note that joint probabilities, the product of two probabilities, commute—i.e., the order of the individual probabilities does not change the result of their product:</p>&#13;
&#13;
<ul class="simplelist">&#13;
	<li><em>P(H and D) = P(D and H)</em></li>&#13;
</ul>&#13;
&#13;
<p>As you can see from the last equation, conditional probabilities do not commute:<a contenteditable="false" data-primary="conditional probabilities" data-secondary="not commuting" data-type="indexterm" id="id843"/><a contenteditable="false" data-primary="null hypothesis significance testing (NHST)" data-secondary="conditional probabilities not commuting" data-type="indexterm" id="id844"/><a contenteditable="false" data-primary="p-values" data-secondary="conditional probabilities not commuting" data-type="indexterm" id="id845"/></p>&#13;
&#13;
<ul class="simplelist">&#13;
	<li><em>P(H|D) ≠ P(D|H)</em></li>&#13;
</ul>&#13;
&#13;
<p>This is a common logical mistake<a contenteditable="false" data-primary="prosecutor’s fallacy" data-secondary="null hypothesis significance testing committing" data-type="indexterm" id="id846"/><a contenteditable="false" data-primary="null hypothesis significance testing (NHST)" data-secondary="prosecutor’s fallacy" data-type="indexterm" id="id847"/> that people make in their thinking and scientists continue to make in their research when using NHST and p-values. This is called the inverse fallacy because you are incorrectly equating a conditional probability, P(D|H), with its inverse, P(H|D), and violating the inverse probability rule. The inverse fallacy is also known as transposed conditional fallacy. As a simple example, consider how the inverse fallacy incorrectly infers statement B from statement A:</p>&#13;
&#13;
<ul class="list_style_type_none">&#13;
	<li>&#13;
	<p>(A) Given that someone is a programmer, it is likely that they are analytical.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>(B) Given that someone is analytical, it is likely that they are a programmer.</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>But P(analytical | programmer) ≠ P(programmer | analytical). As you know, there are many, many analytical people who are not programmers, and such an inference seems absurd when framed in this manner. However, you will see that humans are generally not very good at processing conditional statements and their inverses, especially in complex situations. Indeed, prosecutors have ruined people’s lives by using this flawed logic disguised in arguments that have led judges and juries to make terrible inferences and decisions.<sup><a data-type="noteref" href="ch04.html#ch04fn4" id="ch04fn4-marker">4</a></sup> A common example of the prosecutor’s fallacy goes something like this:</p>&#13;
&#13;
<ul class="list_style_type_none">&#13;
	<li>&#13;
	<p>(A) Say about 0.1% of the 100,000 adults in your city have your blood type.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>(B) A blood stain with your blood type is found on the murder victim.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>(C) Therefore, claims the city prosecutor, there is a 99.9% probability that you are the murderer.</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>That’s clearly absurd. <a contenteditable="false" data-primary="prosecutor’s fallacy" data-secondary="frequentist inference methods using" data-type="indexterm" id="id848"/><a contenteditable="false" data-primary="frequentist view of probability" data-secondary="worse than useless" data-tertiary="using prosecutor’s fallacy" data-type="indexterm" id="id849"/><a contenteditable="false" data-primary="probability" data-secondary="frequentist conventional view" data-tertiary="using prosecutor’s fallacy" data-type="indexterm" id="id850"/><a contenteditable="false" data-primary="dangers of conventional statistical methods" data-secondary="frequentist view worse than useless" data-tertiary="using prosecutor’s fallacy" data-type="indexterm" id="id851"/>What is truly horrifying—and we should all be screaming bloody murder—is that researchers and practitioners are unknowingly using the prosecutor’s fallacious logic when applying NHST and p-values in their statistical inferences. More on NHST in the next section. Let’s expose the prosecutor’s flawed reasoning in this section so that you can see how it is used in the NHST methodology.</p>&#13;
&#13;
<p>The probability of your guilt (G) before the blood stain evidence (E) was discovered to be P(G) = 1/100,000, since every adult in the city is an equally likely suspect. Therefore, the probability of your innocence (I) is P(I) = 99,999/100,000. The probability that the blood stain would match your blood type given you are actually guilty is a certainty, implying P(E | G) = 1. However, even if you are actually innocent, there is still a 0.1% probability that the blood stain would match your blood type merely by its prevalence in the city’s adult population, implying P(E | I) = 0.001. The prosecutor needs to estimate P(G | E), the probability of your guilt given the evidence, with the previously mentioned probabilities. Instead of using the inverse probability rule, the prosecutor uses a fallacious argument as follows:</p>&#13;
&#13;
<ul class="list_style_type_none">&#13;
	<li>&#13;
	<p>(A) Given the evidence, you can be either guilty or innocent, so P(G | E) + P (I | E) = 1</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>(B) Now the prosecutor commits the inverse fallacy by making P(I | E) = P(E | I)</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>(C) Thus the prosecutor’s fallacy gives you P (G | E) = 1 – P(I | E) = 1 – P(E | I)</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>(D) Plugging in the numbers, P(G | E) = 1 – 0.001 = 0.999 or 99.9%</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>Without explicitly using the inverse probability rule, your lawyer could use some common sense and correctly argue that there are 100 adults (0.1% × 100,000) in the city who have the same blood type as you do. Therefore, given evidence of the blood stain alone, there is only a 1 in 100 chance or 1% probability that you are guilty and 99% probability that you are innocent. This is approximately the same probability you would get when applying the inverse probability rule because it just formulates a commonsensical way of counting the possibilities. Let’s do that now and calculate the probability of your innocence given the evidence, P(I | E):</p>&#13;
&#13;
<ul class="list_style_type_none">&#13;
	<li>&#13;
	<p>(A) The inverse probability rule states P (I | E) = P(E | I) × P(I)/ P(E)</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>(B) We use the law of total probability to get P(E) = P(E | I) × P(I) + P(E | G) × P(G)</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>(C) So P(I | E) = 0.001 × 0.99999 / [(0.001 × 0.99999) + ( 1 × 0.00001)] = 0.99 <span class="keep-together">or 99%</span></p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>Before the prosecutor strikes you off the suspect list, it is important to note that it would also be fallacious for your lawyer to now ask the jury to disregard the blood stain as weak evidence of your guilt based on the 1% conditional probability just calculated. <a contenteditable="false" data-primary="defense attorney’s fallacy" data-type="indexterm" id="id852"/><a contenteditable="false" data-primary="Simpson, O. J." data-type="indexterm" id="id853"/>This flawed line of reasoning is called the defense attorney’s fallacy and was used in the notorious O. J. Simpson murder trial. The evidence is not weak, because before the blood stain was found, you had a 1 in 100,000 chance of being the murderer. But after the blood stain was discovered, your chance of being guilty has gone up a thousand times to 1 in 100. That’s very strong evidence indeed and nobody should disregard it. However, it is completely inadequate for a conviction if that is the only piece of evidence presented to the jury. The prosecutor will need additional incriminating evidence to make a valid case against you.</p>&#13;
&#13;
<p>Now let’s look at a realistic financial situation where the inverse fallacy might be harder to spot.<a contenteditable="false" data-primary="economic recessions and prosecutor’s fallacy" data-type="indexterm" id="ch04-rec"/><a contenteditable="false" data-primary="recessions and prosecutor’s fallacy" data-type="indexterm" id="ch04-rec2"/> Economic recessions are notoriously hard to recognize in the early stages of their development. As I write this chapter (in the fall of 2022), there is a debate raging among economists and investors about whether the US economy is currently in a recession or about to enter one. Economists at the National Bureau of Economic Research (NBER), the organization responsible for making the recession official, can only confirm the fact in retrospect. Sometimes the NBER takes over a year to declare when the recession actually started, as it did in the Great Recession of 2007–09. Of course, traders and investors cannot wait that long, and they develop their own indicators for predicting recessions in real time.</p>&#13;
&#13;
<p>Assume that you have developed a proprietary economic indicator that crunches all kinds of data and correctly signals a recession 99% of the time when the US economy is actually in one or about to enter one. You also note that about 20% of the time your indicator signals a recession incorrectly even though the economy is not in one. Say you just found out that your proprietary indicator is flashing a recession signal. What is the probability that the US economy has actually entered into a recession? If you answered that the probability is 99%, as many people instinctively do, you would have committed the inverse fallacy since P(recession given signal) ≠ P(signal given <span class="keep-together">recession).</span></p>&#13;
&#13;
<p>Let’s see why the probability of recession is not 99% but much lower. Assume R is the scenario that the US economy is in a recession and S is the event that your indicator signals that we are in a recession. You have the following conditional probabilities:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>The probability of your indicator giving you a recession signal given we actually are in one is P(S|R) = 0.99 or 99%. This is its true positive rate.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>This implies that the probability your indicator fails to detect a recession given we are actually in one, P(not S|R) = 1 – P(S|R) = 0.01 or 1%. This is its false negative rate.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>The probability your indicator incorrectly alerts you to a recession when there isn’t one is P(S|not R) = 0.20 or 20%. This is its false positive rate.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Similarly, the probability that your indicator successfully detects that the economy is not in a recession, P(not S| not R) = 1 – P(S|not R) = 0.80 or 80%. This is its true negative rate.</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>These conditional probabilities are generally organized in a confusion matrix, as shown in <a data-type="xref" href="#confusion_matrix_for_binary_classificat">Figure 4-1</a>.</p>&#13;
&#13;
<p>Your objective is to estimate P(R|S), the conditional probability that the US economy is in a recession, given your indicator generates such a signal. To calculate this inverse probability P(R|S), you can’t only let the data speak about one specific scenario. Why? Because your economic indicator does not have 100% accuracy. It gives you a false recession signal 20% of the time when the economy is not in one. Could this scenario be 1 of the 5 when it is wrong about the economy being in a recession? Also, 1% of time it fails to detect a recession when the economy is actually in one. So maybe we have already been in a recession for many months, and it is 1 of the 100 instances when your indicator failed to flicker. How would you know just from the data about this particular scenario? You wouldn’t, because you don’t have a clue about the environment you are operating in. You need to leverage prior knowledge so that you can understand the context in which you are running your financial experiments.</p>&#13;
&#13;
<figure><div class="figure" id="confusion_matrix_for_binary_classificat"><img alt="Confusion Matrix of your proprietary recession indicator" src="assets/pmlf_0401.png"/>&#13;
<h6><span class="label">Figure 4-1. </span>Confusion matrix of your proprietary recession indicator<sup><a data-type="noteref" href="ch04.html#ch04fn5" id="ch04fn5-marker">5</a></sup></h6>&#13;
</div></figure>&#13;
&#13;
<p>Your specific dataset is oblivious of how common or uncommon recessions are in the US. Why is that relevant? Because you don’t know if your false positive rate is too high, or low enough, compared to the rate at which recessions tend to occur in the US for your indicator to be useful, despite its 99% true positive rate.</p>&#13;
&#13;
<p>You will need to estimate the probability that the US could be in a recession in any given month P(R) based on past occurrences; this is called the base rate of the particular event/scenario R. Ignoring the base rate leads to a violation of the inverse probability rule and invalid inferences, as we will demonstrate.</p>&#13;
&#13;
<p>Let’s compute the base rate from actual economic data. The NBER’s time series for every month since 1982 that the US was in an economic recession can be downloaded from Federal Reserve Economic Data (FRED), a popular and free data source that has more than half a million economic and financial time series. Let’s use the following Python code to calculate the monthly base rate of economic recessions in the US:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="c1"># Import libraries and FRED datareader</code>&#13;
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>&#13;
<code class="kn">import</code> <code class="nn">pandas</code> <code class="k">as</code> <code class="nn">pd</code>&#13;
<code class="kn">import</code> <code class="nn">pandas_datareader.data</code> <code class="k">as</code> <code class="nn">pdr</code>&#13;
<code class="kn">from</code> <code class="nn">datetime</code> <code class="kn">import</code> <code class="n">datetime</code>&#13;
<code class="n">start</code> <code class="o">=</code> <code class="n">datetime</code><code class="p">(</code><code class="mi">1982</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>&#13;
<code class="n">end</code> <code class="o">=</code> <code class="n">datetime</code><code class="p">(</code><code class="mi">2022</code><code class="p">,</code> <code class="mi">9</code><code class="p">,</code> <code class="mi">30</code><code class="p">)</code>&#13;
<code class="c1"># NBER business cycle classification</code>&#13;
<code class="n">recession</code> <code class="o">=</code> <code class="n">pdr</code><code class="o">.</code><code class="n">DataReader</code><code class="p">(</code><code class="s1">'USREC'</code><code class="p">,</code> <code class="s1">'fred'</code><code class="p">,</code> <code class="n">start</code><code class="p">,</code> <code class="n">end</code><code class="p">)</code>&#13;
<code class="c1"># Percentage of time the US economy was in recession since 1982</code>&#13;
<code class="nb">round</code><code class="p">(</code><code class="n">recession</code><code class="p">[</code><code class="s1">'USREC'</code><code class="p">]</code><code class="o">.</code><code class="n">sum</code><code class="p">()</code><code class="o">/</code><code class="n">recession</code><code class="p">[</code><code class="s1">'USREC'</code><code class="p">]</code><code class="o">.</code><code class="n">count</code><code class="p">()</code><code class="o">*</code><code class="mi">100</code><code class="p">,</code> <code class="mi">2</code><code class="p">)</code></pre>&#13;
&#13;
<p>From this data, the US has been in an economic recession only 9.61% of the time in any given month from January 1982 to September 2022. <a contenteditable="false" data-primary="unconditional probability" data-secondary="US economy in recession" data-secondary-sortas="United States economy" data-type="indexterm" id="id854"/><a contenteditable="false" data-primary="marginal probability" data-secondary="US economy in recession" data-secondary-sortas="United States economy" data-type="indexterm" id="id855"/>Once you have estimated P(R), you can plug it into the law of total probability to get the unconditional probability, or marginal probability P(S), of getting a recession signal from your indicator regardless of the state of the economy. We then use P(R) in the inverse probability rule to calculate the probability the US economy is in recession, given that your proprietary indicator is signaling a recession:</p>&#13;
&#13;
<ul class="simplelist">&#13;
<li>P(S) = P(S|R) × P(R) + P(S|not R) × P(not R) = (0.99 × 0.096) + (0.2 × 0.904) = 0.276</li>&#13;
<li>P(R|S) = P(S|R) × P(R) / P(S) = (0.99 × 0.096) / 0.276 = 0.344</li>&#13;
</ul>&#13;
&#13;
<p>The calculation for P(S) says that you can expect your indicator to generate a signal 27.6% of the time regardless of whether the US economy is in a recession or not. Of the times you do see it flicker, P(R|S) says that in only 34.4% of those scenarios will the signal be correct about the economy being in a recession. Your signal will give you a false alarm P(not R|S) about 65.6% of the time. That’s a very poor indicator—you’re better off ignoring it.</p>&#13;
&#13;
<p>This result seems counterintuitive since your indicator has a 99% true positive rate P(S|R). That’s because you cannot shove your indicator’s false positive rate under the rug and blithely ignore the base rate of US economic recessions with some ideological rubbish of being objective and letting only the data speak. That would be foolish because you would be denying the inverse probability rule and ignoring objective prior data about US economic cycles. Such fallacious inferences and decision making will almost surely see you go broke or be out of a job sooner rather than later.</p>&#13;
&#13;
<p>In the real world of finance and investing, you will need a signal with a false positive rate lower than the base rate to give you a signal with a probability greater than 50% of being correct. To see this, let’s redo the calculation with a revised false positive of 9%, which is slightly less than the 9.61% base rate at which the US economy has been in a recession in any given month since 1982:</p>&#13;
&#13;
<ul class="simplelist">&#13;
<li>P(S) = P(S|R) × P(R) + P(S|not R) × P(not R) = (0.99 × 0.096) + (0.09 × 0.904) = 0.176</li>&#13;
<li>P(R|S) = P(S|R) × P(R) / P(S) = (0.95 × 0.0967)/0.174 = 0.540</li>&#13;
</ul>&#13;
&#13;
<p>With a 54% probability of correctly calling a recession, your indicator will have an edge or positive expectation for better decision making and risk management.</p>&#13;
&#13;
<p class="pagebreak-before">To summarize, the true positive rate of your indicator is important. However, what is equally important is that the false positive rate of the indicator needs to be less than the base rate of the underlying feature in the population you are sampling from. So if you ignore the fact that your indicator is generating false positives P(S|not R) at a 20% rate while the US economy is generating a recessionary month at a 9.61% base rate, your false positives will overwhelm your true positives at a 2 to 1 ratio. It doesn’t seem so far-fetched now to think that unscrupulous prosecutors, snake oil salesmen, and pseudoscientists could fool you (and themselves) with the inverse fallacy.</p>&#13;
&#13;
<p>Since there is still a 34.4% chance that your indicator might be right, randomness could also fool you, too, by granting you a lucky guess, and the US economy could end up being in a recession. However, your probability estimate of 99% would be way off, and your reasoning would be fallacious. A trading or investing strategy based on luck, incorrect reasoning, and poor probability estimates will lead to financial ruin sooner rather than later. Far worse, a statistical methodology like NHST based on the inverse fallacy will overwhelm us with false positive studies, creating confusion and harm. This will ruin the scientific enterprise that we cherish and value so much.<a contenteditable="false" data-primary="" data-startref="ch04-inv" data-type="indexterm" id="id856"/><a contenteditable="false" data-primary="" data-startref="ch04-inv2" data-type="indexterm" id="id857"/><a contenteditable="false" data-primary="" data-startref="ch04-inv3" data-type="indexterm" id="id858"/><a contenteditable="false" data-primary="" data-startref="ch04-inv4" data-type="indexterm" id="id859"/><a contenteditable="false" data-primary="" data-startref="ch04-rec" data-type="indexterm" id="id860"/><a contenteditable="false" data-primary="" data-startref="ch04-rec2" data-type="indexterm" id="id861"/></p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="NHST Is Guilty of the Prosecutor’s Fallacy" data-type="sect1"><div class="sect1" id="nhst_is_guilty_of_the_prosecutorapostro">&#13;
<h1>NHST Is Guilty of the Prosecutor’s Fallacy</h1>&#13;
&#13;
<p>Ronald Fisher, the head architect<a contenteditable="false" data-primary="Fisher, Ronald A." data-type="indexterm" id="id862"/><a contenteditable="false" data-primary="Pearson, Karl" data-type="indexterm" id="id863"/><a contenteditable="false" data-primary="prosecutor’s fallacy" data-secondary="null hypothesis significance testing committing" data-type="indexterm" id="ch04-pros"/><a contenteditable="false" data-primary="null hypothesis significance testing (NHST)" data-secondary="prosecutor’s fallacy" data-type="indexterm" id="ch04-pros2"/><a contenteditable="false" data-primary="dangers of conventional statistical methods" data-secondary="NHST committing prosecutor’s fallacy" data-type="indexterm" id="ch04-pros3"/> of modern statistics, introduced NHST in the 1920s. He also included Karl Pearson’s p-value into his methodology for quantifying uncertainty. This was a postdata methodology and was meant to enable researchers to make statistical inferences from a single experiment based on a null hypothesis that is the negation of the hypothesis that the researcher is trying to prove.</p>&#13;
&#13;
<p>In 1925, Fisher made the absurd and unsubstantiated claim that “the theory of inverse probability is founded upon error and must be wholly rejected.”<sup><a data-type="noteref" href="ch04.html#ch04fn6" id="ch04fn6-marker">6</a></sup> Of course Fisher didn’t and couldn’t provide any proof for this claim. How could he? That would be akin to proving the rules of division are founded on error. As mentioned in the previous chapter, <a contenteditable="false" data-primary="inverse probability rule" data-secondary="Bayes’s theorem misnomer" data-type="indexterm" id="id864"/><a contenteditable="false" data-primary="product rule for conditional probabilities" data-secondary="inverse probability rule derived from" data-tertiary="Bayes’s theorem pejorative" data-type="indexterm" id="id865"/><a contenteditable="false" data-primary="“Bayes’s theorem”" data-primary-sortas="Bayes’s theorem" data-secondary="inverse probability rule as proper name" data-type="indexterm" id="id866"/><a contenteditable="false" data-primary="Bayes, Thomas" data-type="indexterm" id="id867"/>my suspicion is that by renaming the rule after an amateur mathematician, Thomas Bayes, he could cast aspersions on the rule. By rejecting the inverse probability rule, Fisher was able to use the prosecutor’s fallacy to promote his flawed discriminatory ideas under the guise of objectivity and “letting the data speak for themselves.”<sup><a data-type="noteref" href="ch04.html#ch04fn7" id="ch04fn7-marker">7</a></sup> Fisher’s fawning cohorts in industry and slavish acolytes in academia merely repeated the lie about the inverse probability theory and banished it from their practice and curricula—a problem that continues to this day.</p>&#13;
&#13;
<p>NHST is built behind the facade<a contenteditable="false" data-primary="proof by contrapositive" data-type="indexterm" id="id868"/><a contenteditable="false" data-primary="null hypothesis significance testing (NHST)" data-secondary="prosecutor’s fallacy" data-tertiary="proof by contrapositive" data-type="indexterm" id="id869"/> of a valid form of propositional logic known as proof by contrapositive. The logic is as follows: suppose we have two propositions H and D such that if H is true, then D is true. Now if we can prove that D is false, then we can validly conclude that H must be false.</p>&#13;
&#13;
<p>Following the latter logic, researchers<a contenteditable="false" data-primary="null hypothesis (H0) in NHST" data-type="indexterm" id="id870"/> using NHST formulate a hypothesis, called the null hypothesis (H<sub>0</sub>), that they want to disprove before observing any data. H<sub>0</sub> is viewed as the negation of an alternative research hypothesis (H<sub>1</sub>) that they want to establish but is not explicitly specified, i.e., H<sub>1</sub> = not H<sub>0</sub> and P(H<sub>1</sub>) + P(not H<sub>0</sub>) = 1. In this regard, they play the devil’s advocate for the null hypothesis.</p>&#13;
&#13;
<p>The null hypothesis is generally formulated as a summary statistic, such as the difference in the sample means of the data distribution of two groups that need to be compared. It is important to note that researchers do not predict the data that their research hypothesis H<sub>1</sub> is expected to generate, assuming that H<sub>1</sub> is true.</p>&#13;
&#13;
<p>Before starting their experiment,<a contenteditable="false" data-primary="significance level (alpha)" data-secondary="about" data-type="indexterm" id="id871"/><a contenteditable="false" data-primary="alpha for significance level" data-type="indexterm" id="id872"/> researchers also choose a significance level, denoted by alpha, which works as a decision threshold to accept or reject the null hypothesis after observing the data. The convention is to set alpha to 5%. The alpha level is claimed to be the long-run probability that the researcher might incorrectly reject a true null hypothesis, thereby committing a type I error and generating false positive results (a result that is claimed to be true when it is actually false). The alpha level is the most critical element of the experiment, since it determines if the experiment is considered statistically significant or not.</p>&#13;
&#13;
<p>It is important to note that any significance level is entirely subjective, as it is not based on the observed data or the null hypothesis or a scientific reason or any mathematical rule or theorem. The conventional use of the 5% alpha level is a totally arbitrary and self-fulling ritual. Since Fisher used a 5% alpha significance level, researchers and academics blindly follow his example. So much for the vaunted objectivity and scientific rigor of frequentists, not to mention letting the data speak for themselves.</p>&#13;
&#13;
<p>Assuming the null hypothesis is true,<a contenteditable="false" data-primary="p-values" data-secondary="flaws and abuses" data-tertiary="prosecutor’s fallacy" data-type="indexterm" id="id873"/> the researcher computes a statistic called the p-value to quantify the probability of observing the summary statistic of the sample data (D) or something more extreme than it:</p>&#13;
&#13;
<ul class="simplelist">&#13;
	<li>p-value = P(D|H<sub>0</sub>)</li>&#13;
</ul>&#13;
&#13;
<p>If the p-value ≤ alpha, H<sub>0</sub> is rejected as false at the alpha significance level and the alternative hypothesis (H<sub>1</sub>) is accepted as true.</p>&#13;
&#13;
<p>But this logic of NHST is mind-bogglingly absurd.<a contenteditable="false" data-primary="inverse fallacy" data-secondary="NHST and rejecting the null hypothesis" data-type="indexterm" id="id874"/> By rejecting the null hypothesis (H<sub>0</sub>) given the p-value of the test statistic (D), the researcher is committing the inverse fallacy, because P(H<sub>0</sub> | D) ≠ P(D | H<sub>0</sub>). See <a data-type="xref" href="#how_p_values_are_used_in_nhstsource_for">Figure 4-2</a>.</p>&#13;
&#13;
<figure><div class="figure" id="how_p_values_are_used_in_nhstsource_for"><img alt="How p-values are used in NHST" src="assets/pmlf_0402.png"/>&#13;
<h6><span class="label">Figure 4-2. </span>How p-values are used in NHST<sup><a data-type="noteref" href="ch04.html#ch04fn8" id="ch04fn8-marker">8</a></sup></h6>&#13;
</div></figure>&#13;
&#13;
<p>NHST makes an even more absurd leap of logic. NHST commits the prosecutor’s fallacy by allowing researchers to accept the unspecified, alternative research hypothesis, which the data was not modeling in the first place. Go back to the previous section and refresh your memory about how we disentangled the prosecutor’s fallacy.</p>&#13;
&#13;
<p>The researcher wants to determine P(H<sub>1</sub>|D), the probability the research hypothesis (H<sub>1</sub>) is true given the data. But NHST only computes P(D|H<sub>0</sub>), the probability of observing the data assuming the null hypothesis (H<sub>0</sub>) is true. It then uses the p-value statistic to accept or reject the null hypothesis at the alpha significance level. So researchers following the NHST methodology commit the prosecutor’s fallacy as <span class="keep-together">follows:</span></p>&#13;
&#13;
<ul class="list_style_type_none">&#13;
	<li>&#13;
	<p>P(H<sub>1</sub>|D) = 1 – P(H<sub>0</sub>|D) (true statement)</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>P(H<sub>0</sub>|D) = P(D|H<sub>0</sub>) (the inverse fallacy)</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>P(H<sub>1</sub>|D) = 1 – P(D|H<sub>0</sub>) (the prosecutor’s fallacy)</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>How should we validly calculate P(H<sub>1</sub>|D)? The binary logic of proof by contrapositive in a deterministic world needs to be translated into the calculus of conditional probabilities in an uncertain world. This translation is enabled by the inverse probability rule and the law of total probability, as was applied in the previous section:</p>&#13;
&#13;
<ul class="list_style_type_none">&#13;
	<li>&#13;
	<p>P(H<sub>1</sub>|D) = 1 – P(H<sub>0</sub>|D)</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>P(H<sub>1</sub>|D) = 1 – [P(D|H<sub>0</sub>)P(H<sub>0</sub>)/P(D)]</p>&#13;
	</li>&#13;
	<li>&#13;
	<p><strong>P(H<sub>1</sub>|D) = 1 – [P(D|H<sub>0</sub>)P(H<sub>0</sub>) / (P(D|H<sub>0</sub>)P(H<sub>0</sub>) + P(D|H<sub>1</sub>)P(H<sub>1</sub>))]</strong></p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>As this equation, which I have derived, shows, the researcher needs to estimate P(D|H<sub>1</sub>), the probability of observing the data assuming their research hypothesis H<sub>1</sub> is true. Most importantly, the researcher needs to estimate the prior probability or base rate of at least one of their complementary hypotheses, P(H<sub>0</sub>) or P(H<sub>1</sub>). That’s because without the base rate, you cannot compute the evidence or the unconditional probability of observing the data. <a contenteditable="false" data-primary="dangers of conventional statistical methods" data-secondary="frequentist view worse than useless" data-type="indexterm" id="id875"/>This fallacious logic is what makes statistical inferences about either the null hypothesis or the alternative research hypothesis invalid. It is for very good reasons that Jerzy Neyman, an eminent statistician and Fisher’s peer, called Fisher’s work on statistical inference “worse than useless.”<sup><a data-type="noteref" href="ch04.html#ch04fn9" id="ch04fn9-marker">9</a></sup></p>&#13;
&#13;
<p>It is clear that NHST—the cornerstone of education, research, and practice of the social and economic sciences—is <a contenteditable="false" data-primary="null hypothesis significance testing (NHST)" data-secondary="must be rejected" data-type="indexterm" id="id876"/><a contenteditable="false" data-primary="dangers of conventional statistical methods" data-secondary="frequentist view worse than useless" data-tertiary="null hypothesis significance testing" data-type="indexterm" id="id877"/><a contenteditable="false" data-primary="frequentist view of probability" data-secondary="worse than useless" data-tertiary="null hypothesis significance testing" data-type="indexterm" id="id878"/><a contenteditable="false" data-primary="probability" data-secondary="frequentist conventional view" data-tertiary="null hypothesis significance testing" data-type="indexterm" id="id879"/><a contenteditable="false" data-primary="financial theory" data-secondary="flaws of" data-tertiary="NHST" data-type="indexterm" id="id880"/>committing the prosecutor’s fallacy. No wonder most of the published research findings using NHST are false. NHST has wasted billions of research dollars, defamed science, and done a great disservice to humanity with its false positive research studies. All this while professing the farce of rigor and objectivity. NHST continues to wreak havoc on the social and economic sciences, producing too many false research claims to this day despite many failed attempts to abolish it or reform it for over half a century.<sup><a data-type="noteref" href="ch04.html#ch04fn10" id="ch04fn10-marker">10</a></sup> It’s about time we reject the NHST because it “is founded upon error and must be wholly rejected.”<sup><a data-type="noteref" href="ch04.html#ch04fn11" id="ch04fn11-marker">11</a></sup></p>&#13;
&#13;
<p>Many in the social and economic sciences<a contenteditable="false" data-primary="confidence intervals (CIs)" data-secondary="CI theory" data-tertiary="p-values replaced by" data-type="indexterm" id="id881"/><a contenteditable="false" data-primary="p-values" data-secondary="CI theory replacing" data-type="indexterm" id="id882"/> recommend replacing p-values with CI theory, which is touted as a mathematically more rigorous way of quantifying uncertainty. So let’s examine CI theory to see if it is useful.<a contenteditable="false" data-primary="" data-startref="ch04-pros" data-type="indexterm" id="id883"/><a contenteditable="false" data-primary="" data-startref="ch04-pros2" data-type="indexterm" id="id884"/><a contenteditable="false" data-primary="" data-startref="ch04-pros3" data-type="indexterm" id="id885"/></p>&#13;
&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="how_null_hypothesis_significance_testin">&#13;
<h1>How Null Hypothesis Significance Testing Became <span class="keep-together">“Worse Than Useless”</span></h1>&#13;
&#13;
<p>Soon after Fisher introduced NHST,<a contenteditable="false" data-primary="null hypothesis significance testing (NHST)" data-secondary="path to “worse than useless”" data-type="indexterm" id="id886"/><a contenteditable="false" data-primary="Neyman, Jerzy" data-type="indexterm" id="id887"/><a contenteditable="false" data-primary="Fisher, Ronald A." data-type="indexterm" id="id888"/><a contenteditable="false" data-primary="Pearson, Egon" data-type="indexterm" id="id889"/> Jerzy Neyman and Egon Pearson, the other two architects of modern statistics, introduced their methodology of statistical hypothesis testing as a decision framework for industrial quality control.</p>&#13;
&#13;
<p>There was a bitter rivalry between Fisher and Neyman as they advocated for their respective methodologies. Neyman called Fisher’s statistical work “worse than useless,” and Fisher called Neyman’s work “childish” and “horrifying.” Unfortunately, both Neyman and Fisher were right. Fisher’s work on statistical inference is indeed worse than useless because he rejected the inverse probability rule and unabashedly committed the prosecutor’s fallacy with impunity.</p>&#13;
&#13;
<p>Fisher was right in that Neyman’s work is not applicable to social sciences but to industrial quality control. It would be inappropriate to apply it to social systems of creative and free-willed humans instead of factory widgets. Fisher also didn’t understand that Neyman’s theory was a predata theory, and it would be absurd if it were applied as a postdata theory of statistical Inference.</p>&#13;
&#13;
<p>As the two competing methodologies spread throughout the social and economic sciences in the last century, researchers tried to reconcile the ideas of the two bitter rivals. Unfortunately, these social scientists and statisticians did not have a deep understanding of either methodology.<sup><a data-type="noteref" href="ch04.html#ch04fn12" id="ch04fn12-marker">12</a></sup> This inept fusion of the two methodologies is how NHST became “worse than useless.”</p>&#13;
</div></aside>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="The Confidence Game" data-type="sect1"><div class="sect1" id="the_confidence_game">&#13;
<h1>The Confidence Game</h1>&#13;
&#13;
<p>As mentioned in the previous sidebar,<a contenteditable="false" data-primary="Neyman, Jerzy" data-type="indexterm" id="id890"/><a contenteditable="false" data-primary="confidence intervals (CIs)" data-secondary="CI theory" data-type="indexterm" id="ch04-cith"/><a contenteditable="false" data-primary="dangers of conventional statistical methods" data-secondary="confidence intervals" data-tertiary="CI theory as predata" data-type="indexterm" id="id891"/><a contenteditable="false" data-primary="statistical decision theory of Neyman" data-type="indexterm" id="id892"/><a contenteditable="false" data-primary="type I and type II error control by Neyman" data-type="indexterm" id="id893"/> Jerzy Neyman developed a statistical decision theory designed to support industrial quality control. His statistical theory provides a decision framework that seeks to control type I (false positive) and type II (false negative) errors to balance costs versus benefits over the long run based on many experiments. Neyman intentionally left out p-values because it was a nonsensical concept violating basic probabilistic logic.</p>&#13;
&#13;
<p class="pagebreak-before">In 1937, Neyman developed<a contenteditable="false" data-primary="confidence intervals (CIs)" data-secondary="CI theory" data-tertiary="predata theory per Neyman" data-type="indexterm" id="id894"/> CI theory to be a <em>predata theory</em> of statistical inference, intended to inform statistical procedures that have long-run average properties <em>before</em> data are sampled from a population distribution. Neyman made it very clear that his CI theory was not intended to support inferences <em>after</em> data are sampled in a single scientific experiment. CI theory is not a <em>postdata theory of</em> statistical inference despite how it is applied today in research and practice in social and economic sciences.</p>&#13;
&#13;
<p>CI theory quantifies<a contenteditable="false" data-primary="confidence intervals (CIs)" data-secondary="about" data-type="indexterm" id="id895"/> uncertainty of population parameter estimates. <a contenteditable="false" data-primary="confidence intervals (CIs)" data-secondary="about" data-tertiary="90% CI explained" data-type="indexterm" id="id896"/>For example, a 90% confidence interval (CI), as shown in <a data-type="xref" href="#the_interval_left_square_bracketen_dash">Figure 4-3</a>, is generally understood to imply that there is a 90% probability that the true value of a parameter of interest is in the interval [–a, a].</p>&#13;
&#13;
<figure><div class="figure" id="the_interval_left_square_bracketen_dash"><img alt="The interval [–a, a] is called a 90% confidence interval." src="assets/pmlf_0403.png"/>&#13;
<h6><span class="label">Figure 4-3. </span>The interval [–a, a] is called a 90% confidence interval<sup><a data-type="noteref" href="ch04.html#ch04fn13" id="ch04fn13-marker">13</a></sup></h6>&#13;
</div></figure>&#13;
&#13;
<p>Fisher attacked Neyman’s <a contenteditable="false" data-primary="Fisher, Ronald A." data-type="indexterm" id="id897"/>CI theory by claiming it did not serve the needs of scientists and potentially would lead to mutually contradictory inferences from data. Fisher’s criticisms of CI theory have proven to be justified—but not because Neyman’s CI theory is logically or mathematically flawed, as Fisher claimed.</p>&#13;
&#13;
<p class="pagebreak-before">Let’s examine the trio of errors<a contenteditable="false" data-primary="confidence intervals (CIs)" data-secondary="CI theory" data-tertiary="errors from postdata use" data-type="indexterm" id="id898"/><a contenteditable="false" data-primary="dangers of conventional statistical methods" data-secondary="confidence intervals" data-tertiary="errors from CI theory used postdata" data-type="indexterm" id="id899"/><a contenteditable="false" data-primary="confidence intervals (CIs)" data-secondary="errors from postdata use" data-tertiary="about" data-type="indexterm" id="id900"/> that arise from the common practice of misusing Neyman’s CI theory as a postdata theory—i.e., for making inferences about population parameters based on a specific data sample. The three types of errors using CIs are:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>Making probabilistic claims about population parameters</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Making probabilistic claims about a specific confidence interval</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Making probabilistic claims about sampling distributions</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>The frequentist philosophy of<a contenteditable="false" data-primary="frequentist view of probability" data-secondary="worse than useless" data-tertiary="impact on financial economics" data-type="indexterm" id="id901"/><a contenteditable="false" data-primary="probability" data-secondary="frequentist conventional view" data-tertiary="impact on financial economics" data-type="indexterm" id="id902"/><a contenteditable="false" data-primary="dangers of conventional statistical methods" data-secondary="frequentist view worse than useless" data-tertiary="impact on financial economics" data-type="indexterm" id="id903"/> probability and statistical inference has had a profound impact on the theory and practice of financial economics in general and CIs in particular. To explore the implications of confidence intervals (CIs) for our purposes, we begin the next subsection by discussing the fundamental concepts of a simple market model and its relationship to financial theory. Afterward, we utilize Statsmodels, a statistical package in Python, to construct an ordinary least squares (OLS) linear regression model of equity returns to estimate the parameters of our market model. This real-world example allows us to illustrate how CIs are actually applied in financial data analysis. In the next sections, we examine why CIs are logically incoherent and practically useless.<a contenteditable="false" data-primary="" data-startref="ch04-cith" data-type="indexterm" id="id904"/></p>&#13;
&#13;
<section data-pdf-bookmark="Single-Factor Market Model for Equities" data-type="sect2"><div class="sect2" id="single_factor_market_model_for_equities">&#13;
<h2>Single-Factor Market Model for Equities</h2>&#13;
&#13;
<p>Modern portfolio theory assumes that<a contenteditable="false" data-primary="dangers of conventional statistical methods" data-secondary="confidence intervals" data-tertiary="single-factor market model for equities" data-type="indexterm" id="id905"/><a contenteditable="false" data-primary="single-factor market model for equities" data-type="indexterm" id="id906"/><a contenteditable="false" data-primary="equity single-factor market model" data-type="indexterm" id="id907"/><a contenteditable="false" data-primary="modern portfolio theory (MPT)" data-secondary="excess returns for excess risk" data-type="indexterm" id="id908"/><a contenteditable="false" data-primary="excess returns for excess risk" data-type="indexterm" id="id909"/><a contenteditable="false" data-primary="stock single-factor market model" data-type="indexterm" id="id910"/><a contenteditable="false" data-primary="risk" data-secondary="excess returns for excess risk" data-tertiary="single-factor market model for equities" data-type="indexterm" id="id911"/><a contenteditable="false" data-primary="market model for equities (MM)" data-type="indexterm" id="id912"/><a contenteditable="false" data-primary="confidence intervals (CIs)" data-secondary="single-factor market model for equities" data-type="indexterm" id="id913"/> rational, risk-averse investors demand a risk premium, a return in excess of a risk-free asset such as a treasury bill, for investing in risky assets such as equities. A stock’s single-factor market model (MM) is basically a linear regression model of the realized excess returns of a stock (outcome or dependent variable) regressed against the realized excess returns of a single risk factor (predictor or independent variable) such as the overall market, as formulated here:</p>&#13;
&#13;
<ul class="simplelist">&#13;
	<li>&#13;
	<p>( R − F ) = α + β × ( M − F ) + ϵ</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>Where <em>R</em> is the realized return of a stock, <em>F</em> is the return on a risk-free asset such as a US Treasury security, <em>M</em> is the realized return of a market portfolio such as the S&amp;P 500, <em>α</em> (alpha) is the expected stock-specific return, <em>β</em> (beta) is the level of systematic risk exposure to the market, and <em>ε</em> (epsilon) is the unexpected stock-specific return. <a contenteditable="false" data-primary="unexpected stock-specific return (ε)" data-type="indexterm" id="id914"/><a contenteditable="false" data-primary="ε (unexpected stock-specific return)" data-type="indexterm" id="id915"/><a contenteditable="false" data-primary="expected stock-specific return (α)" data-type="indexterm" id="id916"/><a contenteditable="false" data-primary="α (expected stock-specific return)" data-type="indexterm" id="id917"/><a contenteditable="false" data-primary="alpha (α) as expected stock-specific return" data-type="indexterm" id="id918"/><a contenteditable="false" data-primary="equity single-factor market model" data-secondary="alpha as expected stock-specific return" data-type="indexterm" id="id919"/><a contenteditable="false" data-primary="market model for equities (MM)" data-secondary="alpha as expected stock-specific return" data-type="indexterm" id="id920"/><a contenteditable="false" data-primary="single-factor market model for equities" data-secondary="alpha as expected stock-specific return" data-type="indexterm" id="id921"/><a contenteditable="false" data-primary="confidence intervals (CIs)" data-secondary="single-factor market model for equities" data-tertiary="alpha as expected stock-specific return" data-type="indexterm" id="id922"/><a contenteditable="false" data-primary="stock single-factor market model" data-secondary="alpha as expected stock-specific return" data-type="indexterm" id="id923"/><a contenteditable="false" data-primary="β (systemic risk exposure)" data-type="indexterm" id="id924"/><a contenteditable="false" data-primary="systemic risk exposure (β)" data-type="indexterm" id="id925"/><a contenteditable="false" data-primary="beta (β) as systemic risk exposure" data-type="indexterm" id="id926"/><a contenteditable="false" data-primary="risk" data-secondary="systemic risk exposure (β)" data-type="indexterm" id="id927"/><a contenteditable="false" data-primary="confidence intervals (CIs)" data-secondary="single-factor market model for equities" data-tertiary="beta as systemic risk exposure" data-type="indexterm" id="id928"/><a contenteditable="false" data-primary="stock single-factor market model" data-secondary="beta as systemic risk exposure" data-type="indexterm" id="id929"/><a contenteditable="false" data-primary="equity single-factor market model" data-secondary="beta as systemic risk exposure" data-type="indexterm" id="id930"/><a contenteditable="false" data-primary="single-factor market model for equities" data-secondary="beta as systemic risk exposure" data-type="indexterm" id="id931"/><a contenteditable="false" data-primary="market model for equities (MM)" data-secondary="beta as systemic risk exposure" data-type="indexterm" id="id932"/><a contenteditable="false" data-primary="Apple Inc. (AAPL)" data-secondary="market model" data-type="indexterm" id="id933"/><a contenteditable="false" data-primary="epsilon (ε) as unexpected stock-specific return" data-type="indexterm" id="id934"/>The beta of a stock gives the average percentage return response to a 1% change in return of the overall market portfolio. For example, if a stock has a beta of 1.4 and the S&amp;P 500 falls by 1%, the stock is expected to fall by –1.4% <em>on average</em>. See <a data-type="xref" href="#market_model_showing_the_excess_returns">Figure 4-4</a>.</p>&#13;
&#13;
<figure><div class="figure" id="market_model_showing_the_excess_returns"><img alt="Market Model showing the excess returns of Apple Inc. (AAPL) regressed against the excess returns of the S&amp;P 500" src="assets/pmlf_0404.png"/>&#13;
<h6><span class="label">Figure 4-4. </span>Market model showing the excess returns of Apple Inc. (AAPL) regressed against the excess returns of the S&amp;P 500</h6>&#13;
</div></figure>&#13;
&#13;
<p>Note that the MM of an asset is different<a contenteditable="false" data-primary="capital asset pricing model (CAPM)" data-type="indexterm" id="id935"/><a contenteditable="false" data-primary="capital asset pricing model (CAPM)" data-secondary="single-factor market model versus" data-type="indexterm" id="id936"/><a contenteditable="false" data-primary="single-factor market model for equities" data-secondary="capital asset pricing model versus" data-type="indexterm" id="id937"/><a contenteditable="false" data-primary="equity single-factor market model" data-secondary="capital asset pricing model versus" data-type="indexterm" id="id938"/><a contenteditable="false" data-primary="stock single-factor market model" data-secondary="capital asset pricing model versus" data-type="indexterm" id="id939"/><a contenteditable="false" data-primary="market model for equities (MM)" data-secondary="capital asset pricing model versus" data-type="indexterm" id="id940"/><a contenteditable="false" data-primary="confidence intervals (CIs)" data-secondary="single-factor market model for equities" data-tertiary="capital asset pricing model versus" data-type="indexterm" id="id941"/> from its capital asset pricing model (CAPM). The CAPM is the pivotal economic equilibrium model of modern finance that predicts expected returns of an asset based on its <em>β</em> or systematic risk exposure to the overall market. Unlike the CAPM, an asset’s MM is a statistical model about realized returns that has both an idiosyncratic risk term <em>ɑ</em> and an error term <em>ɛ</em> in its formulation.</p>&#13;
&#13;
<p>According to the CAPM, the alpha of an asset’s MM has an expected value of zero because market participants are assumed to hold efficient portfolios that diversify the idiosyncratic risks of any specific asset. Market participants are only rewarded for bearing systematic risk since it cannot be diversified away. In keeping with the general assumptions of an OLS regression model, both CAPM and MM assume that the expected value of the residuals <em>ɛ</em> will be normally distributed with a zero mean and a constant, finite variance.</p>&#13;
&#13;
<p>A financial analyst, relying on<a contenteditable="false" data-primary="modern portfolio theory (MPT)" data-secondary="price data generated by underlying, time-invariant, stochastic process" data-type="indexterm" id="id942"/> modern portfolio theory and practice, assumes there is an underlying, time-invariant, stochastic process generating the price data of Apple Inc., which can be modeled as an OLS linear regression MM. This MM will have population parameters, alpha and beta, which have true, fixed values that can be estimated from reason random samples of Apple’s closing price data.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Simple Linear Regression with Statsmodels" data-type="sect2"><div class="sect2" id="simple_linear_regression_with_statsmode">&#13;
<h2>Simple Linear Regression with Statsmodels</h2>&#13;
&#13;
<p>Let’s run our Python code to<a contenteditable="false" data-primary="dangers of conventional statistical methods" data-secondary="confidence intervals" data-tertiary="alpha and beta estimation code" data-type="indexterm" id="ch04-alph"/><a contenteditable="false" data-primary="confidence intervals (CIs)" data-secondary="alpha and beta estimation code" data-type="indexterm" id="ch04-alph2"/><a contenteditable="false" data-primary="alpha (α) as expected stock-specific return" data-secondary="code for estimating" data-type="indexterm" id="ch04-alph3"/><a contenteditable="false" data-primary="α (expected stock-specific return)" data-secondary="code for estimating" data-type="indexterm" id="ch04-alph4"/><a contenteditable="false" data-primary="expected stock-specific return (α)" data-secondary="code for estimating" data-type="indexterm" id="ch04-alph5"/><a contenteditable="false" data-primary="beta (β) as systemic risk exposure" data-secondary="code for estimating" data-type="indexterm" id="ch04-alph6"/><a contenteditable="false" data-primary="β (systemic risk exposure)" data-secondary="code for estimating" data-type="indexterm" id="ch04-alph7"/><a contenteditable="false" data-primary="systemic risk exposure (β)" data-secondary="code for estimating" data-type="indexterm" id="ch04-alph8"/><a contenteditable="false" data-primary="risk" data-secondary="systemic risk exposure (β)" data-tertiary="code for estimating" data-type="indexterm" id="ch04-alph9"/><a contenteditable="false" data-primary="Statsmodels linear regression to estimate alpha and beta" data-type="indexterm" id="ch04-alphA"/><a contenteditable="false" data-primary="linear regression" data-secondary="estimating alpha and beta" data-type="indexterm" id="ch04-alphB"/><a contenteditable="false" data-primary="equity single-factor market model" data-secondary="code for estimating alpha and beta" data-type="indexterm" id="ch04-alphC"/><a contenteditable="false" data-primary="market model for equities (MM)" data-secondary="code for estimating alpha and beta" data-type="indexterm" id="ch04-alphD"/><a contenteditable="false" data-primary="single-factor market model for equities" data-secondary="code for estimating alpha and beta" data-type="indexterm" id="ch04-alphE"/><a contenteditable="false" data-primary="stock single-factor market model" data-secondary="code for estimating alpha and beta" data-type="indexterm" id="ch04-alphF"/><a contenteditable="false" data-primary="Apple Inc. (AAPL)" data-secondary="code for estimating alpha and beta" data-type="indexterm" id="ch04-alphG"/> estimate alpha and beta based on a sample of five years of daily closing prices of Apple. We can use any holding period return as long as it is used consistently throughout the formula. Using a daily holding period is convenient because it makes price return calculations much easier using pandas DataFrames:</p>&#13;
&#13;
<pre class="pagebreak-before" data-code-language="python" data-type="programlisting">&#13;
<code class="c1"># Install Yahoo finance package</code>&#13;
<code class="err">!</code><code class="n">pip</code> <code class="n">install</code> <code class="n">yfinance</code>&#13;
&#13;
<code class="c1"># Import relevant Python packages</code>&#13;
<code class="kn">import</code> <code class="nn">statsmodels.api</code> <code class="k">as</code> <code class="nn">sm</code>&#13;
<code class="kn">import</code> <code class="nn">pandas</code> <code class="k">as</code> <code class="nn">pd</code>&#13;
<code class="kn">import</code> <code class="nn">yfinance</code> <code class="k">as</code> <code class="nn">yf</code>&#13;
&#13;
<code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="k">as</code> <code class="nn">plt</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">style</code><code class="o">.</code><code class="n">use</code><code class="p">(</code><code class="s1">'seaborn'</code><code class="p">)</code>&#13;
<code class="kn">from</code> <code class="nn">datetime</code> <code class="kn">import</code> <code class="n">datetime</code>&#13;
<code class="c1">#Import financial data</code>&#13;
<code class="n">start</code> <code class="o">=</code> <code class="n">datetime</code><code class="p">(</code><code class="mi">2017</code><code class="p">,</code> <code class="mi">8</code><code class="p">,</code> <code class="mi">3</code><code class="p">)</code>&#13;
<code class="n">end</code> <code class="o">=</code> <code class="n">datetime</code><code class="p">(</code><code class="mi">2022</code><code class="p">,</code> <code class="mi">8</code><code class="p">,</code> <code class="mi">6</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># S&amp;P 500 index is a proxy for the market</code>&#13;
<code class="n">market</code> <code class="o">=</code> <code class="n">yf</code><code class="o">.</code><code class="n">Ticker</code><code class="p">(</code><code class="s1">'SPY'</code><code class="p">)</code><code class="o">.</code><code class="n">history</code><code class="p">(</code><code class="n">start</code><code class="o">=</code><code class="n">start</code><code class="p">,</code> <code class="n">end</code><code class="o">=</code><code class="n">end</code><code class="p">)</code>&#13;
<code class="c1"># Ticker symbol for Apple, the most liquid stock in the world</code>&#13;
<code class="n">stock</code> <code class="o">=</code> <code class="n">yf</code><code class="o">.</code><code class="n">Ticker</code><code class="p">(</code><code class="s1">'AAPL'</code><code class="p">)</code><code class="o">.</code><code class="n">history</code><code class="p">(</code><code class="n">start</code><code class="o">=</code><code class="n">start</code><code class="p">,</code> <code class="n">end</code><code class="o">=</code><code class="n">end</code><code class="p">)</code>&#13;
<code class="c1"># 10 year US treasury note is the proxy for risk free rate</code>&#13;
<code class="n">riskfree_rate</code> <code class="o">=</code> <code class="n">yf</code><code class="o">.</code><code class="n">Ticker</code><code class="p">(</code><code class="s1">'^TNX'</code><code class="p">)</code><code class="o">.</code><code class="n">history</code><code class="p">(</code><code class="n">start</code><code class="o">=</code><code class="n">start</code><code class="p">,</code> <code class="n">end</code><code class="o">=</code><code class="n">end</code><code class="p">)</code>&#13;
<code class="c1"># Create dataframe to hold daily returns of securities</code>&#13;
<code class="n">daily_returns</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">()</code>&#13;
<code class="n">daily_returns</code><code class="p">[</code><code class="s1">'market'</code><code class="p">]</code> <code class="o">=</code> <code class="n">market</code><code class="p">[</code><code class="s1">'Close'</code><code class="p">]</code><code class="o">.</code><code class="n">pct_change</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code><code class="o">*</code><code class="mi">100</code>&#13;
<code class="n">daily_returns</code><code class="p">[</code><code class="s1">'stock'</code><code class="p">]</code> <code class="o">=</code> <code class="n">stock</code><code class="p">[</code><code class="s1">'Close'</code><code class="p">]</code><code class="o">.</code><code class="n">pct_change</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code><code class="o">*</code><code class="mi">100</code>&#13;
<code class="c1"># Compounded daily rate based on 360 days </code>&#13;
<code class="c1"># for the calendar year used in the bond market</code>&#13;
<code class="n">daily_returns</code><code class="p">[</code><code class="s1">'riskfree'</code><code class="p">]</code> <code class="o">=</code> <code class="p">(</code><code class="mi">1</code> <code class="o">+</code> <code class="n">riskfree_rate</code><code class="p">[</code><code class="s1">'Close'</code><code class="p">])</code> <code class="o">**</code> <code class="p">(</code><code class="mi">1</code><code class="o">/</code><code class="mi">360</code><code class="p">)</code> <code class="o">-</code> <code class="mi">1</code>&#13;
<code class="c1"># Plot and summarize the distribution of daily returns</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">hist</code><code class="p">(</code><code class="n">daily_returns</code><code class="p">[</code><code class="s1">'market'</code><code class="p">]),</code> <code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="s1">'Distribution of Market (SPY) </code><code class="w"/>&#13;
<code class="n">Daily</code> <code class="n">Returns</code><code class="s1">'), plt.xlabel('</code><code class="n">Daily</code> <code class="n">Percentage</code> <code class="n">Returns</code><code class="s1">'), </code><code class="w"/>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s1">'Frequency'</code><code class="p">),</code> <code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code>&#13;
<code class="c1"># Analyze descriptive statistics</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="s2">"Descriptive Statistics of the Market's daily percentage returns:</code><code class="se">\n</code><code class="si">{}</code><code class="s2">"</code><code class="o">.</code>&#13;
<code class="nb">format</code><code class="p">(</code><code class="n">daily_returns</code><code class="p">[</code><code class="s1">'market'</code><code class="p">]</code><code class="o">.</code><code class="n">describe</code><code class="p">()))</code>&#13;
&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">hist</code><code class="p">(</code><code class="n">daily_returns</code><code class="p">[</code><code class="s1">'stock'</code><code class="p">]),</code> &#13;
<code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="s1">'Distribution of Apple Inc. (AAPL) Daily Returns'</code><code class="p">),</code> &#13;
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s1">'Daily Percentage Returns'</code><code class="p">),</code> <code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s1">'Frequency'</code><code class="p">),</code> <code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code>&#13;
<code class="c1"># Analyze descriptive statistics</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="s2">"Descriptive Statistics of the Apple's daily percentage returns:</code><code class="se">\n</code><code class="si">{}</code><code class="s2">"</code>&#13;
<code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">daily_returns</code><code class="p">[</code><code class="s1">'stock'</code><code class="p">]</code><code class="o">.</code><code class="n">describe</code><code class="p">()))</code>&#13;
&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">hist</code><code class="p">(</code><code class="n">daily_returns</code><code class="p">[</code><code class="s1">'riskfree'</code><code class="p">]),</code> <code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="s1">'Distribution of the riskfree </code><code class="w"/>&#13;
<code class="n">rate</code> <code class="p">(</code><code class="n">TNX</code><code class="p">)</code> <code class="n">Daily</code> <code class="n">Returns</code><code class="s1">'), plt.xlabel('</code><code class="n">Daily</code> <code class="n">Percentage</code> <code class="n">Returns</code><code class="s1">'), </code><code class="w"/>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s1">'Frequency'</code><code class="p">),</code> <code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code>&#13;
<code class="c1"># Analyze descriptive statistics</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="s2">"Descriptive Statistics of the 10 year note daily percentage returns:</code><code class="se">\n</code><code class="si">{}</code><code class="s2">"</code>&#13;
<code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">daily_returns</code><code class="p">[</code><code class="s1">'riskfree'</code><code class="p">]</code><code class="o">.</code><code class="n">describe</code><code class="p">()))</code>&#13;
<code class="c1"># Examine missing rows in the dataframe</code>&#13;
<code class="n">market</code><code class="o">.</code><code class="n">index</code><code class="o">.</code><code class="n">difference</code><code class="p">(</code><code class="n">riskfree_rate</code><code class="o">.</code><code class="n">index</code><code class="p">)</code>&#13;
<code class="c1"># Fill rows with previous day's risk-free rate since daily rates </code>&#13;
<code class="c1"># are generally stable</code>&#13;
<code class="n">daily_returns</code> <code class="o">=</code> <code class="n">daily_returns</code><code class="o">.</code><code class="n">ffill</code><code class="p">()</code>&#13;
<code class="c1"># Drop NaNs in first row because of percentage calculations</code>&#13;
<code class="n">daily_returns</code> <code class="o">=</code> <code class="n">daily_returns</code><code class="o">.</code><code class="n">dropna</code><code class="p">()</code>&#13;
<code class="c1"># Check dataframe for null values</code>&#13;
<code class="n">daily_returns</code><code class="o">.</code><code class="n">isnull</code><code class="p">()</code><code class="o">.</code><code class="n">sum</code><code class="p">()</code>&#13;
<code class="c1"># Check first five rows of dataframe</code>&#13;
<code class="n">daily_returns</code><code class="o">.</code><code class="n">head</code><code class="p">()</code>&#13;
<code class="c1"># AAPL's Market Model based on daily excess returns</code>&#13;
&#13;
<code class="c1"># Daily excess returns of AAPL</code>&#13;
<code class="n">y</code> <code class="o">=</code> <code class="n">daily_returns</code><code class="p">[</code><code class="s1">'stock'</code><code class="p">]</code> <code class="o">-</code> <code class="n">daily_returns</code><code class="p">[</code><code class="s1">'riskfree'</code><code class="p">]</code>&#13;
<code class="c1"># Daily excess returns of the market</code>&#13;
<code class="n">x</code> <code class="o">=</code> <code class="n">daily_returns</code><code class="p">[</code><code class="s1">'market'</code><code class="p">]</code> <code class="o">-</code> <code class="n">daily_returns</code><code class="p">[</code><code class="s1">'riskfree'</code><code class="p">]</code>&#13;
&#13;
<code class="c1"># Plot the data</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">x</code><code class="p">,</code><code class="n">y</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># Add the constant vector to obtain the intecept</code>&#13;
<code class="n">x</code> <code class="o">=</code> <code class="n">sm</code><code class="o">.</code><code class="n">add_constant</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># Use ordinary least squares algorithm to find the line of best fit</code>&#13;
<code class="n">market_model</code> <code class="o">=</code> <code class="n">sm</code><code class="o">.</code><code class="n">OLS</code><code class="p">(</code><code class="n">y</code><code class="p">,</code> <code class="n">x</code><code class="p">)</code><code class="o">.</code><code class="n">fit</code><code class="p">()</code>&#13;
&#13;
<code class="c1"># Plot the line of best fit</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">x</code><code class="o">*</code><code class="n">market_model</code><code class="o">.</code><code class="n">params</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">+</code><code class="n">market_model</code><code class="o">.</code><code class="n">params</code><code class="p">[</code><code class="s1">'const'</code><code class="p">])</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="s1">'Market Model of AAPL'</code><code class="p">),</code> <code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s1">'SPY Daily Excess Returns'</code><code class="p">),</code> &#13;
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s1">'AAPL Daily Excess Returns'</code><code class="p">),</code> <code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">();</code>&#13;
&#13;
<code class="c1"># Display the values of alpha and beta of AAPL's market model</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="s2">"According to AAPL's Market Model, the security had a realized Alpha of </code><code class="w"/>&#13;
<code class="p">{</code><code class="mi">0</code><code class="p">}</code><code class="o">%</code> <code class="ow">and</code> <code class="n">Beta</code> <code class="n">of</code> <code class="p">{</code><code class="mi">1</code><code class="p">}</code><code class="s2">".format(round(market_model.params['const'],2), </code><code class="w"/>&#13;
<code class="nb">round</code><code class="p">(</code><code class="n">market_model</code><code class="o">.</code><code class="n">params</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code><code class="mi">2</code><code class="p">)))</code>&#13;
<code class="c1"># Summarize and analyze the statistics of your linear regression</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="s2">"The Market Model of AAPL is summarized below:</code><code class="se">\n</code><code class="si">{}</code><code class="s2">"</code>&#13;
<code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">market_model</code><code class="o">.</code><code class="n">summary</code><code class="p">()));</code></pre>&#13;
&#13;
<p>After running our Python code, a financial analyst would estimate that alpha is 0.071% and beta is 1.2385, as shown in the Statsmodels summary output:<a contenteditable="false" data-primary="" data-startref="ch04-alph" data-type="indexterm" id="id943"/><a contenteditable="false" data-primary="" data-startref="ch04-alph2" data-type="indexterm" id="id944"/><a contenteditable="false" data-primary="" data-startref="ch04-alph3" data-type="indexterm" id="id945"/><a contenteditable="false" data-primary="" data-startref="ch04-alph4" data-type="indexterm" id="id946"/><a contenteditable="false" data-primary="" data-startref="ch04-alph5" data-type="indexterm" id="id947"/><a contenteditable="false" data-primary="" data-startref="ch04-alph6" data-type="indexterm" id="id948"/><a contenteditable="false" data-primary="" data-startref="ch04-alph7" data-type="indexterm" id="id949"/><a contenteditable="false" data-primary="" data-startref="ch04-alph8" data-type="indexterm" id="id950"/><a contenteditable="false" data-primary="" data-startref="ch04-alph9" data-type="indexterm" id="id951"/><a contenteditable="false" data-primary="" data-startref="ch04-alphA" data-type="indexterm" id="id952"/><a contenteditable="false" data-primary="" data-startref="ch04-alphB" data-type="indexterm" id="id953"/><a contenteditable="false" data-primary="" data-startref="ch04-alphC" data-type="indexterm" id="id954"/><a contenteditable="false" data-primary="" data-startref="ch04-alphD" data-type="indexterm" id="id955"/><a contenteditable="false" data-primary="" data-startref="ch04-alphE" data-type="indexterm" id="id956"/><a contenteditable="false" data-primary="" data-startref="ch04-alphF" data-type="indexterm" id="id957"/><a contenteditable="false" data-primary="" data-startref="ch04-alphG" data-type="indexterm" id="id958"/></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="n">The</code> <code class="n">Market</code> <code class="n">Model</code> <code class="n">of</code> <code class="n">AAPL</code> <code class="ow">is</code> <code class="n">summarized</code> <code class="n">below</code><code class="p">:</code>&#13;
<code class="n">OLS</code> <code class="n">Regression</code> <code class="n">Results</code>&#13;
<code class="o">=========================================================================</code>&#13;
<code class="n">Dep</code><code class="o">.</code> <code class="n">Variable</code><code class="p">:</code>      <code class="n">y</code>                       <code class="n">R</code><code class="o">-</code><code class="n">squared</code><code class="p">:</code>          <code class="mf">0.624</code>&#13;
<code class="n">Model</code><code class="p">:</code>              <code class="n">OLS</code>                     <code class="n">Adj</code><code class="o">.</code> <code class="n">R</code><code class="o">-</code><code class="n">squared</code><code class="p">:</code>     <code class="mf">0.624</code>&#13;
<code class="n">Method</code><code class="p">:</code>             <code class="n">Least</code> <code class="n">Squares</code>           <code class="n">F</code><code class="o">-</code><code class="n">statistic</code><code class="p">:</code>        <code class="mf">2087.</code>&#13;
<code class="n">Date</code><code class="p">:</code>               <code class="n">Sun</code><code class="p">,</code> <code class="mi">07</code> <code class="n">Aug</code> <code class="mi">2022</code>        <code class="n">Prob</code> <code class="p">(</code><code class="n">F</code><code class="o">-</code><code class="n">statistic</code><code class="p">):</code> <code class="mf">2.02e-269</code>&#13;
<code class="n">Time</code><code class="p">:</code>               <code class="mi">06</code><code class="p">:</code><code class="mi">28</code><code class="p">:</code><code class="mi">33</code>                <code class="n">Log</code><code class="o">-</code><code class="n">Likelihood</code><code class="p">:</code>     <code class="o">-</code><code class="mf">2059.8</code>&#13;
<code class="n">No</code><code class="o">.</code> <code class="n">Observations</code><code class="p">:</code>   <code class="mi">1260</code>                    <code class="n">AIC</code><code class="p">:</code>                <code class="mf">4124.</code>&#13;
<code class="n">Df</code> <code class="n">Residuals</code><code class="p">:</code>       <code class="mi">1258</code>                    <code class="n">BIC</code><code class="p">:</code>                <code class="mf">4134.</code>&#13;
<code class="n">Df</code> <code class="n">Model</code><code class="p">:</code>           <code class="mi">1</code>&#13;
<code class="n">Covariance</code> <code class="n">Type</code><code class="p">:</code>    <code class="n">nonrobust</code> &#13;
<code class="o">========================================================================</code>&#13;
        <code class="n">coef</code>        <code class="n">std</code> <code class="n">err</code>     <code class="n">t</code>        <code class="n">P</code><code class="o">&gt;|</code><code class="n">t</code><code class="o">|</code>     <code class="p">[</code><code class="mf">0.025</code>      <code class="mf">0.975</code><code class="p">]</code>&#13;
<code class="n">const</code>   <code class="mf">0.0710</code>      <code class="mf">0.035</code>     <code class="mf">2.028</code>      <code class="mf">0.043</code>     <code class="mf">0.002</code>       <code class="mf">0.140</code>&#13;
<code class="mi">0</code>       <code class="mf">1.2385</code>      <code class="mf">0.027</code>     <code class="mf">45.684</code>     <code class="mf">0.000</code>     <code class="mf">1.185</code>       <code class="mf">1.292</code>&#13;
<code class="o">========================================================================</code>&#13;
<code class="n">Omnibus</code><code class="p">:</code>        <code class="mf">202.982</code>             <code class="n">Durbin</code><code class="o">-</code><code class="n">Watson</code><code class="p">:</code>          <code class="mf">1.848</code>&#13;
<code class="n">Prob</code><code class="p">(</code><code class="n">Omnibus</code><code class="p">):</code>  <code class="mf">0.000</code>               <code class="n">Jarque</code><code class="o">-</code><code class="n">Bera</code> <code class="p">(</code><code class="n">JB</code><code class="p">):</code>       <code class="mf">1785.931</code>&#13;
<code class="n">Skew</code><code class="p">:</code>           <code class="mf">0.459</code>               <code class="n">Prob</code><code class="p">(</code><code class="n">JB</code><code class="p">):</code>               <code class="mf">0.00</code>&#13;
<code class="n">Kurtosis</code><code class="p">:</code>       <code class="mf">8.760</code>               <code class="n">Cond</code><code class="o">.</code> <code class="n">No</code><code class="o">.</code>               <code class="mf">1.30</code>&#13;
<code class="o">======================================================================</code>&#13;
<code class="n">Warnings</code><code class="p">:</code>&#13;
<code class="p">[</code><code class="mi">1</code><code class="p">]</code> <code class="n">Standard</code> <code class="n">Errors</code> <code class="n">assume</code> <code class="n">that</code> <code class="n">the</code> <code class="n">covariance</code> <code class="n">matrix</code> <code class="n">of</code> <code class="n">the</code> <code class="n">errors</code> &#13;
<code class="ow">is</code> <code class="n">correctly</code> <code class="n">specified</code><code class="o">.</code>&#13;
</pre>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Confidence Intervals for Alpha and Beta" data-type="sect2"><div class="sect2" id="confidence_intervals_for_alpha_and_beta">&#13;
<h2>Confidence Intervals for Alpha and Beta</h2>&#13;
&#13;
<p>Clearly, these point estimates<a contenteditable="false" data-primary="alpha (α) as expected stock-specific return" data-secondary="code for estimating" data-tertiary="confidence intervals" data-type="indexterm" id="id959"/><a contenteditable="false" data-primary="Apple Inc. (AAPL)" data-secondary="code for estimating alpha and beta" data-tertiary="confidence intervals" data-type="indexterm" id="id960"/><a contenteditable="false" data-primary="beta (β) as systemic risk exposure" data-secondary="code for estimating" data-tertiary="confidence intervals" data-type="indexterm" id="id961"/><a contenteditable="false" data-primary="equity single-factor market model" data-secondary="code for estimating alpha and beta" data-tertiary="confidence intervals" data-type="indexterm" id="id962"/><a contenteditable="false" data-primary="expected stock-specific return (α)" data-secondary="code for estimating" data-tertiary="confidence intervals" data-type="indexterm" id="id963"/><a contenteditable="false" data-primary="market model for equities (MM)" data-secondary="code for estimating alpha and beta" data-tertiary="confidence intervals" data-type="indexterm" id="id964"/><a contenteditable="false" data-primary="risk" data-secondary="systemic risk exposure (β)" data-tertiary="estimation confidence intervals" data-type="indexterm" id="id965"/><a contenteditable="false" data-primary="single-factor market model for equities" data-secondary="code for estimating alpha and beta" data-tertiary="confidence intervals" data-type="indexterm" id="id966"/><a contenteditable="false" data-primary="stock single-factor market model" data-secondary="code for estimating alpha and beta" data-tertiary="confidence intervals" data-type="indexterm" id="id967"/><a contenteditable="false" data-primary="systemic risk exposure (β)" data-secondary="code for estimating" data-tertiary="confidence intervals" data-type="indexterm" id="id968"/><a contenteditable="false" data-primary="α (expected stock-specific return)" data-secondary="code for estimating" data-tertiary="confidence intervals" data-type="indexterm" id="id969"/><a contenteditable="false" data-primary="β (systemic risk exposure)" data-secondary="code for estimating" data-tertiary="confidence intervals" data-type="indexterm" id="id970"/><a contenteditable="false" data-primary="frequentist view of probability" data-secondary="confidence intervals for alpha and beta" data-type="indexterm" id="id971"/><a contenteditable="false" data-primary="probability" data-secondary="frequentist conventional view" data-tertiary="confidence intervals for alpha and beta" data-type="indexterm" id="id972"/><a contenteditable="false" data-primary="dangers of conventional statistical methods" data-secondary="confidence intervals" data-tertiary="alpha and beta estimation confidence intervals" data-type="indexterm" id="id973"/><a contenteditable="false" data-primary="frequentist view of probability" data-secondary="dangerous" data-see="dangers of conventional statistical methods" data-type="indexterm" id="id974"/><a contenteditable="false" data-primary="probability" data-secondary="frequentist conventional view" data-see="dangers of conventional statistical methods" data-tertiary="dangerous" data-type="indexterm" id="id975"/> of alpha and beta will vary depending on the sample size as well as start and end dates used in our random samples, with each estimate reflecting Apple’s idiosyncratic price fluctuations during that specific time period. Even though the population parameters alpha and beta are unknown, and possibly unknowable, the financial analyst considers them to be true constants of a stochastic process. It is the random sampling of Apple’s price data that introduces uncertainty in the estimates of constant population parameters. It is the data, and every statistic derived from the data, such as CIs, that are treated as random by frequentists. Financial analysts calculate CIs from random samples to express the uncertainty around point estimates of constant population parameters.</p>&#13;
&#13;
<p>CIs provide a range of values<a contenteditable="false" data-primary="confidence intervals (CIs)" data-secondary="about" data-type="indexterm" id="id976"/><a contenteditable="false" data-primary="confidence intervals (CIs)" data-secondary="about" data-tertiary="95% CI for Apple alpha and beta" data-type="indexterm" id="id977"/> with a probability value or significance level attached to that range. For instance, in Apple’s MM, a financial analyst could calculate the 95% confidence interval by calculating the standard error (SE) of alpha and beta. Since the residuals <em>ɛ</em> are assumed to be normally distributed with an unknown, constant variance, the t-statistic would need to be used in computing CIs. However, because the sample size is greater than 30, the t-distribution converges to the standard normal distribution, and the t-statistic values are the same as the Z-scores of a standard normal distribution. So the analyst would multiply each SE by +/– the Z-score for a 95% CI and then add the result to the point estimate of alpha and beta to obtain its CI. From the previous Statsmodels regression results, the 95% CI for alpha and beta were computed as follows:</p>&#13;
&#13;
<ul class="simplelist">&#13;
<li>α+/– (SE × t-statistic / Z-score for 95% CI) = 0.0710 % +/– (0.035 % × 1.96) = [0.002%, 0.140%]</li>&#13;
<li>β+/- (SE × t-statistic / Z-score for 95% CI) = 1.2385 +/– (0.027 × 1.96) = [1.185, 1.292])</li></ul>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Unveiling the Confidence Game" data-type="sect1"><div class="sect1" id="unveiling_the_confidence_game">&#13;
<h1>Unveiling the Confidence Game</h1>&#13;
&#13;
<p>To understand this trio of errors,<a contenteditable="false" data-primary="confidence intervals (CIs)" data-secondary="errors from postdata use" data-tertiary="about" data-type="indexterm" id="id978"/><a contenteditable="false" data-primary="probability" data-secondary="frequentist conventional view" data-type="indexterm" id="id979"/><a contenteditable="false" data-primary="frequentist view of probability" data-type="indexterm" id="id980"/><a contenteditable="false" data-primary="dangers of conventional statistical methods" data-secondary="confidence intervals" data-tertiary="errors from CI theory used postdata" data-type="indexterm" id="ch04-error"/><a contenteditable="false" data-primary="confidence intervals (CIs)" data-secondary="CI theory" data-tertiary="errors from postdata use" data-type="indexterm" id="ch04-error2"/> we need to understand probability and statistical inference from the perspective of a modern statistician. As discussed in <a data-type="xref" href="ch02.html#analyzing_and_quantifying_uncertainty">Chapter 2</a>, frequentists, such as Fisher and Neyman, claim that probability is a natural, static property of an event and is measured empirically as its long-run relative frequency.</p>&#13;
&#13;
<p>Frequentists postulate that the underlying stochastic process that generates data has statistical properties that do not change in the long run: the probability distribution is stationary ergodic. Even though the parameters of this underlying process may be unknown or unknowable, frequentists believe that these parameters are constant and have “true” values. Population parameters may be estimated from random samples of data. It is the randomness of data that creates uncertainty in the estimates of the true, fixed population parameters.</p>&#13;
&#13;
<p>What most people think they<a contenteditable="false" data-primary="confidence intervals (CIs)" data-secondary="about" data-tertiary="95% CI incorrect interpretation" data-type="indexterm" id="id981"/> are getting from a 95% CI is a 95% probability that the true population parameter is within the limits of the <em>specific</em> interval calculated from a specific data sample. For instance, based on the Statsmodels results, you would think there is a 95% probability that the true value of beta of Apple is in the range [1.185, 1.292]. Strictly speaking, your interpretation of such a CI would be wrong.</p>&#13;
&#13;
<p>According to Neyman’s CI theory,<a contenteditable="false" data-primary="confidence intervals (CIs)" data-secondary="about" data-tertiary="95% CI interpretation corrected" data-type="indexterm" id="id982"/> what a 95% CI actually means is that if we were to draw 100 random samples from Apple’s underlying stock return distribution, we would end up with 100 different confidence intervals, and we can be confident that 95 of them will contain the true population parameter within their limits. However, we won’t know which specific 95 CIs of the 100 CIs include the true value of the population parameter and which 5 CIs do not. We are assured that only the long-run ratio of the CIs that include the population parameter to the ones that do not will approach 95% as we draw random samples ad nauseam.</p>&#13;
&#13;
<p>Winston Churchill could just as well have been talking about CIs instead of Russia’s world war strategy when he said, “It is a riddle, wrapped in a mystery, inside an enigma; but perhaps there is a key.” Indeed, we do present a key in this chapter. Let’s investigate the triumvirate of fallacies that arise from misusing CI as a postdata theory in financial data analysis.</p>&#13;
&#13;
<section data-pdf-bookmark="Errors in Making Probabilistic Claims About Population Parameters" data-type="sect2"><div class="sect2" id="errors_in_making_probabilistic_claims">&#13;
<h2>Errors in Making Probabilistic Claims About Population Parameters</h2>&#13;
&#13;
<p>Recall that a frequentist statistician<a contenteditable="false" data-primary="confidence intervals (CIs)" data-secondary="errors from postdata use" data-tertiary="probabilistic claims about population parameters" data-type="indexterm" id="id983"/><a contenteditable="false" data-primary="frequentist view of probability" data-secondary="population parameters constants with “true” values" data-type="indexterm" id="id984"/><a contenteditable="false" data-primary="probability" data-secondary="frequentist conventional view" data-tertiary="population parameters constants with “true” values" data-type="indexterm" id="id985"/><a contenteditable="false" data-primary="population parameters" data-secondary="frequentist view as constants with “true” values" data-type="indexterm" id="id986"/><a contenteditable="false" data-primary="population parameters" data-secondary="error from postdata use of confidence intervals" data-type="indexterm" id="id987"/> considers a population parameter to be a constant with a “true” value. This value may be unknown or even unknowable. But that does not change the fact that its value is fixed. Therefore, a population parameter is either in a CI or it is not. For instance, if you believe the theory that capital markets are highly efficient, you would also believe that the true value of alpha is 0. Now 0 is definitely not in the interval [0.002%, 0.14%] calculated in the previous Statsmodels regression results. Therefore, the probability that alpha is in our CI is 0% and not 95% or any other value.</p>&#13;
&#13;
<p>Because population parameters are believed to be constants by frequentists, there can be absolutely no ambiguity about them: the probability that the true value of a population parameter is within <em>any</em> CI is either 0% or 100%. So it is erroneous to make probabilistic claims about any population parameter under a frequentist interpretation of probability.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Errors in Making Probabilistic Claims About a Specific Confidence Interval" data-type="sect2"><div class="sect2" id="errors_in_making_probabilistic_claims_a">&#13;
<h2>Errors in Making Probabilistic Claims About a Specific <span class="keep-together">Confidence Interval</span></h2>&#13;
&#13;
<p>A more sophisticated interpretation of CIs<a contenteditable="false" data-primary="confidence intervals (CIs)" data-secondary="errors from postdata use" data-tertiary="probabilistic claims about specific confidence interval" data-type="indexterm" id="id988"/><a contenteditable="false" data-primary="frequentist view of probability" data-secondary="long-run frequencies of repeatable events" data-type="indexterm" id="id989"/><a contenteditable="false" data-primary="probability" data-secondary="frequentist conventional view" data-tertiary="long-run frequencies of repeatable events" data-type="indexterm" id="id990"/> found in the literature and textbooks goes as follows: hypothetically speaking, if we were to repeat our linear regression many times, the interval [1.185, 1.292] would contain the true value of beta within its limits about 95% of the time.</p>&#13;
&#13;
<p>Recall that probabilities in the frequentist world apply only to long-run frequencies of <em>repeatable</em> events. By definition, the probability of a unique event, such as a specific CI, is undefined and makes no sense to a frequentist. Therefore, a frequentist cannot assign a 95% probability to either of the specific intervals for alpha and beta that we have calculated. In other words, we can’t really infer much from a specific CI.</p>&#13;
&#13;
<p>But that is the main objective of our exercise! This limitation of CIs makes it utterly useless for data scientists who want to make inferences about population parameters from their specific data samples: i.e., they want to make postdata inferences. But, as was mentioned earlier, Neyman intended his CI theory to be used for only predata inferences based on long-term frequencies.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Errors in Making Probabilistic Claims About Sampling Distributions" data-type="sect2"><div class="sect2" id="errors_in_making_probabilistic_claim">&#13;
<h2>Errors in Making Probabilistic Claims About Sampling Distributions</h2>&#13;
&#13;
<p>How do financial analysts justify<a contenteditable="false" data-primary="sampling distributions of sample mean" data-type="indexterm" id="id991"/><a contenteditable="false" data-primary="confidence intervals (CIs)" data-secondary="errors from postdata use" data-tertiary="probabilistic claims about sampling distributions" data-type="indexterm" id="ch04-samp"/><a contenteditable="false" data-primary="sampling distributions of sample mean" data-secondary="errors from confidence interval postdata use" data-type="indexterm" id="ch04-samp2"/> making these probabilistic claims about CIs in research and practice? How do they square the circle? What is the key to applying CIs in a commonsensical way? Statisticians can, in theory or in practice, repeatedly sample data from a population distribution. The point estimates of sample means computed from many different random samples create a pattern called the sampling distribution of the sample mean. <a contenteditable="false" data-primary="central limit theorem (CLT)" data-secondary="sampling distributions" data-type="indexterm" id="id992"/>Sampling distributions enable frequentists to invoke the central limit theorem (CLT) in calculating the uncertainty around sample point estimates of population parameters. In particular, as was discussed in the previous chapter, the CLT states that if many samples are drawn randomly from a population with a finite mean and variance, the sampling distribution of the sample mean approaches a normal distribution asymptotically. The shape of the underlying population distribution is immaterial and can only affect the speed of this inexorable convergence to normality. See <a data-type="xref" href="ch03.html#the_sampling_distribution_of_the_sample">Figure 3-7</a> in the previous chapter.</p>&#13;
&#13;
<p>The frequentist definition of probability<a contenteditable="false" data-primary="frequentist view of probability" data-secondary="long-run frequencies of repeatable events" data-type="indexterm" id="id993"/><a contenteditable="false" data-primary="probability" data-secondary="frequentist conventional view" data-tertiary="long-run frequencies of repeatable events" data-type="indexterm" id="id994"/> as a long-run relative frequency of repeatable events resonates with the CLT’s repeated drawing of random samples from a population distribution to generate its sampling distributions. So statisticians square the circle by invoking the CLT and claiming that their sampling distributions almost surely converge to a normal distribution, regardless of the shape of the underlying population distribution. This also enables them to compute CIs using the Z-scores of the standard normal distribution, as shown in the previous Statsmodels regression results. This is the key to the enigmatic use of CI as a postdata theory.</p>&#13;
&#13;
<p>However, as financial executives<a contenteditable="false" data-primary="central limit theorem (CLT)" data-secondary="confidence intervals violating assumptions" data-type="indexterm" id="id995"/><a contenteditable="false" data-primary="confidence intervals (CIs)" data-secondary="central limit theorem assumptions violated" data-type="indexterm" id="id996"/><a contenteditable="false" data-primary="dangers of conventional statistical methods" data-secondary="confidence intervals" data-tertiary="central limit theorem assumptions violated" data-type="indexterm" id="id997"/> and investors putting our capital at risk, we need to read the fine print of the CLT: specifically, we need to note its assumption that the underlying population distribution needs to have a finite mean and variance. While most distributions satisfy these two conditions, there are many that do not, especially in finance and economics. For these types of population distributions, the CLT cannot be invoked to save CIs. The key does not work on these doors—it is not a magic key. <a contenteditable="false" data-primary="Cauchy distribution" data-secondary="not having finite mean or variances" data-type="indexterm" id="id998"/><a contenteditable="false" data-primary="probability distributions" data-secondary="fat-tailed Cauchy distribution" data-tertiary="not having finite mean or variances" data-type="indexterm" id="id999"/><a contenteditable="false" data-primary="Pareto distribution not having finite mean or variances" data-type="indexterm" id="id1000"/><a contenteditable="false" data-primary="Lorentzian distribution" data-see="Cauchy distribution" data-type="indexterm" id="id1001"/><a contenteditable="false" data-primary="fat-tailed probability distributions" data-secondary="Cauchy distribution" data-tertiary="not having finite mean or variances" data-type="indexterm" id="id1002"/>For instance, the Cauchy and Pareto distributions are fat-tailed distributions that do not have finite means or variances. As was mentioned in the previous chapter and is worth repeating, a Cauchy (or Lorentzian) distribution looks deceptively similar to a normal distribution, but has very fat tails because of its infinite variance. See <a data-type="xref" href="#compare_cauchysoliduslorentzian_distrib">Figure 4-5</a>.</p>&#13;
&#13;
<p>The diagnostic tests computed by Statsmodels in <a data-type="xref" href="#market_model_showing_the_excess_returns">Figure 4-4</a> show us<a contenteditable="false" data-primary="single-factor market model for equities" data-secondary="key assumptions wrecked" data-type="indexterm" id="id1003"/><a contenteditable="false" data-primary="market model for equities (MM)" data-secondary="key assumptions wrecked" data-type="indexterm" id="id1004"/><a contenteditable="false" data-primary="stock single-factor market model" data-secondary="key assumptions wrecked" data-type="indexterm" id="id1005"/><a contenteditable="false" data-primary="equity single-factor market model" data-secondary="key assumptions wrecked" data-type="indexterm" id="id1006"/><a contenteditable="false" data-primary="dangers of conventional statistical methods" data-secondary="confidence intervals" data-tertiary="market model assumptions wrecked" data-type="indexterm" id="id1007"/><a contenteditable="false" data-primary="confidence intervals (CIs)" data-secondary="key assumptions wrecked" data-tertiary="market model" data-type="indexterm" id="id1008"/> that the equity market has wrecked the key assumptions of our MM. Specifically, the Bera-Jarque and Omnibus normality tests show the probability that the residuals <em>ɛ</em> that are normally distributed are almost surely zero. This distribution is positively skewed and has very fat tails—a kurtosis that is about three times that of a standard normal distribution.</p>&#13;
&#13;
<figure><div class="figure" id="compare_cauchysoliduslorentzian_distrib"><img alt="Compare Cauchy/Lorentzian distribution with the Normal distribution." src="assets/pmlf_0405.png"/>&#13;
<h6><span class="label">Figure 4-5. </span>Compare Cauchy distribution with the normal distribution<sup><a data-type="noteref" href="ch04.html#ch04fn14" id="ch04fn14-marker">14</a></sup></h6>&#13;
</div></figure>&#13;
&#13;
<p>How about making the sample size even larger? Won’t the distribution of the residuals get more normal with a much larger sample size, as claimed by financial theory? <a contenteditable="false" data-primary="Apple Inc. (AAPL)" data-secondary="market model" data-type="indexterm" id="id1009"/>Let’s run our MM using 25 years of Apple’s daily closing prices—a quarter of a century’s worth of data. Here are the results:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="n">The</code> <code class="n">Market</code> <code class="n">Model</code> <code class="n">of</code> <code class="n">AAPL</code> <code class="ow">is</code> <code class="n">summarized</code> <code class="n">below</code><code class="p">:</code>&#13;
<code class="n">OLS</code> <code class="n">Regression</code> <code class="n">Results</code>&#13;
<code class="o">=========================================================================</code>&#13;
<code class="n">Dep</code><code class="o">.</code> <code class="n">Variable</code><code class="p">:</code>      <code class="n">y</code>                       <code class="n">R</code><code class="o">-</code><code class="n">squared</code><code class="p">:</code>          <code class="mf">0.270</code>&#13;
<code class="n">Model</code><code class="p">:</code>              <code class="n">OLS</code>                     <code class="n">Adj</code><code class="o">.</code> <code class="n">R</code><code class="o">-</code><code class="n">squared</code><code class="p">:</code>     <code class="mf">0.270</code>&#13;
<code class="n">Method</code><code class="p">:</code>             <code class="n">Least</code> <code class="n">Squares</code>           <code class="n">F</code><code class="o">-</code><code class="n">statistic</code><code class="p">:</code>        <code class="mf">2331.</code>&#13;
<code class="n">Date</code><code class="p">:</code>               <code class="n">Sun</code><code class="p">,</code> <code class="mi">07</code> <code class="n">Aug</code> <code class="mi">2022</code>        <code class="n">Prob</code> <code class="p">(</code><code class="n">F</code><code class="o">-</code><code class="n">statistic</code><code class="p">):</code> <code class="mf">0.00</code>&#13;
<code class="n">Time</code><code class="p">:</code>               <code class="mi">07</code><code class="p">:</code><code class="mi">03</code><code class="p">:</code><code class="mi">34</code>                <code class="n">Log</code><code class="o">-</code><code class="n">Likelihood</code><code class="p">:</code>     <code class="o">-</code><code class="mf">14187.</code>&#13;
<code class="n">No</code><code class="o">.</code> <code class="n">Observations</code><code class="p">:</code>   <code class="mi">6293</code>                    <code class="n">AIC</code><code class="p">:</code>                <code class="mf">2.838e+04</code>&#13;
<code class="n">Df</code> <code class="n">Residuals</code><code class="p">:</code>       <code class="mi">6291</code>                    <code class="n">BIC</code><code class="p">:</code>                <code class="mf">2.839e+04</code>&#13;
<code class="n">Df</code> <code class="n">Model</code><code class="p">:</code>           <code class="mi">1</code>&#13;
<code class="n">Covariance</code> <code class="n">Type</code><code class="p">:</code>    <code class="n">nonrobust</code> &#13;
<code class="o">========================================================================</code>&#13;
        <code class="n">coef</code>        <code class="n">std</code> <code class="n">err</code>     <code class="n">t</code>        <code class="n">P</code><code class="o">&gt;|</code><code class="n">t</code><code class="o">|</code>     <code class="p">[</code><code class="mf">0.025</code>      <code class="mf">0.975</code><code class="p">]</code>&#13;
<code class="n">const</code>   <code class="mf">0.1063</code>      <code class="mf">0.029</code>     <code class="mf">3.656</code>      <code class="mf">0.000</code>     <code class="mf">0.049</code>       <code class="mf">0.163</code>&#13;
<code class="mi">0</code>       <code class="mf">1.1208</code>      <code class="mf">0.023</code>     <code class="mf">48.281</code>     <code class="mf">0.000</code>     <code class="mf">1.075</code>       <code class="mf">1.166</code>&#13;
<code class="o">========================================================================</code>&#13;
<code class="n">Omnibus</code><code class="p">:</code>        <code class="mf">2566.940</code>             <code class="n">Durbin</code><code class="o">-</code><code class="n">Watson</code><code class="p">:</code>          <code class="mf">2.020</code>&#13;
<code class="n">Prob</code><code class="p">(</code><code class="n">Omnibus</code><code class="p">):</code>  <code class="mf">0.000</code>                <code class="n">Jarque</code><code class="o">-</code><code class="n">Bera</code> <code class="p">(</code><code class="n">JB</code><code class="p">):</code>       <code class="mf">66298.825</code>&#13;
<code class="n">Skew</code><code class="p">:</code>          <code class="o">-</code><code class="mf">0.736</code>                <code class="n">Prob</code><code class="p">(</code><code class="n">JB</code><code class="p">):</code>               <code class="mf">0.00</code>&#13;
<code class="n">Kurtosis</code><code class="p">:</code>       <code class="mf">53.262</code>               <code class="n">Cond</code><code class="o">.</code> <code class="n">No</code><code class="o">.</code>               <code class="mf">1.25</code>&#13;
<code class="o">======================================================================</code>&#13;
<code class="n">Warnings</code><code class="p">:</code>&#13;
<code class="p">[</code><code class="mi">1</code><code class="p">]</code> <code class="n">Standard</code> <code class="n">Errors</code> <code class="n">assume</code> <code class="n">that</code> <code class="n">the</code> <code class="n">covariance</code> <code class="n">matrix</code> <code class="n">of</code> <code class="n">the</code> <code class="n">errors</code> &#13;
<code class="ow">is</code> <code class="n">correctly</code> <code class="n">specified</code><code class="o">.</code>&#13;
</pre>&#13;
<p>All the diagnostic test results make<a contenteditable="false" data-primary="capital asset pricing model (CAPM)" data-secondary="key assumptions wrecked" data-type="indexterm" id="id1010"/><a contenteditable="false" data-primary="dangers of conventional statistical methods" data-secondary="confidence intervals" data-tertiary="capital asset pricing model assumptions wrecked" data-type="indexterm" id="id1011"/><a contenteditable="false" data-primary="confidence intervals (CIs)" data-secondary="key assumptions wrecked" data-tertiary="capital asset pricing model" data-type="indexterm" id="id1012"/> it clear that the equity market has savaged the “Nobel-prize-winning” CAPM (and related MM) theory. Even with a sample size that includes a quarter of a century of daily closing prices, the distribution of our model’s residuals is grossly more non-normal than before. It is now very negatively skewed with an absurdly high kurtosis—almost 18 times that of a standard normal distribution. Most notably, the CI of our 25-year beta is [1.075, 1.166], which is outside the range of the CI of our 5-year beta [1.185,1.292]. In fact, the beta of AAPL seems to be regressing toward 1, the beta value of the S&amp;P 500.</p>&#13;
&#13;
<p>Invoking some version of the CLT and claiming asymptotic normality for the <em>sampling distributions</em> of the residuals or the coefficients of our regression model seem futile, if not invalid. There is a compelling body of economic research claiming that the underlying distributions of all financial asset price returns do not have finite variances. Financial analysts should not be so certain that they can summon the powers of the CLT and assert asymptotic normality in their CI computations. Furthermore, they need to be sure that convergence to asymptotic normality is reasonably fast because, <a contenteditable="false" data-primary="Keynes, Maynard" data-type="indexterm" id="id1013"/>as the eminent economist Maynard Keynes found out the hard way with his personal equity investments, “The market can stay irrational longer than you can stay solvent.”<sup><a data-type="noteref" href="ch04.html#ch04fn15" id="ch04fn15-marker">15</a></sup> For an equity trade, a quarter of a century is an eternity.<a contenteditable="false" data-primary="" data-startref="ch04-samp" data-type="indexterm" id="id1014"/><a contenteditable="false" data-primary="" data-startref="ch04-samp2" data-type="indexterm" id="id1015"/><a contenteditable="false" data-primary="" data-startref="ch04-error" data-type="indexterm" id="id1016"/><a contenteditable="false" data-primary="" data-startref="ch04-error2" data-type="indexterm" id="id1017"/></p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="conclusions-id00002">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>Because of the errors detailed in this chapter with NHST, p-values, and CIs, I have no confidence in them (or the CAPM) and do not use them in my financial data analyses. I would not waste a penny trading or investing based on the estimated CIs of alpha and beta of any frequentist MM computed by Statsmodels or any other software application. I would also throw any social or economic study that uses NHST, p-values, or confidence intervals in the trash, where junk belongs and should not be recycled.</p>&#13;
&#13;
<p class="pagebreak-before">Statistical hypothesis testing developed by Neyman and Pearson only makes sense as a predata decision theory for mechanical processes like industrial quality control. The mish-mash of the competing statistical theories of Fisher and Neyman was created by nonstatisticians (or incompetent statisticians) to please two bitter rivals, and they ended up creating a nonsensical, confusing blend of the two. Of course, this has not stopped data scientists from using NHST, p-values, and CIs blindly or academics from teaching it as a mathematically rigorous postdata theory of statistical inference.</p>&#13;
&#13;
<p>CIs are not designed for making postdata inferences about population parameters from a single experiment. The use of CIs as a postdata theory is epistemologically flawed. It flagrantly violates the frail philosophical foundation of frequentist probability on which it rests. Yet, orthodox statisticians have concocted a mind-bending, spurious rationale for doing exactly that. You might get away with misusing Neyman’s CI theory if the CLT applies to your data analysis—i.e., the underlying population distribution has a finite mean and variance resulting in asymptotic normality of its sampling distributions.</p>&#13;
&#13;
<p>However, it is common knowledge among academics and practitioners that price returns of all financial assets are not normally distributed. It is likely that these fat tails are a consequence of infinite variances of their underlying population distributions. So the theoretical powers of the CLT cannot be utilized by analysts to rescue CIs from the non-normal, fat-tailed, ugly realities of financial markets. Even if asymptotic normality is theoretically possible in some situations, the desired convergence may not be quick enough for it to be of any practical value for trading and investing. Financial analysts should heed another of Keynes’s warnings when hoping for asymptotic normality of their sampling distributions: “In the long run we are all dead.”<sup><a data-type="noteref" href="ch04.html#ch04fn16" id="ch04fn16-marker">16</a></sup> And almost surely broke.</p>&#13;
&#13;
<p>Regardless, financial data analysts using CIs as a postdata theory are making invalid inferences and grossly misestimating the uncertainties in their point estimates. Unorthodox statistical thinking, ground-breaking numerical algorithms, and modern computing technology make the use of “worse than useless” NHST, p-values, and CI theory in financial data analysis unnecessary. The second half of this book is dedicated to exploring and applying epistemic inference and probabilistic machine learning to finance and investing.</p>&#13;
</div></section>&#13;
&#13;
<section class="pagebreak-before" data-pdf-bookmark="References" data-type="sect1"><div class="sect1" id="references-id00015">&#13;
<h1>References</h1>&#13;
&#13;
<p>Aldrich, John. “R. A. Fisher on Bayes and Bayes’ Theorem.” <em>International Society for Bayesian Analysis</em> 3, no. 1 (2008): 161–70.</p>&#13;
&#13;
<p>Colquhoun, David. “An Investigation of the False Discovery Rate and the Misinterpretation of p-values.” <em>Royal Society Open Science</em> 1, no. 3 (November 2014). <a href="http://doi.org/10.1098/rsos.140216"><em class="hyperlink">http://doi.org/10.1098/rsos.140216</em></a>.</p>&#13;
&#13;
<p>Gigerenzer, Gerd. “Statistical Rituals: The Replication Delusion and How We Got There.” <em>Advances in Methods and Practices in Psychological Science</em> (June 2018): 198–218. <a href="https://doi.org/10.1177/2515245918771329"><em class="hyperlink">https://doi.org/10.1177/2515245918771329</em></a>.</p>&#13;
&#13;
<p>Harvey, Campbell R., Yan Liu, and Heqing Zhu. “…And the Cross-Section of Expected Returns.” <em>The Review of Financial Studies</em> 29, no. 1 (January 2016): 5–68. <a href="https://www.jstor.org/stable/43866011"><em class="hyperlink">https://www.jstor.org/stable/43866011</em></a>.</p>&#13;
&#13;
<p>Ioannidis, John P. A. “Why Most Published Research Findings Are False.” <em>PLOS Medicine</em> 2, no. 8 (2005), e124. <a href="https://doi.org/10.1371/journal.pmed.0020124"><em class="hyperlink">https://doi.org/10.1371/journal.pmed.0020124</em></a>.</p>&#13;
&#13;
<p>Lambdin, Charles. “Significance Tests as Sorcery: Science Is Empirical—Significance Tests Are Not.” <em>Theory &amp; Psychology</em> 22, no. 1 (2012): 67–90. <a href="https://doi.org/10.1177/0959354311429854"><em class="hyperlink">https://doi.org/10.1177/0959354311429854</em></a>.</p>&#13;
&#13;
<p>Lenhard, Johannes. “Models and Statistical Inference: The Controversy Between Fisher and Neyman-Pearson.” <em>The British Journal for the Philosophy of Science</em> 57, no. 1 (2006): 69–91. <a href="http://www.jstor.org/stable/3541653"><em class="hyperlink">http://www.jstor.org/stable/3541653</em></a>.</p>&#13;
&#13;
<p>Louçã, Francisco. “Emancipation Through Interaction—How Eugenics and Statistics Converged and Diverged.” <em>Journal of the History of Biology</em> 42, no. 4 (2009): 649–684. <a href="http://www.jstor.org/stable/25650625"><em class="hyperlink">http://www.jstor.org/stable/25650625</em></a>.</p>&#13;
&#13;
<p>Morey, R. D., R. Hoekstra, J. N. Rouder, M. D. Lee, and E. J. Wagenmakers. “The Fallacy of Placing Confidence in Confidence Intervals.” <em>Psychonomic Bulletin &amp; Review</em> 23, no. 1 (2016): 103–123. <a href="https://doi.org/10.3758/s13423-015-0947-8"><em class="hyperlink">https://doi.org/10.3758/s13423-015-0947-8</em></a>.</p>&#13;
&#13;
<p>Szucs, Dénes, and John P. A. Ioannidis. “When Null Hypothesis Significance Testing Is Unsuitable for Research: A Reassessment.” <em>Frontiers in Human Neuroscience</em> 11, no. 390 (August 2017). doi: 10.3389/fnhum.2017.00390.</p>&#13;
&#13;
<p>Thompson, W. C., and E. L. Schumann. “Interpretation of Statistical Evidence in Criminal Trials: The Prosecutor’s Fallacy and the Defense Attorney’s Fallacy.” <em>Law and Human Behavior</em> 11, no. 3 (1987): 167–187. <a href="http://www.jstor.org/stable/1393631"><em class="hyperlink">http://www.jstor.org/stable/1393631</em></a>.</p>&#13;
</div></section>&#13;
&#13;
<section class="pagebreak-before" data-pdf-bookmark="Further Reading" data-type="sect1"><div class="sect1" id="further_reading-id00007">&#13;
<h1>Further Reading</h1>&#13;
&#13;
<p>Jaynes, E. T. <em>Probability Theory: The Logic of Science</em>. Edited by G. Larry Bretthorst. New York: Cambridge University Press, 2003.</p>&#13;
&#13;
<p>McElreath, Richard. <em>Statistical Rethinking: A Bayesian Course with Examples in R and Stan</em>. Boca Raton, FL: Chapman and Hall/CRC, 2016.</p>&#13;
&#13;
<p>Leamer, Edward E. “Let’s Take the Con Out of Econometrics,” <em>The American Economic Review</em> 73, No. 1 (March 1983): 31-43 </p>&#13;
</div></section>&#13;
<div data-type="footnotes"><p data-type="footnote" id="ch04fn1"><sup><a href="ch04.html#ch04fn1-marker">1</a></sup> John P. A. Ioannidis, “Why Most Published Research Findings Are False,” <em>PLOS Medicine</em> 2, no. 8 (2005), e124, <a href="https://doi.org/10.1371/journal.pmed.0020124"><em class="hyperlink">https://doi.org/10.1371/journal.pmed.0020124</em></a>; Campbell R. Harvey, Yan Liu, and Heqing Zhu, “…And the Cross-Section of Expected Returns,” <em>The Review of Financial Studies</em> 29, no. 1 (January 2016): 5–68, <a href="https://www.jstor.org/stable/43866011"><em class="hyperlink">https://www.jstor.org/stable/43866011</em></a>.</p><p data-type="footnote" id="ch04fn2"><sup><a href="ch04.html#ch04fn2-marker">2</a></sup> David Colquhoun, “An Investigation of the False Discovery Rate and the Misinterpretation of p-values,” <em>Royal Society Open Science</em> (November 2014), <a href="http://doi.org/10.1098/rsos.140216"><em class="hyperlink">http://doi.org/10.1098/rsos.140216</em></a>; Charles Lambdin, “Significance Tests as Sorcery: Science Is Empirical—Significance Tests Are Not,” <em>Theory &amp; Psychology</em> 22, no. 1 (2012): 67–90, <a href="https://doi.org/10.1177/0959354311429854"><em class="hyperlink">https://doi.org/10.1177/0959354311429854</em></a>.</p><p data-type="footnote" id="ch04fn3"><sup><a href="ch04.html#ch04fn3-marker">3</a></sup> R. D. Morey, R. Hoekstra, J. N. Rouder, M. D. Lee, and E. J. Wagenmakers, “The Fallacy of Placing Confidence in Confidence Intervals,” <em>Psychonomic Bulletin &amp; Review</em> 23, no. 1 (2016): 103–123, <a href="https://doi.org/10.3758/s13423-015-0947-8"><em class="hyperlink">https://doi.org/10.3758/s13423-015-0947-8</em></a>.</p><p data-type="footnote" id="ch04fn4"><sup><a href="ch04.html#ch04fn4-marker">4</a></sup> W. C. Thompson and E. L. Schumann, “Interpretation of Statistical Evidence in Criminal Trials: The Prosecutor’s Fallacy and the Defense Attorney’s Fallacy,” <em>Law and Human Behavior</em> 11, no. 3 (1987): 167–187, <a href="http://www.jstor.org/stable/1393631"><em class="hyperlink">http://www.jstor.org/stable/1393631</em></a>.</p><p data-type="footnote" id="ch04fn5"><sup><a href="ch04.html#ch04fn5-marker">5</a></sup> Adapted from an image on Wikimedia Commons.</p><p data-type="footnote" id="ch04fn6"><sup><a href="ch04.html#ch04fn6-marker">6</a></sup> Quoted in John Aldrich, “R. A. Fisher on Bayes and Bayes’ Theorem,” <em>International Society for Bayesian Analysis</em> 3, no. 1 (2008): 163.</p><p data-type="footnote" id="ch04fn7"><sup><a href="ch04.html#ch04fn7-marker">7</a></sup> Francisco Louçã, “Emancipation Through Interaction—How Eugenics and Statistics Converged and Diverged,” <em>Journal of the History of Biology</em> 42, no. 4 (2009): 649–684, <a href="http://www.jstor.org/stable/25650625"><em class="hyperlink">http://www.jstor.org/stable/25650625</em></a>.</p><p data-type="footnote" id="ch04fn8"><sup><a href="ch04.html#ch04fn8-marker">8</a></sup> Adapted from an image on Wikimedia Commons.</p><p data-type="footnote" id="ch04fn9"><sup><a href="ch04.html#ch04fn9-marker">9</a></sup> Johannes Lenhard, “Models and Statistical Inference: The Controversy Between Fisher and Neyman-Pearson,” <em>The British Journal for the Philosophy of Science</em> 57, no. 1 (2006): 69–91, <a href="http://www.jstor.org/stable/3541653"><em class="hyperlink">http://www.jstor.org/stable/3541653</em></a>.</p><p data-type="footnote" id="ch04fn10"><sup><a href="ch04.html#ch04fn10-marker">10</a></sup> Dénes Szucs and John P. A. Ioannidis, “When Null Hypothesis Significance Testing Is Unsuitable for Research: A Reassessment,” <em>Frontiers in Human Neuroscience</em> 11, no. 390 (August 2017), doi: 10.3389/fnhum.2017.00390.</p><p data-type="footnote" id="ch04fn11"><sup><a href="ch04.html#ch04fn11-marker">11</a></sup> Aldrich, “R. A. Fisher on Bayes and Bayes’ Theorem,” 163.</p><p data-type="footnote" id="ch04fn12"><sup><a href="ch04.html#ch04fn12-marker">12</a></sup> Gerd Gigerenzer, “Statistical Rituals: The Replication Delusion and How We Got There,” <em>Advances in Methods and Practices in Psychological Science</em> (June 2018): 198–218, <a href="https://doi.org/10.1177/2515245918771329"><em class="hyperlink">https://doi.org/10.1177/2515245918771329</em></a>.</p><p data-type="footnote" id="ch04fn13"><sup><a href="ch04.html#ch04fn13-marker">13</a></sup> Adapted from an image on Wikimedia Commons.</p><p data-type="footnote" id="ch04fn14"><sup><a href="ch04.html#ch04fn14-marker">14</a></sup> Adapted from an image on Wikimedia Commons.</p><p data-type="footnote" id="ch04fn15"><sup><a href="ch04.html#ch04fn15-marker">15</a></sup> “Keynes the Speculator,” John Maynard Keynes, accessed June 23, 2023, <a href="https://www.maynardkeynes.org/keynes-the-speculator.html"><em class="hyperlink">https://www.maynardkeynes.org/keynes-the-speculator.html</em></a>.</p><p data-type="footnote" id="ch04fn16"><sup><a href="ch04.html#ch04fn16-marker">16</a></sup> Paul Lay, “Keynes in the Long Run,” History Today, accessed June 23, 2023, <a href="https://www.historytoday.com/keynes-long-run"><em class="hyperlink">https://www.historytoday.com/keynes-long-run</em></a>.</p></div></div></section></body></html>