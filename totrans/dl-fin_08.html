<html><head></head><body><section data-pdf-bookmark="Chapter 8. Deep Learning for Time Series Prediction I" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch08">&#13;
<h1><span class="label">Chapter 8. </span>Deep Learning for Time Series Prediction I</h1>&#13;
&#13;
<p><a contenteditable="false" data-primary="time series prediction (deep learning algorithms for)" data-type="indexterm" id="Chapter_8.html0"/><em>Deep learning</em> <a contenteditable="false" data-primary="deep learning (generally)" data-secondary="machine learning versus" data-type="indexterm" id="id681"/><a contenteditable="false" data-primary="machine learning, deep learning versus" data-type="indexterm" id="id682"/>is a slightly more complex and more detailed field than machine learning. Machine learning and deep learning both fall under the umbrella of data science. As you will see, deep learning is mostly about neural networks, a highly sophisticated and powerful algorithm that has enjoyed a lot of coverage and hype, and for good reason: it is very powerful and able to catch highly complex nonlinear relationships between different variables.</p>&#13;
&#13;
<p>The aim of this chapter is to explain the functioning of neural networks before using them to predict financial time series in Python, just like you saw in <a data-type="xref" href="ch07.html#ch07">Chapter 7</a>.</p>&#13;
&#13;
<section data-pdf-bookmark="A Walk Through Neural Networks" data-type="sect1"><div class="sect1" id="id60">&#13;
<h1>A Walk Through Neural Networks</h1>&#13;
&#13;
<p><a contenteditable="false" data-primary="artificial neural networks (ANNs)" data-type="indexterm" id="Chapter_8.html1"/><a contenteditable="false" data-primary="time series prediction (deep learning algorithms for)" data-secondary="neural networks" data-type="indexterm" id="Chapter_8.html2"/><em>Artificial neural networks </em>(ANNs) have their roots in the study of neurology, where researchers sought to comprehend how the human brain and its intricate network of interconnected neurons functioned. ANNs are designed to produce computational representations of biological neural network behavior.</p>&#13;
&#13;
<p>ANNs have been around since the 1940s, when academics first started looking into ways to build computational models based on the human brain. <a contenteditable="false" data-primary="McCulloch, Warren" data-type="indexterm" id="id683"/><a contenteditable="false" data-primary="Pitts, Walter" data-type="indexterm" id="id684"/>Logician Walter Pitts<em> </em>and neurophysiologist Warren McCulloch were among the early pioneers in this subject. They published the idea of a computational model based on simplified artificial neurons in a paper.<sup><a data-type="noteref" href="ch08.html#id685" id="id685-marker">1</a></sup></p>&#13;
&#13;
<p><a contenteditable="false" data-primary="perceptrons" data-primary-seealso="multilayer perceptrons [MLPs]" data-type="indexterm" id="id686"/><a contenteditable="false" data-primary="Rosenblatt, Frank" data-type="indexterm" id="id687"/>The development of artificial neural networks gained further momentum in the 1950s and 1960s when researchers like Frank Rosenblatt worked on the <em>perceptron</em>, a type of artificial neuron that could learn from its inputs. Rosenblatt’s work paved the way for the development of single-layer neural networks capable of pattern <span class="keep-together">recognition</span> tasks.</p>&#13;
&#13;
<p>With the creation of multilayer neural networks, also known as <em>deep neural networks</em>, and the introduction of more potent algorithms, artificial neural networks made significant strides in the 1980s and 1990s. This innovation made it possible for neural networks to learn hierarchical data representations, which enhanced their performance on challenging tasks. <a contenteditable="false" data-primary="Hinton, Geoffrey" data-type="indexterm" id="id688"/>Although multiple researchers contributed to the development and advancement of artificial neural networks, one influential figure is Geoffrey Hinton. Hinton, along with his collaborators, made significant contributions to the field by developing new learning algorithms and architectures for neural networks. His work on deep learning has been instrumental in the recent resurgence and success of artificial neural networks.</p>&#13;
&#13;
<p><a contenteditable="false" data-primary="artificial neural networks (ANNs)" data-secondary="layers" data-type="indexterm" id="id689"/>An ANN consists of interconnected nodes, called artificial neurons, organized into layers. The layers are typically divided into three types:</p>&#13;
&#13;
<dl>&#13;
	<dt>Input layer</dt>&#13;
	<dd><p>The input layer receives input data, which could be numerical, categorical, or even raw sensory data. Input layers are explanatory variables that are supposed to be predictive in nature.</p></dd>&#13;
	<dt>Hidden layers</dt>&#13;
	<dd><p>The hidden layers (one or more) process the input data through their interconnected neurons. Each neuron in a layer receives inputs, performs a computation (discussed later), and passes the output to the next layer.</p></dd>&#13;
	<dt>Output layer</dt>&#13;
	<dd><p>The output layer produces the final result or prediction based on the processed information from the hidden layers. The number of neurons in the output layer depends on the type of problem the network is designed to solve.</p></dd>&#13;
</dl>&#13;
&#13;
<p><a data-type="xref" href="#figure-8-1">Figure 8-1</a> shows an illustration of an artificial neural network where the information flows from left to right. It begins with the two inputs being connected to the four hidden layers where calculation is done before outputting a weighted prediction in the output layer.</p>&#13;
&#13;
<figure><div class="figure" id="figure-8-1"><img alt="" class="iimagesdlf_0801png" src="assets/dlff_0801.png"/>&#13;
<h6><span class="label">Figure 8-1. </span>A simple illustration of an artificial neural network.</h6>&#13;
</div></figure>&#13;
&#13;
<p>Each neuron in the ANN performs two main operations:</p>&#13;
&#13;
<ol>&#13;
	<li>The neuron receives inputs from the previous layer or directly from the input data. Each input is multiplied by a weight value, which represents the strength or importance of that connection. The weighted inputs are then summed together.</li>&#13;
	<li>After the weighted sum, an activation function (discussed in the next section) is applied to introduce nonlinearity into the output of the neuron. The activation function determines the neuron’s output value based on the summed inputs.</li>&#13;
</ol>&#13;
&#13;
<p>During the training process, the ANN adjusts the weights of its connections to improve its performance. This is typically done through an iterative optimization algorithm, such as gradient descent, where the network’s performance is evaluated using a defined loss function. The algorithm computes the gradient of the loss function with respect to the network’s weights, allowing the weights to be updated in a way that minimizes the error.</p>&#13;
&#13;
<p>ANNs have the ability to learn and generalize from data, making them suitable for tasks like pattern recognition and regression. With the advancements in deep learning, ANNs with multiple hidden layers have shown exceptional performance on complex tasks, leveraging their ability to learn hierarchical representations and capture intricate patterns in the data.</p>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>It is worth noting that the process from inputs to outputs is referred to as <em>forward propagation</em>.</p>&#13;
</div>&#13;
&#13;
<section data-pdf-bookmark="Activation Functions" data-type="sect2"><div class="sect2" id="id61">&#13;
<h2>Activation Functions</h2>&#13;
&#13;
<p><a contenteditable="false" data-primary="activation functions" data-primary-seealso="specific functions" data-type="indexterm" id="Chapter_8.html3"/><a contenteditable="false" data-primary="artificial neural networks (ANNs)" data-secondary="activation functions" data-type="indexterm" id="Chapter_8.html4"/><em>Activation functions</em> in neural networks introduce nonlinearity to the output of a neuron, allowing neural networks to model complex relationships and learn from nonlinear data. They determine the output of a neuron based on the weighted sum of its inputs. Let’s discuss these activation functions in detail.</p>&#13;
&#13;
<p><a contenteditable="false" data-primary="sigmoid activation function" data-type="indexterm" id="id690"/>The <em>sigmoid activation function</em> maps the input to a range between 0 and 1, making it suitable for binary classification problems or as a smooth approximation of a step function. The mathematical representation of the function is as follows:</p>&#13;
&#13;
<div data-type="equation"><p><math alttext="upper S left-parenthesis x right-parenthesis equals StartFraction 1 Over 1 plus e Superscript negative x Baseline EndFraction">&#13;
  <mrow>&#13;
    <mi>S</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>x</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <mfrac><mn>1</mn> <mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mi>x</mi></mrow> </msup></mrow></mfrac>&#13;
  </mrow>&#13;
</math></p></div>&#13;
&#13;
&#13;
<p><a data-type="xref" href="#figure-8-2">Figure 8-2</a> shows the sigmoid function.</p>&#13;
&#13;
<figure><div class="figure" id="figure-8-2"><img alt="" class="iimagesdlf_0802png" src="assets/dlff_0802.png"/>&#13;
<h6><span class="label">Figure 8-2. </span>Graph of the sigmoid function.</h6>&#13;
</div></figure>&#13;
&#13;
<p>Among the advantages of the sigmoid activation function are the following:</p>&#13;
&#13;
<ul>&#13;
	<li>It is a smooth as well as differentiable function that facilitates gradient-based <span class="keep-together">optimization</span> algorithms.</li>&#13;
	<li>It squashes the input to a bounded range, which can be interpreted as a <span class="keep-together">probability</span> or confidence level.</li>&#13;
</ul>&#13;
&#13;
<p class="pagebreak-before">However, it has its limitations as well:</p>&#13;
&#13;
<ul>&#13;
	<li><a contenteditable="false" data-primary="vanishing gradient problem" data-type="indexterm" id="id691"/>It suffers from the <em>vanishing gradient problem</em>, where gradients become very small for extreme input values. This can hinder the learning process.</li>&#13;
	<li>Outputs are not zero centered, making it less suitable for certain situations, such as optimizing weights using symmetric update rules like the gradient descent.</li>&#13;
</ul>&#13;
&#13;
<p>The next activation function is the <em>hyperbolic tangent function</em> (tanh), which you saw in <a data-type="xref" href="ch04.html#ch04">Chapter 4</a>. The mathematical representation of the function is as follows:</p>&#13;
&#13;
<div data-type="equation"><p><math alttext="t a n h left-parenthesis x right-parenthesis equals StartFraction e Superscript x Baseline minus e Superscript negative x Baseline Over e Superscript x Baseline plus e Superscript negative x Baseline EndFraction">&#13;
  <mrow>&#13;
    <mi>t</mi>&#13;
    <mi>a</mi>&#13;
    <mi>n</mi>&#13;
    <mi>h</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>x</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <mstyle displaystyle="false" scriptlevel="0">&#13;
      <mfrac><mrow><msup><mi>e</mi> <mi>x</mi> </msup><mo>-</mo><msup><mi>e</mi> <mrow><mo>-</mo><mi>x</mi></mrow> </msup></mrow> <mrow><msup><mi>e</mi> <mi>x</mi> </msup><mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mi>x</mi></mrow> </msup></mrow></mfrac>&#13;
    </mstyle>&#13;
  </mrow>&#13;
</math></p></div>&#13;
&#13;
<p>Among the advantages of the hyperbolic tangent function are the following:</p>&#13;
&#13;
<ul>&#13;
	<li>It is similar to the sigmoid function but is zero centered, which helps alleviate the issue of asymmetric updates in weight optimization.</li>&#13;
	<li>Its nonlinearity can capture a wider range of data variations compared to the <span class="keep-together">sigmoid</span> function.</li>&#13;
</ul>&#13;
&#13;
<p>The following are among its limitations:</p>&#13;
&#13;
<ul>&#13;
	<li>It suffers from the vanishing gradient problem, particularly in deep <span class="keep-together">networks.</span></li>&#13;
	&#13;
		<li class="fix_tracking">Outputs are still susceptible to saturation at the extremes, resulting in gradients close to zero.</li>&#13;
</ul>&#13;
&#13;
&#13;
<p><a data-type="xref" href="#figure-8-3">Figure 8-3</a> shows the hyperbolic tangent function.</p>&#13;
&#13;
<figure><div class="figure" id="figure-8-3"><img alt="" class="iimagesdlf_0803png" src="assets/dlff_0803.png"/>&#13;
<h6><span class="label">Figure 8-3. </span>Graph of the hyperbolic tangent function.</h6>&#13;
</div></figure>&#13;
&#13;
<p><a contenteditable="false" data-primary="ReLU (rectified linear unit) activation function" data-type="indexterm" id="Chapter_8.html5"/><a contenteditable="false" data-primary="rectified linear unit (ReLU) activation function" data-type="indexterm" id="Chapter_8.html5a"/>The next function is called the <em>ReLU activation function</em>. ReLU stands for <em>rectified linear unit</em>. This function sets negative values to zero and keeps the positive values unchanged. It is efficient and helps avoid the vanishing gradient problem. The mathematical representation of the function is as follows:</p>&#13;
&#13;
<div data-type="equation"><p><math alttext="f left-parenthesis x right-parenthesis equals m a x left-parenthesis 0 comma x right-parenthesis">&#13;
  <mrow>&#13;
    <mi>f</mi>&#13;
    <mo>(</mo>&#13;
    <mi>x</mi>&#13;
    <mo>)</mo>&#13;
    <mo>=</mo>&#13;
    <mi>m</mi>&#13;
    <mi>a</mi>&#13;
    <mi>x</mi>&#13;
    <mo>(</mo>&#13;
    <mn>0</mn>&#13;
    <mo>,</mo>&#13;
    <mi>x</mi>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math></p></div>&#13;
&#13;
<p>Among the advantages of the ReLU function are the following:</p>&#13;
&#13;
<ul>&#13;
	<li>It is simple to implement, as it only involves taking the maximum of 0 and the input value. The simplicity of ReLU leads to faster computation and training compared to more complex activation functions.</li>&#13;
	<li class="pagebreak-before less_space">It helps mitigate the vanishing gradient problem that can occur during deep neural network training. The derivative of ReLU is either 0 or 1, which means that the gradients can flow more freely and avoid becoming exponentially small as the network gets deeper.</li>&#13;
</ul>&#13;
&#13;
<p>Among the limitations of the function are the following:</p>&#13;
&#13;
<ul>&#13;
	<li>It outputs 0 for negative input values, which can lead to information loss. In some cases, it may be beneficial to have activation functions that can produce negative outputs as well.</li>&#13;
	<li>It is not a smooth function, because its derivative is discontinuous at 0. This can cause optimization difficulties in certain scenarios.</li>&#13;
</ul>&#13;
&#13;
<p><a data-type="xref" href="#figure-8-4">Figure 8-4</a> shows the ReLU function.</p>&#13;
&#13;
<figure><div class="figure" id="figure-8-4"><img alt="" class="iimagesdlf_0804png" src="assets/dlff_0804.png"/>&#13;
<h6><span class="label">Figure 8-4. </span>Graph of the ReLU function.</h6>&#13;
</div></figure>&#13;
&#13;
<p><a contenteditable="false" data-primary="leaky ReLU (rectified linear unit) activation function" data-type="indexterm" id="Chapter_8.html6"/>The final activation function to discuss is the <em>leaky ReLU activation function</em>. This activation function is an extension of the ReLU function that introduces a small slope for negative inputs. The mathematical representation of the function is as follows:</p>&#13;
&#13;
<div data-type="equation"><p><math alttext="f left-parenthesis x right-parenthesis equals m a x left-parenthesis 0.01 x comma x right-parenthesis">&#13;
  <mrow>&#13;
    <mi>f</mi>&#13;
    <mo>(</mo>&#13;
    <mi>x</mi>&#13;
    <mo>)</mo>&#13;
    <mo>=</mo>&#13;
    <mi>m</mi>&#13;
    <mi>a</mi>&#13;
    <mi>x</mi>&#13;
    <mo>(</mo>&#13;
    <mn>0</mn>&#13;
    <mo lspace="0%" rspace="0%">.</mo>&#13;
    <mn>01</mn>&#13;
    <mi>x</mi>&#13;
    <mo>,</mo>&#13;
    <mi>x</mi>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math></p></div>&#13;
&#13;
<p>Leaky ReLU addresses the dead neuron problem in ReLU and allows some activation for negative inputs, which can help with the flow of gradients during training.</p>&#13;
&#13;
<p>Among the advantages of the leaky ReLU function are the following:</p>&#13;
&#13;
<ul>&#13;
	<li>It overcomes the issue of dead neurons that can occur with ReLU. By introducing a small slope for negative inputs, leaky ReLU ensures that even if a neuron is not activated, it can still contribute to the gradient flow during training.</li>&#13;
	<li>It is a continuous function, even at negative input values. The nonzero slope for negative inputs allows the activation function to have a defined derivative throughout its input range.</li>&#13;
</ul>&#13;
&#13;
<p>The following are among the limitations of the function:</p>&#13;
&#13;
<ul>&#13;
	<li>The slope of the leaky part is a hyperparameter that needs to be set manually. It requires careful tuning to strike a balance between avoiding dead neurons and preventing too much leakage that may hinder the nonlinearity of the activation function.</li>&#13;
	<li>Although leaky ReLU provides a nonzero response for negative inputs, it does not provide the same level of negative activation as some other activation functions, such as the hyperbolic tangent (tanh) and sigmoid. In scenarios where a strong negative activation response is desired, other activation functions might be more suitable.</li>&#13;
</ul>&#13;
&#13;
<p><a data-type="xref" href="#figure-8-5">Figure 8-5</a> shows the leaky ReLU function.</p>&#13;
&#13;
<p>Your choice of activation function depends on the nature of the problem, the architecture of the network, and the desired behavior of the neurons in the network.<a contenteditable="false" data-primary="" data-startref="Chapter_8.html5a" data-type="indexterm" id="id692"/><a contenteditable="false" data-primary="" data-startref="Chapter_8.html5" data-type="indexterm" id="id693"/></p>&#13;
&#13;
<p>Activation functions typically take the weighted sum of inputs to a neuron and apply a nonlinear transformation to it. The transformed value is then passed on as the output of the neuron to the next layer of the network. The specific form and behavior of activation functions can vary, but their overall purpose is to introduce nonlinearities that allow the network to learn complex patterns and relationships in the data.</p>&#13;
&#13;
<figure><div class="figure" id="figure-8-5"><img alt="" class="iimagesdlf_0805png" src="assets/dlff_0805.png"/>&#13;
<h6><span class="label">Figure 8-5. </span>Graph of the leaky ReLU function.</h6>&#13;
</div></figure>&#13;
&#13;
&#13;
&#13;
<p>To sum up, activation functions play a crucial role in ANNs by introducing nonlinearity into the network’s computations. They are applied to the outputs of individual neurons or intermediate layers and help determine whether a neuron should be activated or not based on the input it receives. Without activation functions, the network would only be able to learn linear relationships between the input and output. However, most real-world problems (especially financial time series) involve complex, nonlinear relationships, so activation functions are essential for enabling neural networks to learn and represent such relationships effectively.<a contenteditable="false" data-primary="" data-startref="Chapter_8.html4" data-type="indexterm" id="id694"/><a contenteditable="false" data-primary="" data-startref="Chapter_8.html3" data-type="indexterm" id="id695"/></p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Backpropagation" data-type="sect2"><div class="sect2" id="id62">&#13;
<h2>Backpropagation</h2>&#13;
&#13;
<p><a contenteditable="false" data-primary="artificial neural networks (ANNs)" data-secondary="backpropagation" data-type="indexterm" id="Chapter_8.html7"/><a contenteditable="false" data-primary="backpropagation" data-type="indexterm" id="Chapter_8.html8"/><em>Backpropagation</em> is a fundamental algorithm used to train neural networks. It allows the network to update its weights in a way that minimizes the difference between the predicted output and the desired output.</p>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Backpropagation is a shortened term for <em>backward propagation of errors.</em></p>&#13;
</div>&#13;
&#13;
<p>Training neural networks involves the following steps:</p>&#13;
&#13;
<ol>&#13;
	<li>&#13;
	<p>Randomly initialize the weights and biases of the neural network. This allows you to have a first step when you do not have initial information.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p><a contenteditable="false" data-primary="forward propagation" data-type="indexterm" id="id696"/>Perform <em>forward propagation</em>, a technique to calculate the predicted outputs of the network for a given input. As a reminder, this step involves calculating the weighted sum of inputs for each neuron, applying the activation function to the weighted sum, passing the value to the next layer (if it’s not the last), and continuing the process until reaching the output layer (prediction).</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Compare the predicted output with the actual output (test data) and calculate the loss, which represents the difference between them. The choice of the loss function (e.g., MAE or MSE) depends on the specific problem being solved.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Perform backpropagation to calculate the gradients of the loss with respect to the weights and biases. In this step, the algorithm will start from the output layer (the last layer) and go backward. It will compute the gradient of the loss with respect to the output of each neuron in the current layer. Then it will calculate the gradient of the loss with respect to the weighted sum of inputs for each neuron in the current layer by applying the chain rule. After that, it will compute the gradient of the loss with respect to the weights and biases of each neuron in the current layer using the gradients from the previous steps. These steps are repeated until the gradients are calculated for all layers.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Update the weights and biases of the network by using the calculated gradients and a chosen optimization algorithm run on a specific number of batches of data, which are controlled by the hyperparameter (referred to as the batch size). Updating the weights is done by subtracting the product of the learning rate and the gradient of the weights. Adjusting the biases is done by subtracting the product of the learning rate and the gradient of the biases. Repeat the preceding steps until the weights and biases are updated for all layers.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>The algorithm then repeats steps 2–5 for a specified number of epochs or until a convergence criterion is met. <a contenteditable="false" data-primary="epoch" data-type="indexterm" id="id697"/>An <em>epoch </em>represents one complete pass through the entire training dataset (the whole process entails passing through the training dataset multiple times ideally).</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Once the training is completed, evaluate the performance of the trained neural network on a separate validation or test dataset.</p>&#13;
	</li>&#13;
</ol>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p><a contenteditable="false" data-primary="learning rate hyperparameter" data-type="indexterm" id="id698"/>The <em>learning rate</em> is a hyperparameter that determines the step size at which a neural network’s weights are updated during the training process. It controls how quickly or slowly the model learns from the data it’s being trained on.</p>&#13;
&#13;
<p><a contenteditable="false" data-primary="batch size hyperparameter" data-type="indexterm" id="id699"/>The <em>batch size</em> is a hyperparameter that determines the number of samples processed before updating the model’s weights during each iteration of the training process. In other words, it specifies how many training examples are used at a time to calculate the gradients and update the weights.</p>&#13;
</div>&#13;
&#13;
<p>Choosing an appropriate batch size is essential for efficient training and can impact the convergence speed and memory requirements. There is no one-size-fits-all answer to the ideal batch size, as it depends on various factors, such as the dataset size, available computational resources, and the complexity of the model.</p>&#13;
&#13;
&#13;
<p>Commonly used batch sizes for training MLPs range from small values (such as 16, 32, or 64) to larger ones (such as 128, 256, or even larger). Smaller batch sizes can offer more frequent weight updates and may help the model converge more quickly, especially when the dataset is large or has a lot of variations. However, smaller batch sizes may also introduce more noise and slower convergence due to frequent updates with less accurate gradients. On the other hand, larger batch sizes can provide more stable gradients and better utilization of parallel processing capabilities, leading to faster training on modern hardware. However, they might require more memory, and the updates are less frequent, which could slow down convergence or make the training process less robust.</p>&#13;
&#13;
<p>As a general rule of thumb, you can start with a moderate batch size like 32 and experiment with different values to find the best trade-off between convergence speed and computational efficiency for your specific MLP model and dataset.</p>&#13;
&#13;
&#13;
<p>The backpropagation algorithm leverages the chain rule (refer to <a data-type="xref" href="ch04.html#ch04">Chapter 4</a> for more information on calculus) to calculate the gradients by propagating the errors backward through the network.</p>&#13;
&#13;
<p>By iteratively adjusting the weights based on the error propagated backward through the network, backpropagation enables the network to learn and improve its predictions over time. Backpropagation is a key algorithm in training neural networks and has contributed to significant advancements in various fields.<a contenteditable="false" data-primary="" data-startref="Chapter_8.html8" data-type="indexterm" id="id700"/><a contenteditable="false" data-primary="" data-startref="Chapter_8.html7" data-type="indexterm" id="id701"/></p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Optimization Algorithms" data-type="sect2"><div class="sect2" id="id63">&#13;
<h2>Optimization Algorithms</h2>&#13;
&#13;
<p><a contenteditable="false" data-primary="artificial neural networks (ANNs)" data-secondary="optimization algorithms" data-type="indexterm" id="id702"/><a contenteditable="false" data-primary="optimizers (optimization algorithms)" data-type="indexterm" id="id703"/>In neural networks, optimization algorithms, also known as <em>optimizers</em>, are used to update the parameters (weights and biases) of the network during the training process. These algorithms aim to minimize the loss function and find the optimal values for the parameters that result in the best performance of the network. There are several types of optimizers:</p>&#13;
&#13;
<dl>&#13;
<dt>Gradient descent (GD)</dt> &#13;
<dd><p><a contenteditable="false" data-primary="GD (gradient descent)" data-type="indexterm" id="id704"/><a contenteditable="false" data-primary="gradient descent (GD)" data-type="indexterm" id="id705"/>Gradient descent is the most fundamental optimization algorithm. It updates the network’s weights and biases in the direction opposite to the gradient of the loss function with respect to the parameters. It adjusts the parameters by taking steps proportional to the negative of the gradient, multiplied by a learning rate.</p></dd>&#13;
&#13;
<dt>Stochastic gradient descent (SGD)</dt>&#13;
<dd><p><a contenteditable="false" data-primary="adaptive moment estimation (Adam)" data-type="indexterm" id="id706"/><a contenteditable="false" data-primary="SGD (stochastic gradient descent) algorithm" data-type="indexterm" id="id707"/><a contenteditable="false" data-primary="stochastic gradient descent (SGD) algorithm" data-type="indexterm" id="id708"/>SGD is a variant of gradient descent that randomly selects a single training example or a mini batch of examples to compute the gradient and update the parameters. It provides a computationally efficient approach and introduces noise in the training process, which can help escape local optima.</p></dd>&#13;
&#13;
<dt>Adaptive moment estimation (Adam)</dt>&#13;
<dd><p>Adam is an adaptive optimization algorithm that computes adaptive learning rates for each parameter based on estimates of the first and second moments of the gradients. Adam is widely used due to its effectiveness and efficiency in various applications.</p></dd>&#13;
&#13;
<dt>Root mean square propagation (RMSprop)</dt>&#13;
<dd><p><a contenteditable="false" data-primary="root mean square propagation (RMSprop)" data-type="indexterm" id="id709"/><a contenteditable="false" data-primary="RMSprop (root mean square propagation)" data-type="indexterm" id="id710"/>The purpose of RMSprop is to address some of the limitations of the standard gradient descent algorithm, such as slow convergence and oscillations in different directions. RMSprop adjusts the learning rate for each parameter based on the average of the recent squared gradients. It calculates an exponentially weighted moving average of the squared gradients over time.</p></dd>&#13;
</dl>&#13;
&#13;
<p>Each optimizer has its own characteristics, advantages, and limitations, and their performance can vary depending on the dataset and the network architecture. Experimentation and tuning are often necessary to determine the best optimizer for a specific task.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Regularization Techniques" data-type="sect2"><div class="sect2" id="id64">&#13;
<h2>Regularization Techniques</h2>&#13;
&#13;
<p><a contenteditable="false" data-primary="artificial neural networks (ANNs)" data-secondary="regularization techniques" data-type="indexterm" id="id711"/><a contenteditable="false" data-primary="regularization" data-secondary="techniques" data-type="indexterm" id="id712"/><em>Regularization techniques</em> in neural networks are methods used to prevent overfitting, which can lead to poor performance and reduced ability of the model to make accurate predictions on new examples. Regularization techniques help to control the complexity of a neural network and improve its ability to generalize to unseen data.</p>&#13;
&#13;
<p><a contenteditable="false" data-primary="dropout (regularization technique)" data-type="indexterm" id="id713"/><em>Dropout </em>is a regularization technique commonly used in neural networks to prevent overfitting (refer to <a data-type="xref" href="ch07.html#ch07">Chapter 7</a> for detailed information on overfitting). It involves randomly omitting (dropping) a fraction of the neurons during training by setting their outputs to zero. This temporarily removes the neurons and their corresponding connections from the network, forcing the remaining neurons to learn more robust and independent representations.</p>&#13;
&#13;
<p>The key idea behind dropout is that it acts as a form of model averaging or ensemble learning. By randomly dropping out neurons, the network becomes less reliant on specific neurons or connections and learns more robust features. Dropout also helps prevent co-adaptation, where certain neurons rely heavily on others, reducing their individual learning capability. As a result, dropout can improve the network’s generalization ability and reduce overfitting.</p>&#13;
&#13;
<p><a contenteditable="false" data-primary="early stopping (regularization technique)" data-type="indexterm" id="id714"/><em>Early stopping</em> is a technique that also prevents overfitting by monitoring the model’s performance on a validation set during training. It works by stopping the training process when the model’s performance on the validation set starts to deteriorate. The idea behind early stopping is that as the model continues to train, it may start to overfit the training data, causing a decrease in performance on unseen data.</p>&#13;
&#13;
<p>The training process is typically divided into epochs, where each epoch represents a complete pass over the training data. During training, the model’s performance on the validation set is evaluated after each epoch. If the validation loss or a chosen metric starts to worsen consistently for a certain number of epochs, training is stopped, and the model’s parameters from the epoch with the best performance are used as the final model.</p>&#13;
&#13;
<p>Early stopping helps prevent overfitting by finding the optimal point at which the model has learned the most useful patterns without memorizing noise or irrelevant details from the training data. Both dropout and early stopping are key regularization techniques that help prevent overfitting and help stabilize the model.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Multilayer Perceptrons" data-type="sect2"><div class="sect2" id="id65">&#13;
<h2>Multilayer Perceptrons</h2>&#13;
&#13;
<p><a contenteditable="false" data-primary="artificial neural networks (ANNs)" data-secondary="multilayer perceptrons" data-type="indexterm" id="Chapter_8.html9"/><a contenteditable="false" data-primary="MLPs (multilayer perceptrons)" data-type="indexterm" id="Chapter_8.html10a"/><a contenteditable="false" data-primary="multilayer perceptrons (MLPs)" data-type="indexterm" id="Chapter_8.html10"/>A <em>multilayer perceptron</em> (MLP) is a type of ANN that consists of multiple layers of artificial neurons, or nodes, arranged in a sequential manner. <a contenteditable="false" data-primary="feedforward neural network" data-type="indexterm" id="id715"/>It is a <em>feedforward neural network</em>, meaning that information flows through the network in one direction, from the input layer to the output layer, without any loops or feedback connections (you will learn more about this later in <a data-type="xref" href="#recurrent_neural_networks">“Recurrent Neural Networks”</a>).</p>&#13;
&#13;
<p><a contenteditable="false" data-primary="perceptrons" data-type="indexterm" id="id716"/>The basic building block of an MLP is a <em>perceptron</em>, an artificial neuron that takes multiple inputs, applies weights to those inputs, performs a weighted sum, and passes the result through an activation function to produce an output (basically, the neuron that you have seen already). An MLP contains multiple perceptrons organized in <span class="keep-together">layers.</span> It typically consists of an input layer, one or more hidden layers (the more layers, the deeper the learning process up to a certain point), and an output layer.</p>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>The term <em>perceptron </em>is sometimes used more broadly to refer to a single-layer neural network based on a perceptron-like architecture. In this context, the term <em>perceptron </em>can be used interchangeably with <em>neural network</em> or <em>single-layer perceptron</em>.</p>&#13;
</div>&#13;
&#13;
<p>As a reminder, the input layer receives the raw input data, such as features from a dataset (e.g., the stationary values of a moving average). The hidden layers, which are intermediate layers between the input and output layers, perform complex transformations on the input data. Each neuron in a hidden layer takes inputs from all neurons in the previous layer, applies weights, performs the weighted sum, and passes the result through an activation function. The output layer produces the final output of the network.</p>&#13;
&#13;
<p><a contenteditable="false" data-primary="backpropagation" data-type="indexterm" id="id717"/>MLPs are trained using backpropagation, which adjusts the weights of the neurons in the network to minimize the difference between the predicted output and the desired output. They are known for their ability to learn complex, nonlinear relationships in data, making them suitable for a wide range of tasks, including pattern recognition. <a data-type="xref" href="#figure-8-6">Figure 8-6</a> shows an example of a deep MLP architecture.</p>&#13;
&#13;
<figure><div class="figure" id="figure-8-6"><img alt="" class="iimagesdlf_0806png" src="assets/dlff_0806.png"/>&#13;
<h6><span class="label">Figure 8-6. </span>A simple illustration of an MLP with two hidden layers.</h6>&#13;
</div></figure>&#13;
&#13;
<p>At this stage, you should understand that deep learning is basically neural networks with many hidden layers that add to the complexity of the learning process.</p>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>It is important to download <em>master_function.py</em> from this book’s <a href="https://oreil.ly/5YGHI">GitHub repository</a> to access the functions seen in this book. After downloading it, you must set your Python’s interpreter directory as the path where <em>master_function.py</em> is stored.</p>&#13;
</div>&#13;
&#13;
<p>The aim of this section is to create an MLP to forecast daily S&amp;P 500 returns. Import the required libraries:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">keras.models</code> <code class="kn">import</code> <code class="n">Sequential</code>&#13;
<code class="kn">from</code> <code class="nn">keras.layers</code> <code class="kn">import</code> <code class="n">Dense</code>&#13;
<code class="kn">import</code> <code class="nn">keras</code>&#13;
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>&#13;
<code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="k">as</code> <code class="nn">plt</code>&#13;
<code class="kn">import</code> <code class="nn">pandas_datareader</code> <code class="k">as</code> <code class="nn">pdr</code>&#13;
<code class="kn">from</code> <code class="nn">master_function</code> <code class="kn">import</code> <code class="n">data_preprocessing</code><code class="p">,</code> <code class="n">plot_train_test_values</code>&#13;
<code class="kn">from</code> <code class="nn">master_function</code> <code class="kn">import</code> <code class="n">calculate_accuracy</code><code class="p">,</code> <code class="n">model_bias</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">mean_squared_error</code>&#13;
</pre>&#13;
&#13;
<p>Now import the historical data and transform it:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="c1"># Set the start and end dates for the data</code>&#13;
<code class="n">start_date</code> <code class="o">=</code> <code class="s1">'1990-01-01'</code>&#13;
<code class="n">end_date</code>   <code class="o">=</code> <code class="s1">'2023-06-01'</code>&#13;
<code class="c1"># Fetch S&amp;P 500 price data</code>&#13;
<code class="n">data</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">((</code><code class="n">pdr</code><code class="o">.</code><code class="n">get_data_fred</code><code class="p">(</code><code class="s1">'SP500'</code><code class="p">,</code> <code class="n">start</code> <code class="o">=</code> <code class="n">start_date</code><code class="p">,</code> &#13;
                                   <code class="n">end</code> <code class="o">=</code> <code class="n">end_date</code><code class="p">))</code><code class="o">.</code><code class="n">dropna</code><code class="p">())</code>&#13;
<code class="c1"># Difference the data and make it stationary</code>&#13;
<code class="n">data</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">diff</code><code class="p">(</code><code class="n">data</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">])</code>&#13;
</pre>&#13;
&#13;
<p>Set the hyperparameters for the model:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="n">num_lags</code> <code class="o">=</code> <code class="mi">100</code>&#13;
<code class="n">train_test_split</code> <code class="o">=</code> <code class="mf">0.80</code>&#13;
<code class="n">num_neurons_in_hidden_layers</code> <code class="o">=</code> <code class="mi">20</code>&#13;
<code class="n">num_epochs</code> <code class="o">=</code> <code class="mi">500</code>&#13;
<code class="n">batch_size</code> <code class="o">=</code> <code class="mi">16</code></pre>&#13;
&#13;
<p>Use the data preprocessing function to create the four required arrays:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="c1"># Creating the training and test sets</code>&#13;
<code class="n">x_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">x_test</code><code class="p">,</code> <code class="n">y_test</code> <code class="o">=</code> <code class="n">data_preprocessing</code><code class="p">(</code><code class="n">data</code><code class="p">,</code> <code class="n">num_lags</code><code class="p">,</code> &#13;
                                                      <code class="n">train_test_split</code><code class="p">)</code>&#13;
</pre>&#13;
&#13;
<p>The following code block shows how to build the MLP architecture in <em>keras</em>. Make sure you understand the notes in the code:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="c1"># Designing the architecture of the model</code>&#13;
<code class="n">model</code> <code class="o">=</code> <code class="n">Sequential</code><code class="p">()</code>&#13;
<code class="c1"># First hidden layer with ReLU as activation function</code>&#13;
<code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dense</code><code class="p">(</code><code class="n">num_neurons_in_hidden_layers</code><code class="p">,</code> <code class="n">input_dim</code> <code class="o">=</code> <code class="n">num_lags</code><code class="p">,</code> &#13;
                <code class="n">activation</code> <code class="o">=</code> <code class="s1">'relu'</code><code class="p">))</code>  &#13;
<code class="c1"># Second hidden layer with ReLU as activation function</code>&#13;
<code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dense</code><code class="p">(</code><code class="n">num_neurons_in_hidden_layers</code><code class="p">,</code> <code class="n">activation</code> <code class="o">=</code> <code class="s1">'relu'</code><code class="p">))</code>  &#13;
<code class="c1"># Output layer</code>&#13;
<code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dense</code><code class="p">(</code><code class="mi">1</code><code class="p">))</code>&#13;
<code class="c1"># Compiling</code>&#13;
<code class="n">model</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">loss</code> <code class="o">=</code> <code class="s1">'mean_squared_error'</code><code class="p">,</code> <code class="n">optimizer</code> <code class="o">=</code> <code class="s1">'adam'</code><code class="p">)</code>&#13;
<code class="c1"># Fitting the model</code>&#13;
<code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">x_train</code><code class="p">,</code> <code class="n">np</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">y_train</code><code class="p">,</code> <code class="p">(</code><code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">)),</code> <code class="n">epochs</code> <code class="o">=</code> <code class="n">num_epochs</code><code class="p">,</code> &#13;
          <code class="n">batch_size</code> <code class="o">=</code> <code class="n">batch_size</code><code class="p">)</code>&#13;
<code class="c1"># Predicting in-sample</code>&#13;
<code class="n">y_predicted_train</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">x_train</code><code class="p">),</code> <code class="p">(</code><code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">))</code>&#13;
<code class="c1"># Predicting out-of-sample</code>&#13;
<code class="n">y_predicted</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">x_test</code><code class="p">),</code> <code class="p">(</code><code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">))</code>&#13;
</pre>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>When creating a <code>Dense</code> layer, you need to specify the <code>input_dim</code> parameter in the first layer of your neural network. For subsequent <code>Dense</code> layers, the <code>input_dim</code> is automatically inferred from the previous layer’s output.</p>&#13;
</div>&#13;
&#13;
<p>Let’s plot the results and analyze the performance:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="n">Accuracy</code> <code class="n">Train</code> <code class="o">=</code>  <code class="mf">92.4</code> <code class="o">%</code>&#13;
<code class="n">Accuracy</code> <code class="n">Test</code> <code class="o">=</code>  <code class="mf">54.85</code> <code class="o">%</code>&#13;
<code class="n">RMSE</code> <code class="n">Train</code> <code class="o">=</code>  <code class="mf">4.3602984254</code>&#13;
<code class="n">RMSE</code> <code class="n">Test</code> <code class="o">=</code>  <code class="mf">75.7542774467</code>&#13;
<code class="n">Correlation</code> <code class="n">In</code><code class="o">-</code><code class="n">Sample</code> <code class="n">Predicted</code><code class="o">/</code><code class="n">Train</code> <code class="o">=</code>  <code class="mf">0.989</code>&#13;
<code class="n">Correlation</code> <code class="n">Out</code><code class="o">-</code><code class="n">of</code><code class="o">-</code><code class="n">Sample</code> <code class="n">Predicted</code><code class="o">/</code><code class="n">Test</code> <code class="o">=</code>  <code class="mf">0.044</code>&#13;
<code class="n">Model</code> <code class="n">Bias</code> <code class="o">=</code>  <code class="mf">1.03</code>&#13;
</pre>&#13;
&#13;
<p><a data-type="xref" href="#figure-8-7">Figure 8-7</a> shows the evolution of the forecasting task from the last values of <code>y_train</code> to the first values of <code>y_test</code> and <code>y_predicted</code>.</p>&#13;
&#13;
<figure><div class="figure" id="figure-8-7"><img alt="" class="iimagesdlf_0807png" src="assets/dlff_0807.png"/>&#13;
<h6><span class="label">Figure 8-7. </span>Training data followed by test data (dashed line) and the predicted data (thin line); the vertical dashed line represents the start of the test period. The model used is the MLP regression algorithm.</h6>&#13;
</div></figure>&#13;
&#13;
<p>The results are extremely volatile when changing the hyperparameters. This is why using sophisticated models on complex data requires a lot of tweaks and optimizations. Consider the following improvements to enhance the results of the model:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>Select relevant features (inputs) that capture the underlying patterns and characteristics of the financial time series. This can involve calculating technical indicators (e.g., moving averages and the RSI) or deriving other meaningful variables from the data.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Review the architecture of the model. Consider increasing the number of layers or neurons to provide the model with more capacity to learn complex patterns. Experiment with different activation functions and regularization techniques such as dropout and early stopping (see <a data-type="xref" href="ch09.html#ch09">Chapter 9</a> for an application of regularization techniques).</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Fine-tune the hyperparameters of your MLP model. Parameters like the batch size and the number of epochs can significantly impact the model’s ability to converge and generalize.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Combine multiple MLP models into an ensemble. This can involve training several models with different initializations or using different subsets of the data. Aggregating their predictions can lead to better results than using a single model.</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>As the model trains, the loss function should decrease due to the learning process. This can be seen using the following code (to be run after compiling the model):</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="kn">import</code> <code class="nn">tensorflow</code> <code class="k">as</code> <code class="nn">tf</code>&#13;
<code class="n">losses</code> <code class="o">=</code> <code class="p">[]</code>&#13;
<code class="n">epochs</code> <code class="o">=</code> <code class="p">[]</code>&#13;
<code class="k">class</code> <code class="nc">LossCallback</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">keras</code><code class="o">.</code><code class="n">callbacks</code><code class="o">.</code><code class="n">Callback</code><code class="p">):</code>&#13;
    <code class="k">def</code> <code class="nf">on_epoch_end</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">epoch</code><code class="p">,</code> <code class="n">logs</code> <code class="o">=</code> <code class="kc">None</code><code class="p">):</code>&#13;
        <code class="n">losses</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">logs</code><code class="p">[</code><code class="s1">'loss'</code><code class="p">])</code>&#13;
        <code class="n">epochs</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">epoch</code> <code class="o">+</code> <code class="mi">1</code><code class="p">)</code>&#13;
        <code class="n">plt</code><code class="o">.</code><code class="n">clf</code><code class="p">()</code>&#13;
        <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">epochs</code><code class="p">,</code> <code class="n">losses</code><code class="p">,</code> <code class="n">marker</code> <code class="o">=</code> <code class="s1">'o'</code><code class="p">)</code>&#13;
        <code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="s1">'Loss Curve'</code><code class="p">)</code>&#13;
        <code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s1">'Epoch'</code><code class="p">)</code>&#13;
        <code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s1">'Loss Value'</code><code class="p">)</code>&#13;
        <code class="n">plt</code><code class="o">.</code><code class="n">grid</code><code class="p">(</code><code class="kc">True</code><code class="p">)</code>&#13;
        <code class="n">plt</code><code class="o">.</code><code class="n">pause</code><code class="p">(</code><code class="mf">0.01</code><code class="p">)</code>&#13;
<code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">x_train</code><code class="p">,</code> <code class="n">np</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">y_train</code><code class="p">,</code> <code class="p">(</code><code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">)),</code> <code class="n">epochs</code> <code class="o">=</code> <code class="mi">100</code><code class="p">,</code> &#13;
          <code class="n">verbose</code> <code class="o">=</code> <code class="mi">0</code><code class="p">,</code> <code class="n">callbacks</code> <code class="o">=</code> <code class="p">[</code><code class="n">LossCallback</code><code class="p">()])</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code>&#13;
</pre>&#13;
&#13;
<p>The previous code block plots the loss at the end of every epoch, thus creating a dynamic loss curve visualized in real time. Notice how it falls until reaching a plateau where it struggles to decrease. <a data-type="xref" href="#figure-8-8">Figure 8-8</a> shows the decreasing loss function <span class="keep-together">across epochs</span><a contenteditable="false" data-primary="" data-startref="Chapter_8.html10a" data-type="indexterm" id="id718"/><a contenteditable="false" data-primary="" data-startref="Chapter_8.html10" data-type="indexterm" id="id719"/><a contenteditable="false" data-primary="" data-startref="Chapter_8.html9" data-type="indexterm" id="id720"/>.<a contenteditable="false" data-primary="" data-startref="Chapter_8.html2" data-type="indexterm" id="id721"/><a contenteditable="false" data-primary="" data-startref="Chapter_8.html1" data-type="indexterm" id="id722"/></p>&#13;
&#13;
<figure><div class="figure" id="figure-8-8"><img alt="" class="iimagesdlf_0808png" src="assets/dlff_0808.png"/>&#13;
<h6><span class="label">Figure 8-8. </span>Loss value across epochs.</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Recurrent Neural Networks" data-type="sect1"><div class="sect1" id="recurrent_neural_networks">&#13;
<h1>Recurrent Neural Networks</h1>&#13;
&#13;
<p><a contenteditable="false" data-primary="recurrent neural networks (RNNs)" data-type="indexterm" id="Chapter_8.html11"/><a contenteditable="false" data-primary="RNNs (recurrent neural networks)" data-type="indexterm" id="Chapter_8.html11a"/><a contenteditable="false" data-primary="time series prediction (deep learning algorithms for)" data-secondary="recurrent neural networks" data-type="indexterm" id="Chapter_8.html12"/>A <em>recurrent neural network</em> (RNN) is a type of artificial neural network that is designed to process sequential data or data with temporal dependencies. Unlike feedforward neural networks, which process data in a single pass from input to output, RNNs maintain internal memory or hidden states to capture information from previous inputs and utilize it in the processing of subsequent inputs.</p>&#13;
&#13;
<p>The key feature of an RNN is the presence of <em>recurrent connections</em>, which create a loop in the network. This loop allows the network to persist information across time steps, making it well suited for tasks that involve sequential or time-dependent data.</p>&#13;
&#13;
<p>At each time step, an RNN takes an input vector and combines it with the previous hidden state. It then applies activation functions to compute the new hidden state and produces an output. This process is repeated for each time step, with the hidden state being updated and passed along as information flows through the network.</p>&#13;
&#13;
<p>The recurrent connections enable RNNs to capture dependencies and patterns in sequential data. They can model the context and temporal dynamics of the data, making them useful in time series prediction.</p>&#13;
&#13;
<p><a contenteditable="false" data-primary="vanishing gradient problem" data-type="indexterm" id="id723"/>However, traditional RNNs suffer from the vanishing gradient problem, where the gradients that are backpropagated through the recurrent connections can become very small or very large, leading to difficulties in training the network. The vanishing gradient problem is resolved in the next section with an enhanced type of neural network. For now, let’s focus on RNNs and their specificities.</p>&#13;
&#13;
<p><a data-type="xref" href="#figure-8-9">Figure 8-9</a> shows an example of an RNN architecture.</p>&#13;
&#13;
<figure><div class="figure" id="figure-8-9"><img alt="" class="iimagesrnnpng" src="assets/dlff_0809.png"/>&#13;
<h6><span class="label">Figure 8-9. </span>A simple illustration of an RNN with two hidden layers.</h6>&#13;
</div></figure>&#13;
&#13;
<p>Let’s deploy an RNN algorithm to forecast S&amp;P 500 daily returns. As usual, import the required libraries:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">keras.models</code> <code class="kn">import</code> <code class="n">Sequential</code>&#13;
<code class="kn">from</code> <code class="nn">keras.layers</code> <code class="kn">import</code> <code class="n">Dense</code><code class="p">,</code> <code class="n">SimpleRNN</code>&#13;
<code class="kn">import</code> <code class="nn">keras</code>&#13;
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>&#13;
<code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="k">as</code> <code class="nn">plt</code>&#13;
<code class="kn">import</code> <code class="nn">pandas_datareader</code> <code class="k">as</code> <code class="nn">pdr</code>&#13;
<code class="kn">from</code> <code class="nn">master_function</code> <code class="kn">import</code> <code class="n">data_preprocessing</code><code class="p">,</code> <code class="n">plot_train_test_values</code>&#13;
<code class="kn">from</code> <code class="nn">master_function</code> <code class="kn">import</code> <code class="n">calculate_accuracy</code><code class="p">,</code> <code class="n">model_bias</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">mean_squared_error</code>&#13;
</pre>&#13;
&#13;
<p>Now set the hyperparameters of the model:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="n">num_lags</code> <code class="o">=</code> <code class="mi">100</code>&#13;
<code class="n">train_test_split</code> <code class="o">=</code> <code class="mf">0.80</code>&#13;
<code class="n">num_neurons_in_hidden_layers</code> <code class="o">=</code> <code class="mi">20</code>&#13;
<code class="n">num_epochs</code> <code class="o">=</code> <code class="mi">500</code>&#13;
<code class="n">batch_size</code> <code class="o">=</code> <code class="mi">16</code>&#13;
</pre>&#13;
&#13;
<p>The following code block shows how to build the RNN architecture in <em>keras</em>:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="c1"># Designing the architecture of the model</code>&#13;
<code class="n">model</code> <code class="o">=</code> <code class="n">Sequential</code><code class="p">()</code>&#13;
<code class="c1"># First hidden layer</code>&#13;
<code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dense</code><code class="p">(</code><code class="n">num_neurons_in_hidden_layers</code><code class="p">,</code> <code class="n">input_dim</code> <code class="o">=</code> <code class="n">num_lags</code><code class="p">,</code> &#13;
                <code class="n">activation</code> <code class="o">=</code> <code class="s1">'relu'</code><code class="p">))</code>  &#13;
<code class="c1"># Second hidden layer</code>&#13;
<code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dense</code><code class="p">(</code><code class="n">num_neurons_in_hidden_layers</code><code class="p">,</code> <code class="n">activation</code> <code class="o">=</code> <code class="s1">'relu'</code><code class="p">))</code>  &#13;
<code class="c1"># Output layer</code>&#13;
<code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dense</code><code class="p">(</code><code class="mi">1</code><code class="p">))</code>&#13;
<code class="c1"># Compiling</code>&#13;
<code class="n">model</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">loss</code> <code class="o">=</code> <code class="s1">'mean_squared_error'</code><code class="p">,</code> <code class="n">optimizer</code> <code class="o">=</code> <code class="s1">'adam'</code><code class="p">)</code>&#13;
<code class="c1"># Fitting the model</code>&#13;
<code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">x_train</code><code class="p">,</code> <code class="n">np</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">y_train</code><code class="p">,</code> <code class="p">(</code><code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">)),</code> <code class="n">epochs</code> <code class="o">=</code> <code class="n">num_epochs</code><code class="p">,</code> &#13;
          <code class="n">batch_size</code> <code class="o">=</code> <code class="n">batch_size</code><code class="p">)</code>&#13;
<code class="c1"># Predicting in-sample</code>&#13;
<code class="n">y_predicted_train</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">x_train</code><code class="p">),</code> <code class="p">(</code><code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">))</code>&#13;
<code class="c1"># Predicting out-of-sample</code>&#13;
<code class="n">y_predicted</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">x_test</code><code class="p">),</code> <code class="p">(</code><code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">))</code>&#13;
</pre>&#13;
&#13;
<p>Let’s plot the results and analyze the performance:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="n">Accuracy</code> <code class="n">Train</code> <code class="o">=</code>  <code class="mf">67.16</code> <code class="o">%</code>&#13;
<code class="n">Accuracy</code> <code class="n">Test</code> <code class="o">=</code>  <code class="mf">52.11</code> <code class="o">%</code>&#13;
<code class="n">RMSE</code> <code class="n">Train</code> <code class="o">=</code>  <code class="mf">22.7704952044</code>&#13;
<code class="n">RMSE</code> <code class="n">Test</code> <code class="o">=</code>  <code class="mf">60.3443059267</code>&#13;
<code class="n">Correlation</code> <code class="n">In</code><code class="o">-</code><code class="n">Sample</code> <code class="n">Predicted</code><code class="o">/</code><code class="n">Train</code> <code class="o">=</code>  <code class="mf">0.642</code>&#13;
<code class="n">Correlation</code> <code class="n">Out</code><code class="o">-</code><code class="n">of</code><code class="o">-</code><code class="n">Sample</code> <code class="n">Predicted</code><code class="o">/</code><code class="n">Test</code> <code class="o">=</code>  <code class="err">–</code><code class="mf">0.022</code>&#13;
<code class="n">Model</code> <code class="n">Bias</code> <code class="o">=</code>  <code class="mf">2.18</code>&#13;
</pre>&#13;
&#13;
<p><a data-type="xref" href="#figure-8-10">Figure 8-10</a> shows the evolution of the forecasting task from the last values of <code>y_train</code> to the first values of <code>y_test</code> and <code>y_predicted</code>.</p>&#13;
&#13;
<figure><div class="figure" id="figure-8-10"><img alt="" class="iimagesdlf_0810png" src="assets/dlff_0810.png"/>&#13;
<h6><span class="label">Figure 8-10. </span>Training data followed by test data (dashed line) and the predicted data (thin line); the vertical dashed line represents the start of the test period. The model used is the RNN regression algorithm.</h6>&#13;
</div></figure>&#13;
&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p>A good task for you to do is to create an optimization function that loops around different hyperparameters and selects the best ones or averages the best ones. This way, you may be able to obtain a robust model based on the ensembling technique. You can also backtest different markets and different time horizons. Note that these techniques are valid not only for financial time series, but for all types of time series.</p>&#13;
</div>&#13;
&#13;
<p>In summary, RNNs are neural networks that can process sequential data by maintaining internal memory and capturing temporal dependencies. They are powerful models for tasks involving time series or sequential data. As a reminder, stationarity is an essential property for successful time series forecasting. A stationary time series exhibits constant mean, variance, and autocovariance over time. RNNs (among other deep learning models) assume that the underlying time series is stationary, which means the statistical properties of the data do not change over time. If the time series is nonstationary, it may contain trends, seasonality, or other patterns that can affect the performance of RNNs. The optimization and enhancement recommendations on MLPs are also valid on RNNs.<a contenteditable="false" data-primary="" data-startref="Chapter_8.html12" data-type="indexterm" id="id724"/><a contenteditable="false" data-primary="" data-startref="Chapter_8.html11" data-type="indexterm" id="id725"/><a contenteditable="false" data-primary="" data-startref="Chapter_8.html11a" data-type="indexterm" id="id726"/></p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Long Short-Term Memory" data-type="sect1"><div class="sect1" id="id67">&#13;
<h1>Long Short-Term Memory</h1>&#13;
&#13;
<p><a contenteditable="false" data-primary="long short-term memory (LSTM)" data-type="indexterm" id="Chapter_8.html13"/><a contenteditable="false" data-primary="time series prediction (deep learning algorithms for)" data-secondary="long short-term memory" data-type="indexterm" id="Chapter_8.html15"/><em>Long short-term memory</em> (LSTM) is a type of RNN that addresses the vanishing gradient problem and allows the network to capture long-term dependencies in sequential data. LSTMs were introduced by Hochreiter and Schmidhuber in 1997.</p>&#13;
&#13;
<p>LSTMs are designed to overcome the limitations of traditional RNNs when dealing with long sequences of data. They achieve this by incorporating specialized memory cells that can retain information over extended time periods. The key idea behind LSTMs is the use of a gating mechanism that controls the flow of information through the memory cells.</p>&#13;
&#13;
<p>The LSTM architecture consists of memory cells, input gates, forget gates, and output gates. The memory cells store and update information at each time step, while the gates regulate the flow of information. Here’s how LSTMs work:</p>&#13;
&#13;
<dl>&#13;
 <dt>Input gate</dt>&#13;
 <dd><p>The input gate determines which information from the current time step should be stored in the memory cell. It takes the current input and the previous hidden state as inputs, and then it applies a sigmoid activation function to generate a value between 0 and 1 for each component of the memory cell.</p></dd>&#13;
 &#13;
 <dt>Forget gate</dt>&#13;
 <dd><p>The forget gate determines which information from the previous memory cell should be forgotten. It takes the current input and the previous hidden state as inputs, and then it applies a sigmoid activation function to produce a forget vector. This vector is then multiplied element-wise with the previous memory cell values, allowing the LSTM to forget irrelevant information.</p></dd>&#13;
 &#13;
 <dt class="pagebreak-before less_space">Update</dt>&#13;
 <dd><p>The update step combines the information from the input gate and the forget gate. It takes the current input and the previous hidden state as inputs, and then it applies a tanh activation function. The resulting vector is then multiplied element-wise with the input gate output, and the product is added to the product of the forget gate and the previous memory cell values. This update operation determines which new information to store in the memory cell.</p></dd>&#13;
 &#13;
 <dt>Output gate</dt>&#13;
 <dd><p>The output gate determines the output of the LSTM at the current time step. It takes the current input and the previous hidden state as inputs, and then it applies a sigmoid activation function. The updated memory cell values are passed through a hyperbolic tangent (tanh) activation function and then multiplied element-wise with the output gate. The resulting vector becomes the current hidden state and is also the output of the LSTM at that time step.</p></dd>&#13;
</dl>&#13;
&#13;
&#13;
<p>The gating mechanisms in LSTMs allow them to selectively remember or forget information over long sequences, making them well suited for tasks involving long-term dependencies. By addressing the vanishing gradient problem and capturing long-term dependencies, LSTMs have become a popular choice for sequential data processing and have been instrumental in advancing the field of deep learning.</p>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Theoretically, RNNs are capable of learning long-term dependencies, but in practice, they do not, hence the need for LSTMs.</p>&#13;
</div>&#13;
&#13;
<p>As usual, let’s apply LSTMs to the same time series problem. Note, however, that the results do not mean anything since the explanatory variables are arbitrary and the hyperparameters are not tuned. The aim of doing such exercises is to understand the code and the logic behind the algorithm. Afterward, it will be up to you to select the inputs and the variables that you deem worthy to be tested out.</p>&#13;
&#13;
<p>Import the required libraries as follows:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">keras.models</code> <code class="kn">import</code> <code class="n">Sequential</code>&#13;
<code class="kn">from</code> <code class="nn">keras.layers</code> <code class="kn">import</code> <code class="n">Dense</code><code class="p">,</code> <code class="n">LSTM</code>&#13;
<code class="kn">import</code> <code class="nn">keras</code>&#13;
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>&#13;
<code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="k">as</code> <code class="nn">plt</code>&#13;
<code class="kn">import</code> <code class="nn">pandas_datareader</code> <code class="k">as</code> <code class="nn">pdr</code>&#13;
<code class="kn">from</code> <code class="nn">master_function</code> <code class="kn">import</code> <code class="n">data_preprocessing</code><code class="p">,</code> <code class="n">plot_train_test_values</code>&#13;
<code class="kn">from</code> <code class="nn">master_function</code> <code class="kn">import</code> <code class="n">calculate_accuracy</code><code class="p">,</code> <code class="n">model_bias</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">mean_squared_error</code>&#13;
</pre>&#13;
&#13;
<p class="pagebreak-before">Now set the hyperparameters of the model:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="n">num_lags</code> <code class="o">=</code> <code class="mi">100</code>&#13;
<code class="n">train_test_split</code> <code class="o">=</code> <code class="mf">0.80</code>&#13;
<code class="n">num_neurons_in_hidden_layers</code> <code class="o">=</code> <code class="mi">20</code>&#13;
<code class="n">num_epochs</code> <code class="o">=</code> <code class="mi">100</code>&#13;
<code class="n">batch_size</code> <code class="o">=</code> <code class="mi">32</code>&#13;
</pre>&#13;
&#13;
<p>The LSTM model requires three-dimensional arrays of features. This can be done using the following code:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
x_train = x_train.reshape((–1, num_lags, 1))&#13;
x_test = x_test.reshape((–1, num_lags, 1))</pre>&#13;
&#13;
<p>The following code block shows how to build the LSTM architecture in <em>keras</em>:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="c1"># Create the LSTM model</code>&#13;
<code class="n">model</code> <code class="o">=</code> <code class="n">Sequential</code><code class="p">()</code>&#13;
<code class="c1"># First LSTM layer</code>&#13;
<code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">LSTM</code><code class="p">(</code><code class="n">units</code> <code class="o">=</code> <code class="n">num_neurons_in_hidden_layers</code><code class="p">,</code> &#13;
               <code class="n">input_shape</code> <code class="o">=</code> <code class="p">(</code><code class="n">num_lags</code><code class="p">,</code> <code class="mi">1</code><code class="p">)))</code>&#13;
<code class="c1"># Second hidden layer</code>&#13;
<code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dense</code><code class="p">(</code><code class="n">num_neurons_in_hidden_layers</code><code class="p">,</code> <code class="n">activation</code> <code class="o">=</code> <code class="s1">'relu'</code><code class="p">))</code>  &#13;
<code class="c1"># Output layer</code>&#13;
<code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dense</code><code class="p">(</code><code class="n">units</code> <code class="o">=</code> <code class="mi">1</code><code class="p">))</code>&#13;
<code class="c1"># Compile the model</code>&#13;
<code class="n">model</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">loss</code> <code class="o">=</code> <code class="s1">'mean_squared_error'</code><code class="p">,</code> <code class="n">optimizer</code> <code class="o">=</code> <code class="s1">'adam'</code><code class="p">)</code>&#13;
<code class="c1"># Train the model</code>&#13;
<code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">x_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">epochs</code> <code class="o">=</code> <code class="n">num_epochs</code><code class="p">,</code> <code class="n">batch_size</code> <code class="o">=</code> <code class="n">batch_size</code><code class="p">)</code>&#13;
<code class="c1"># Predicting in-sample</code>&#13;
<code class="n">y_predicted_train</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">x_train</code><code class="p">),</code> <code class="p">(</code><code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">))</code>&#13;
<code class="c1"># Predicting out-of-sample</code>&#13;
<code class="n">y_predicted</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">x_test</code><code class="p">),</code> <code class="p">(</code><code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">))</code>&#13;
</pre>&#13;
&#13;
<p>Let’s plot the results and analyze the performance:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="n">Accuracy</code> <code class="n">Train</code> <code class="o">=</code>  <code class="mf">65.63</code> <code class="o">%</code>&#13;
<code class="n">Accuracy</code> <code class="n">Test</code> <code class="o">=</code>  <code class="mf">50.42</code> <code class="o">%</code>&#13;
<code class="n">RMSE</code> <code class="n">Train</code> <code class="o">=</code>  <code class="mf">25.5619843783</code>&#13;
<code class="n">RMSE</code> <code class="n">Test</code> <code class="o">=</code>  <code class="mf">55.1133475721</code>&#13;
<code class="n">Correlation</code> <code class="n">In</code><code class="o">-</code><code class="n">Sample</code> <code class="n">Predicted</code><code class="o">/</code><code class="n">Train</code> <code class="o">=</code>  <code class="mf">0.515</code>&#13;
<code class="n">Correlation</code> <code class="n">Out</code><code class="o">-</code><code class="n">of</code><code class="o">-</code><code class="n">Sample</code> <code class="n">Predicted</code><code class="o">/</code><code class="n">Test</code> <code class="o">=</code>  <code class="mf">0.057</code>&#13;
<code class="n">Model</code> <code class="n">Bias</code> <code class="o">=</code>  <code class="mf">2.56</code>&#13;
</pre>&#13;
&#13;
<p><a data-type="xref" href="#figure-8-11">Figure 8-11</a> shows the evolution of the forecasting task from the last values of <code>y_train</code> to the first values of <code>y_test</code> and <code>y_predicted</code>. Note that the <span class="keep-together">hyperparameters</span> are the same as the ones used in the RNN model.</p>&#13;
&#13;
<figure><div class="figure" id="figure-8-11"><img alt="" class="iimagesdlf_0811png" src="assets/dlff_0811.png"/>&#13;
<h6><span class="label">Figure 8-11. </span>Training data followed by test data (dashed line) and the predicted data (thin line); the vertical dashed line represents the start of the test period. The model used is the LSTM regression algorithm.</h6>&#13;
</div></figure>&#13;
&#13;
<p>It is worth seeing how well the algorithm is fitted to the training data. <a data-type="xref" href="#figure-8-12">Figure 8-12</a> shows the values from <code>y_predicted_train</code> and <code>y_train</code>.</p>&#13;
&#13;
<figure><div class="figure" id="figure-8-12"><img alt="" class="iimagesdlf_0812png" src="assets/dlff_0812.png"/>&#13;
<h6><span class="label">Figure 8-12. </span>In-sample predictions using the LSTM regression algorithm.</h6>&#13;
</div></figure>&#13;
&#13;
<p class="pagebreak-before">In the context of LSTMs, a three-dimensional array represents the shape of the input data that is fed into the models. It is typically used to accommodate sequential or time series data in the form of input sequences. The dimensions of a three-dimensional array have specific meanings:</p>&#13;
&#13;
<dl>&#13;
 <dt>Dimension 1 (samples)</dt>&#13;
 <dd><p>This dimension represents the number of samples or examples in the dataset. Each sample corresponds to a specific sequence or time series instance. For example, if you have 1,000 time series sequences in your dataset, dimension 1 would be 1,000.</p></dd>&#13;
 &#13;
 <dt>Dimension 2 (time steps)</dt>&#13;
 <dd><p>This dimension represents the number of time steps or data points in each sequence. It defines the length of the input sequence that the LSTM or RNN model processes at each time step. For instance, if your input sequences have a length of 10 time steps, dimension 2 would be 10.</p></dd>&#13;
 &#13;
 <dt>Dimension 3 (features)</dt>&#13;
 <dd><p>This dimension represents the number of features or variables associated with each time step in the sequence. It defines the dimensionality of each time step’s data. In the case of univariate time series data, where only a single value is considered at each time step, dimension 3 would typically be 1. For multivariate time series, where multiple variables are observed at each time step, dimension 3 would be greater than 1.</p></dd>&#13;
</dl>&#13;
&#13;
&#13;
<p>Let’s take a quick break and discuss an interesting topic. Using simple linear algorithms to model complex, nonlinear relationships is most likely to give bad results. At the same time, using extremely complex methods such as LSTMs on simple and predictable data may not be necessary even though it may provide positive results. <a data-type="xref" href="#figure-8-13">Figure 8-13</a> shows an ascending time series that looks like it’s oscillating in <span class="keep-together">regular intervals.</span></p>&#13;
&#13;
<figure><div class="figure" id="figure-8-13"><img class="iimagessynthetic_data_chap8png" src="assets/dlff_0813.png"/>&#13;
<h6><span class="label">Figure 8-13. </span>A generated ascending time series with oscillating properties.</h6>&#13;
</div></figure>&#13;
&#13;
<p>Believe it or not, linear regression can actually model this raw time series quite well. By assuming an autoregressive model with 100 features (which means that to predict the next value, the model looks at the last 100 values), the linear regression algorithm can be trained on in-sample data and output the out-of-sample results shown in <a data-type="xref" href="#figure-8-14">Figure 8-14</a>.</p>&#13;
&#13;
<figure><div class="figure" id="figure-8-14"><img class="iimagespred_synthpng" src="assets/dlff_0814.png"/>&#13;
<h6><span class="label">Figure 8-14. </span>Prediction over the ascending time series using linear regression.</h6>&#13;
</div></figure>&#13;
&#13;
<p>But let’s take its first order difference and make it stationary. Take a look at <a data-type="xref" href="#figure-8-15">Figure 8-15</a>, which shows a stationary time series created from differencing the time series shown in <a data-type="xref" href="#figure-8-13">Figure 8-13</a>.</p>&#13;
&#13;
<figure><div class="figure" id="figure-8-15"><img class="iimagessynthetic_data_diff_chap8png" src="assets/dlff_0815.png"/>&#13;
<h6><span class="label">Figure 8-15. </span>A generated ascending time series with oscillating properties (differenced).</h6>&#13;
</div></figure>&#13;
&#13;
<p>The linear regression algorithm can be trained on in-sample data and output the out-of-sample results shown in <a data-type="xref" href="#figure-8-16">Figure 8-16</a> with extreme accuracy.</p>&#13;
&#13;
<figure><div class="figure" id="figure-8-16"><img class="iimagespred_synth_2png" src="assets/dlff_0816.png"/>&#13;
<h6><span class="label">Figure 8-16. </span>Prediction over the differenced time series using linear regression.</h6>&#13;
</div></figure>&#13;
&#13;
<p><a contenteditable="false" data-primary="coefficient of determination (R²)" data-type="indexterm" id="id727"/>Another way of assessing the goodness of fit of a linear regression model is to use R². Also known as the <em>coefficient of determination</em>, <em>R²</em> is a statistical measure that indicates the proportion of the variance in the dependent variable that can be explained by the independent variable(s) in a regression model.</p>&#13;
&#13;
<p>R² ranges from 0 to 1 and is often expressed as a percentage. A value of 0 indicates that the independent variable(s) cannot explain any of the variability in the dependent variable, while a value of 1 indicates that the independent variable(s) can completely explain the variability in the dependent variable.</p>&#13;
&#13;
<p>In simple terms, R² represents the proportion of the dependent variable’s variability that can be attributed to the independent variable(s) included in the model. It provides a measure of how well the regression model fits the observed data. However, it does not indicate the causal relationship between variables or the overall quality of the model. It is also worth noting that R² is the squared correlation between the two variables. The R² metric for the  differenced time series is 0.935, indicating extremely good fit.</p>&#13;
&#13;
<p>In parallel, using an MLP with some optimization also yields good results. <a data-type="xref" href="#figure-8-17">Figure 8-17</a> shows the results of the differenced values when using a simple MLP model (with two hidden layers, each containing 24 neurons and a batch size of 128 run through 50 epochs).</p>&#13;
&#13;
<figure><div class="figure" id="figure-8-17"><img class="iimagespred_synth_diff_mlppng" src="assets/dlff_0817.png"/>&#13;
<h6><span class="label">Figure 8-17. </span>Prediction over the differenced time series using MLP.</h6>&#13;
</div></figure>&#13;
&#13;
<p>However, the added complexity of using a deep learning method to predict such a simple time series may not be worth it.<a contenteditable="false" data-primary="" data-startref="Chapter_8.html15" data-type="indexterm" id="id728"/><a contenteditable="false" data-primary="" data-startref="Chapter_8.html13" data-type="indexterm" id="id729"/></p>&#13;
&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Temporal Convolutional Neural Networks" data-type="sect1"><div class="sect1" id="id68">&#13;
<h1>Temporal Convolutional Neural Networks</h1>&#13;
&#13;
<p><a contenteditable="false" data-primary="convolutional neural networks (CNNs)" data-type="indexterm" id="Chapter_8.html16"/><a contenteditable="false" data-primary="CNNs (convolutional neural networks)" data-type="indexterm" id="Chapter_8.html16a"/><a contenteditable="false" data-primary="temporal convolutional neural networks" data-type="indexterm" id="Chapter_8.html17"/><a contenteditable="false" data-primary="time series prediction (deep learning algorithms for)" data-secondary="temporal convolutional neural networks" data-type="indexterm" id="Chapter_8.html18"/><em>Convolutional neural networks </em>(CNNs) are a class of deep learning models designed to process structured grid-like data, with a particular emphasis on images and other grid-like data such as time series (less commonly used) and audio spectrograms. CNNs are good at learning and extracting hierarchical patterns and features from input data, making them powerful tools for tasks like image recognition, object detection, image segmentation, and more.</p>&#13;
&#13;
<p><a contenteditable="false" data-primary="convolutional layers" data-type="indexterm" id="id730"/>The core building blocks of CNNs are the <em>convolutional layers</em>. These layers perform convolution operations by applying a set of learnable filters to input data, resulting in feature maps that capture relevant spatial patterns and local dependencies. <a contenteditable="false" data-primary="pooling layers" data-type="indexterm" id="id731"/>Another important concept with CNNs is <em>pooling layers,</em> which downsample the feature maps produced by convolutional layers. Common pooling operations include <em>max pooling</em> (selecting the maximum value in a neighborhood) and <em>average pooling</em> (computing the average value). Pooling helps reduce spatial dimensions, extract dominant features, and improve computational efficiency.</p>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>A CNN that is specifically used for time series forecasting is often referred to as a 1D-CNN or a<em> temporal convolutional network</em>.</p>&#13;
&#13;
<p>The term <em>1D-CNN</em> indicates that the convolutional operations are applied along the temporal dimension of the input data, which is characteristic of time series data. This distinguishes it from traditional CNNs that operate on spatial dimensions in tasks such as image recognition.</p>&#13;
</div>&#13;
&#13;
<p>A typical CNN architecture consists of three main components: an input layer, several alternating convolutional and pooling layers, and fully connected layers at the end. Convolutional layers are responsible for feature extraction, while pooling layers downsample the data. The fully connected layers provide the final predictions.</p>&#13;
&#13;
<p>CNN architectures can vary greatly depending on the specific task. These architectures often employ additional techniques such as dropout regularization to improve performance and address challenges like overfitting.</p>&#13;
&#13;
<p>CNNs can be used for time series prediction by leveraging their ability to capture local patterns and extract relevant features from the input data. The framework of the process is as follows:</p>&#13;
&#13;
<ol>&#13;
	<li>&#13;
	<p>CNNs use convolutional layers to perform localized feature extraction. The convolutional layers consist of a set of learnable filters that are convolved with the input data. Each filter extracts different features from the input data by applying element-wise multiplications and summations in a sliding window manner. The result is a feature map that highlights important patterns or features at different locations in the input data.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Pooling layers are often employed after convolutional layers to reduce the spatial dimensionality of the feature maps. Max pooling is a common technique, where the maximum value within a local neighborhood is selected, effectively downsampling the feature map. Pooling helps in capturing the most salient features while reducing the computational complexity and enhancing the network’s <span class="keep-together">ability</span> to generalize.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>After the convolutional and pooling layers, the resulting feature maps are typically flattened into a one-dimensional vector. This flattening operation transforms the spatially distributed features into a linear sequence, which can then be passed to fully connected layers.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Fully connected layers receive the flattened feature vector as input and learn to map it to the desired output. These layers enable the network to learn complex combinations of features and model the nonlinear relationships between input features and target predictions. The last fully connected layer typically represents the output layer, which predicts the target values for the time series.</p>&#13;
	</li>&#13;
</ol>&#13;
&#13;
<p>Before moving to the algorithm creation steps, let’s review some key concepts seen with CNNs. In time series forecasting with CNNs, <em>filters </em>are applied along the temporal dimension of the input data. Instead of considering spatial features as in image data, the filters are designed to capture temporal patterns or dependencies within the time series. Each filter slides across the time series, processing a subset of consecutive time steps at a time. The filter learns to detect specific temporal patterns or features in the input data. For example, it might capture short-term trends, seasonality, or recurring patterns that are relevant for the forecasting task. Multiple filters can be used in each convolutional layer, allowing the network to learn a diverse set of temporal features. Each filter captures different aspects of the time series, enabling the model to capture complex temporal relationships.</p>&#13;
&#13;
<p>Another concept is the <em>kernel size</em>, which refers to the length or the number of consecutive time steps that the filter considers during the convolution operation. It defines the receptive field of the filter and influences the size of the extracted temporal patterns. The choice of kernel size depends on the characteristics of the time series data and the patterns to be captured. Smaller kernel sizes, such as 3 or 5, focus on capturing short-term patterns, while larger kernel sizes, such as 7 or 10, are suitable for capturing longer-term dependencies. Experimentation with different kernel sizes can help identify the optimal receptive field that captures the relevant temporal <span class="keep-together">patterns</span> for accurate forecasting. It’s common to have multiple convolutional layers with different kernel sizes to capture patterns at various temporal scales.</p>&#13;
&#13;
<p>Now let’s see how to create a temporal CNN to forecast S&amp;P 500 returns using its <span class="keep-together">lagged</span> values. Import the required libraries as follows:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="kn">from</code> <code class="nn">keras.models</code> <code class="kn">import</code> <code class="n">Sequential</code>&#13;
<code class="kn">from</code> <code class="nn">keras.layers</code> <code class="kn">import</code> <code class="n">Conv1D</code><code class="p">,</code> <code class="n">MaxPooling1D</code><code class="p">,</code> <code class="n">Flatten</code><code class="p">,</code> <code class="n">Dense</code>&#13;
<code class="kn">import</code> <code class="nn">keras</code>&#13;
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>&#13;
<code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="k">as</code> <code class="nn">plt</code>&#13;
<code class="kn">import</code> <code class="nn">pandas_datareader</code> <code class="k">as</code> <code class="nn">pdr</code>&#13;
<code class="kn">from</code> <code class="nn">master_function</code> <code class="kn">import</code> <code class="n">data_preprocessing</code><code class="p">,</code> <code class="n">plot_train_test_values</code>&#13;
<code class="kn">from</code> <code class="nn">master_function</code> <code class="kn">import</code> <code class="n">calculate_accuracy</code><code class="p">,</code> <code class="n">model_bias</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">mean_squared_error</code>&#13;
</pre>&#13;
&#13;
<p>Next, set the hyperparameters of the model:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="n">num_lags</code> <code class="o">=</code> <code class="mi">100</code> &#13;
<code class="n">train_test_split</code> <code class="o">=</code> <code class="mf">0.80</code> &#13;
<code class="n">filters</code> <code class="o">=</code> <code class="mi">64</code> &#13;
<code class="n">kernel_size</code> <code class="o">=</code> <code class="mi">4</code>&#13;
<code class="n">pool_size</code> <code class="o">=</code> <code class="mi">2</code>&#13;
<code class="n">num_epochs</code> <code class="o">=</code> <code class="mi">100</code> &#13;
<code class="n">batch_size</code> <code class="o">=</code> <code class="mi">8</code>&#13;
</pre>&#13;
&#13;
<p>Reshape the features arrays into three-dimensional data structures:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="n">x_train</code> <code class="o">=</code> <code class="n">x_train</code><code class="o">.</code><code class="n">reshape</code><code class="p">((</code><code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="n">num_lags</code><code class="p">,</code> <code class="mi">1</code><code class="p">))</code>&#13;
<code class="n">x_test</code> <code class="o">=</code> <code class="n">x_test</code><code class="o">.</code><code class="n">reshape</code><code class="p">((</code><code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="n">num_lags</code><code class="p">,</code> <code class="mi">1</code><code class="p">))</code></pre>&#13;
&#13;
<p>Now create the architecture of the temporal convolutional network (TCN) and run the algorithm:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="c1"># Create the temporal convolutional network model</code>&#13;
<code class="n">model</code> <code class="o">=</code> <code class="n">Sequential</code><code class="p">()</code>&#13;
<code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Conv1D</code><code class="p">(</code><code class="n">filters</code> <code class="o">=</code> <code class="n">filters</code><code class="p">,</code> <code class="n">kernel_size</code> <code class="o">=</code> <code class="n">kernel_size</code><code class="p">,</code> &#13;
                 <code class="n">activation</code> <code class="o">=</code> <code class="s1">'relu'</code><code class="p">,</code> <code class="n">input_shape</code> <code class="o">=</code> <code class="p">(</code><code class="n">num_lags</code><code class="p">,</code> <code class="mi">1</code><code class="p">)))</code>&#13;
<code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">MaxPooling1D</code><code class="p">(</code><code class="n">pool_size</code> <code class="o">=</code> <code class="n">pool_size</code><code class="p">))</code>&#13;
<code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Flatten</code><code class="p">())</code>&#13;
<code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dense</code><code class="p">(</code><code class="n">units</code> <code class="o">=</code> <code class="mi">1</code><code class="p">))</code>&#13;
<code class="c1"># Compile the model</code>&#13;
<code class="n">model</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">loss</code> <code class="o">=</code> <code class="s1">'mean_squared_error'</code><code class="p">,</code> <code class="n">optimizer</code> <code class="o">=</code> <code class="s1">'adam'</code><code class="p">)</code>&#13;
<code class="c1"># Train the model</code>&#13;
<code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">x_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">epochs</code> <code class="o">=</code> <code class="n">num_epochs</code> <code class="p">,</code> <code class="n">batch_size</code> <code class="o">=</code> <code class="n">batch_size</code><code class="p">)</code>&#13;
<code class="c1"># Predicting in-sample</code>&#13;
<code class="n">y_predicted_train</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">x_train</code><code class="p">),</code> <code class="p">(</code><code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">))</code>&#13;
<code class="c1"># Predicting out-of-sample</code>&#13;
<code class="n">y_predicted</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">x_test</code><code class="p">),</code> <code class="p">(</code><code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">))</code>&#13;
</pre>&#13;
&#13;
<p>Let’s plot the results and analyze the performance:</p>&#13;
&#13;
<pre data-code-language="python" data-executable="true" data-type="programlisting">&#13;
<code class="n">Accuracy</code> <code class="n">Train</code> <code class="o">=</code>  <code class="mf">68.9</code> <code class="o">%</code>&#13;
<code class="n">Accuracy</code> <code class="n">Test</code> <code class="o">=</code>  <code class="mf">49.16</code> <code class="o">%</code>&#13;
<code class="n">RMSE</code> <code class="n">Train</code> <code class="o">=</code>  <code class="mf">18.3047790152</code>&#13;
<code class="n">RMSE</code> <code class="n">Test</code> <code class="o">=</code>  <code class="mf">63.4069105299</code>&#13;
<code class="n">Correlation</code> <code class="n">In</code><code class="o">-</code><code class="n">Sample</code> <code class="n">Predicted</code><code class="o">/</code><code class="n">Train</code> <code class="o">=</code>  <code class="mf">0.786</code>&#13;
<code class="n">Correlation</code> <code class="n">Out</code><code class="o">-</code><code class="n">of</code><code class="o">-</code><code class="n">Sample</code> <code class="n">Predicted</code><code class="o">/</code><code class="n">Test</code> <code class="o">=</code>  <code class="mf">0.041</code>&#13;
<code class="n">Model</code> <code class="n">Bias</code> <code class="o">=</code>  <code class="mf">0.98</code>&#13;
</pre>&#13;
&#13;
<p><a data-type="xref" href="#figure-8-18">Figure 8-18</a> shows the evolution of the forecasting task from the last values of <code>y_train</code> to the first values of <code>y_test</code> and <code>y_predicted</code>.</p>&#13;
&#13;
<figure><div class="figure" id="figure-8-18"><img alt="" class="iimagesdlf_0818png" src="assets/dlff_0818.png"/>&#13;
<h6><span class="label">Figure 8-18. </span>Training data followed by test data (dashed line) and the predicted data (thin line); the vertical dashed line represents the start of the test period. The model used is the CNN regression algorithm.</h6>&#13;
</div></figure>&#13;
&#13;
<p>It is important to use performance metrics that reflect your choice and to search for a better algorithm. Accuracy may be one of the base metrics to give you a quick glance at the predictive abilities of your model, but on its own, it is not enough. The results seen in this chapter reflect only the training using the selected hyperparameters. Optimization will allow you to achieve very good results on certain models.</p>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>There is no strict rule defining the number of hidden layers required to consider a neural network as deep. However, a common convention is that a neural network with two or more hidden layers is typically considered a deep neural network.<a contenteditable="false" data-primary="" data-startref="Chapter_8.html18" data-type="indexterm" id="id732"/><a contenteditable="false" data-primary="" data-startref="Chapter_8.html17" data-type="indexterm" id="id733"/><a contenteditable="false" data-primary="" data-startref="Chapter_8.html16a" data-type="indexterm" id="id734"/><a contenteditable="false" data-primary="" data-startref="Chapter_8.html16" data-type="indexterm" id="id735"/></p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="id69">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>Applying deep learning algorithms to time series data can offer several benefits and challenges. Deep learning algorithms have shown great utility in time series analysis by effectively capturing complex patterns, extracting meaningful features, and making accurate predictions. However, their success relies heavily on the quality of the data and the chosen features.</p>&#13;
&#13;
<p>The utility of applying deep learning algorithms on time series data stems from their ability to automatically learn hierarchical representations and model intricate temporal dependencies. They can handle nonlinear relationships and capture both local and global patterns, making them suitable for a wide range of time series tasks like forecasting, anomaly detection, classification, and signal processing.</p>&#13;
&#13;
<p><a contenteditable="false" data-primary="time series prediction (deep learning algorithms for)" data-secondary="challenges when applying algorithms" data-type="indexterm" id="id736"/>However, applying deep learning algorithms to time series can present challenges:</p>&#13;
&#13;
<dl>&#13;
 <dt>Data quality</dt>&#13;
 <dd><p>Deep learning models heavily rely on large amounts of high-quality, labeled data for training. Insufficient or noisy data can hinder the performance of the models, leading to inaccurate predictions or unreliable insights. Data preprocessing, cleaning, and addressing missing values become crucial steps to ensure the <span class="keep-together">quality</span> of the data.</p></dd>&#13;
 &#13;
 <dt>Feature engineering</dt>&#13;
 <dd><p>Deep learning models can automatically learn relevant features from the data. However, the choice and extraction of informative features can significantly impact the model’s performance. Domain knowledge, data exploration, and feature engineering techniques are important in selecting or transforming features that enhance the model’s ability to capture relevant patterns.</p></dd>&#13;
 &#13;
 <dt>Model complexity</dt>&#13;
 <dd><p>Deep learning models are typically complex with a large number of parameters. Training such models requires substantial computational resources, longer training times, and careful hyperparameter tuning. Overfitting, where the model memorizes the training data without generalizing well to unseen data, is also a common challenge.</p></dd>&#13;
 &#13;
 <dt>Interpretability</dt>&#13;
 <dd><p>Deep learning models are often considered mystery boxes, making it challenging to interpret the learned representations and understand the reasoning behind predictions. This can be a concern in domains where interpretability and explainability are crucial, such as finance.</p></dd>&#13;
</dl>&#13;
&#13;
&#13;
<p>To overcome these challenges and harness the power of deep learning algorithms for time series analysis, careful consideration of data quality, appropriate feature engineering, model architecture selection, regularization techniques, and interpretability approaches are essential. It is crucial to understand the specific characteristics and requirements of the time series data and the task at hand to choose and tailor the deep learning approach accordingly.<a contenteditable="false" data-primary="" data-startref="Chapter_8.html0" data-type="indexterm" id="id737"/></p>&#13;
</div></section>&#13;
<div data-type="footnotes"><p data-type="footnote" id="id685"><sup><a href="ch08.html#id685-marker">1</a></sup> W. S. McCulloch and W. Pitts, “A Logical Calculus of the Ideas Immanent in Nervous Activity,” <em>Bulletin of Mathematical Biophysics</em> 5 (1943): 115–33.</p></div></div></section></body></html>