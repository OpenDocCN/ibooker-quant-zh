<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 5. Supervised Learning: Regression &#10;(Including Time Series Models)"><div class="chapter" id="Chapter5">
<h1><span class="label">Chapter 5. </span>Supervised Learning: Regression 
<span class="keep-together">(Including Time Series Models)</span></h1>


<p><a data-type="indexterm" data-primary="regression" id="ix_Chapter5-asciidoc0"/>Supervised regression–based machine learning is a predictive form of modeling in which the goal is to model the relationship between a target and the predictor variable(s) in order to estimate a continuous set of possible outcomes. These are the most used machine learning models in finance.</p>

<p>One of the focus areas of analysts in financial institutions (and finance in general) is to predict investment opportunities, typically predictions of asset prices and asset returns. Supervised regression–based machine learning models are inherently suitable in this context. These models help investment and financial managers understand the properties of the predicted variable and its relationship with other variables, and help them identify significant factors that drive asset returns. This helps investors estimate return profiles, trading costs, technical and financial investment required in infrastructure, and thus ultimately the risk profile and profitability of a strategy or portfolio.</p>

<p>With the availability of large volumes of data and processing techniques, supervised regression–based machine learning isn’t just limited to asset price prediction.
These models are applied to a wide range of areas within finance, including portfolio management, insurance pricing, instrument pricing, hedging, and risk management.</p>

<p>In this chapter we cover three supervised regression–based case studies that span diverse areas, including asset price prediction, instrument pricing, and portfolio management. All of the case studies follow the standardized seven-step model development process presented in <a data-type="xref" href="ch02.xhtml#Chapter2">Chapter 2</a>; those steps include defining the problem, loading the data, exploratory data analysis, data preparation, model evaluation, and model tuning.<sup><a data-type="noteref" id="idm45174931460040-marker" href="ch05.xhtml#idm45174931460040">1</a></sup> The case studies are designed not only to cover a diverse set of topics from the finance standpoint but also to cover multiple machine learning and modeling concepts, including models from basic linear regression to advanced deep learning that were presented in <a data-type="xref" href="ch04.xhtml#Chapter4">Chapter 4</a>.</p>

<p>A substantial amount of asset modeling and prediction problems in the financial industry involve a time component and estimation of a continuous output. As such, it is also important to address <em>time series models</em>. <a data-type="indexterm" data-primary="time series analysis (defined)" id="idm45174931456904"/>In its broadest form, time series analysis is about inferring what has happened to a series of data points in the past and attempting to predict what will happen to it in the future. <a data-type="indexterm" data-primary="regression" data-secondary="time series models versus" id="idm45174931455832"/><a data-type="indexterm" data-primary="supervised regression" data-secondary="time series models versus" id="idm45174931454872"/><a data-type="indexterm" data-primary="time series models" data-secondary="supervised regression versus" id="idm45174931453912"/>There have been a lot of comparisons and debates in academia and the industry regarding the differences between supervised regression and time series models. <a data-type="indexterm" data-primary="nonparametric models" id="idm45174931452648"/><a data-type="indexterm" data-primary="parametric models" id="idm45174931451976"/>Most time series models are <em>parametric</em> (i.e., a known function is assumed to represent the data), while the majority of supervised regression models are <em>nonparametric</em>. Time series models primarily use historical data of the predicted variables for prediction, and supervised learning algorithms use <em>exogenous variables</em> as predictor variables.<sup><a data-type="noteref" id="idm45174931449640-marker" href="ch05.xhtml#idm45174931449640">2</a></sup> However, supervised regression can embed the historical data of the predicted variable through a time-delay approach (covered later in this chapter), and a time series model (such as ARIMAX, also covered later in this chapter) can use exogenous variables for prediction. Hence, time series and supervised regression models are similar in the sense that both can use exogenous variables as well as historical data of the predicted variable for forecasting. In terms of the final output, both supervised regression and time series models estimate a continuous set of possible outcomes of a variable.</p>

<p>In <a data-type="xref" href="ch04.xhtml#Chapter4">Chapter 4</a>, we covered the concepts of models that are common between supervised regression and supervised classification. Given that time series models are more closely aligned with supervised regression than supervised classification, we will go through the concepts of time series models separately in this chapter. We will also demonstrate how we can use time series models on financial data to predict future values. Comparison of time series models against the supervised regression models will be presented in the case studies. Additionally, some machine learning and deep learning models (such as LSTM) can be directly used for time series forecasting, and those will be discussed as well.</p>

<p class="pagebreak-before">In <a data-type="xref" href="#CaseStudy1SR">“Case Study 1: Stock Price Prediction”</a>, we demonstrate one of the most popular prediction problems in finance, that of predicting stock returns. In addition to predicting future stock prices accurately, the purpose of this case study is to discuss the machine learning–based framework for general asset class price prediction in finance. In this case study we will discuss several machine learning and time series concepts, along with focusing on visualization and model tuning.</p>

<p>In <a data-type="xref" href="#CaseStudy2SR">“Case Study 2: Derivative Pricing”</a>, we will delve into derivative pricing using supervised regression and show how to deploy machine learning techniques in the context of traditional quant problems. As compared to traditional derivative pricing models, machine learning techniques can lead to faster derivative pricing without relying on the several impractical assumptions. Efficient numerical computation using machine learning can be increasingly beneficial in areas such as financial risk management, where a trade-off between efficiency and accuracy is often inevitable.</p>

<p>In <a data-type="xref" href="#CaseStudy3SR">“Case Study 3: Investor Risk Tolerance and Robo-Advisors”</a>, we illustrate supervised regression–based framework to estimate the risk tolerance of investors. In this case study, we build a robo-advisor dashboard in Python and implement this risk tolerance prediction model in the dashboard. We demonstrate how such models can lead to the automation of portfolio management processes, including the use of robo-advisors for investment management. The purpose is also to illustrate how machine learning can efficiently be used to overcome the problem of traditional risk tolerance profiling or risk tolerance questionnaires that suffer from several behavioral biases.</p>

<p>In <a data-type="xref" href="#CaseStudy4SR">“Case Study 4: Yield Curve Prediction”</a>, we use a supervised regression–based framework to forecast different yield curve tenors simultaneously. We demonstrate how we can produce multiple tenors at the same time to model the yield curve using machine learning models.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45174931439128">
<h5/>
<p>In this chapter, we will learn about the following concepts related to supervised regression and time series techniques:</p>

<ul>
<li>
<p>Application and comparison of different time series and machine learning 
<span class="keep-together">models</span>.</p>
</li>
<li>
<p>Interpretation of the models and results. Understanding the potential overfitting and underfitting and intuition behind linear versus nonlinear models.</p>
</li>
<li>
<p>Performing data preparation and transformations to be used in machine learning models.</p>
</li>
<li>
<p>Performing feature selection and engineering to improve model performance.</p>
</li>
<li>
<p>Using data visualization and data exploration to understand outcomes.</p>
</li>
<li>
<p>Algorithm tuning to improve model performance. Understanding, implementing, and tuning time series models such as ARIMA for prediction.</p>
</li>
<li>
<p>Framing a problem statement related to portfolio management and behavioral finance in a regression-based machine learning framework.</p>
</li>
<li>
<p>Understanding how deep learning–based models such as LSTM can be used for time series prediction.</p>
</li>
</ul>
</div></aside>

<p>The models used for supervised regression were presented in Chapters <a href="ch03.xhtml#Chapter3">3</a> and <a href="ch04.xhtml#Chapter4">4</a>. Prior to the case studies, we will discuss time series models. We highly recommend readers turn to <em>Time Series Analysis and Its Applications</em>, 4th Edition, by Robert H. Shumway and David S. Stoffer (Springer) for a more in-depth understanding of time series concepts, and to <em>Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow</em>, 2nd Edition, by Aurélien Géron (O’Reilly) for more on concepts in supervised regression models.</p>
<div data-type="note" epub:type="note"><h1>This Chapter’s Code Repository</h1>
<p>A Python-based master template for supervised regression, a time series model template, and the Jupyter notebook for all case studies presented in this chapter are included in the folder <a href="https://oreil.ly/sJFV0"><em>Chapter 5 - Sup. Learning - Regression and Time Series models</em></a> of the code repository for this book.</p>

<p>For any new supervised regression–based case study, use the common template from the code repository, modify the elements specific to the case study, and borrow the concepts and insights from the case studies presented in this chapter. The template also includes the implementation and tuning of the ARIMA and LSTM models.<sup><a data-type="noteref" id="idm45174931422552-marker" href="ch05.xhtml#idm45174931422552">3</a></sup> The templates are designed to run on the cloud (i.e., Kaggle, Google Colab, and AWS). All the case studies have been designed on a uniform regression template.<sup><a data-type="noteref" id="idm45174931421656-marker" href="ch05.xhtml#idm45174931421656">4</a></sup></p>
</div>






<section data-type="sect1" data-pdf-bookmark="Time Series Models"><div class="sect1" id="idm45174931420728">
<h1>Time Series Models</h1>

<p><a data-type="indexterm" data-primary="time series models" data-secondary="basics" id="ix_Chapter5-asciidoc1"/>A <a data-type="indexterm" data-primary="time series (defined)" id="idm45174931417944"/><em>time series</em> is a sequence of numbers that are ordered by a time index.</p>

<p>In this section we will cover the following aspects of time series models, which we further leverage in the case studies:</p>

<ul>
<li>
<p>The components of a time series</p>
</li>
<li>
<p>Autocorrelation and stationarity of time series</p>
</li>
<li>
<p>Traditional time series models (e.g., ARIMA)</p>
</li>
<li>
<p>Use of deep learning models for time series modeling</p>
</li>
<li>
<p>Conversion of time series data for use in a supervised learning framework</p>
</li>
</ul>








<section data-type="sect2" data-pdf-bookmark="Time Series Breakdown"><div class="sect2" id="idm45174931411096">
<h2>Time Series Breakdown</h2>

<p><a data-type="indexterm" data-primary="time series models" data-secondary="time series components" id="idm45174931409496"/>A time series can be broken down into the following components:</p>
<dl>
<dt>Trend Component</dt>
<dd>
<p><a data-type="indexterm" data-primary="trend, defined" id="idm45174931406968"/>A trend is a consistent directional movement in a time series. These trends
will be either <em>deterministic</em> or <em>stochastic</em>. The former allows us to provide an underlying
rationale for the trend, while the latter is a random feature of a series that we will be
unlikely to explain. Trends often appear in financial series, and many trading models use sophisticated trend identification algorithms.</p>
</dd>
<dt>Seasonal Component</dt>
<dd>
<p><a data-type="indexterm" data-primary="seasonal variation" id="idm45174931403720"/>Many time series contain seasonal variation. This is particularly
true in series representing business sales or climate levels. In quantitative finance we often
see seasonal variation, particularly in series related to holiday seasons or annual temperature variation (such as natural gas).</p>
</dd>
</dl>

<p>We can write the components of a time series <math alttext="y Subscript t">
  <msub><mi>y</mi> <mi>t</mi> </msub>
</math> as</p>
<div data-type="equation">
<math alttext="y Subscript t Baseline equals upper S Subscript t Baseline plus upper T Subscript t Baseline plus upper R Subscript t" display="block">
  <mrow>
    <msub><mi>y</mi> <mi>t</mi> </msub>
    <mo>=</mo>
    <msub><mi>S</mi> <mi>t</mi> </msub>
    <mo>+</mo>
    <msub><mi>T</mi> <mi>t</mi> </msub>
    <mo>+</mo>
    <msub><mi>R</mi> <mi>t</mi> </msub>
  </mrow>
</math>
</div>

<p>where <math alttext="upper S Subscript t">
  <msub><mi>S</mi> <mi>t</mi> </msub>
</math> is the seasonal component, <math alttext="upper T Subscript t">
  <msub><mi>T</mi> <mi>t</mi> </msub>
</math> is the trend component, and <math alttext="upper R Subscript t">
  <msub><mi>R</mi> <mi>t</mi> </msub>
</math> represents the remainder component of the time series not captured by seasonal or trend 
<span class="keep-together">component</span>.</p>

<p>The Python code for breaking down a time series (<em>Y</em>) into its component is as follows:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">import</code> <code class="nn">statsmodels.api</code> <code class="k">as</code> <code class="nn">sm</code>
<code class="n">sm</code><code class="o">.</code><code class="n">tsa</code><code class="o">.</code><code class="n">seasonal_decompose</code><code class="p">(</code><code class="n">Y</code><code class="p">,</code><code class="n">freq</code><code class="o">=</code><code class="mi">52</code><code class="p">)</code><code class="o">.</code><code class="n">plot</code><code class="p">()</code></pre>

<p><a data-type="xref" href="#TimeSeriesComponents">Figure 5-1</a> shows the time series broken down into trend, seasonality, and remainder components. Breaking down a time series into these components may help us better understand the time series and identify its behavior for better prediction.</p>

<p>The three time series components are shown separately in the bottom three panels. These components can be added together to reconstruct the actual time series shown in the top panel (shown as “observed”). Notice that the time series shows a trending component after 2017. Hence, the prediction model for this time series should incorporate the information regarding the trending behavior after 2017. In terms of seasonality there is some increase in the magnitude in the beginning of the calendar year. The residual component shown in the bottom panel is what is left over when the seasonal and trend components have been subtracted from the data. The residual component is mostly flat with some spikes and noise around 2018 and 2019. Also, each of the plots are on different scales, and the trend component has maximum range as shown by the scale on the plot.</p>

<figure><div id="TimeSeriesComponents" class="figure">
<img src="Images/mlbf_0501.png" alt="mlbf 0501" width="1425" height="784"/>
<h6><span class="label">Figure 5-1. </span>Time series components</h6>
</div></figure>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Autocorrelation and Stationarity"><div class="sect2" id="idm45174931410472">
<h2>Autocorrelation and Stationarity</h2>

<p>When we are given one or more time series, it is relatively straightforward to decompose the time series into trend, seasonality, and residual components. However, there are other aspects that come into play when dealing with time series data, particularly in finance.</p>










<section data-type="sect3" data-pdf-bookmark="Autocorrelation"><div class="sect3" id="idm45174931364200">
<h3>Autocorrelation</h3>

<p><a data-type="indexterm" data-primary="autocorrelation" id="idm45174931359848"/><a data-type="indexterm" data-primary="time series models" data-secondary="autocorrelation" id="idm45174931359144"/>There are many situations in which consecutive elements of a time series exhibit correlation. That is, the behavior of sequential points in the series affect each other in a dependent manner. <em>Autocorrelation</em> is the similarity between observations as a function of the time lag between them. Such relationships can be modeled using an autoregression model. The term <em>autoregression</em> indicates that it is a regression of the variable against itself.</p>

<p>In an autoregression model, we forecast the variable of interest using a linear combination of past values of the variable.</p>

<p>Thus, an autoregressive model of order <math alttext="p">
  <mi>p</mi>
</math> can be written as</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <msub><mi>y</mi> <mi>t</mi> </msub>
    <mo>=</mo>
    <mi>c</mi>
    <mo>+</mo>
    <msub><mi>ϕ</mi> <mn>1</mn> </msub>
    <msub><mi>y</mi> <mrow><mi>t</mi><mo>–</mo><mn>1</mn></mrow> </msub>
    <mo>+</mo>
    <msub><mi>ϕ</mi> <mn>2</mn> </msub>
    <msub><mi>y</mi> <mrow><mi>t</mi><mo>–</mo><mn>2</mn></mrow> </msub>
    <mo>+</mo>
    <mo>.</mo>
    <mo>.</mo>
    <mo>.</mo>
    <mo>.</mo>
    <msub><mi>ϕ</mi> <mi>p</mi> </msub>
    <msub><mi>y</mi> <mrow><mi>t</mi><mo>–</mo><mi>p</mi></mrow> </msub>
    <mo>+</mo>
    <mi>ϵ</mi>
  </mrow>
</math>
</div>

<p>where
<math alttext="epsilon Subscript t">
  <msub><mi>ϵ</mi> <mi>t</mi> </msub>
</math> is white noise.<sup><a data-type="noteref" id="idm45174931308056-marker" href="ch05.xhtml#idm45174931308056">5</a></sup> An autoregressive model is like a multiple regression but with lagged values of <math alttext="y Subscript t">
  <msub><mi>y</mi> <mi>t</mi> </msub>
</math> as predictors. We refer to this as an AR(<em>p</em>) model, an autoregressive model of order <em>p</em>.
Autoregressive models are remarkably flexible at handling a wide range of different time series patterns.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Stationarity"><div class="sect3" id="idm45174931304200">
<h3>Stationarity</h3>

<p><a data-type="indexterm" data-primary="stationary time series" id="idm45174931302760"/><a data-type="indexterm" data-primary="time series models" data-secondary="stationarity" id="idm45174931302056"/>A time series is said to be stationary if its statistical properties do not change over time. Thus a time series with trend or with seasonality is not stationary, as the trend and seasonality will affect the value of the time series at different times. On the other hand, a white noise series is stationary, as it does not matter when you observe it; it should look similar at any point in time.</p>

<p><a data-type="xref" href="#NonStationaryPlots">Figure 5-2</a> shows some examples of nonstationary series.</p>

<figure><div id="NonStationaryPlots" class="figure">
<img src="Images/mlbf_0502.png" alt="mlbf 0502" width="1276" height="363"/>
<h6><span class="label">Figure 5-2. </span>Nonstationary plots</h6>
</div></figure>

<p>In the first plot, we can clearly see that the mean varies (increases) with time, resulting in an upward trend. Thus this is a nonstationary series. For a series to be classified as stationary, it should not exhibit a trend. Moving on to the second plot, we certainly do not see a trend in the series, but the variance of the series is a function of time. A stationary series must have a constant variance; hence this series is a nonstationary series as well. In the third plot, the spread becomes closer as the time increases, which implies that the covariance is a function of time. The three examples shown in <a data-type="xref" href="#NonStationaryPlots">Figure 5-2</a> represent nonstationary time series. Now look at a fourth plot, as shown in <a data-type="xref" href="#StationaryPlots">Figure 5-3</a>.</p>

<figure><div id="StationaryPlots" class="figure">
<img src="Images/mlbf_0503.png" alt="mlbf 0503" width="353" height="359"/>
<h6><span class="label">Figure 5-3. </span>Stationary plot</h6>
</div></figure>

<p>In this case, the mean, variance, and covariance are constant with time. This is what a stationary time series looks like. Predicting future values using this fourth plot would be easier. Most statistical models require the series to be stationary to make effective and precise predictions.</p>

<p>The two major reasons behind nonstationarity of a time series are trend and seasonality, as shown in <a data-type="xref" href="#NonStationaryPlots">Figure 5-2</a>. In order to use time series forecasting models, we generally convert any nonstationary series to a stationary series, making it easier to model since statistical properties don’t change over time.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Differencing"><div class="sect3" id="idm45174931290408">
<h3>Differencing</h3>

<p><a data-type="indexterm" data-primary="time series models" data-secondary="differencing" id="idm45174931289000"/>Differencing is one of the methods used to make a time series stationary. In this method, we compute the difference of consecutive terms in the series. Differencing is typically performed to get rid of the varying mean. Mathematically, differencing can be written as:</p>
<div data-type="equation">
<math>
  <mrow>
    <msubsup><mi>y</mi> <mi>t</mi> <mo>′</mo> </msubsup>
    <mo>=</mo>
    <msub><mi>y</mi> <mi>t</mi> </msub>
    <mo>–</mo>
    <msub><mi>y</mi> <mrow><mi>t</mi><mo>–</mo><mn>1</mn></mrow> </msub>
  </mrow>
</math>
</div>

<p>where <math alttext="y Subscript t">
  <msub><mi>y</mi> <mi>t</mi> </msub>
</math> is the value at a time <em>t</em>.</p>

<p>When the differenced series is white noise, the original series is referred to as a nonstationary series of degree one.</p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Traditional Time Series Models (Including the ARIMA Model)"><div class="sect2" id="idm45174931276824">
<h2>Traditional Time Series Models (Including the ARIMA Model)</h2>

<p><a data-type="indexterm" data-primary="time series models" data-secondary="traditional models" id="ix_Chapter5-asciidoc2"/>There are many ways to model a time series in order to make predictions. Most of the time series models aim at incorporating the trend, seasonality, and remainder components while addressing the autocorrelation and stationarity embedded in the time series. For example, the autoregressive (AR) model discussed in the previous section addresses the autocorrelation in the time series.</p>

<p>One of the most widely used models in time series forecasting is the ARIMA model.</p>










<section data-type="sect3" data-pdf-bookmark="ARIMA"><div class="sect3" id="idm45174931273368">
<h3>ARIMA</h3>

<p><a data-type="indexterm" data-primary="ARIMA (autoregressive integrated moving average) model" id="idm45174931271768"/><a data-type="indexterm" data-primary="time series models" data-secondary="ARIMA" id="idm45174931271032"/>If we combine stationarity with autoregression and a moving average model (discussed further on in this section), we obtain an ARIMA model. <em>ARIMA</em> is an acronym for AutoRegressive Integrated Moving Average, and it has the following components:</p>
<dl>
<dt>AR(p)</dt>
<dd>
<p>It represents autoregression, i.e., regression of the time series onto itself, as discussed in the previous section, with an assumption that current series values depend on its previous values with some lag (or several lags). The maximum lag in the model is referred to as <em>p</em>.</p>
</dd>
<dt>I(d)</dt>
<dd>
<p>It represents order of integration. It is simply the number of differences needed to make the series stationary.</p>
</dd>
<dt>MA(q)</dt>
<dd>
<p>It represents moving average. Without going into detail, it models the error of the time series; again, the assumption is that current error depends on the previous with some lag, which is referred to as <em>q</em>.</p>
</dd>
</dl>

<p>The moving average equation is written as:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <msub><mi>y</mi> <mi>t</mi> </msub>
    <mo>=</mo>
    <mi>c</mi>
    <mo>+</mo>
    <msub><mi>ϵ</mi> <mi>t</mi> </msub>
    <mo>+</mo>
    <msub><mi>θ</mi> <mn>1</mn> </msub>
    <msub><mi>ϵ</mi> <mrow><mi>t</mi><mo>–</mo><mn>1</mn></mrow> </msub>
    <mo>+</mo>
    <msub><mi>θ</mi> <mn>2</mn> </msub>
    <msub><mi>ϵ</mi> <mrow><mi>t</mi><mo>–</mo><mn>2</mn></mrow> </msub>
  </mrow>
</math>
</div>

<p>where, <math alttext="epsilon Subscript t">
  <msub><mi>ϵ</mi> <mi>t</mi> </msub>
</math> is white noise. We refer to this as an <em>MA(q)</em> model of order <em>q</em>.</p>

<p>Combining all the components, the full ARIMA model can be written as:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <msubsup><mi>y</mi> <mi>t</mi> <mo>′</mo> </msubsup>
    <mo>=</mo>
    <mi>c</mi>
    <mo>+</mo>
    <msub><mi>ϕ</mi> <mn>1</mn> </msub>
    <msubsup><mi>y</mi> <mrow><mi>t</mi><mo>–</mo><mn>1</mn></mrow> <mo>′</mo> </msubsup>
    <mo>+</mo>
    <mo>⋯</mo>
    <mo>+</mo>
    <msub><mi>ϕ</mi> <mi>p</mi> </msub>
    <msubsup><mi>y</mi> <mrow><mi>t</mi><mo>–</mo><mi>p</mi></mrow> <mo>′</mo> </msubsup>
    <mo>+</mo>
    <msub><mi>θ</mi> <mn>1</mn> </msub>
    <msub><mi>ε</mi> <mrow><mi>t</mi><mo>–</mo><mn>1</mn></mrow> </msub>
    <mo>+</mo>
    <mo>⋯</mo>
    <mo>+</mo>
    <msub><mi>θ</mi> <mi>q</mi> </msub>
    <msub><mi>ε</mi> <mrow><mi>t</mi><mo>–</mo><mi>q</mi></mrow> </msub>
    <mo>+</mo>
    <msub><mi>ε</mi> <mi>t</mi> </msub>
  </mrow>
</math>
</div>

<p>where <math alttext="y prime Subscript t">
  <msubsup><mi>y</mi> <mi>t</mi> <mo>'</mo> </msubsup>
</math> is the differenced series (it may have been differenced more than once). The predictors on the right-hand side include both lagged values of <math alttext="y prime Subscript t">
  <msubsup><mi>y</mi> <mi>t</mi> <mo>'</mo> </msubsup>
</math> and lagged errors. We call this an ARIMA(<em>p,d,q</em>) model, where <em>p</em> is the order of the autoregressive part, <em>d</em> is the degree of first differencing involved, and <em>q</em> is the order of the moving average part. The same stationarity and invertibility conditions that are used for autoregressive and moving average models also apply to an ARIMA model.</p>

<p>The Python code to fit the ARIMA model of the order (1,0,0) is shown in the 
<span class="keep-together">following</span>:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">from</code> <code class="nn">statsmodels.tsa.arima_model</code> <code class="k">import</code> <code class="n">ARIMA</code>
<code class="n">model</code><code class="o">=</code><code class="n">ARIMA</code><code class="p">(</code><code class="n">endog</code><code class="o">=</code><code class="n">Y_train</code><code class="p">,</code><code class="n">order</code><code class="o">=</code><code class="p">[</code><code class="mi">1</code><code class="p">,</code><code class="mi">0</code><code class="p">,</code><code class="mi">0</code><code class="p">])</code></pre>

<p class="pagebreak-before">The family of ARIMA models has several variants, and some of them are as follows:</p>
<dl>
<dt>ARIMAX</dt>
<dd>
<p>ARIMA models with exogenous variables included. We will be using this model in case study 1.</p>
</dd>
<dt>SARIMA</dt>
<dd>
<p>“S” in this model stands for seasonal, and this model is targeted at modeling the seasonality component embedded in the time series, along with other 
<span class="keep-together">components</span>.</p>
</dd>
<dt>VARMA</dt>
<dd>
<p>This is the extension of the model to multivariate case, when there are many variables to be predicted simultaneously. We predict many variables simultaneously in <a data-type="xref" href="#CaseStudy4SR">“Case Study 4: Yield Curve Prediction”</a>.<a data-type="indexterm" data-startref="ix_Chapter5-asciidoc2" id="idm45174931181816"/></p>
</dd>
</dl>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Deep Learning Approach to Time Series Modeling"><div class="sect2" id="idm45174931180856">
<h2>Deep Learning Approach to Time Series Modeling</h2>

<p><a data-type="indexterm" data-primary="deep learning" data-secondary="time series modeling with" id="ix_Chapter5-asciidoc3"/><a data-type="indexterm" data-primary="time series models" data-secondary="deep learning approach to modeling" id="ix_Chapter5-asciidoc4"/>The traditional time series models such as ARIMA are well understood
and effective on many problems. However, these traditional methods also suffer from several limitations. Traditional time series models are linear functions, or simple transformations of linear functions, and they require manually diagnosed parameters, such as time dependence, and don’t perform well with corrupt or missing data.</p>

<p>If we look at the advancements in the field of deep learning for time series prediction, we see that <em>recurrent neural network</em> (RNN) has gained increasing attention in recent years. These methods can identify structure and patterns such as nonlinearity, can seamlessly model problems with multiple input variables, and are relatively robust to missing data. The RNN models can retain state from one iteration to the next by using their own output as input for the next step. These deep learning models can be referred to as time series models, as they can make future predictions using the data points in the past, similar to traditional time series models such as ARIMA. Therefore, there are a wide range of applications in finance where these deep learning models can be leveraged. Let us look at the deep learning models for time series forecasting.</p>










<section data-type="sect3" data-pdf-bookmark="RNNs"><div class="sect3" id="idm45174931174760">
<h3>RNNs</h3>

<p><a data-type="indexterm" data-primary="deep learning" data-secondary="RNNs and" id="idm45174931173352"/><a data-type="indexterm" data-primary="recurrent neural networks (RNNs)" id="idm45174931172376"/>Recurrent neural networks (RNNs) are called “recurrent” because they perform the same task for every element of a sequence, with the output being dependent on the previous computations. RNN models have a memory, which captures information about what has been calculated so far. As shown in <a data-type="xref" href="#RNN">Figure 5-4</a>, a recurrent neural network can be thought of as multiple copies of the same network, each passing a message to a successor.</p>

<figure><div id="RNN" class="figure">
<img src="Images/mlbf_0504.png" alt="mlbf 0504" width="780" height="423"/>
<h6><span class="label">Figure 5-4. </span>Recurrent Neural Network</h6>
</div></figure>

<p>In <a data-type="xref" href="#RNN">Figure 5-4</a>:</p>

<ul>
<li>
<p><em>X<sub>t</sub></em> is the input at time step <em>t</em>.</p>
</li>
<li>
<p><em>O<sub>t</sub></em> is the output at time step <em>t</em>.</p>
</li>
<li>
<p><em>S<sub>t</sub></em> is the hidden state at time step <em>t</em>. It’s the memory of the network. It is calculated based on the previous hidden state and the input at the current step.</p>
</li>
</ul>

<p>The main feature of an RNN is this hidden state, which captures some information about a sequence and uses it accordingly whenever needed.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Long short-term memory"><div class="sect3" id="idm45174931141640">
<h3>Long short-term memory</h3>

<p><a data-type="indexterm" data-primary="long short-term memory (LSTM)" id="idm45174931140360"/><a data-type="indexterm" data-primary="recurrent neural networks (RNNs)" data-secondary="LSTM" id="idm45174931139640"/><em>Long short-term memory</em> (LSTM) is a special kind of RNN explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically default behavior for an LSTM model.<sup><a data-type="noteref" id="idm45174931138088-marker" href="ch05.xhtml#idm45174931138088">6</a></sup> These models are composed of a set of cells with features to memorize the sequence of data. These cells capture and store the data streams. Further, the cells interconnect one module of the past to another module of the present to convey information from several past time instants to the present one. Due to the use of gates in each cell, data in each cell can be disposed, filtered, or added for the next cells.</p>

<p><a data-type="indexterm" data-primary="gates" id="idm45174931136024"/>The <em>gates</em>, based on artificial neural network layers, enable the cells to optionally let data pass through or be disposed. Each layer yields numbers in the range of zero to one, depicting the amount of every segment of data that ought to be let through in each cell. More precisely, an estimation of zero value implies “let nothing pass through.” An estimation of one indicates “let everything pass through.” Three types of gates are involved in each LSTM, with the goal of controlling the state of each cell:</p>
<dl>
<dt>Forget Gate</dt>
<dd>
<p>Outputs a number between zero and one, where one shows “completely keep this” and zero implies “completely ignore this.” This gate conditionally decides whether the past should be forgotten or preserved.</p>
</dd>
<dt>Input Gate</dt>
<dd>
<p>Chooses which new data needs to be stored in the cell.</p>
</dd>
<dt>Output Gate</dt>
<dd>
<p>Decides what will yield out of each cell. The yielded value will be based on the cell state along with the filtered and newly added data.</p>
</dd>
</dl>

<p>Keras wraps the efficient numerical computation libraries and functions and allows us to define and train LSTM neural network models in a few short lines of code. In the following code, LSTM module from <code>keras.layers</code> is used for implementing LSTM network. The network is trained with the variable <code>X_train_LSTM</code>. The network has a hidden layer with 50 LSTM blocks or neurons and an output layer that makes a single value prediction. Also refer to <a data-type="xref" href="ch03.xhtml#Chapter3">Chapter 3</a> for a more detailed description of all the terms (i.e., sequential, learning rate, momentum, epoch, and batch size).</p>

<p>A sample Python code for implementing an LSTM model in Keras is shown below:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">from</code> <code class="nn">keras.models</code> <code class="k">import</code> <code class="n">Sequential</code>
<code class="kn">from</code> <code class="nn">keras.layers</code> <code class="k">import</code> <code class="n">Dense</code>
<code class="kn">from</code> <code class="nn">keras.optimizers</code> <code class="k">import</code> <code class="n">SGD</code>
<code class="kn">from</code> <code class="nn">keras.layers</code> <code class="k">import</code> <code class="n">LSTM</code>

<code class="k">def</code> <code class="nf">create_LSTMmodel</code><code class="p">(</code><code class="n">learn_rate</code> <code class="o">=</code> <code class="mf">0.01</code><code class="p">,</code> <code class="n">momentum</code><code class="o">=</code><code class="mi">0</code><code class="p">):</code>
       <code class="c"># create model</code>
   <code class="n">model</code> <code class="o">=</code> <code class="n">Sequential</code><code class="p">()</code>
   <code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">LSTM</code><code class="p">(</code><code class="mi">50</code><code class="p">,</code> <code class="n">input_shape</code><code class="o">=</code><code class="p">(</code><code class="n">X_train_LSTM</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code>\
      <code class="n">X_train_LSTM</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">2</code><code class="p">])))</code>
   <code class="c">#More number of cells can be added if needed</code>
   <code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dense</code><code class="p">(</code><code class="mi">1</code><code class="p">))</code>
   <code class="n">optimizer</code> <code class="o">=</code> <code class="n">SGD</code><code class="p">(</code><code class="n">lr</code><code class="o">=</code><code class="n">learn_rate</code><code class="p">,</code> <code class="n">momentum</code><code class="o">=</code><code class="n">momentum</code><code class="p">)</code>
   <code class="n">model</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">loss</code><code class="o">=</code><code class="s">'mse'</code><code class="p">,</code> <code class="n">optimizer</code><code class="o">=</code><code class="s">'adam'</code><code class="p">)</code>
   <code class="k">return</code> <code class="n">model</code>
<code class="n">LSTMModel</code> <code class="o">=</code> <code class="n">create_LSTMmodel</code><code class="p">(</code><code class="n">learn_rate</code> <code class="o">=</code> <code class="mf">0.01</code><code class="p">,</code> <code class="n">momentum</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
<code class="n">LSTMModel_fit</code> <code class="o">=</code> <code class="n">LSTMModel</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train_LSTM</code><code class="p">,</code> <code class="n">Y_train_LSTM</code><code class="p">,</code> <code class="n">validation_data</code><code class="o">=</code>\
  <code class="p">(</code><code class="n">X_test_LSTM</code><code class="p">,</code> <code class="n">Y_test_LSTM</code><code class="p">),</code><code class="n">epochs</code><code class="o">=</code><code class="mi">330</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">72</code><code class="p">,</code> <code class="n">verbose</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">shuffle</code><code class="o">=</code><code class="k">False</code><code class="p">)</code></pre>

<p>In terms of both learning and implementation, LSTM provides considerably more options for fine-tuning compared to ARIMA models. Although deep learning models have several advantages over traditional time series models, deep learning models are more complicated and difficult to train<a data-type="indexterm" data-startref="ix_Chapter5-asciidoc4" id="idm45174931124312"/><a data-type="indexterm" data-startref="ix_Chapter5-asciidoc3" id="idm45174931123704"/>.<sup><a data-type="noteref" id="idm45174931122968-marker" href="ch05.xhtml#idm45174931122968">7</a></sup></p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Modifying Time Series Data for Supervised Learning Models"><div class="sect2" id="idm45174930991640">
<h2>Modifying Time Series Data for Supervised Learning Models</h2>

<p><a data-type="indexterm" data-primary="time series models" data-secondary="modifying data for supervised learning model" id="idm45174930990264"/>A time series is a sequence of numbers that are ordered by a time index. Supervised learning is where we have input variables (<em>X</em>) and an output variable (<em>Y</em>). Given a sequence of numbers for a time series dataset, we can restructure the data into a set of predictor and predicted variables, just like in a supervised learning problem. We can do this by using previous time steps as input variables and using the next time step as the output variable. Let’s make this concrete with an example.</p>

<p>We can restructure a time series shown in the left table in <a data-type="xref" href="#Ts2SL">Figure 5-5</a> as a supervised learning problem by using the value at the previous time step to predict the value at the next time step. Once we’ve reorganized the time series dataset this way, the data would look like the table on the right.</p>

<figure><div id="Ts2SL" class="figure">
<img src="Images/mlbf_0505.png" alt="mlbf 0505" width="637" height="409"/>
<h6><span class="label">Figure 5-5. </span>Modifying time series for supervised learning models</h6>
</div></figure>

<p>We can see that the previous time step is the input (<em>X</em>) and the next time step is the output (<em>Y</em>) in our supervised learning problem. The order between the observations is preserved and must continue to be preserved when using this dataset to train a supervised model. We will delete the first and last row while training our supervised model as we don’t have values for either <em>X</em> or <em>Y</em>.</p>

<p>In Python, the main function to help transform time series data into a supervised learning problem is the <code>shift()</code> function from the Pandas library. We will demonstrate this approach in the case studies. The use of prior time steps to predict the next time step is called the <em>sliding window</em>, <em>time delay</em>, or <em>lag</em> method.<a data-type="indexterm" data-startref="ix_Chapter5-asciidoc1" id="idm45174930979384"/></p>

<p>Having discussed all the concepts of supervised learning and time series models, let us move to the case studies.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Case Study 1: Stock Price Prediction"><div class="sect1" id="CaseStudy1SR">
<h1>Case Study 1: Stock Price Prediction</h1>

<p><a data-type="indexterm" data-primary="stock price prediction" id="ix_Chapter5-asciidoc5"/>One of the biggest challenges in finance is predicting stock prices. However, with the onset of recent advancements in machine learning applications, the field has been evolving to utilize nondeterministic solutions that learn what is going on in order to make more accurate predictions. Machine learning techniques naturally lend 
<span class="keep-together">themselves</span> to stock price prediction based on historical data. Predictions can be made for a single time point ahead or for a set of future time points.</p>

<p><a data-type="indexterm" data-primary="stock price prediction" data-secondary="features useful for" id="idm45174930973880"/>As a high-level overview, other than the historical price of the stock itself, the features that are generally useful for stock price prediction are as follows:</p>
<dl>
<dt>Correlated assets</dt>
<dd>
<p>An organization depends on and interacts with many external factors, including its competitors, clients, the global economy, the geopolitical situation, fiscal and monetary policies, access to capital, and so on. Hence, its stock price may be correlated not only with the stock price of other companies but also with other assets such as commodities, FX, broad-based indices, or even fixed income 
<span class="keep-together">securities</span>.</p>
</dd>
<dt>Technical indicators</dt>
<dd>
<p>A lot of investors follow technical indicators. Moving average, exponential moving average, and momentum are the most popular indicators.</p>
</dd>
</dl>
<dl>
<dt>Fundamental analysis</dt>
<dd><p>Two primary data sources to glean features that can be used in fundamental analysis include:</p>
<dl>
<dt>Performance reports</dt>
<dd><p>Annual and quarterly reports of companies can be used to extract or determine key metrics, such as ROE (Return on Equity) and P/E (Price-to-Earnings).</p></dd>
<dt>News</dt>
<dd><p>News can indicate upcoming events that can potentially move the stock price in a certain direction.</p></dd>
</dl>
</dd>
</dl>

<p>In this case study, we will use various supervised learning–based models to predict the stock price of Microsoft using correlated assets and its own historical data. By the end of this case study, readers will be familiar with a general machine learning approach to stock prediction modeling, from gathering and cleaning data to building and tuning different models.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45174930962920">
<h5/>
<p>In this case study, we will focus on:</p>

<ul>
<li>
<p>Looking at various machine learning and time series models, ranging in complexity, that can be used to predict stock returns.</p>
</li>
<li>
<p>Visualization of the data using different kinds of charts (i.e., density, correlation, scatterplot, etc.)</p>
</li>
<li>
<p>Using deep learning (LSTM) models for time series forecasting.</p>
</li>
<li>
<p>Implementation of the grid search for time series models (i.e., ARIMA model).</p>
</li>
<li>
<p>Interpretation of the results and examining potential overfitting and underfitting of the data across the models.</p>
</li>
</ul>
</div></aside>
<img class="blueprint top" src="Images/bracket_top.png" width="1625" height="32"/>








<section data-type="sect2" class="blueprint" data-pdf-bookmark="Blueprint for Using Supervised Learning Models to Predict a Stock Price"><div class="sect2" id="idm45174930955400">
<h2>Blueprint for Using Supervised Learning Models to Predict a Stock Price</h2>










<section data-type="sect3" data-pdf-bookmark="1. Problem definition"><div class="sect3" id="idm45174930953720">
<h3>1. Problem definition</h3>

<p>In the supervised regression framework used for this case study, the weekly return of Microsoft stock is the predicted variable. We need to understand what affects Microsoft stock price and incorporate as much information into the model. Out of correlated assets, technical indicators, and fundamental analysis (discussed in the section before), we will focus on correlated assets as features in this case study.<sup><a data-type="noteref" id="idm45174930951640-marker" href="ch05.xhtml#idm45174930951640">8</a></sup></p>

<p>For this case study, other than the historical data of Microsoft, the independent variables used are the following potentially correlated assets:</p>
<dl>
<dt>Stocks</dt>
<dd>
<p>IBM (IBM) and Alphabet (GOOGL)</p>
</dd>
<dt>Currency<sup><a data-type="noteref" id="idm45174930944792-marker" href="ch05.xhtml#idm45174930944792">9</a></sup></dt>
<dd>
<p>USD/JPY and GBP/USD</p>
</dd>
<dt>Indices</dt>
<dd>
<p>S&amp;P 500, Dow Jones, and VIX</p>
</dd>
</dl>

<p>The dataset used for this case study is extracted from Yahoo Finance and <a href="https://fred.stlouisfed.org">the FRED website</a>. In addition to predicting the stock price accurately, this case study will also demonstrate the infrastructure and framework for each step of time series and supervised regression–based modeling for stock price prediction. We will use the daily closing price of the last 10 years, from 2010 onward.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="2. Getting started—loading the data and Python packages"><div class="sect3" id="idm45174930940264">
<h3>2. Getting started—loading the data and Python packages</h3>












<section data-type="sect4" data-pdf-bookmark="2.1. Loading the Python packages"><div class="sect4" id="idm45174930939256">
<h4>2.1. Loading the Python packages</h4>

<p><a data-type="indexterm" data-primary="stock price prediction" data-secondary="loading Python packages" id="idm45174930937608"/>The list of the libraries used for data loading, data analysis, data preparation, model evaluation, and model tuning are shown below. The packages used for different purposes have been segregated in the Python code that follows. The details of most of these packages and functions were provided in Chapters <a href="ch02.xhtml#Chapter2">2</a> and <a href="ch04.xhtml#Chapter4">4</a>. The use of these packages will be demonstrated in different steps of the model development process.</p>

<p><code>Function and modules for the supervised regression models</code></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="k">import</code> <code class="n">LinearRegression</code>
<code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="k">import</code> <code class="n">Lasso</code>
<code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="k">import</code> <code class="n">ElasticNet</code>
<code class="kn">from</code> <code class="nn">sklearn.tree</code> <code class="k">import</code> <code class="n">DecisionTreeRegressor</code>
<code class="kn">from</code> <code class="nn">sklearn.neighbors</code> <code class="k">import</code> <code class="n">KNeighborsRegressor</code>
<code class="kn">from</code> <code class="nn">sklearn.svm</code> <code class="k">import</code> <code class="n">SVR</code>
<code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="k">import</code> <code class="n">RandomForestRegressor</code>
<code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="k">import</code> <code class="n">GradientBoostingRegressor</code>
<code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="k">import</code> <code class="n">ExtraTreesRegressor</code>
<code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="k">import</code> <code class="n">AdaBoostRegressor</code>
<code class="kn">from</code> <code class="nn">sklearn.neural_network</code> <code class="k">import</code> <code class="n">MLPRegressor</code></pre>

<p><code>Function and modules for data analysis and model evaluation</code></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="k">import</code> <code class="n">train_test_split</code>
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="k">import</code> <code class="n">KFold</code>
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="k">import</code> <code class="n">cross_val_score</code>
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="k">import</code> <code class="n">GridSearchCV</code>
<code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="k">import</code> <code class="n">mean_squared_error</code>
<code class="kn">from</code> <code class="nn">sklearn.feature_selection</code> <code class="k">import</code> <code class="n">SelectKBest</code>
<code class="kn">from</code> <code class="nn">sklearn.feature_selection</code> <code class="k">import</code> <code class="n">chi2</code><code class="p">,</code> <code class="n">f_regression</code></pre>

<p><code>Function and modules for deep learning models</code></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">from</code> <code class="nn">keras.models</code> <code class="k">import</code> <code class="n">Sequential</code>
<code class="kn">from</code> <code class="nn">keras.layers</code> <code class="k">import</code> <code class="n">Dense</code>
<code class="kn">from</code> <code class="nn">keras.optimizers</code> <code class="k">import</code> <code class="n">SGD</code>
<code class="kn">from</code> <code class="nn">keras.layers</code> <code class="k">import</code> <code class="n">LSTM</code>
<code class="kn">from</code> <code class="nn">keras.wrappers.scikit_learn</code> <code class="k">import</code> <code class="n">KerasRegressor</code></pre>

<p><code>Function and modules for time series models</code></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">from</code> <code class="nn">statsmodels.tsa.arima_model</code> <code class="k">import</code> <code class="n">ARIMA</code>
<code class="kn">import</code> <code class="nn">statsmodels.api</code> <code class="k">as</code> <code class="nn">sm</code></pre>

<p><code>Function and modules for data preparation and visualization</code></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c"># pandas, pandas_datareader, numpy and matplotlib</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">import</code> <code class="nn">pandas</code> <code class="k">as</code> <code class="nn">pd</code>
<code class="kn">import</code> <code class="nn">pandas_datareader.data</code> <code class="k">as</code> <code class="nn">web</code>
<code class="kn">from</code> <code class="nn">matplotlib</code> <code class="k">import</code> <code class="n">pyplot</code>
<code class="kn">from</code> <code class="nn">pandas.plotting</code> <code class="k">import</code> <code class="n">scatter_matrix</code>
<code class="kn">import</code> <code class="nn">seaborn</code> <code class="k">as</code> <code class="nn">sns</code>
<code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="k">import</code> <code class="n">StandardScaler</code>
<code class="kn">from</code> <code class="nn">pandas.plotting</code> <code class="k">import</code> <code class="n">scatter_matrix</code>
<code class="kn">from</code> <code class="nn">statsmodels.graphics.tsaplots</code> <code class="k">import</code> <code class="n">plot_acf</code></pre>
</div></section>













<section data-type="sect4" data-pdf-bookmark="2.2. Loading the data"><div class="sect4" id="idm45174930938632">
<h4>2.2. Loading the data</h4>

<p><a data-type="indexterm" data-primary="stock price prediction" data-secondary="loading data" id="idm45174930597048"/>One of the most important steps in machine learning and predictive modeling is gathering good data. The following steps demonstrate the loading of data from the Yahoo Finance and FRED websites using the Pandas <code>DataReader</code> function:<sup><a data-type="noteref" id="idm45174930595336-marker" href="ch05.xhtml#idm45174930595336">10</a></sup></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">stk_tickers</code> <code class="o">=</code> <code class="p">[</code><code class="s">'MSFT'</code><code class="p">,</code> <code class="s">'IBM'</code><code class="p">,</code> <code class="s">'GOOGL'</code><code class="p">]</code>
<code class="n">ccy_tickers</code> <code class="o">=</code> <code class="p">[</code><code class="s">'DEXJPUS'</code><code class="p">,</code> <code class="s">'DEXUSUK'</code><code class="p">]</code>
<code class="n">idx_tickers</code> <code class="o">=</code> <code class="p">[</code><code class="s">'SP500'</code><code class="p">,</code> <code class="s">'DJIA'</code><code class="p">,</code> <code class="s">'VIXCLS'</code><code class="p">]</code>

<code class="n">stk_data</code> <code class="o">=</code> <code class="n">web</code><code class="o">.</code><code class="n">DataReader</code><code class="p">(</code><code class="n">stk_tickers</code><code class="p">,</code> <code class="s">'yahoo'</code><code class="p">)</code>
<code class="n">ccy_data</code> <code class="o">=</code> <code class="n">web</code><code class="o">.</code><code class="n">DataReader</code><code class="p">(</code><code class="n">ccy_tickers</code><code class="p">,</code> <code class="s">'fred'</code><code class="p">)</code>
<code class="n">idx_data</code> <code class="o">=</code> <code class="n">web</code><code class="o">.</code><code class="n">DataReader</code><code class="p">(</code><code class="n">idx_tickers</code><code class="p">,</code> <code class="s">'fred'</code><code class="p">)</code></pre>

<p>Next, we define our dependent <em>(Y)</em> and independent <em>(X)</em> variables. The predicted variable is the weekly return of Microsoft (MSFT). The number of trading days in a week is assumed to be five, and we compute the return using five trading days. For independent variables we use the correlated assets and the historical return of MSFT at different frequencies.</p>

<p>The variables used as independent variables are lagged five-day return of stocks (IBM and GOOG), currencies (USD/JPY and GBP/USD), and indices (S&amp;P 500, Dow Jones, and VIX), along with lagged 5-day, 15-day, 30-day and 60-day return of MSFT.</p>

<p>The lagged five-day variables embed the time series component by using a time-delay approach, where the lagged variable is included as one of the independent variables. This step is reframing the time series data into a supervised regression–based model framework.</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">return_period</code> <code class="o">=</code> <code class="mi">5</code>
<code class="n">Y</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">log</code><code class="p">(</code><code class="n">stk_data</code><code class="o">.</code><code class="n">loc</code><code class="p">[:,</code> <code class="p">(</code><code class="s">'Adj Close'</code><code class="p">,</code> <code class="s">'MSFT'</code><code class="p">)])</code><code class="o">.</code><code class="n">diff</code><code class="p">(</code><code class="n">return_period</code><code class="p">)</code><code class="o">.</code>\
<code class="n">shift</code><code class="p">(</code><code class="o">-</code><code class="n">return_period</code><code class="p">)</code>
<code class="n">Y</code><code class="o">.</code><code class="n">name</code> <code class="o">=</code> <code class="n">Y</code><code class="o">.</code><code class="n">name</code><code class="p">[</code><code class="o">-</code><code class="mi">1</code><code class="p">]</code><code class="o">+</code><code class="s">'_pred'</code>

<code class="n">X1</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">log</code><code class="p">(</code><code class="n">stk_data</code><code class="o">.</code><code class="n">loc</code><code class="p">[:,</code> <code class="p">(</code><code class="s">'Adj Close'</code><code class="p">,</code> <code class="p">(</code><code class="s">'GOOGL'</code><code class="p">,</code> <code class="s">'IBM'</code><code class="p">))])</code><code class="o">.</code><code class="n">diff</code><code class="p">(</code><code class="n">return_period</code><code class="p">)</code>
<code class="n">X1</code><code class="o">.</code><code class="n">columns</code> <code class="o">=</code> <code class="n">X1</code><code class="o">.</code><code class="n">columns</code><code class="o">.</code><code class="n">droplevel</code><code class="p">()</code>
<code class="n">X2</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">log</code><code class="p">(</code><code class="n">ccy_data</code><code class="p">)</code><code class="o">.</code><code class="n">diff</code><code class="p">(</code><code class="n">return_period</code><code class="p">)</code>
<code class="n">X3</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">log</code><code class="p">(</code><code class="n">idx_data</code><code class="p">)</code><code class="o">.</code><code class="n">diff</code><code class="p">(</code><code class="n">return_period</code><code class="p">)</code>

<code class="n">X4</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">concat</code><code class="p">([</code><code class="n">np</code><code class="o">.</code><code class="n">log</code><code class="p">(</code><code class="n">stk_data</code><code class="o">.</code><code class="n">loc</code><code class="p">[:,</code> <code class="p">(</code><code class="s">'Adj Close'</code><code class="p">,</code> <code class="s">'MSFT'</code><code class="p">)])</code><code class="o">.</code><code class="n">diff</code><code class="p">(</code><code class="n">i</code><code class="p">)</code> \
<code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="p">[</code><code class="n">return_period</code><code class="p">,</code> <code class="n">return_period</code><code class="o">*</code><code class="mi">3</code><code class="p">,</code>\
<code class="n">return_period</code><code class="o">*</code><code class="mi">6</code><code class="p">,</code> <code class="n">return_period</code><code class="o">*</code><code class="mi">12</code><code class="p">]],</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code><code class="o">.</code><code class="n">dropna</code><code class="p">()</code>
<code class="n">X4</code><code class="o">.</code><code class="n">columns</code> <code class="o">=</code> <code class="p">[</code><code class="s">'MSFT_DT'</code><code class="p">,</code> <code class="s">'MSFT_3DT'</code><code class="p">,</code> <code class="s">'MSFT_6DT'</code><code class="p">,</code> <code class="s">'MSFT_12DT'</code><code class="p">]</code>

<code class="n">X</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">concat</code><code class="p">([</code><code class="n">X1</code><code class="p">,</code> <code class="n">X2</code><code class="p">,</code> <code class="n">X3</code><code class="p">,</code> <code class="n">X4</code><code class="p">],</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>

<code class="n">dataset</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">concat</code><code class="p">([</code><code class="n">Y</code><code class="p">,</code> <code class="n">X</code><code class="p">],</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code><code class="o">.</code><code class="n">dropna</code><code class="p">()</code><code class="o">.</code><code class="n">iloc</code><code class="p">[::</code><code class="n">return_period</code><code class="p">,</code> <code class="p">:]</code>
<code class="n">Y</code> <code class="o">=</code> <code class="n">dataset</code><code class="o">.</code><code class="n">loc</code><code class="p">[:,</code> <code class="n">Y</code><code class="o">.</code><code class="n">name</code><code class="p">]</code>
<code class="n">X</code> <code class="o">=</code> <code class="n">dataset</code><code class="o">.</code><code class="n">loc</code><code class="p">[:,</code> <code class="n">X</code><code class="o">.</code><code class="n">columns</code><code class="p">]</code></pre>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="3. Exploratory data analysis"><div class="sect3" id="idm45174930522952">
<h3>3. Exploratory data analysis</h3>

<p><a data-type="indexterm" data-primary="stock price prediction" data-secondary="exploratory data analysis" id="ix_Chapter5-asciidoc6"/>We will look at descriptive statistics, data visualization, and time series analysis in this section.</p>












<section data-type="sect4" data-pdf-bookmark="3.1. Descriptive statistics"><div class="sect4" id="idm45174930215256">
<h4>3.1. Descriptive statistics</h4>

<p><a data-type="indexterm" data-primary="stock price prediction" data-secondary="descriptive statistics" id="idm45174930213880"/>Let’s have a look at the dataset we have:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">dataset</code><code class="o">.</code><code class="n">head</code><code class="p">()</code></pre>

<p><code>Output</code></p>

<figure><div class="figure">
<img src="Images/mlbf_05in01.png" alt="mlbf 05in01" width="966" height="155"/>
<h6/>
</div></figure>

<p>The variable MSFT_pred is the return of Microsoft stock and is the predicted variable. The dataset contains the lagged series of other correlated stocks, currencies, and indices. Additionally, it also consists of the lagged historical returns of MSFT.</p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="3.2. Data visualization"><div class="sect4" id="idm45174930209992">
<h4>3.2. Data visualization</h4>

<p><a data-type="indexterm" data-primary="stock price prediction" data-secondary="data visualization" id="ix_Chapter5-asciidoc7"/>The fastest way to learn more about the data is to visualize it. The visualization involves independently understanding each attribute of the dataset. We will look at the scatterplot and the correlation matrix. These plots give us a sense of the interdependence of the data. Correlation can be calculated and displayed for each pair of the variables by creating a correlation matrix. Hence, besides the relationship between independent and dependent variables, it also shows the correlation among the independent variables. This is useful to know because some machine learning algorithms like linear and logistic regression can have poor performance if there are highly correlated input variables in the data:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">correlation</code> <code class="o">=</code> <code class="n">dataset</code><code class="o">.</code><code class="n">corr</code><code class="p">()</code>
<code class="n">pyplot</code><code class="o">.</code><code class="n">figure</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">15</code><code class="p">,</code><code class="mi">15</code><code class="p">))</code>
<code class="n">pyplot</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="s">'Correlation Matrix'</code><code class="p">)</code>
<code class="n">sns</code><code class="o">.</code><code class="n">heatmap</code><code class="p">(</code><code class="n">correlation</code><code class="p">,</code> <code class="n">vmax</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">square</code><code class="o">=</code><code class="k">True</code><code class="p">,</code><code class="n">annot</code><code class="o">=</code><code class="k">True</code><code class="p">,</code><code class="n">cmap</code><code class="o">=</code><code class="s">'cubehelix'</code><code class="p">)</code></pre>

<p class="pagebreak-before"><code>Output</code></p>

<figure><div class="figure">
<img src="Images/mlbf_05in02.png" alt="mlbf 05in02" width="800" height="811"/>
<h6/>
</div></figure>

<p>Looking at the correlation plot (full-size version available on <a href="https://oreil.ly/g3wVU">GitHub</a>), we see some correlation of the predicted variable with the lagged 5-day, 15-day, 30-day, and 60-day returns of MSFT. Also, we see a higher negative correlation of many asset returns versus VIX, which is intuitive.</p>

<p>Next, we can visualize the relationship between all the variables in the regression using the scatterplot matrix shown below:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">pyplot</code><code class="o">.</code><code class="n">figure</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">15</code><code class="p">,</code><code class="mi">15</code><code class="p">))</code>
<code class="n">scatter_matrix</code><code class="p">(</code><code class="n">dataset</code><code class="p">,</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">12</code><code class="p">,</code><code class="mi">12</code><code class="p">))</code>
<code class="n">pyplot</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>

<p class="pagebreak-before"><code>Output</code></p>

<figure><div class="figure">
<img src="Images/mlbf_05in03.png" alt="mlbf 05in03" width="785" height="698"/>
<h6/>
</div></figure>

<p>Looking at the scatterplot (full-size version available on <a href="https://oreil.ly/g3wVU">GitHub</a>), we see some linear relationship of the predicted variable with the lagged 15-day, 30-day, and 60-day returns of MSFT. Otherwise, we do not see any special relationship between our predicted variable and the features.<a data-type="indexterm" data-startref="ix_Chapter5-asciidoc7" id="idm45174930114264"/></p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="3.3. Time series analysis"><div class="sect4" id="idm45174930209368">
<h4>3.3. Time series analysis</h4>

<p><a data-type="indexterm" data-primary="stock price prediction" data-secondary="time series analysis" id="idm45174930112184"/>Next, we delve into the time series analysis and look at the decomposition of the time series of the predicted variable into trend and seasonality components:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">res</code> <code class="o">=</code> <code class="n">sm</code><code class="o">.</code><code class="n">tsa</code><code class="o">.</code><code class="n">seasonal_decompose</code><code class="p">(</code><code class="n">Y</code><code class="p">,</code><code class="n">freq</code><code class="o">=</code><code class="mi">52</code><code class="p">)</code>
<code class="n">fig</code> <code class="o">=</code> <code class="n">res</code><code class="o">.</code><code class="n">plot</code><code class="p">()</code>
<code class="n">fig</code><code class="o">.</code><code class="n">set_figheight</code><code class="p">(</code><code class="mi">8</code><code class="p">)</code>
<code class="n">fig</code><code class="o">.</code><code class="n">set_figwidth</code><code class="p">(</code><code class="mi">15</code><code class="p">)</code>
<code class="n">pyplot</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>

<p class="pagebreak-before"><code>Output</code></p>

<figure><div class="figure">
<img src="Images/mlbf_05in04.png" alt="mlbf 05in04" width="933" height="496"/>
<h6/>
</div></figure>

<p>We can see that for MSFT there has been a general upward trend in the return series. This may be due to the large run-up of MSFT in the recent years, causing more positive weekly return data points than negative.<sup><a data-type="noteref" id="idm45174929976312-marker" href="ch05.xhtml#idm45174929976312">11</a></sup> The trend may show up in the constant/bias terms in our models. The residual (or white noise) term is relatively small over the entire time series.<a data-type="indexterm" data-startref="ix_Chapter5-asciidoc6" id="idm45174929975336"/></p>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="4. Data preparation"><div class="sect3" id="idm45174929974536">
<h3>4. Data preparation</h3>

<p><a data-type="indexterm" data-primary="stock price prediction" data-secondary="data preparation" id="idm45174929973368"/>This step typically involves data processing, data cleaning, looking at feature importance, and performing feature reduction. The data obtained for this case study is relatively clean and doesn’t require further processing. Feature reduction might be useful here, but given the relatively small number of variables considered, we will keep all of them as is. We will demonstrate data preparation in some of the subsequent case studies in detail.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="5. Evaluate models"><div class="sect3" id="idm45174929971544">
<h3>5. Evaluate models</h3>












<section data-type="sect4" data-pdf-bookmark="5.1. Train-test split and evaluation metrics"><div class="sect4" id="idm45174929970328">
<h4>5.1. Train-test split and evaluation metrics</h4>

<p><a data-type="indexterm" data-primary="stock price prediction" data-secondary="evaluation of models" id="ix_Chapter5-asciidoc8"/>As described in <a data-type="xref" href="ch02.xhtml#Chapter2">Chapter 2</a>, it is a good idea to partition the original dataset into a <em>training set</em> and a <em>test set</em>. The test set is a sample of the data that we hold back from our analysis and modeling. We use it right at the end of our project to confirm the performance of our final model. It is the final test that gives us confidence on our estimates of accuracy on unseen data. We will use 80% of the dataset for modeling and use 20% for testing.
With time series data, the sequence of values is important. So we do not distribute the dataset into training and test sets in random fashion, but we select an arbitrary split point in the ordered list of observations and create two new datasets:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">validation_size</code> <code class="o">=</code> <code class="mf">0.2</code>
<code class="n">train_size</code> <code class="o">=</code> <code class="nb">int</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">X</code><code class="p">)</code> <code class="o">*</code> <code class="p">(</code><code class="mi">1</code><code class="o">-</code><code class="n">validation_size</code><code class="p">))</code>
<code class="n">X_train</code><code class="p">,</code> <code class="n">X_test</code> <code class="o">=</code> <code class="n">X</code><code class="p">[</code><code class="mi">0</code><code class="p">:</code><code class="n">train_size</code><code class="p">],</code> <code class="n">X</code><code class="p">[</code><code class="n">train_size</code><code class="p">:</code><code class="nb">len</code><code class="p">(</code><code class="n">X</code><code class="p">)]</code>
<code class="n">Y_train</code><code class="p">,</code> <code class="n">Y_test</code> <code class="o">=</code> <code class="n">Y</code><code class="p">[</code><code class="mi">0</code><code class="p">:</code><code class="n">train_size</code><code class="p">],</code> <code class="n">Y</code><code class="p">[</code><code class="n">train_size</code><code class="p">:</code><code class="nb">len</code><code class="p">(</code><code class="n">X</code><code class="p">)]</code></pre>
</div></section>













<section data-type="sect4" data-pdf-bookmark="5.2. Test options and evaluation metrics"><div class="sect4" id="idm45174929963976">
<h4>5.2. Test options and evaluation metrics</h4>

<p>To optimize the various hyperparameters of the models, we use ten-fold cross validation (CV) and recalculate the results ten times to account for the inherent randomness in some of the models and the CV process. We will evaluate algorithms using the mean squared error metric. This metric gives an idea of the performance of the supervised regression models. All these concepts, including cross validation and evaluation metrics, have been described in <a data-type="xref" href="ch04.xhtml#Chapter4">Chapter 4</a>:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">num_folds</code> <code class="o">=</code> <code class="mi">10</code>
<code class="n">scoring</code> <code class="o">=</code> <code class="s">'neg_mean_squared_error'</code></pre>
</div></section>













<section data-type="sect4" data-pdf-bookmark="5.3. Compare models and algorithms"><div class="sect4" id="idm45174929884792">
<h4>5.3. Compare models and algorithms</h4>

<p>Now that we have completed the data loading and designed the test harness, we need to choose a model.</p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="5.3.1. Machine learning models from Scikit-learn"><div class="sect4" id="idm45174929882168">
<h4>5.3.1. Machine learning models from Scikit-learn</h4>

<p>In this step, the supervised regression models are implemented using the sklearn package:</p>

<p><code>Regression and tree regression algorithms</code></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">models</code> <code class="o">=</code> <code class="p">[]</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'LR'</code><code class="p">,</code> <code class="n">LinearRegression</code><code class="p">()))</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'LASSO'</code><code class="p">,</code> <code class="n">Lasso</code><code class="p">()))</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'EN'</code><code class="p">,</code> <code class="n">ElasticNet</code><code class="p">()))</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'KNN'</code><code class="p">,</code> <code class="n">KNeighborsRegressor</code><code class="p">()))</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'CART'</code><code class="p">,</code> <code class="n">DecisionTreeRegressor</code><code class="p">()))</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'SVR'</code><code class="p">,</code> <code class="n">SVR</code><code class="p">()))</code></pre>

<p><code>Neural network algorithms</code></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'MLP'</code><code class="p">,</code> <code class="n">MLPRegressor</code><code class="p">()))</code></pre>

<p><code>Ensemble models</code></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c"># Boosting methods</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'ABR'</code><code class="p">,</code> <code class="n">AdaBoostRegressor</code><code class="p">()))</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'GBR'</code><code class="p">,</code> <code class="n">GradientBoostingRegressor</code><code class="p">()))</code>
<code class="c"># Bagging methods</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'RFR'</code><code class="p">,</code> <code class="n">RandomForestRegressor</code><code class="p">()))</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'ETR'</code><code class="p">,</code> <code class="n">ExtraTreesRegressor</code><code class="p">()))</code></pre>

<p class="pagebreak-before">Once we have selected all the models, we loop over each of them. First, we run the <em>k</em>-fold analysis. Next, we run the model on the entire training and testing dataset.</p>

<p>All the algorithms use default tuning parameters. We will calculate the mean and standard deviation of the evaluation metric for each algorithm and collect the results for model comparison later:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">names</code> <code class="o">=</code> <code class="p">[]</code>
<code class="n">kfold_results</code> <code class="o">=</code> <code class="p">[]</code>
<code class="n">test_results</code> <code class="o">=</code> <code class="p">[]</code>
<code class="n">train_results</code> <code class="o">=</code> <code class="p">[]</code>
<code class="k">for</code> <code class="n">name</code><code class="p">,</code> <code class="n">model</code> <code class="ow">in</code> <code class="n">models</code><code class="p">:</code>
    <code class="n">names</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">name</code><code class="p">)</code>
    <code class="c">## k-fold analysis:</code>
    <code class="n">kfold</code> <code class="o">=</code> <code class="n">KFold</code><code class="p">(</code><code class="n">n_splits</code><code class="o">=</code><code class="n">num_folds</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="n">seed</code><code class="p">)</code>
    <code class="c">#converted mean squared error to positive. The lower the better</code>
    <code class="n">cv_results</code> <code class="o">=</code> <code class="o">-</code><code class="mi">1</code><code class="o">*</code> <code class="n">cross_val_score</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">X_train</code><code class="p">,</code> <code class="n">Y_train</code><code class="p">,</code> <code class="n">cv</code><code class="o">=</code><code class="n">kfold</code><code class="p">,</code> \
      <code class="n">scoring</code><code class="o">=</code><code class="n">scoring</code><code class="p">)</code>
    <code class="n">kfold_results</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">cv_results</code><code class="p">)</code>
    <code class="c"># Full Training period</code>
    <code class="n">res</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">Y_train</code><code class="p">)</code>
    <code class="n">train_result</code> <code class="o">=</code> <code class="n">mean_squared_error</code><code class="p">(</code><code class="n">res</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_train</code><code class="p">),</code> <code class="n">Y_train</code><code class="p">)</code>
    <code class="n">train_results</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">train_result</code><code class="p">)</code>
    <code class="c"># Test results</code>
    <code class="n">test_result</code> <code class="o">=</code> <code class="n">mean_squared_error</code><code class="p">(</code><code class="n">res</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test</code><code class="p">),</code> <code class="n">Y_test</code><code class="p">)</code>
    <code class="n">test_results</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">test_result</code><code class="p">)</code></pre>

<p>Let’s compare the algorithms by looking at the cross validation results:</p>

<p><code>Cross validation results</code></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">fig</code> <code class="o">=</code> <code class="n">pyplot</code><code class="o">.</code><code class="n">figure</code><code class="p">()</code>
<code class="n">fig</code><code class="o">.</code><code class="n">suptitle</code><code class="p">(</code><code class="s">'Algorithm Comparison: Kfold results'</code><code class="p">)</code>
<code class="n">ax</code> <code class="o">=</code> <code class="n">fig</code><code class="o">.</code><code class="n">add_subplot</code><code class="p">(</code><code class="mi">111</code><code class="p">)</code>
<code class="n">pyplot</code><code class="o">.</code><code class="n">boxplot</code><code class="p">(</code><code class="n">kfold_results</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_xticklabels</code><code class="p">(</code><code class="n">names</code><code class="p">)</code>
<code class="n">fig</code><code class="o">.</code><code class="n">set_size_inches</code><code class="p">(</code><code class="mi">15</code><code class="p">,</code><code class="mi">8</code><code class="p">)</code>
<code class="n">pyplot</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>

<p class="pagebreak-before"><code>Output</code></p>

<figure><div class="figure">
<img src="Images/mlbf_05in05.png" alt="mlbf 05in05" width="945" height="496"/>
<h6/>
</div></figure>

<p><a data-type="indexterm" data-primary="elastic net (EN)" id="idm45174929368456"/><a data-type="indexterm" data-primary="lasso regression (LASSO)" id="idm45174929367752"/>Although the results of a couple of the models look good, we see that the linear regression and the regularized regression including the lasso regression (LASSO) and  elastic net (EN) seem to perform best. This indicates a strong linear relationship between the dependent and independent variables. Going back to the exploratory analysis, we saw a good correlation and linear relationship of the target variables with the different lagged MSFT variables.</p>

<p>Let us look at the errors of the test set as well:</p>

<p><code>Training and test error</code></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c"># compare algorithms</code>
<code class="n">fig</code> <code class="o">=</code> <code class="n">pyplot</code><code class="o">.</code><code class="n">figure</code><code class="p">()</code>

<code class="n">ind</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">arange</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">names</code><code class="p">))</code>  <code class="c"># the x locations for the groups</code>
<code class="n">width</code> <code class="o">=</code> <code class="mf">0.35</code>  <code class="c"># the width of the bars</code>

<code class="n">fig</code><code class="o">.</code><code class="n">suptitle</code><code class="p">(</code><code class="s">'Algorithm Comparison'</code><code class="p">)</code>
<code class="n">ax</code> <code class="o">=</code> <code class="n">fig</code><code class="o">.</code><code class="n">add_subplot</code><code class="p">(</code><code class="mi">111</code><code class="p">)</code>
<code class="n">pyplot</code><code class="o">.</code><code class="n">bar</code><code class="p">(</code><code class="n">ind</code> <code class="o">-</code> <code class="n">width</code><code class="o">/</code><code class="mi">2</code><code class="p">,</code> <code class="n">train_results</code><code class="p">,</code>  <code class="n">width</code><code class="o">=</code><code class="n">width</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s">'Train Error'</code><code class="p">)</code>
<code class="n">pyplot</code><code class="o">.</code><code class="n">bar</code><code class="p">(</code><code class="n">ind</code> <code class="o">+</code> <code class="n">width</code><code class="o">/</code><code class="mi">2</code><code class="p">,</code> <code class="n">test_results</code><code class="p">,</code> <code class="n">width</code><code class="o">=</code><code class="n">width</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s">'Test Error'</code><code class="p">)</code>
<code class="n">fig</code><code class="o">.</code><code class="n">set_size_inches</code><code class="p">(</code><code class="mi">15</code><code class="p">,</code><code class="mi">8</code><code class="p">)</code>
<code class="n">pyplot</code><code class="o">.</code><code class="n">legend</code><code class="p">()</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_xticks</code><code class="p">(</code><code class="n">ind</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_xticklabels</code><code class="p">(</code><code class="n">names</code><code class="p">)</code>
<code class="n">pyplot</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>

<p class="pagebreak-before"><code>Output</code></p>

<figure><div class="figure">
<img src="Images/mlbf_05in06.png" alt="mlbf 05in06" width="1183" height="614"/>
<h6/>
</div></figure>

<p>Examining the training and test error, we still see a stronger performance from the linear models. Some of the algorithms, such as the decision tree regressor (CART), overfit on the training data and produced very high error on the test set. Ensemble models such as gradient boosting regression (GBR) and random forest regression (RFR) have low bias but high variance. We also see that the artificial neural network algorithm (shown as MLP in the chart) shows higher errors in both the training and test sets. This is perhaps due to the linear relationship of the variables not captured accurately by ANN, improper hyperparameters, or insufficient training of the model. Our original intuition from the cross validation results and the scatterplots also seem to demonstrate a better performance of linear models.</p>

<p>We now look at some of the time series and deep learning models that can be used. Once we are done creating these, we will compare their performance against that of the supervised regression–based models. Due to the nature of time series models, we are not able to run a <em>k</em>-fold analysis. We can still compare our results to the other models based on the full training and testing results.</p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="5.3.2. Time series–based models: ARIMA and LSTM"><div class="sect4" id="idm45174929881224">
<h4>5.3.2. Time series–based models: ARIMA and LSTM</h4>

<p><a data-type="indexterm" data-primary="ARIMA (autoregressive integrated moving average) model" id="idm45174929296104"/>The models used so far already embed the time series component by using a time-delay approach, where the lagged variable is included as one of the independent variables. However, for the time series–based models we do not need the lagged variables of MSFT as the independent variables. Hence, as a first step we remove MSFT’s previous returns for these models. We use all other variables as the exogenous variables in these models.</p>

<p>Let us first prepare the dataset for ARIMA models by having only the correlated varriables as exogenous variables:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">X_train_ARIMA</code><code class="o">=</code><code class="n">X_train</code><code class="o">.</code><code class="n">loc</code><code class="p">[:,</code> <code class="p">[</code><code class="s">'GOOGL'</code><code class="p">,</code> <code class="s">'IBM'</code><code class="p">,</code> <code class="s">'DEXJPUS'</code><code class="p">,</code> <code class="s">'SP500'</code><code class="p">,</code> <code class="s">'DJIA'</code><code class="p">,</code> \
<code class="s">'VIXCLS'</code><code class="p">]]</code>
<code class="n">X_test_ARIMA</code><code class="o">=</code><code class="n">X_test</code><code class="o">.</code><code class="n">loc</code><code class="p">[:,</code> <code class="p">[</code><code class="s">'GOOGL'</code><code class="p">,</code> <code class="s">'IBM'</code><code class="p">,</code> <code class="s">'DEXJPUS'</code><code class="p">,</code> <code class="s">'SP500'</code><code class="p">,</code> <code class="s">'DJIA'</code><code class="p">,</code> \
<code class="s">'VIXCLS'</code><code class="p">]]</code>
<code class="n">tr_len</code> <code class="o">=</code> <code class="nb">len</code><code class="p">(</code><code class="n">X_train_ARIMA</code><code class="p">)</code>
<code class="n">te_len</code> <code class="o">=</code> <code class="nb">len</code><code class="p">(</code><code class="n">X_test_ARIMA</code><code class="p">)</code>
<code class="n">to_len</code> <code class="o">=</code> <code class="nb">len</code> <code class="p">(</code><code class="n">X</code><code class="p">)</code></pre>

<p>We now configure the ARIMA model with the order <em>(1,0,0)</em> and use the independent variables as the exogenous variables in the model. The version of the ARIMA model where the exogenous variables are also used is known as the <em>ARIMAX</em> model, where "<em>X</em>" represents exogenous variables:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">modelARIMA</code><code class="o">=</code><code class="n">ARIMA</code><code class="p">(</code><code class="n">endog</code><code class="o">=</code><code class="n">Y_train</code><code class="p">,</code><code class="n">exog</code><code class="o">=</code><code class="n">X_train_ARIMA</code><code class="p">,</code><code class="n">order</code><code class="o">=</code><code class="p">[</code><code class="mi">1</code><code class="p">,</code><code class="mi">0</code><code class="p">,</code><code class="mi">0</code><code class="p">])</code>
<code class="n">model_fit</code> <code class="o">=</code> <code class="n">modelARIMA</code><code class="o">.</code><code class="n">fit</code><code class="p">()</code></pre>

<p>Now we fit the ARIMA model:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">error_Training_ARIMA</code> <code class="o">=</code> <code class="n">mean_squared_error</code><code class="p">(</code><code class="n">Y_train</code><code class="p">,</code> <code class="n">model_fit</code><code class="o">.</code><code class="n">fittedvalues</code><code class="p">)</code>
<code class="n">predicted</code> <code class="o">=</code> <code class="n">model_fit</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">start</code> <code class="o">=</code> <code class="n">tr_len</code> <code class="o">-</code><code class="mi">1</code> <code class="p">,</code><code class="n">end</code> <code class="o">=</code> <code class="n">to_len</code> <code class="o">-</code><code class="mi">1</code><code class="p">,</code> \
  <code class="n">exog</code> <code class="o">=</code> <code class="n">X_test_ARIMA</code><code class="p">)[</code><code class="mi">1</code><code class="p">:]</code>
<code class="n">error_Test_ARIMA</code> <code class="o">=</code> <code class="n">mean_squared_error</code><code class="p">(</code><code class="n">Y_test</code><code class="p">,</code><code class="n">predicted</code><code class="p">)</code>
<code class="n">error_Test_ARIMA</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">0.0005931919240399084</pre>

<p>Error of this ARIMA model is reasonable.</p>

<p><a data-type="indexterm" data-primary="long short-term memory (LSTM)" data-secondary="stock price prediction" id="idm45174929048968"/>Now let’s prepare the dataset for the LSTM model. We need the data in the form of arrays of all the input variables and the output variables.</p>

<p>The logic behind the LSTM is that data is taken from the previous day (the data of all the other features for that day—correlated assets and the lagged variables of MSFT) and we try to predict the next day. Then we move the one-day window with one day and again predict the next day. We iterate like this over the whole dataset (of course in batches). The code below will create a dataset in which <em>X</em> is the set of independent variables at a given time (<em>t</em>) and <em>Y</em> is the target variable at the next time (<em>t + 1</em>):</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">seq_len</code> <code class="o">=</code> <code class="mi">2</code> <code class="c">#Length of the seq for the LSTM</code>

<code class="n">Y_train_LSTM</code><code class="p">,</code> <code class="n">Y_test_LSTM</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">Y_train</code><code class="p">)[</code><code class="n">seq_len</code><code class="o">-</code><code class="mi">1</code><code class="p">:],</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">Y_test</code><code class="p">)</code>
<code class="n">X_train_LSTM</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">zeros</code><code class="p">((</code><code class="n">X_train</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">+</code><code class="mi">1</code><code class="o">-</code><code class="n">seq_len</code><code class="p">,</code> <code class="n">seq_len</code><code class="p">,</code> <code class="n">X_train</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">]))</code>
<code class="n">X_test_LSTM</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">zeros</code><code class="p">((</code><code class="n">X_test</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="n">seq_len</code><code class="p">,</code> <code class="n">X</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">]))</code>
<code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">seq_len</code><code class="p">):</code>
    <code class="n">X_train_LSTM</code><code class="p">[:,</code> <code class="n">i</code><code class="p">,</code> <code class="p">:]</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">X_train</code><code class="p">)[</code><code class="n">i</code><code class="p">:</code><code class="n">X_train</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">+</code><code class="n">i</code><code class="o">+</code><code class="mi">1</code><code class="o">-</code><code class="n">seq_len</code><code class="p">,</code> <code class="p">:]</code>
    <code class="n">X_test_LSTM</code><code class="p">[:,</code> <code class="n">i</code><code class="p">,</code> <code class="p">:]</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>\
    <code class="p">[</code><code class="n">X_train</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">+</code><code class="n">i</code><code class="o">-</code><code class="mi">1</code><code class="p">:</code><code class="n">X</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">+</code><code class="n">i</code><code class="o">+</code><code class="mi">1</code><code class="o">-</code><code class="n">seq_len</code><code class="p">,</code> <code class="p">:]</code></pre>

<p>In the next step, we create the LSTM architecture. As we can see, the input of the LSTM is in <code>X_train_LSTM</code>, which goes into 50 hidden units in the LSTM layer and then is transformed to a single output—the stock return value. The hyperparameters (i.e., learning rate, optimizer, activation function, etc.) were discussed in <a data-type="xref" href="ch03.xhtml#Chapter3">Chapter 3</a> of the book:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c"># LSTM Network</code>
<code class="k">def</code> <code class="nf">create_LSTMmodel</code><code class="p">(</code><code class="n">learn_rate</code> <code class="o">=</code> <code class="mf">0.01</code><code class="p">,</code> <code class="n">momentum</code><code class="o">=</code><code class="mi">0</code><code class="p">):</code>
        <code class="c"># create model</code>
    <code class="n">model</code> <code class="o">=</code> <code class="n">Sequential</code><code class="p">()</code>
    <code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">LSTM</code><code class="p">(</code><code class="mi">50</code><code class="p">,</code> <code class="n">input_shape</code><code class="o">=</code><code class="p">(</code><code class="n">X_train_LSTM</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code>\
      <code class="n">X_train_LSTM</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">2</code><code class="p">])))</code>
    <code class="c">#More cells can be added if needed</code>
    <code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dense</code><code class="p">(</code><code class="mi">1</code><code class="p">))</code>
    <code class="n">optimizer</code> <code class="o">=</code> <code class="n">SGD</code><code class="p">(</code><code class="n">lr</code><code class="o">=</code><code class="n">learn_rate</code><code class="p">,</code> <code class="n">momentum</code><code class="o">=</code><code class="n">momentum</code><code class="p">)</code>
    <code class="n">model</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">loss</code><code class="o">=</code><code class="s">'mse'</code><code class="p">,</code> <code class="n">optimizer</code><code class="o">=</code><code class="s">'adam'</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">model</code>
<code class="n">LSTMModel</code> <code class="o">=</code> <code class="n">create_LSTMmodel</code><code class="p">(</code><code class="n">learn_rate</code> <code class="o">=</code> <code class="mf">0.01</code><code class="p">,</code> <code class="n">momentum</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
<code class="n">LSTMModel_fit</code> <code class="o">=</code> <code class="n">LSTMModel</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train_LSTM</code><code class="p">,</code> <code class="n">Y_train_LSTM</code><code class="p">,</code> \
  <code class="n">validation_data</code><code class="o">=</code><code class="p">(</code><code class="n">X_test_LSTM</code><code class="p">,</code> <code class="n">Y_test_LSTM</code><code class="p">),</code>\
  <code class="n">epochs</code><code class="o">=</code><code class="mi">330</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">72</code><code class="p">,</code> <code class="n">verbose</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">shuffle</code><code class="o">=</code><code class="k">False</code><code class="p">)</code></pre>

<p>Now we fit the LSTM model with the data and look at the change in the model performance metric over time simultaneously in the training set and the test set:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">pyplot</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">LSTMModel_fit</code><code class="o">.</code><code class="n">history</code><code class="p">[</code><code class="s">'loss'</code><code class="p">],</code> <code class="n">label</code><code class="o">=</code><code class="s">'train'</code><code class="p">,</code> <code class="p">)</code>
<code class="n">pyplot</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">LSTMModel_fit</code><code class="o">.</code><code class="n">history</code><code class="p">[</code><code class="s">'val_loss'</code><code class="p">],</code> <code class="s">'--'</code><code class="p">,</code><code class="n">label</code><code class="o">=</code><code class="s">'test'</code><code class="p">,)</code>
<code class="n">pyplot</code><code class="o">.</code><code class="n">legend</code><code class="p">()</code>
<code class="n">pyplot</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>

<p><code>Output</code></p>

<figure class="width-75"><div class="figure">
<img src="Images/mlbf_05in07.png" alt="mlbf 05in07" width="377" height="233"/>
<h6/>
</div></figure>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">error_Training_LSTM</code> <code class="o">=</code> <code class="n">mean_squared_error</code><code class="p">(</code><code class="n">Y_train_LSTM</code><code class="p">,</code>\
  <code class="n">LSTMModel</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_train_LSTM</code><code class="p">))</code>
<code class="n">predicted</code> <code class="o">=</code> <code class="n">LSTMModel</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test_LSTM</code><code class="p">)</code>
<code class="n">error_Test_LSTM</code> <code class="o">=</code> <code class="n">mean_squared_error</code><code class="p">(</code><code class="n">Y_test</code><code class="p">,</code><code class="n">predicted</code><code class="p">)</code></pre>

<p>Now, in order to compare the time series and the deep learning models, we append the result of these models to the results of the supervised regression–based models:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">test_results</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">error_Test_ARIMA</code><code class="p">)</code>
<code class="n">test_results</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">error_Test_LSTM</code><code class="p">)</code>

<code class="n">train_results</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">error_Training_ARIMA</code><code class="p">)</code>
<code class="n">train_results</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">error_Training_LSTM</code><code class="p">)</code>

<code class="n">names</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="s">"ARIMA"</code><code class="p">)</code>
<code class="n">names</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="s">"LSTM"</code><code class="p">)</code></pre>

<p><code>Output</code></p>

<figure><div class="figure">
<img src="Images/mlbf_05in08.png" alt="mlbf 05in08" width="1436" height="773"/>
<h6/>
</div></figure>

<p><a data-type="indexterm" data-primary="ARIMA (autoregressive integrated moving average) model" id="idm45174928483560"/>Looking at the chart, we find the time series–based ARIMA model comparable to the linear supervised regression models: linear regression (LR), lasso regression (LASSO), and elastic net (EN). This can primarily be due to the strong linear relationship as discussed before. The LSTM model performs decently; however, the ARIMA model outperforms the LSTM model in the test set. Hence, we select the ARIMA model for model tuning.<a data-type="indexterm" data-startref="ix_Chapter5-asciidoc8" id="idm45174928482184"/></p>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="6. Model tuning and grid search"><div class="sect3" id="idm45174929296792">
<h3>6. Model tuning and grid search</h3>

<p><a data-type="indexterm" data-primary="stock price prediction" data-secondary="model tuning and grid search" id="ix_Chapter5-asciidoc9"/>Let us perform the model tuning of the ARIMA model.</p>
<div data-type="tip"><h1>Model Tuning for the Supervised Learning or Time Series Models</h1>
<p>The detailed implementation of grid search for all the supervised learning–based models, along with the ARIMA and LSTM models, is provided in the Regression-Master template under the <a href="https://oreil.ly/9S8h_">GitHub repository for this book</a>. For the grid search of the ARIMA and LSTM models, refer to the “ARIMA and LSTM Grid Search” section of the Regression-Master template.</p>
</div>

<p>The ARIMA model is generally represented as ARIMA<em>(p,d,q) </em>model, where <em>p</em> is the order of the autoregressive part, <em>d</em> is the degree of first differencing involved, and <em>q</em> is the order of the moving average part.
The order of the ARIMA model was set to <em>(1,0,0)</em>. So we perform a grid search with different <em>p</em>, <em>d</em>, and <em>q</em> combinations in the ARIMA model’s order and select the combination that minimizes the fitting error:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="k">def</code> <code class="nf">evaluate_arima_model</code><code class="p">(</code><code class="n">arima_order</code><code class="p">):</code>
    <code class="c">#predicted = list()</code>
    <code class="n">modelARIMA</code><code class="o">=</code><code class="n">ARIMA</code><code class="p">(</code><code class="n">endog</code><code class="o">=</code><code class="n">Y_train</code><code class="p">,</code><code class="n">exog</code><code class="o">=</code><code class="n">X_train_ARIMA</code><code class="p">,</code><code class="n">order</code><code class="o">=</code><code class="n">arima_order</code><code class="p">)</code>
    <code class="n">model_fit</code> <code class="o">=</code> <code class="n">modelARIMA</code><code class="o">.</code><code class="n">fit</code><code class="p">()</code>
    <code class="n">error</code> <code class="o">=</code> <code class="n">mean_squared_error</code><code class="p">(</code><code class="n">Y_train</code><code class="p">,</code> <code class="n">model_fit</code><code class="o">.</code><code class="n">fittedvalues</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">error</code>

<code class="c"># evaluate combinations of p, d and q values for an ARIMA model</code>
<code class="k">def</code> <code class="nf">evaluate_models</code><code class="p">(</code><code class="n">p_values</code><code class="p">,</code> <code class="n">d_values</code><code class="p">,</code> <code class="n">q_values</code><code class="p">):</code>
    <code class="n">best_score</code><code class="p">,</code> <code class="n">best_cfg</code> <code class="o">=</code> <code class="nb">float</code><code class="p">(</code><code class="s">"inf"</code><code class="p">),</code> <code class="k">None</code>
    <code class="k">for</code> <code class="n">p</code> <code class="ow">in</code> <code class="n">p_values</code><code class="p">:</code>
        <code class="k">for</code> <code class="n">d</code> <code class="ow">in</code> <code class="n">d_values</code><code class="p">:</code>
            <code class="k">for</code> <code class="n">q</code> <code class="ow">in</code> <code class="n">q_values</code><code class="p">:</code>
                <code class="n">order</code> <code class="o">=</code> <code class="p">(</code><code class="n">p</code><code class="p">,</code><code class="n">d</code><code class="p">,</code><code class="n">q</code><code class="p">)</code>
                <code class="k">try</code><code class="p">:</code>
                    <code class="n">mse</code> <code class="o">=</code> <code class="n">evaluate_arima_model</code><code class="p">(</code><code class="n">order</code><code class="p">)</code>
                    <code class="k">if</code> <code class="n">mse</code> <code class="o">&lt;</code> <code class="n">best_score</code><code class="p">:</code>
                        <code class="n">best_score</code><code class="p">,</code> <code class="n">best_cfg</code> <code class="o">=</code> <code class="n">mse</code><code class="p">,</code> <code class="n">order</code>
                    <code class="nb">print</code><code class="p">(</code><code class="s">'ARIMA%s MSE=%.7f'</code> <code class="o">%</code> <code class="p">(</code><code class="n">order</code><code class="p">,</code><code class="n">mse</code><code class="p">))</code>
                <code class="k">except</code><code class="p">:</code>
                    <code class="k">continue</code>
    <code class="nb">print</code><code class="p">(</code><code class="s">'Best ARIMA%s MSE=%.7f'</code> <code class="o">%</code> <code class="p">(</code><code class="n">best_cfg</code><code class="p">,</code> <code class="n">best_score</code><code class="p">))</code>

<code class="c"># evaluate parameters</code>
<code class="n">p_values</code> <code class="o">=</code> <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">]</code>
<code class="n">d_values</code> <code class="o">=</code> <code class="nb">range</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">2</code><code class="p">)</code>
<code class="n">q_values</code> <code class="o">=</code> <code class="nb">range</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">2</code><code class="p">)</code>
<code class="n">warnings</code><code class="o">.</code><code class="n">filterwarnings</code><code class="p">(</code><code class="s">"ignore"</code><code class="p">)</code>
<code class="n">evaluate_models</code><code class="p">(</code><code class="n">p_values</code><code class="p">,</code> <code class="n">d_values</code><code class="p">,</code> <code class="n">q_values</code><code class="p">)</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">ARIMA(0, 0, 0) MSE=0.0009879
ARIMA(0, 0, 1) MSE=0.0009721
ARIMA(1, 0, 0) MSE=0.0009696
ARIMA(1, 0, 1) MSE=0.0009685
ARIMA(2, 0, 0) MSE=0.0009684
ARIMA(2, 0, 1) MSE=0.0009683
Best ARIMA(2, 0, 1) MSE=0.0009683</pre>

<p>We see that the ARIMA model with the order <em>(2,0,1)</em> is the best performer out of all the combinations tested in the grid search, although there isn’t a significant difference in the mean squared error (MSE) with other combinations. This means that the model with the autoregressive lag of two and moving average of one yields the best result. We should not forget the fact that there are other exogenous variables in the model that influence the order of the best ARIMA model as well.<a data-type="indexterm" data-startref="ix_Chapter5-asciidoc9" id="idm45174928283192"/></p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="7. Finalize the model"><div class="sect3" id="idm45174928282360">
<h3>7. Finalize the model</h3>

<p><a data-type="indexterm" data-primary="stock price prediction" data-secondary="finalizing the model" id="idm45174928281192"/>In the last step we will check the finalized model on the test set.</p>












<section data-type="sect4" data-pdf-bookmark="7.1. Results on the test dataset"><div class="sect4" id="idm45174928279960">
<h4>7.1. Results on the test dataset</h4>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c"># prepare model</code>
<code class="n">modelARIMA_tuned</code><code class="o">=</code><code class="n">ARIMA</code><code class="p">(</code><code class="n">endog</code><code class="o">=</code><code class="n">Y_train</code><code class="p">,</code><code class="n">exog</code><code class="o">=</code><code class="n">X_train_ARIMA</code><code class="p">,</code><code class="n">order</code><code class="o">=</code><code class="p">[</code><code class="mi">2</code><code class="p">,</code><code class="mi">0</code><code class="p">,</code><code class="mi">1</code><code class="p">])</code>
<code class="n">model_fit_tuned</code> <code class="o">=</code> <code class="n">modelARIMA_tuned</code><code class="o">.</code><code class="n">fit</code><code class="p">()</code></pre>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c"># estimate accuracy on validation set</code>
<code class="n">predicted_tuned</code> <code class="o">=</code> <code class="n">model_fit</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">start</code> <code class="o">=</code> <code class="n">tr_len</code> <code class="o">-</code><code class="mi">1</code> <code class="p">,</code>\
  <code class="n">end</code> <code class="o">=</code> <code class="n">to_len</code> <code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="n">exog</code> <code class="o">=</code> <code class="n">X_test_ARIMA</code><code class="p">)[</code><code class="mi">1</code><code class="p">:]</code>
<code class="nb">print</code><code class="p">(</code><code class="n">mean_squared_error</code><code class="p">(</code><code class="n">Y_test</code><code class="p">,</code><code class="n">predicted_tuned</code><code class="p">))</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">0.0005970582461404503</pre>

<p>The MSE of the model on the test set looks good and is actually less than that of the training set.</p>

<p>In the last step, we will visualize the output of the selected model and compare the modeled data against the actual data. In order to visualize the chart, we convert the return time series to a price time series. We also assume the price at the beginning of the test set as one for the sake of simplicity. Let us look at the plot of actual versus predicted data:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c"># plotting the actual data versus predicted data</code>
<code class="n">predicted_tuned</code><code class="o">.</code><code class="n">index</code> <code class="o">=</code> <code class="n">Y_test</code><code class="o">.</code><code class="n">index</code>
<code class="n">pyplot</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">exp</code><code class="p">(</code><code class="n">Y_test</code><code class="p">)</code><code class="o">.</code><code class="n">cumprod</code><code class="p">(),</code> <code class="s">'r'</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s">'actual'</code><code class="p">,)</code>

<code class="c"># plotting t, a separately</code>
<code class="n">pyplot</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">exp</code><code class="p">(</code><code class="n">predicted_tuned</code><code class="p">)</code><code class="o">.</code><code class="n">cumprod</code><code class="p">(),</code> <code class="s">'b--'</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s">'predicted'</code><code class="p">)</code>
<code class="n">pyplot</code><code class="o">.</code><code class="n">legend</code><code class="p">()</code>
<code class="n">pyplot</code><code class="o">.</code><code class="n">rcParams</code><code class="p">[</code><code class="s">"figure.figsize"</code><code class="p">]</code> <code class="o">=</code> <code class="p">(</code><code class="mi">8</code><code class="p">,</code><code class="mi">5</code><code class="p">)</code>
<code class="n">pyplot</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>

<figure class="width-90"><div class="figure">
<img src="Images/mlbf_05in09.png" alt="mlbf 05in09" width="488" height="288"/>
<h6/>
</div></figure>

<p>Looking at the chart, we clearly see the trend has been captured perfectly by the model. The predicted series is less volatile compared to the actual time series, and it aligns with the actual data for the first few months of the test set. A point to note is that the purpose of the model is to compute the next day’s return given the data observed up to the present day, and not to predict the stock price several days in the future given the current data. Hence, a deviation from the actual data is expected as we move away from the beginning of the test set. The model seems to perform well for the first few months, with deviation from the actual data increasing six to seven months after the beginning of the test set.</p>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="Conclusion"><div class="sect3" id="idm45174928021896">
<h3>Conclusion</h3>

<p>We can conclude that simple models—linear regression, regularized regression (i.e., Lasso and elastic net)—along with the time series models, such as ARIMA, are promising modeling approaches for stock price prediction problems. This approach helps us deal with overfitting and underfitting, which are some of the key challenges in predicting problems in finance.</p>

<p>We should also note that we can use a wider set of indicators, such as P/E ratio, trading volume, technical indicators, or news data, which might lead to better results. We will demonstrate this in some of the future case studies in the book.</p>

<p>Overall, we created a supervised-regression and time series modeling framework that allows us to perform stock price prediction using historical data. This framework generates results to analyze risk and profitability before risking any capital.<a data-type="indexterm" data-startref="ix_Chapter5-asciidoc5" id="idm45174928018600"/></p>
<img class="blueprint bottom" src="Images/bracket_bottom.png" width="1625" height="32"/>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Case Study 2: Derivative Pricing"><div class="sect1" id="CaseStudy2SR">
<h1>Case Study 2: Derivative Pricing</h1>

<p><a data-type="indexterm" data-primary="derivative pricing" id="ix_Chapter5-asciidoc10"/>In computational finance and risk management, several numerical methods (e.g., finite differences, fourier methods, and Monte Carlo simulation) are commonly used for the valuation of financial derivatives.</p>

<p><a data-type="indexterm" data-primary="Black–Scholes formula" id="idm45174928014008"/>The <em>Black-Scholes formula</em> is probably one of the most widely cited and used models in derivative pricing. Numerous variations and extensions of this formula are used to price many kinds of financial derivatives. However, the model is based on several assumptions. <a data-type="indexterm" data-primary="geometric Brownian motion (GBM)" id="idm45174928012504"/>It assumes a specific form of movement for the derivative price, namely a <em>Geometric Brownian Motion</em> (GBM). It also assumes a conditional payment at maturity of the option and economic constraints, such as no-arbitrage. Several other derivative pricing models have similarly impractical model assumptions. Finance practitioners are well aware that these assumptions are violated in practice, and prices from these models are further adjusted using practitioner judgment.</p>

<p>Another aspect of the many traditional derivative pricing models is model calibration, which is typically done not by historical asset prices but by means of derivative prices (i.e., by matching the market prices of heavily traded options to the derivative prices from the mathematical model). In the process of model calibration, thousands of derivative prices need to be determined in order to fit the parameters of the model, and the overall process is time consuming. Efficient numerical computation is increasingly important in financial risk management, especially when we deal with real-time risk management (e.g., high frequency trading). However, due to the requirement of a highly efficient computation, certain high-quality asset models and methodologies are discarded during model calibration of traditional derivative pricing models.</p>

<p>Machine learning can potentially be used to tackle these drawbacks related to impractical model assumptions and inefficient model calibration. Machine learning algorithms have the ability to tackle more nuances with very few theoretical assumptions and can be effectively used for derivative pricing, even in a world with frictions. With the advancements in hardware, we can train machine learning models on high performance CPUs, GPUs, and other specialized hardware to achieve a speed increase of several orders of magnitude as compared to the traditional derivative pricing models.</p>

<p>Additionally, market data is plentiful, so it is possible to train a machine learning algorithm to learn the function that is collectively generating derivative prices in the market. Machine learning models can capture subtle nonlinearities in the data that are not obtainable through other statistical approaches.</p>

<p>In this case study, we look at derivative pricing from a machine learning standpoint and use a supervised regression–based model to price an option from simulated data. The main idea here is to come up with a machine learning framework for derivative pricing. Achieving a machine learning model with high accuracy would mean that we can leverage the efficient numerical calculation of machine learning for derivative pricing with fewer underlying model assumptions.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45174928006888">
<h5/>
<p>In this case study, we will focus on:</p>

<ul>
<li>
<p>Developing a machine learning–based framework for derivative pricing.</p>
</li>
<li>
<p>Comparison of linear and nonlinear supervised regression models in the context of derivative pricing.</p>
</li>
</ul>
</div></aside>
<img class="blueprint top" src="Images/bracket_top.png" width="1625" height="32"/>








<section data-type="sect2" class="blueprint" data-pdf-bookmark="Blueprint for Developing a Machine Learning Model for Derivative Pricing"><div class="sect2" id="idm45174928002296">
<h2>Blueprint for Developing a Machine Learning Model for Derivative Pricing</h2>










<section data-type="sect3" data-pdf-bookmark="1. Problem definition"><div class="sect3" id="idm45174928000616">
<h3>1. Problem definition</h3>

<p>In the supervised regression framework we used for this case study, the predicted variable is the price of the option, and the predictor variables are the market data used as inputs to the Black-Scholes option pricing model.</p>

<p>The variables selected to estimate the market price of the option are stock price, strike price, time to expiration, volatility, interest rate, and dividend yield. The predicted variable for this case study was generated using random inputs and feeding them into the well-known Black-Scholes model.<sup><a data-type="noteref" id="idm45174927997912-marker" href="ch05.xhtml#idm45174927997912">12</a></sup></p>

<p><a data-type="indexterm" data-primary="Black–Scholes model" data-secondary="for call option price" id="idm45174927996856"/>The price of a call option per the Black-Scholes option pricing model is defined in <a data-type="xref" href="#BSEq">Equation 5-1</a>.</p>
<div data-type="equation" id="BSEq">
<h5><span class="label">Equation 5-1. </span>Black-Scholes equation for call option</h5>
<math display="block">
  <mrow>
    <mi>S</mi>
    <msup><mi>e</mi> <mrow><mo>–</mo><mi>q</mi><mi>τ</mi></mrow> </msup>
    <mi>Φ</mi>
    <mrow>
      <mo>(</mo>
      <msub><mi>d</mi> <mn>1</mn> </msub>
      <mo>)</mo>
    </mrow>
    <mo>–</mo>
    <msup><mi>e</mi> <mrow><mo>–</mo><mi>r</mi><mi>τ</mi></mrow> </msup>
    <mi>K</mi>
    <mi>Φ</mi>
    <mrow>
      <mo>(</mo>
      <msub><mi>d</mi> <mn>2</mn> </msub>
      <mo>)</mo>
    </mrow>
  </mrow>
</math>
</div>

<p>with</p>

<p><math display="inline">
  <mrow>
    <msub><mi>d</mi> <mn>1</mn> </msub>
    <mo>=</mo>
    <mfrac><mrow><mo form="prefix">ln</mo><mrow><mo>(</mo><mi>S</mi><mo>/</mo><mi>K</mi><mo>)</mo></mrow><mo>+</mo><mrow><mo>(</mo><mi>r</mi><mo>–</mo><mi>q</mi><mo>+</mo><msup><mi>σ</mi> <mn>2</mn> </msup><mo>/</mo><mn>2</mn><mo>)</mo></mrow><mi>τ</mi></mrow> <mrow><mi>σ</mi><msqrt><mi>τ</mi></msqrt></mrow></mfrac>
  </mrow>
</math></p>

<p>and</p>

<p><math display="inline">
  <mrow>
    <msub><mi>d</mi> <mn>2</mn> </msub>
    <mo>=</mo>
    <mfrac><mrow><mo form="prefix">ln</mo><mrow><mo>(</mo><mi>S</mi><mo>/</mo><mi>K</mi><mo>)</mo></mrow><mo>+</mo><mrow><mo>(</mo><mi>r</mi><mo>–</mo><mi>q</mi><mo>–</mo><msup><mi>σ</mi> <mn>2</mn> </msup><mo>/</mo><mn>2</mn><mo>)</mo></mrow><mi>τ</mi></mrow> <mrow><mi>σ</mi><msqrt><mi>τ</mi></msqrt></mrow></mfrac>
    <mo>=</mo>
    <msub><mi>d</mi> <mn>1</mn> </msub>
    <mo>–</mo>
    <mi>σ</mi>
    <msqrt>
      <mi>τ</mi>
    </msqrt>
  </mrow>
</math></p>

<p>where we have stock price <math alttext="upper S">
  <mi>S</mi>
</math>; strike price
<math alttext="upper K">
  <mi>K</mi>
</math>; risk-free rate <math alttext="r">
  <mi>r</mi>
</math>; annual dividend yield
<math alttext="q">
  <mi>q</mi>
</math>; time to maturity <math display="inline">
  <mrow>
    <mi>τ</mi>
    <mo>=</mo>
    <mi>T</mi>
    <mo>–</mo>
    <mi>t</mi>
  </mrow>
</math> (represented
as a unitless fraction of one year); and volatility <math alttext="sigma">
  <mi>σ</mi>
</math>.</p>

<p><a data-type="indexterm" data-primary="moneyness" id="idm45174927911672"/>To make the logic simpler, we define moneyness as
<math alttext="upper M equals upper K slash upper S">
  <mrow>
    <mi>M</mi>
    <mo>=</mo>
    <mi>K</mi>
    <mo>/</mo>
    <mi>S</mi>
  </mrow>
</math> and look at the prices in terms of per unit of
current stock price. We also set <math alttext="q">
  <mi>q</mi>
</math> as 0.</p>

<p>This simplifies the formula to the following:</p>
<div data-type="equation">
<math>
  <mrow>
    <msup><mi>e</mi> <mrow><mo>–</mo><mi>q</mi><mi>τ</mi></mrow> </msup>
    <mi>Φ</mi>
    <mfenced separators="" open="(" close=")">
      <mfrac><mrow><mo>–</mo><mo form="prefix">ln</mo><mrow><mo>(</mo><mi>M</mi><mo>)</mo></mrow><mo>+</mo><mrow><mo>(</mo><mi>r</mi><mo>+</mo><msup><mi>σ</mi> <mn>2</mn> </msup><mo>/</mo><mn>2</mn><mo>)</mo></mrow><mi>τ</mi></mrow> <mrow><mi>σ</mi><msqrt><mi>τ</mi></msqrt></mrow></mfrac>
    </mfenced>
    <mo>–</mo>
    <msup><mi>e</mi> <mrow><mo>–</mo><mi>r</mi><mi>τ</mi></mrow> </msup>
    <mi>M</mi>
    <mi>Φ</mi>
    <mfenced separators="" open="(" close=")">
      <mfrac><mrow><mo>–</mo><mo form="prefix">ln</mo><mrow><mo>(</mo><mi>M</mi><mo>)</mo></mrow><mo>+</mo><mrow><mo>(</mo><mi>r</mi><mo>–</mo><msup><mi>σ</mi> <mn>2</mn> </msup><mo>/</mo><mn>2</mn><mo>)</mo></mrow><mi>τ</mi></mrow> <mrow><mi>σ</mi><msqrt><mi>τ</mi></msqrt></mrow></mfrac>
    </mfenced>
    <mspace width="0.166667em"/>
  </mrow>
</math>
</div>

<p>Looking at the equation above, the parameters that feed into the Black-Scholes option pricing model are moneyness, risk-free rate, volatility, and time to maturity.</p>

<p><a data-type="indexterm" data-primary="volatility" id="idm45174927880728"/>The parameter that plays the central role in derivative market is volatility, as it is directly related to the movement of the stock prices. With the increase in the volatility, the range of share price movements becomes much wider than that of a low volatility stock.</p>

<p>In the options market, there isn’t a single volatility used to price all the options. This volatility depends on the option moneyness and time to maturity. In general, the volatility increases with higher time to maturity and with moneyness. This behavior is referred to as volatility smile/skew. We often derive the volatility from the price of the options existing in the market, and this volatility is referred to as “implied” volatility.
In this exercise, we assume the structure of the volatility surface and use function in <a data-type="xref" href="#VolEq">Equation 5-2</a>, where volatility depends on the option moneyness and time to maturity to generate the option volatility surface.</p>
<div data-type="equation" id="VolEq">
<h5><span class="label">Equation 5-2. </span>Equation for vloatility</h5>
<math display="block">
  <mrow>
    <mi>σ</mi>
    <mrow>
      <mo>(</mo>
      <mi>M</mi>
      <mo>,</mo>
      <mi>τ</mi>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <msub><mi>σ</mi> <mn>0</mn> </msub>
    <mo>+</mo>
    <mi>α</mi>
    <mi>τ</mi>
    <mo>+</mo>
    <mi>β</mi>
    <msup><mrow><mo>(</mo><mi>M</mi><mo>–</mo><mn>1</mn><mo>)</mo></mrow> <mn>2</mn> </msup>
  </mrow>
</math>
</div>
</div></section>













<section data-type="sect3" data-pdf-bookmark="2. Getting started—loading the data and Python packages"><div class="sect3" id="idm45174927999992">
<h3>2. Getting started—loading the data and Python packages</h3>












<section data-type="sect4" data-pdf-bookmark="2.1. Loading the Python packages"><div class="sect4" id="idm45174927864136">
<h4>2.1. Loading the Python packages</h4>

<p><a data-type="indexterm" data-primary="derivative pricing" data-secondary="loading data and Python packages" id="ix_Chapter5-asciidoc11"/>The loading of Python packages is similar to case study 1 in this chapter. Please refer to the Jupyter notebook of this case study for more details.</p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="2.2. Defining functions and parameters"><div class="sect4" id="idm45174927861208">
<h4>2.2. Defining functions and parameters</h4>

<p><a data-type="indexterm" data-primary="derivative pricing" data-secondary="defining functions and parameters" id="idm45174927859832"/>To generate the dataset, we need to simulate the input parameters and then create the predicted variable.</p>

<p>As a first step we define the constant parameters. The constant parameters required for the volatility surface are defined below. These parameters are not expected to have a significant impact on the option price; therefore, these parameters are set to some meaningful values:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">true_alpha</code> <code class="o">=</code> <code class="mf">0.1</code>
<code class="n">true_beta</code> <code class="o">=</code> <code class="mf">0.1</code>
<code class="n">true_sigma0</code> <code class="o">=</code> <code class="mf">0.2</code></pre>

<p>The risk-free rate, which is an input to the Black-Scholes option pricing model, is defined as follows:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">risk_free_rate</code> <code class="o">=</code> <code class="mf">0.05</code></pre>
</div></section>













<section data-type="sect4" data-pdf-bookmark="Volatility and option pricing functions"><div class="sect4" id="idm45174927804744">
<h4>Volatility and option pricing functions</h4>

<p>In this step we define the function to compute the volatility and price of a call option as per Equations <a href="#BSEq">5-1</a> and <a href="#VolEq">5-2</a>:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="k">def</code> <code class="nf">option_vol_from_surface</code><code class="p">(</code><code class="n">moneyness</code><code class="p">,</code> <code class="n">time_to_maturity</code><code class="p">):</code>
    <code class="k">return</code> <code class="n">true_sigma0</code> <code class="o">+</code> <code class="n">true_alpha</code> <code class="o">*</code> <code class="n">time_to_maturity</code> <code class="o">+</code>\
     <code class="n">true_beta</code> <code class="o">*</code> <code class="n">np</code><code class="o">.</code><code class="n">square</code><code class="p">(</code><code class="n">moneyness</code> <code class="o">-</code> <code class="mi">1</code><code class="p">)</code>

<code class="k">def</code> <code class="nf">call_option_price</code><code class="p">(</code><code class="n">moneyness</code><code class="p">,</code> <code class="n">time_to_maturity</code><code class="p">,</code> <code class="n">option_vol</code><code class="p">):</code>
    <code class="n">d1</code><code class="o">=</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">log</code><code class="p">(</code><code class="mi">1</code><code class="o">/</code><code class="n">moneyness</code><code class="p">)</code><code class="o">+</code><code class="p">(</code><code class="n">risk_free_rate</code><code class="o">+</code><code class="n">np</code><code class="o">.</code><code class="n">square</code><code class="p">(</code><code class="n">option_vol</code><code class="p">))</code><code class="o">*</code>\
    <code class="n">time_to_maturity</code><code class="p">)</code><code class="o">/</code> <code class="p">(</code><code class="n">option_vol</code><code class="o">*</code><code class="n">np</code><code class="o">.</code><code class="n">sqrt</code><code class="p">(</code><code class="n">time_to_maturity</code><code class="p">))</code>
    <code class="n">d2</code><code class="o">=</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">log</code><code class="p">(</code><code class="mi">1</code><code class="o">/</code><code class="n">moneyness</code><code class="p">)</code><code class="o">+</code><code class="p">(</code><code class="n">risk_free_rate</code><code class="o">-</code><code class="n">np</code><code class="o">.</code><code class="n">square</code><code class="p">(</code><code class="n">option_vol</code><code class="p">))</code><code class="o">*</code>\
    <code class="n">time_to_maturity</code><code class="p">)</code><code class="o">/</code><code class="p">(</code><code class="n">option_vol</code><code class="o">*</code><code class="n">np</code><code class="o">.</code><code class="n">sqrt</code><code class="p">(</code><code class="n">time_to_maturity</code><code class="p">))</code>
    <code class="n">N_d1</code> <code class="o">=</code> <code class="n">norm</code><code class="o">.</code><code class="n">cdf</code><code class="p">(</code><code class="n">d1</code><code class="p">)</code>
    <code class="n">N_d2</code> <code class="o">=</code> <code class="n">norm</code><code class="o">.</code><code class="n">cdf</code><code class="p">(</code><code class="n">d2</code><code class="p">)</code>

    <code class="k">return</code> <code class="n">N_d1</code> <code class="o">-</code> <code class="n">moneyness</code> <code class="o">*</code> <code class="n">np</code><code class="o">.</code><code class="n">exp</code><code class="p">(</code><code class="o">-</code><code class="n">risk_free_rate</code><code class="o">*</code><code class="n">time_to_maturity</code><code class="p">)</code> <code class="o">*</code> <code class="n">N_d2</code></pre>
</div></section>













<section data-type="sect4" data-pdf-bookmark="2.3. Data generation"><div class="sect4" id="idm45174927800824">
<h4>2.3. Data generation</h4>

<p><a data-type="indexterm" data-primary="derivative pricing" data-secondary="data generation" id="idm45174927799576"/>We generate the input and output variables in the following steps:</p>

<ul>
<li>
<p>Time to maturity (<em>Ts</em>) is generated using the <code>np.random.random</code> function, which generates a uniform random variable between zero and one.</p>
</li>
<li>
<p>Moneyness (<em>Ks</em>) is generated using the <code>np.random.randn</code> function, which generates a normally distributed random variable. The random number multiplied by 0.25 generates the deviation of strike from spot price,<sup><a data-type="noteref" id="idm45174927663880-marker" href="ch05.xhtml#idm45174927663880">13</a></sup> and the overall equation ensures that the moneyness is greater than zero.</p>
</li>
<li>
<p>Volatility (<em>sigma</em>) is generated as a function of time to maturity and moneyness using <a data-type="xref" href="#VolEq">Equation 5-2</a>.</p>
</li>
<li>
<p>The option price is generated using <a data-type="xref" href="#BSEq">Equation 5-1</a> for the Black-Scholes option price.</p>
</li>
</ul>

<p>In total we generate 10,000 data points (<em>N</em>):</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">N</code> <code class="o">=</code> <code class="mi">10000</code>

<code class="n">Ks</code> <code class="o">=</code> <code class="mi">1</code><code class="o">+</code><code class="mf">0.25</code><code class="o">*</code><code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">randn</code><code class="p">(</code><code class="n">N</code><code class="p">)</code>
<code class="n">Ts</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">random</code><code class="p">(</code><code class="n">N</code><code class="p">)</code>
<code class="n">Sigmas</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="n">option_vol_from_surface</code><code class="p">(</code><code class="n">k</code><code class="p">,</code><code class="n">t</code><code class="p">)</code> <code class="k">for</code> <code class="n">k</code><code class="p">,</code><code class="n">t</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">Ks</code><code class="p">,</code><code class="n">Ts</code><code class="p">)])</code>
<code class="n">Ps</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="n">call_option_price</code><code class="p">(</code><code class="n">k</code><code class="p">,</code><code class="n">t</code><code class="p">,</code><code class="n">sig</code><code class="p">)</code> <code class="k">for</code> <code class="n">k</code><code class="p">,</code><code class="n">t</code><code class="p">,</code><code class="n">sig</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">Ks</code><code class="p">,</code><code class="n">Ts</code><code class="p">,</code><code class="n">Sigmas</code><code class="p">)])</code></pre>

<p>Now we create the variables for predicted and predictor variables:<a data-type="indexterm" data-startref="ix_Chapter5-asciidoc11" id="idm45174927443208"/></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">Y</code> <code class="o">=</code> <code class="n">Ps</code>
<code class="n">X</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">concatenate</code><code class="p">([</code><code class="n">Ks</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code><code class="mi">1</code><code class="p">),</code> <code class="n">Ts</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code><code class="mi">1</code><code class="p">),</code> <code class="n">Sigmas</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code><code class="mi">1</code><code class="p">)],</code> \
<code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>

<code class="n">dataset</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">concatenate</code><code class="p">([</code><code class="n">Y</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code><code class="mi">1</code><code class="p">),</code> <code class="n">X</code><code class="p">],</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">),</code>
                       <code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s">'Price'</code><code class="p">,</code> <code class="s">'Moneyness'</code><code class="p">,</code> <code class="s">'Time'</code><code class="p">,</code> <code class="s">'Vol'</code><code class="p">])</code></pre>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="3. Exploratory data analysis"><div class="sect3" id="idm45174927620392">
<h3>3. Exploratory data analysis</h3>

<p><a data-type="indexterm" data-primary="derivative pricing" data-secondary="exploratory data analysis" id="idm45174927619384"/>Let’s have a look at the dataset we have.</p>












<section data-type="sect4" data-pdf-bookmark="3.1. Descriptive statistics"><div class="sect4" id="idm45174927394600">
<h4>3.1. Descriptive statistics</h4>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">dataset</code><code class="o">.</code><code class="n">head</code><code class="p">()</code></pre>

<p><code>Output</code></p>
<table>

<thead>
<tr>
<th/>
<th>Price</th>
<th>Moneyness</th>
<th>Time</th>
<th>Vol</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>0</p></td>
<td><p>1.390e-01</p></td>
<td><p>0.898</p></td>
<td><p>0.221</p></td>
<td><p>0.223</p></td>
</tr>
<tr>
<td><p>1</p></td>
<td><p>3.814e-06</p></td>
<td><p>1.223</p></td>
<td><p>0.052</p></td>
<td><p>0.210</p></td>
</tr>
<tr>
<td><p>2</p></td>
<td><p>1.409e-01</p></td>
<td><p>0.969</p></td>
<td><p>0.391</p></td>
<td><p>0.239</p></td>
</tr>
<tr>
<td><p>3</p></td>
<td><p>1.984e-01</p></td>
<td><p>0.950</p></td>
<td><p>0.628</p></td>
<td><p>0.263</p></td>
</tr>
<tr>
<td><p>4</p></td>
<td><p>2.495e-01</p></td>
<td><p>0.914</p></td>
<td><p>0.810</p></td>
<td><p>0.282</p></td>
</tr>
</tbody>
</table>

<p>The dataset contains <em>price</em>—which is the price of the option and is the predicted variable—along with <em>moneyness</em> (the ratio of strike and spot price), <em>time to maturity</em>, and <em>volatility</em>, which are the features in the model.</p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="3.2. Data visualization"><div class="sect4" id="idm45174927346664">
<h4>3.2. Data visualization</h4>

<p>In this step we look at scatterplot to understand the interaction between different variables:<sup><a data-type="noteref" id="idm45174927344984-marker" href="ch05.xhtml#idm45174927344984">14</a></sup></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">pyplot</code><code class="o">.</code><code class="n">figure</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">15</code><code class="p">,</code><code class="mi">15</code><code class="p">))</code>
<code class="n">scatter_matrix</code><code class="p">(</code><code class="n">dataset</code><code class="p">,</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">12</code><code class="p">,</code><code class="mi">12</code><code class="p">))</code>
<code class="n">pyplot</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>

<p><code>Output</code></p>

<figure><div class="figure">
<img src="Images/mlbf_05in10.png" alt="mlbf 05in10" width="779" height="690"/>
<h6/>
</div></figure>

<p>The scatterplot reveals very interesting dependencies and relationships between the variables. Let us look at the first row of the chart to see the relationship of price to different variables. We observe that as moneyness decreases (i.e., strike price decreases as compared to the stock price), there is an increase in the price, which is in line with the rationale described in the previous section. Looking at the price versus time to maturity, we see an increase in the option price. The price versus volatility chart also shows an increase in the price with the volatility. However, option price seems to exhibit a nonlinear relationship with most of the variables. This means that we expect our nonlinear models to do a better job than our linear models.</p>

<p>Another interesting relationship is between volatility and strike. The more we deviate from the moneyness of one, the higher the volatility we observe. This behavior is shown due to the volatility function we defined before and illustrates the volatility smile/skew.</p>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="4. Data preparation and analysis"><div class="sect3" id="idm45174927325192">
<h3>4. Data preparation and analysis</h3>

<p><a data-type="indexterm" data-primary="derivative pricing" data-secondary="data preparation/analysis" id="idm45174927296328"/>We performed most of the data preparation steps (i.e., getting the dependent and independent variables) in the preceding sections. In this step we look at the feature importance.</p>












<section data-type="sect4" data-pdf-bookmark="4.1. Univariate feature selection"><div class="sect4" id="idm45174927294936">
<h4>4.1. Univariate feature selection</h4>

<p>We start by looking at each feature individually and, using the single variable regression fit as the criteria, look at the most important variables:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">bestfeatures</code> <code class="o">=</code> <code class="n">SelectKBest</code><code class="p">(</code><code class="n">k</code><code class="o">=</code><code class="s">'all'</code><code class="p">,</code> <code class="n">score_func</code><code class="o">=</code><code class="n">f_regression</code><code class="p">)</code>
<code class="n">fit</code> <code class="o">=</code> <code class="n">bestfeatures</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code><code class="n">Y</code><code class="p">)</code>
<code class="n">dfscores</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">fit</code><code class="o">.</code><code class="n">scores_</code><code class="p">)</code>
<code class="n">dfcolumns</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">([</code><code class="s">'Moneyness'</code><code class="p">,</code> <code class="s">'Time'</code><code class="p">,</code> <code class="s">'Vol'</code><code class="p">])</code>
<code class="c">#concat two dataframes for better visualization</code>
<code class="n">featureScores</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">concat</code><code class="p">([</code><code class="n">dfcolumns</code><code class="p">,</code><code class="n">dfscores</code><code class="p">],</code><code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
<code class="n">featureScores</code><code class="o">.</code><code class="n">columns</code> <code class="o">=</code> <code class="p">[</code><code class="s">'Specs'</code><code class="p">,</code><code class="s">'Score'</code><code class="p">]</code>  <code class="c">#naming the dataframe columns</code>
<code class="n">featureScores</code><code class="o">.</code><code class="n">nlargest</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code><code class="s">'Score'</code><code class="p">)</code><code class="o">.</code><code class="n">set_index</code><code class="p">(</code><code class="s">'Specs'</code><code class="p">)</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">Moneyness : 30282.309
Vol : 2407.757
Time : 1597.452</pre>

<p>We observe that the moneyness is the most important variable for the option price, followed by volatility and time to maturity. Given there are only three predictor variables, we retain all the variables for modeling.</p>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="5. Evaluate models"><div class="sect3" id="idm45174927160184">
<h3>5. Evaluate models</h3>












<section data-type="sect4" data-pdf-bookmark="5.1. Train-test split and evaluation metrics"><div class="sect4" id="idm45174927159208">
<h4>5.1. Train-test split and evaluation metrics</h4>

<p><a data-type="indexterm" data-primary="derivative pricing" data-secondary="evaluation of models" id="ix_Chapter5-asciidoc12"/>First, we separate the training set and test set:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">validation_size</code> <code class="o">=</code> <code class="mf">0.2</code>

<code class="n">train_size</code> <code class="o">=</code> <code class="nb">int</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">X</code><code class="p">)</code> <code class="o">*</code> <code class="p">(</code><code class="mi">1</code><code class="o">-</code><code class="n">validation_size</code><code class="p">))</code>
<code class="n">X_train</code><code class="p">,</code> <code class="n">X_test</code> <code class="o">=</code> <code class="n">X</code><code class="p">[</code><code class="mi">0</code><code class="p">:</code><code class="n">train_size</code><code class="p">],</code> <code class="n">X</code><code class="p">[</code><code class="n">train_size</code><code class="p">:</code><code class="nb">len</code><code class="p">(</code><code class="n">X</code><code class="p">)]</code>
<code class="n">Y_train</code><code class="p">,</code> <code class="n">Y_test</code> <code class="o">=</code> <code class="n">Y</code><code class="p">[</code><code class="mi">0</code><code class="p">:</code><code class="n">train_size</code><code class="p">],</code> <code class="n">Y</code><code class="p">[</code><code class="n">train_size</code><code class="p">:</code><code class="nb">len</code><code class="p">(</code><code class="n">X</code><code class="p">)]</code></pre>

<p>We use the prebuilt sklearn models to run a <em>k</em>-fold analysis on our training data. We then train the model on the full training data and use it for prediction of the test data. We will evaluate algorithms using the mean squared error metric. The parameters for the <em>k</em>-fold analysis and evaluation metrics are defined as follows:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">num_folds</code> <code class="o">=</code> <code class="mi">10</code>
<code class="n">seed</code> <code class="o">=</code> <code class="mi">7</code>
<code class="n">scoring</code> <code class="o">=</code> <code class="s">'neg_mean_squared_error'</code></pre>
</div></section>













<section data-type="sect4" data-pdf-bookmark="5.2. Compare models and algorithms"><div class="sect4" id="idm45174927158840">
<h4>5.2. Compare models and algorithms</h4>

<p>Now that we have completed the data loading and have designed the test harness, we need to choose a model out of the suite of the supervised regression models.</p>

<p><code>Linear models and regression trees</code></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">models</code> <code class="o">=</code> <code class="p">[]</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'LR'</code><code class="p">,</code> <code class="n">LinearRegression</code><code class="p">()))</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'KNN'</code><code class="p">,</code> <code class="n">KNeighborsRegressor</code><code class="p">()))</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'CART'</code><code class="p">,</code> <code class="n">DecisionTreeRegressor</code><code class="p">()))</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'SVR'</code><code class="p">,</code> <code class="n">SVR</code><code class="p">()))</code></pre>

<p><code>Artificial neural network</code></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'MLP'</code><code class="p">,</code> <code class="n">MLPRegressor</code><code class="p">()))</code></pre>

<p><code>Boosting and bagging methods</code></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c"># Boosting methods</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'ABR'</code><code class="p">,</code> <code class="n">AdaBoostRegressor</code><code class="p">()))</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'GBR'</code><code class="p">,</code> <code class="n">GradientBoostingRegressor</code><code class="p">()))</code>
<code class="c"># Bagging methods</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'RFR'</code><code class="p">,</code> <code class="n">RandomForestRegressor</code><code class="p">()))</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'ETR'</code><code class="p">,</code> <code class="n">ExtraTreesRegressor</code><code class="p">()))</code></pre>

<p>Once we have selected all the models, we loop over each of them. First, we run the <em>k</em>-fold analysis. Next, we run the model on the entire training and testing dataset.</p>

<p>The algorithms use default tuning parameters. We will calculate the mean and standard deviation of error metric and save the results for use later.</p>

<p><code>Output</code></p>

<figure><div class="figure">
<img src="Images/mlbf_05in11.png" alt="mlbf 05in11" width="1179" height="623"/>
<h6/>
</div></figure>

<p>The Python code for the <em>k</em>-fold analysis step is similar to that used in case study 1. Readers can also refer to the Jupyter notebook of this case study in the code repository for more details. Let us look at the performance of the models in the training set.</p>

<p><a data-type="indexterm" data-primary="classification and regression trees (CART)" data-secondary="derivative pricing with" id="idm45174926882536"/>We see clearly that the nonlinear models, including classification and regression tree (CART), ensemble models, and artificial neural network (represented by MLP in the chart above), perform a lot better that the linear algorithms. This is intuitive given the nonlinear relationships we observed in the scatterplot.</p>

<p><a data-type="indexterm" data-primary="artificial neural networks (ANNs)" data-secondary="derivative pricing with" id="idm45174926880808"/>Artificial neural networks (ANN) have the natural ability to model any function with fast experimentation and deployment times (definition, training, testing, inference). ANN can effectively be used in complex derivative pricing situations. Hence, out of all the models with good performance, we choose ANN for further analysis.</p>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="6. Model tuning and finalizing the model"><div class="sect3" id="idm45174927128104">
<h3>6. Model tuning and finalizing the model</h3>

<p><a data-type="indexterm" data-primary="derivative pricing" data-secondary="tuning/finalizing model" id="idm45174926877848"/>Determining the proper number of nodes for the middle layer of an ANN is more of an art than a science, as discussed in <a data-type="xref" href="ch03.xhtml#Chapter3">Chapter 3</a>. Too many nodes in the middle layer, and thus too many connections, produce a neural network that memorizes the input data and lacks the ability to generalize. Therefore, increasing the number of nodes in the middle layer will improve performance on the training set, while decreasing the number of nodes in the middle layer will improve performance on a new dataset.</p>

<p>As discussed in <a data-type="xref" href="ch03.xhtml#Chapter3">Chapter 3</a>, the ANN model has several other hyperparameters such as learning rate, momentum, activation function, number of epochs, and batch size. All these hyperparameters can be tuned during the grid search process. However, in this step, we stick to performing grid search on the number of hidden layers for the purpose of simplicity. The approach to perform grid search on other hyperparameters is the same as described in the following code snippet:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="sd">'''</code>
<code class="sd">hidden_layer_sizes : tuple, length = n_layers - 2, default (100,)</code>
<code class="sd">    The ith element represents the number of neurons in the ith</code>
<code class="sd">    hidden layer.</code>
<code class="sd">'''</code>
<code class="n">param_grid</code><code class="o">=</code><code class="p">{</code><code class="s">'hidden_layer_sizes'</code><code class="p">:</code> <code class="p">[(</code><code class="mi">20</code><code class="p">,),</code> <code class="p">(</code><code class="mi">50</code><code class="p">,),</code> <code class="p">(</code><code class="mi">20</code><code class="p">,</code><code class="mi">20</code><code class="p">),</code> <code class="p">(</code><code class="mi">20</code><code class="p">,</code> <code class="mi">30</code><code class="p">,</code> <code class="mi">20</code><code class="p">)]}</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">MLPRegressor</code><code class="p">()</code>
<code class="n">kfold</code> <code class="o">=</code> <code class="n">KFold</code><code class="p">(</code><code class="n">n_splits</code><code class="o">=</code><code class="n">num_folds</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="n">seed</code><code class="p">)</code>
<code class="n">grid</code> <code class="o">=</code> <code class="n">GridSearchCV</code><code class="p">(</code><code class="n">estimator</code><code class="o">=</code><code class="n">model</code><code class="p">,</code> <code class="n">param_grid</code><code class="o">=</code><code class="n">param_grid</code><code class="p">,</code> <code class="n">scoring</code><code class="o">=</code><code class="n">scoring</code><code class="p">,</code> \
  <code class="n">cv</code><code class="o">=</code><code class="n">kfold</code><code class="p">)</code>
<code class="n">grid_result</code> <code class="o">=</code> <code class="n">grid</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">Y_train</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="s">"Best: %f using %s"</code> <code class="o">%</code> <code class="p">(</code><code class="n">grid_result</code><code class="o">.</code><code class="n">best_score_</code><code class="p">,</code> <code class="n">grid_result</code><code class="o">.</code><code class="n">best_params_</code><code class="p">))</code>
<code class="n">means</code> <code class="o">=</code> <code class="n">grid_result</code><code class="o">.</code><code class="n">cv_results_</code><code class="p">[</code><code class="s">'mean_test_score'</code><code class="p">]</code>
<code class="n">stds</code> <code class="o">=</code> <code class="n">grid_result</code><code class="o">.</code><code class="n">cv_results_</code><code class="p">[</code><code class="s">'std_test_score'</code><code class="p">]</code>
<code class="n">params</code> <code class="o">=</code> <code class="n">grid_result</code><code class="o">.</code><code class="n">cv_results_</code><code class="p">[</code><code class="s">'params'</code><code class="p">]</code>
<code class="k">for</code> <code class="n">mean</code><code class="p">,</code> <code class="n">stdev</code><code class="p">,</code> <code class="n">param</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">means</code><code class="p">,</code> <code class="n">stds</code><code class="p">,</code> <code class="n">params</code><code class="p">):</code>
    <code class="nb">print</code><code class="p">(</code><code class="s">"%f (%f) with: %r"</code> <code class="o">%</code> <code class="p">(</code><code class="n">mean</code><code class="p">,</code> <code class="n">stdev</code><code class="p">,</code> <code class="n">param</code><code class="p">))</code></pre>

<p class="pagebreak-before"><code>Output</code></p>

<pre data-type="programlisting">Best: -0.000024 using {'hidden_layer_sizes': (20, 30, 20)}
-0.000580 (0.000601) with: {'hidden_layer_sizes': (20,)}
-0.000078 (0.000041) with: {'hidden_layer_sizes': (50,)}
-0.000090 (0.000140) with: {'hidden_layer_sizes': (20, 20)}
-0.000024 (0.000011) with: {'hidden_layer_sizes': (20, 30, 20)}</pre>

<p>The best model has three layers, with 20, 30, and 20 nodes in each hidden layer, respectively. Hence, we prepare a model with this configuration and check its performance on the test set. This is a crucial step, because a greater number of layers may lead to overfitting and have poor performance in the test set.</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c"># prepare model</code>
<code class="n">model_tuned</code> <code class="o">=</code> <code class="n">MLPRegressor</code><code class="p">(</code><code class="n">hidden_layer_sizes</code><code class="o">=</code><code class="p">(</code><code class="mi">20</code><code class="p">,</code> <code class="mi">30</code><code class="p">,</code> <code class="mi">20</code><code class="p">))</code>
<code class="n">model_tuned</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">Y_train</code><code class="p">)</code></pre>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c"># estimate accuracy on validation set</code>
<code class="c"># transform the validation dataset</code>
<code class="n">predictions</code> <code class="o">=</code> <code class="n">model_tuned</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">mean_squared_error</code><code class="p">(</code><code class="n">Y_test</code><code class="p">,</code> <code class="n">predictions</code><code class="p">))</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">3.08127276609567e-05</pre>

<p>We see that the root mean squared error (RMSE) is 3.08e–5, which is less than one cent. Hence, the ANN model does an excellent job of fitting the Black-Scholes option pricing model. A greater number of layers and tuning of other hyperparameters may enable the ANN model to capture the complex relationship and nonlinearity in the data even better. Overall, the results suggest that ANN may be used to train an option pricing model that matches market prices.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="7. Additional analysis: removing the volatility data"><div class="sect3" id="idm45174926878792">
<h3>7. Additional analysis: removing the volatility data</h3>

<p><a data-type="indexterm" data-primary="derivative pricing" data-secondary="removing volatility data" id="idm45174926642504"/>As an additional analysis, we make the process harder by trying to predict the price without the volatility data. If the model performance is good, we will eliminate the need to have a volatility function as described before. In this step, we further compare the performance of the linear and nonlinear models.
In the following code snippet, we remove the volatility variable from the dataset of the predictor variable and define the training set and test set again:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">X</code> <code class="o">=</code> <code class="n">X</code><code class="p">[:,</code> <code class="p">:</code><code class="mi">2</code><code class="p">]</code>
<code class="n">validation_size</code> <code class="o">=</code> <code class="mf">0.2</code>
<code class="n">train_size</code> <code class="o">=</code> <code class="nb">int</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">X</code><code class="p">)</code> <code class="o">*</code> <code class="p">(</code><code class="mi">1</code><code class="o">-</code><code class="n">validation_size</code><code class="p">))</code>
<code class="n">X_train</code><code class="p">,</code> <code class="n">X_test</code> <code class="o">=</code> <code class="n">X</code><code class="p">[</code><code class="mi">0</code><code class="p">:</code><code class="n">train_size</code><code class="p">],</code> <code class="n">X</code><code class="p">[</code><code class="n">train_size</code><code class="p">:</code><code class="nb">len</code><code class="p">(</code><code class="n">X</code><code class="p">)]</code>
<code class="n">Y_train</code><code class="p">,</code> <code class="n">Y_test</code> <code class="o">=</code> <code class="n">Y</code><code class="p">[</code><code class="mi">0</code><code class="p">:</code><code class="n">train_size</code><code class="p">],</code> <code class="n">Y</code><code class="p">[</code><code class="n">train_size</code><code class="p">:</code><code class="nb">len</code><code class="p">(</code><code class="n">X</code><code class="p">)]</code></pre>

<p>Next, we run the suite of the models (except the regularized regression model) with the new dataset, with the same parameters and similar Python code as before. The performance of all the models after removing the volatility data is as follows:</p>

<figure><div class="figure">
<img src="Images/mlbf_05in12.png" alt="mlbf 05in12" width="1175" height="623"/>
<h6/>
</div></figure>

<p>Looking at the result, we have a similar conclusion as before and see a poor performance of the linear regression and good performance of the ensemble and ANN models. The linear regression now does even a worse job than before. However, the performance of ANN and other ensemble models does not deviate much from their previous performance. This implies the information of the volatility is likely captured in other variables, such as moneyness and time to maturity. Overall, it is good news as it means that fewer variables might be needed to achieve the same performance.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Conclusion"><div class="sect3" id="idm45174926500296">
<h3>Conclusion</h3>

<p>We know that derivative pricing is a nonlinear problem. As expected, our linear regression model did not do as well as our nonlinear models, and the non-linear models have a very good overall performance. We also observed that removing the volatility increases the difficulty of the prediction problem for the linear regression. However, the nonlinear models such as ensemble models and ANN are still able to do well at the prediction process. This does indicate that one might be able to sidestep the development of an option volatility surface and achieve a good prediction with a smaller number of variables.</p>

<p>We saw that an artificial neural network (ANN) can reproduce the Black-Scholes option pricing formula for a call option to a high degree of accuracy, meaning we can leverage efficient numerical calculation of machine learning in derivative pricing without relying on the impractical assumptions made in the traditional derivative pricing models. The ANN and the related machine learning architecture can easily be extended to pricing derivatives in the real world, with no knowledge of the theory of derivative pricing. The use of machine learning techniques can lead to much faster derivative pricing compared to traditional derivative pricing models. The price we might have to pay for this extra speed is some loss of accuracy. However, this reduced accuracy is often well within reasonable limits and acceptable from a practical point of view. New technology has commoditized the use of ANN, so it might be worthwhile for banks, hedge funds, and financial institutions to explore these models for derivative pricing<a data-type="indexterm" data-startref="ix_Chapter5-asciidoc12" id="idm45174926496888"/>.<a data-type="indexterm" data-startref="ix_Chapter5-asciidoc10" id="idm45174926496056"/></p>
<img class="blueprint bottom" src="Images/bracket_bottom.png" width="1625" height="32"/>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Case Study 3: Investor Risk Tolerance and Robo-Advisors"><div class="sect1" id="CaseStudy3SR">
<h1>Case Study 3: Investor Risk Tolerance and Robo-Advisors</h1>

<p><a data-type="indexterm" data-primary="risk tolerance and robo-advisor case study" id="ix_Chapter5-asciidoc14"/>The risk tolerance of an investor is one of the most important inputs to the portfolio allocation and rebalancing steps of the portfolio management process. There is a wide variety of risk profiling tools that take varied approaches to understanding the risk tolerance of an investor. Most of these approaches include qualitative judgment and involve significant manual effort. In most of the cases, the risk tolerance of an investor is decided based on a risk tolerance questionnaire.</p>

<p>Several studies have shown that these risk tolerance questionnaires are prone to error, as investors suffer from behavioral biases and are poor judges of their own risk perception, especially during stressed markets. Also, given that these questionnaires must be manually completed by investors, they eliminate the possibility of automating the entire investment management process.</p>

<p>So can machine learning provide a better understanding of an investor’s risk profile than a risk tolerance questionnaire can? Can machine learning contribute to automating the entire portfolio management process by cutting the client out of the loop? Could an algorithm be written to develop a personality profile for the client that would be a better representation of how they would deal with different market 
<span class="keep-together">scenarios</span>?</p>

<p>The goal of this case study is to answer these questions. We first build a supervised regression–based model to predict the risk tolerance of an investor. We then build a robo-advisor dashboard in Python and implement the risk tolerance prediction model in the dashboard. The overall purpose is to demonstrate the automation of the manual steps in the portfolio management process with the help of machine learning. This can prove to be immensely useful, specifically for robo-advisors.</p>

<p>A <em>dashboard</em> is one of the key features of a robo-advisor as it provides access to important information and allows users to interact with their accounts free of any human dependency, making the portfolio management process highly efficient.</p>

<p><a data-type="xref" href="#ROBO1">Figure 5-6</a> provides a quick glance at the robo-advisor dashboard built for this case study. The dashboard performs end-to-end asset allocation for an investor, embedding the machine learning–based risk tolerance model constructed in this case study.</p>

<figure><div id="ROBO1" class="figure">
<img src="Images/mlbf_0506.png" alt="mlbf 0506" width="1333" height="711"/>
<h6><span class="label">Figure 5-6. </span>Robo-advisor dashboard</h6>
</div></figure>

<p>This dashboard has been built in Python and is described in detail in an additional step in this case study. Although it has been built in the context of robo-advisors, it can be extended to other areas in finance and can embed the machine learning models discussed in other case studies, providing finance decision makers with a graphical interface for analyzing and interpreting model results.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45174926482440">
<h5/>
<p>In this case study, we will focus on:</p>

<ul>
<li>
<p>Feature elimination and feature importance/intuition.</p>
</li>
<li>
<p>Using machine learning to automate manual processes involved in portfolio management process.</p>
</li>
<li>
<p>Using machine learning to quantify and model the behavioral bias of investors/individuals.</p>
</li>
<li>
<p>Embedding machine learning models into user interfaces or dashboards using Python.</p>
</li>
</ul>
</div></aside>
<img class="blueprint top" src="Images/bracket_top.png" width="1625" height="32"/>








<section data-type="sect2" class="blueprint" data-pdf-bookmark="Blueprint for Modeling Investor Risk Tolerance and Enabling a Machine Learning–Based Robo-Advisor"><div class="sect2" id="idm45174926476056">
<h2>Blueprint for Modeling Investor Risk Tolerance and Enabling a Machine Learning–Based Robo-Advisor</h2>










<section data-type="sect3" data-pdf-bookmark="1. Problem definition"><div class="sect3" id="idm45174926474264">
<h3>1. Problem definition</h3>

<p>In the supervised regression framework used for this case study, the predicted variable is the “true” risk tolerance of an individual,<sup><a data-type="noteref" id="idm45174926472776-marker" href="ch05.xhtml#idm45174926472776">15</a></sup> and the predictor variables are demographic, financial, and behavioral attributes of an individual.</p>

<p>The data used for this case study is from the <a href="https://oreil.ly/2vxJ6">Survey of Consumer Finances (SCF)</a>, which is conducted by the Federal Reserve Board. The survey includes responses about household demographics, net worth, financial, and nonfinancial assets for the same set of individuals in 2007 (precrisis) and 2009 (postcrisis). This enables us to see how each household’s allocation changed after the 2008 global financial crisis. Refer to the <a href="https://oreil.ly/_L8vS">data dictionary</a> for more information on this survey.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="2. Getting started—loading the data and Python packages"><div class="sect3" id="idm45174926469448">
<h3>2. Getting started—loading the data and Python packages</h3>












<section data-type="sect4" data-pdf-bookmark="2.1. Loading the Python packages"><div class="sect4" id="idm45174926468440">
<h4>2.1. Loading the Python packages</h4>

<p><a data-type="indexterm" data-primary="risk tolerance and robo-advisor case study" data-secondary="loading data and Python packages" id="idm45174926467272"/>The details on loading the standard Python packages were presented in the previous case studies. Refer to the Jupyter notebook for this case study for more details.</p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="2.2. Loading the data"><div class="sect4" id="idm45174926465576">
<h4>2.2. Loading the data</h4>

<p>In this step we load the data from the Survey of Consumer Finances and look at the data shape:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c"># load dataset</code>
<code class="n">dataset</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">read_excel</code><code class="p">(</code><code class="s">'SCFP2009panel.xlsx'</code><code class="p">)</code></pre>

<p>Let us look at the size of the data:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">dataset</code><code class="o">.</code><code class="n">shape</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">(19285, 515)</pre>

<p>As we can see, the dataset has a total of 19,285 observations with 515 columns. The number of columns represents the number of features.</p>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="3. Data preparation and feature selection"><div class="sect3" id="idm45174926414984">
<h3>3. Data preparation and feature selection</h3>

<p>In this step we prepare the predicted and predictor variables to be used for modeling.</p>












<section data-type="sect4" data-pdf-bookmark="3.1. Preparing the predicted variable"><div class="sect4" id="prep_pred_var">
<h4>3.1. Preparing the predicted variable</h4>

<p><a data-type="indexterm" data-primary="risk tolerance and robo-advisor case study" data-secondary="data preparation" id="ix_Chapter5-asciidoc15"/>In the first step, we prepare the predicted variable, which is the true risk tolerance.</p>

<p>The steps to compute the true risk tolerance are as follows:</p>
<ol>
<li>
<p>Compute the risky assets and the risk-free assets for all the individuals in the survey data. <a data-type="indexterm" data-primary="risk-free assets" id="idm45174926402888"/><a data-type="indexterm" data-primary="risky assets" id="idm45174926402184"/>Risky and risk-free assets are defined as follows:</p>
<dl>
<dt>Risky assets</dt>
<dd>
<p>Investments in mutual funds, stocks, and bonds.</p>
</dd>
<dt>Risk-free assets</dt>
<dd>
<p>Checking and savings balances, certificates of deposit, and other cash balances and equivalents.</p>
</dd>
</dl>
</li>
<li>
<p>Take the ratio of risky assets to total assets (where total assets is the sum of risky and risk-free assets) of an individual and consider that as a measure of the individual’s risk tolerance.<sup><a data-type="noteref" id="idm45174926394984-marker" href="ch05.xhtml#idm45174926394984">16</a></sup> From the SCF, we have the data of risky and risk-free assets for the individuals in 2007 and 2009. We use this data and normalize the risky assets with the price of a stock index (S&amp;P500) in 2007 versus 2009 to get risk tolerance.</p>
</li>
<li>
<p><a data-type="indexterm" data-primary="intelligent investors" id="idm45174926393176"/>Identify the “intelligent” investors. Some literature describes an intelligent investor as one who does not change their risk tolerance during changes in the market. So we consider the investors who changed their risk tolerance by less than 10% between 2007 and 2009 as the intelligent investors. Of course, this is a qualitative judgment, and there can be several other ways of defining an intelligent investor. However, as mentioned before, beyond coming up with a precise definition of true risk tolerance, the purpose of this case study is to demonstrate the usage of machine learning and provide a machine learning–based framework in portfolio management that can be further leveraged for more detailed analysis.</p>
</li>

</ol>

<p>Let us compute the predicted variable. First, we get the risky and risk-free assets and compute the risk tolerance for 2007 and 2009 in the following code snippet:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c"># Compute the risky assets and risk-free assets for 2007</code>
<code class="n">dataset</code><code class="p">[</code><code class="s">'RiskFree07'</code><code class="p">]</code><code class="o">=</code> <code class="n">dataset</code><code class="p">[</code><code class="s">'LIQ07'</code><code class="p">]</code> <code class="o">+</code> <code class="n">dataset</code><code class="p">[</code><code class="s">'CDS07'</code><code class="p">]</code> <code class="o">+</code> <code class="n">dataset</code><code class="p">[</code><code class="s">'SAVBND07'</code><code class="p">]</code>\
 <code class="o">+</code> <code class="n">dataset</code><code class="p">[</code><code class="s">'CASHLI07'</code><code class="p">]</code>
<code class="n">dataset</code><code class="p">[</code><code class="s">'Risky07'</code><code class="p">]</code> <code class="o">=</code> <code class="n">dataset</code><code class="p">[</code><code class="s">'NMMF07'</code><code class="p">]</code> <code class="o">+</code> <code class="n">dataset</code><code class="p">[</code><code class="s">'STOCKS07'</code><code class="p">]</code> <code class="o">+</code> <code class="n">dataset</code><code class="p">[</code><code class="s">'BOND07'</code><code class="p">]</code>

<code class="c"># Compute the risky assets and risk-free assets for 2009</code>
<code class="n">dataset</code><code class="p">[</code><code class="s">'RiskFree09'</code><code class="p">]</code><code class="o">=</code> <code class="n">dataset</code><code class="p">[</code><code class="s">'LIQ09'</code><code class="p">]</code> <code class="o">+</code> <code class="n">dataset</code><code class="p">[</code><code class="s">'CDS09'</code><code class="p">]</code> <code class="o">+</code> <code class="n">dataset</code><code class="p">[</code><code class="s">'SAVBND09'</code><code class="p">]</code>\
<code class="o">+</code> <code class="n">dataset</code><code class="p">[</code><code class="s">'CASHLI09'</code><code class="p">]</code>
<code class="n">dataset</code><code class="p">[</code><code class="s">'Risky09'</code><code class="p">]</code> <code class="o">=</code> <code class="n">dataset</code><code class="p">[</code><code class="s">'NMMF09'</code><code class="p">]</code> <code class="o">+</code> <code class="n">dataset</code><code class="p">[</code><code class="s">'STOCKS09'</code><code class="p">]</code> <code class="o">+</code> <code class="n">dataset</code><code class="p">[</code><code class="s">'BOND09'</code><code class="p">]</code>

<code class="c"># Compute the risk tolerance for 2007</code>
<code class="n">dataset</code><code class="p">[</code><code class="s">'RT07'</code><code class="p">]</code> <code class="o">=</code> <code class="n">dataset</code><code class="p">[</code><code class="s">'Risky07'</code><code class="p">]</code><code class="o">/</code><code class="p">(</code><code class="n">dataset</code><code class="p">[</code><code class="s">'Risky07'</code><code class="p">]</code><code class="o">+</code><code class="n">dataset</code><code class="p">[</code><code class="s">'RiskFree07'</code><code class="p">])</code>

<code class="c">#Average stock index for normalizing the risky assets in 2009</code>
<code class="n">Average_SP500_2007</code><code class="o">=</code><code class="mi">1478</code>
<code class="n">Average_SP500_2009</code><code class="o">=</code><code class="mi">948</code>

<code class="c"># Compute the risk tolerance for 2009</code>
<code class="n">dataset</code><code class="p">[</code><code class="s">'RT09'</code><code class="p">]</code> <code class="o">=</code> <code class="n">dataset</code><code class="p">[</code><code class="s">'Risky09'</code><code class="p">]</code><code class="o">/</code><code class="p">(</code><code class="n">dataset</code><code class="p">[</code><code class="s">'Risky09'</code><code class="p">]</code><code class="o">+</code><code class="n">dataset</code><code class="p">[</code><code class="s">'RiskFree09'</code><code class="p">])</code><code class="o">*</code>\
                <code class="p">(</code><code class="n">Average_SP500_2009</code><code class="o">/</code><code class="n">Average_SP500_2007</code><code class="p">)</code></pre>

<p>Let us look at the details of the data:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">dataset</code><code class="o">.</code><code class="n">head</code><code class="p">()</code></pre>

<p><code>Output</code></p>

<figure><div class="figure">
<img src="Images/mlbf_05in13.png" alt="mlbf 05in13" width="724" height="187"/>
<h6/>
</div></figure>

<p>The data above displays some of the columns out of the 521 columns of the dataset.</p>

<p>Let us compute the percentage change in risk tolerance between 2007 and 2009:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">dataset</code><code class="p">[</code><code class="s">'PercentageChange'</code><code class="p">]</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">abs</code><code class="p">(</code><code class="n">dataset</code><code class="p">[</code><code class="s">'RT09'</code><code class="p">]</code><code class="o">/</code><code class="n">dataset</code><code class="p">[</code><code class="s">'RT07'</code><code class="p">]</code><code class="o">-</code><code class="mi">1</code><code class="p">)</code></pre>

<p>Next, we drop the rows containing “NA” or “NaN”:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c"># Drop the rows containing NA</code>
<code class="n">dataset</code><code class="o">=</code><code class="n">dataset</code><code class="o">.</code><code class="n">dropna</code><code class="p">(</code><code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>

<code class="n">dataset</code><code class="o">=</code><code class="n">dataset</code><code class="p">[</code><code class="o">~</code><code class="n">dataset</code><code class="o">.</code><code class="n">isin</code><code class="p">([</code><code class="n">np</code><code class="o">.</code><code class="n">nan</code><code class="p">,</code> <code class="n">np</code><code class="o">.</code><code class="n">inf</code><code class="p">,</code> <code class="o">-</code><code class="n">np</code><code class="o">.</code><code class="n">inf</code><code class="p">])</code><code class="o">.</code><code class="n">any</code><code class="p">(</code><code class="mi">1</code><code class="p">)]</code></pre>

<p>Let us investigate the risk tolerance behavior of individuals in 2007 versus 2009. First we look at the risk tolerance in 2007:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">sns</code><code class="o">.</code><code class="n">distplot</code><code class="p">(</code><code class="n">dataset</code><code class="p">[</code><code class="s">'RT07'</code><code class="p">],</code> <code class="n">hist</code><code class="o">=</code><code class="k">True</code><code class="p">,</code> <code class="n">kde</code><code class="o">=</code><code class="k">False</code><code class="p">,</code>
             <code class="n">bins</code><code class="o">=</code><code class="nb">int</code><code class="p">(</code><code class="mi">180</code><code class="o">/</code><code class="mi">5</code><code class="p">),</code> <code class="n">color</code> <code class="o">=</code> <code class="s">'blue'</code><code class="p">,</code>
             <code class="n">hist_kws</code><code class="o">=</code><code class="p">{</code><code class="s">'edgecolor'</code><code class="p">:</code><code class="s">'black'</code><code class="p">})</code></pre>

<p class="pagebreak-before"><code>Output</code></p>

<figure><div class="figure">
<img src="Images/mlbf_05in14.png" alt="mlbf 05in14" width="367" height="247"/>
<h6/>
</div></figure>

<p>Looking at the risk tolerance in 2007, we see that a significant number of individuals had a risk tolerance close to one, meaning investments were skewed more toward the risky assets. Now let us look at the risk tolerance in 2009:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">sns</code><code class="o">.</code><code class="n">distplot</code><code class="p">(</code><code class="n">dataset</code><code class="p">[</code><code class="s">'RT09'</code><code class="p">],</code> <code class="n">hist</code><code class="o">=</code><code class="k">True</code><code class="p">,</code> <code class="n">kde</code><code class="o">=</code><code class="k">False</code><code class="p">,</code>
             <code class="n">bins</code><code class="o">=</code><code class="nb">int</code><code class="p">(</code><code class="mi">180</code><code class="o">/</code><code class="mi">5</code><code class="p">),</code> <code class="n">color</code> <code class="o">=</code> <code class="s">'blue'</code><code class="p">,</code>
             <code class="n">hist_kws</code><code class="o">=</code><code class="p">{</code><code class="s">'edgecolor'</code><code class="p">:</code><code class="s">'black'</code><code class="p">})</code></pre>

<p><code>Output</code></p>

<figure><div class="figure">
<img src="Images/mlbf_05in15.png" alt="mlbf 05in15" width="367" height="247"/>
<h6/>
</div></figure>

<p>Clearly, the behavior of the individuals reversed after the crisis. Overall risk tolerance decreased, which is shown by the outsized proportion of households having risk tolerance close to zero in 2009. Most of the investments of these individuals were in risk-free assets.</p>

<p class="pagebreak-before">In the next step, we pick the intelligent investors whose change in risk tolerance between 2007 and 2009 was less than 10%, as described in <a data-type="xref" href="#prep_pred_var">“3.1. Preparing the predicted variable”</a>:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">dataset3</code> <code class="o">=</code> <code class="n">dataset</code><code class="p">[</code><code class="n">dataset</code><code class="p">[</code><code class="s">'PercentageChange'</code><code class="p">]</code><code class="o">&lt;=.</code><code class="mi">1</code><code class="p">]</code></pre>

<p>We assign the true risk tolerance as the average risk tolerance of these intelligent investors between 2007 and 2009:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">dataset3</code><code class="p">[</code><code class="s">'TrueRiskTolerance'</code><code class="p">]</code> <code class="o">=</code> <code class="p">(</code><code class="n">dataset3</code><code class="p">[</code><code class="s">'RT07'</code><code class="p">]</code> <code class="o">+</code> <code class="n">dataset3</code><code class="p">[</code><code class="s">'RT09'</code><code class="p">])</code><code class="o">/</code><code class="mi">2</code></pre>

<p>This is the predicted variable for this case study.</p>

<p>Let us drop other labels that might not be needed for the prediction:<a data-type="indexterm" data-startref="ix_Chapter5-asciidoc15" id="idm45174925887080"/></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">dataset3</code><code class="o">.</code><code class="n">drop</code><code class="p">(</code><code class="n">labels</code><code class="o">=</code><code class="p">[</code><code class="s">'RT07'</code><code class="p">,</code> <code class="s">'RT09'</code><code class="p">],</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">inplace</code><code class="o">=</code><code class="k">True</code><code class="p">)</code>
<code class="n">dataset3</code><code class="o">.</code><code class="n">drop</code><code class="p">(</code><code class="n">labels</code><code class="o">=</code><code class="p">[</code><code class="s">'PercentageChange'</code><code class="p">],</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">inplace</code><code class="o">=</code><code class="k">True</code><code class="p">)</code></pre>
</div></section>













<section data-type="sect4" data-pdf-bookmark="3.2. Feature selection—limit the feature space"><div class="sect4" id="idm45174925739576">
<h4>3.2. Feature selection—limit the feature space</h4>

<p>In this section, we will explore ways to condense the feature space.</p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="3.2.1. Feature elimination"><div class="sect4" id="idm45174925781304">
<h4>3.2.1. Feature elimination</h4>

<p><a data-type="indexterm" data-primary="risk tolerance and robo-advisor case study" data-secondary="feature selection" id="ix_Chapter5-asciidoc16"/>To filter the features further, we check the description in the <a href="https://oreil.ly/_L8vS">data dictionary</a> and keep only the features that are 
<span class="keep-together">relevant</span>.</p>

<p>Looking at the entire data, we have more than <em>500</em> features in the dataset. However, academic literature and industry practice indicate risk tolerance is heavily influenced by investor demographic, financial, and behavioral attributes, such as age, current income, net worth, and willingness to take risk. All these attributes were available in the dataset and are summarized in the following section. These attributes are used as features to predict investors’ risk tolerance.</p>

<figure><div class="figure">
<img src="Images/mlbf_05in16.png" alt="mlbf 05in16" width="1029" height="342"/>
<h6/>
</div></figure>

<p>In the dataset, each of the columns contains a numeric value corresponding to the value of the attribute. The details are as follows:</p>
<dl>
<dt>AGE</dt>
<dd>
<p>There are six age categories, where 1 represents age less than 35 and 6 represents age more than 75.</p>
</dd>
<dt>EDUC</dt>
<dd>
<p>There are four education categories, where 1 represents no high school and 4 represents college degree.</p>
</dd>
<dt>MARRIED</dt>
<dd>
<p>There are two categories to represent marital status, where 1 represents married and 2 represents unmarried.</p>
</dd>
<dt>OCCU</dt>
<dd>
<p>This represents occupation category. A value of 1 represents managerial status and 4 represents unemployed.</p>
</dd>
<dt>KIDS</dt>
<dd>
<p>Number of children.</p>
</dd>
<dt>WSAVED</dt>
<dd>
<p>This represents the individual’s spending versus income, split into three categories. For example, 1 represents spending exceeded income.</p>
</dd>
<dt>NWCAT</dt>
<dd>
<p>This represents net worth category. There are five categories, where 1 represents net worth less than the 25th percentile and 5 represents net worth more than the 90th percentile.</p>
</dd>
<dt>INCCL</dt>
<dd>
<p>This represents income category. There are five categories, where 1 represents income less than $10,000 and 5 represents income more than $100,000.</p>
</dd>
<dt>RISK</dt>
<dd>
<p>This represents the willingness to take risk on a scale of 1 to 4, where 1 represents the highest level of willingness to take risk.</p>
</dd>
</dl>

<p>We keep only the intuitive features as of 2007 and remove all the intermediate features and features related to 2009, as the variables of 2007 are the only ones required for predicting the risk tolerance:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">keep_list2</code> <code class="o">=</code> <code class="p">[</code><code class="s">'AGE07'</code><code class="p">,</code><code class="s">'EDCL07'</code><code class="p">,</code><code class="s">'MARRIED07'</code><code class="p">,</code><code class="s">'KIDS07'</code><code class="p">,</code><code class="s">'OCCAT107'</code><code class="p">,</code><code class="s">'INCOME07'</code><code class="p">,</code>\
<code class="s">'RISK07'</code><code class="p">,</code><code class="s">'NETWORTH07'</code><code class="p">,</code><code class="s">'TrueRiskTolerance'</code><code class="p">]</code>

<code class="n">drop_list2</code> <code class="o">=</code> <code class="p">[</code><code class="n">col</code> <code class="k">for</code> <code class="n">col</code> <code class="ow">in</code> <code class="n">dataset3</code><code class="o">.</code><code class="n">columns</code> <code class="k">if</code> <code class="n">col</code> <code class="ow">not</code> <code class="ow">in</code> <code class="n">keep_list2</code><code class="p">]</code>

<code class="n">dataset3</code><code class="o">.</code><code class="n">drop</code><code class="p">(</code><code class="n">labels</code><code class="o">=</code><code class="n">drop_list2</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">inplace</code><code class="o">=</code><code class="k">True</code><code class="p">)</code></pre>

<p class="pagebreak-before">Now let us look at the correlation among the features:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c"># correlation</code>
<code class="n">correlation</code> <code class="o">=</code> <code class="n">dataset3</code><code class="o">.</code><code class="n">corr</code><code class="p">()</code>
<code class="n">plt</code><code class="o">.</code><code class="n">figure</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">15</code><code class="p">,</code><code class="mi">15</code><code class="p">))</code>
<code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="s">'Correlation Matrix'</code><code class="p">)</code>
<code class="n">sns</code><code class="o">.</code><code class="n">heatmap</code><code class="p">(</code><code class="n">correlation</code><code class="p">,</code> <code class="n">vmax</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">square</code><code class="o">=</code><code class="k">True</code><code class="p">,</code><code class="n">annot</code><code class="o">=</code><code class="k">True</code><code class="p">,</code><code class="n">cmap</code><code class="o">=</code><code class="s">'cubehelix'</code><code class="p">)</code></pre>

<p><code>Output</code></p>

<figure><div class="figure">
<img src="Images/mlbf_05in17.png" alt="mlbf 05in17" width="807" height="847"/>
<h6/>
</div></figure>

<p>Looking at the correlation chart (full-size version available on <a href="https://oreil.ly/iQpk4">GitHub</a>), net worth and income are positively correlated with risk tolerance. With a greater number of kids and marriage, risk tolerance decreases. As the willingness to take risks decreases, the risk tolerance decreases. With age there is a positive relationship of the risk 
<span class="keep-together">tolerance</span>. As per Hui Wang and Sherman Hanna’s paper “Does Risk Tolerance Decrease with Age?,” risk tolerance increases as people age (i.e., the proportion of net wealth invested in risky assets increases as people age) when other variables are held constant.</p>

<p>So in summary, the relationship of these variables with risk tolerance seems intuitive.<a data-type="indexterm" data-startref="ix_Chapter5-asciidoc16" id="idm45174925588184"/></p>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="4. Evaluate models"><div class="sect3" id="idm45174925780712">
<h3>4. Evaluate models</h3>












<section data-type="sect4" data-pdf-bookmark="4.1. Train-test split"><div class="sect4" id="idm45174925586504">
<h4>4.1. Train-test split</h4>

<p><a data-type="indexterm" data-primary="risk tolerance and robo-advisor case study" data-secondary="evaluation of models" id="idm45174925585272"/>Let us split the data into training and test set:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">Y</code><code class="o">=</code> <code class="n">dataset3</code><code class="p">[</code><code class="s">"TrueRiskTolerance"</code><code class="p">]</code>
<code class="n">X</code> <code class="o">=</code> <code class="n">dataset3</code><code class="o">.</code><code class="n">loc</code><code class="p">[:,</code> <code class="n">dataset3</code><code class="o">.</code><code class="n">columns</code> <code class="o">!=</code> <code class="s">'TrueRiskTolerance'</code><code class="p">]</code>
<code class="n">validation_size</code> <code class="o">=</code> <code class="mf">0.2</code>
<code class="n">seed</code> <code class="o">=</code> <code class="mi">3</code>
<code class="n">X_train</code><code class="p">,</code> <code class="n">X_validation</code><code class="p">,</code> <code class="n">Y_train</code><code class="p">,</code> <code class="n">Y_validation</code> <code class="o">=</code> \
<code class="n">train_test_split</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">Y</code><code class="p">,</code> <code class="n">test_size</code><code class="o">=</code><code class="n">validation_size</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="n">seed</code><code class="p">)</code></pre>
</div></section>













<section data-type="sect4" data-pdf-bookmark="4.2. Test options and evaluation metrics"><div class="sect4" id="idm45174925559656">
<h4>4.2. Test options and evaluation metrics</h4>

<p>We use R<sup>2</sup> as the evaluation metric and select 10 as the number of folds for cross validation.<sup><a data-type="noteref" id="idm45174925517336-marker" href="ch05.xhtml#idm45174925517336">17</a></sup></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">num_folds</code> <code class="o">=</code> <code class="mi">10</code>
<code class="n">scoring</code> <code class="o">=</code> <code class="s">'r2'</code></pre>
</div></section>













<section data-type="sect4" data-pdf-bookmark="4.3. Compare models and algorithms"><div class="sect4" id="idm45174925513608">
<h4>4.3. Compare models and algorithms</h4>

<p><a data-type="indexterm" data-primary="k-folds cross validation" id="idm45174925512360"/>Next, we select the suite of the regression model and perform the <em>k</em>-folds cross validation.</p>

<p><code>Regression Models</code></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c"># spot-check the algorithms</code>
<code class="n">models</code> <code class="o">=</code> <code class="p">[]</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'LR'</code><code class="p">,</code> <code class="n">LinearRegression</code><code class="p">()))</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'LASSO'</code><code class="p">,</code> <code class="n">Lasso</code><code class="p">()))</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'EN'</code><code class="p">,</code> <code class="n">ElasticNet</code><code class="p">()))</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'KNN'</code><code class="p">,</code> <code class="n">KNeighborsRegressor</code><code class="p">()))</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'CART'</code><code class="p">,</code> <code class="n">DecisionTreeRegressor</code><code class="p">()))</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'SVR'</code><code class="p">,</code> <code class="n">SVR</code><code class="p">()))</code>
<code class="c">#Ensemble Models</code>
<code class="c"># Boosting methods</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'ABR'</code><code class="p">,</code> <code class="n">AdaBoostRegressor</code><code class="p">()))</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'GBR'</code><code class="p">,</code> <code class="n">GradientBoostingRegressor</code><code class="p">()))</code>
<code class="c"># Bagging methods</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'RFR'</code><code class="p">,</code> <code class="n">RandomForestRegressor</code><code class="p">()))</code>
<code class="n">models</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="s">'ETR'</code><code class="p">,</code> <code class="n">ExtraTreesRegressor</code><code class="p">()))</code></pre>

<p>The Python code for the <em>k</em>-fold analysis step is similar to that of previous case studies. Readers can also refer to the Jupyter notebook of this case study in the code repository for more details. Let us look at the performance of the models in the training set.</p>

<figure><div class="figure">
<img src="Images/mlbf_05in18.png" alt="mlbf 05in18" width="1172" height="622"/>
<h6/>
</div></figure>

<p>The nonlinear models perform better than the linear models, which means that there is a nonlinear relationship between the risk tolerance and the variables used to predict it. Given random forest regression is one of the best methods, we use it for further grid search.</p>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="5. Model tuning and grid search"><div class="sect3" id="idm45174925339912">
<h3>5. Model tuning and grid search</h3>

<p><a data-type="indexterm" data-primary="risk tolerance and robo-advisor case study" data-secondary="model tuning and grid search" id="idm45174925338504"/>As discussed in <a data-type="xref" href="ch04.xhtml#Chapter4">Chapter 4</a>, random forest has many hyperparameters that can be tweaked while performing the grid search. However, we will confine our grid search to number of estimators (<code>n_estimators</code>) as it is one of the most important hyperparameters. It represents the number of trees in the random forest model. Ideally, this should be increased until no further improvement is seen in the model:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c"># 8. Grid search : RandomForestRegressor</code>
<code class="sd">'''</code>
<code class="sd">n_estimators : integer, optional (default=10)</code>
<code class="sd">    The number of trees in the forest.</code>
<code class="sd">'''</code>
<code class="n">param_grid</code> <code class="o">=</code> <code class="p">{</code><code class="s">'n_estimators'</code><code class="p">:</code> <code class="p">[</code><code class="mi">50</code><code class="p">,</code><code class="mi">100</code><code class="p">,</code><code class="mi">150</code><code class="p">,</code><code class="mi">200</code><code class="p">,</code><code class="mi">250</code><code class="p">,</code><code class="mi">300</code><code class="p">,</code><code class="mi">350</code><code class="p">,</code><code class="mi">400</code><code class="p">]}</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">RandomForestRegressor</code><code class="p">()</code>
<code class="n">kfold</code> <code class="o">=</code> <code class="n">KFold</code><code class="p">(</code><code class="n">n_splits</code><code class="o">=</code><code class="n">num_folds</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="n">seed</code><code class="p">)</code>
<code class="n">grid</code> <code class="o">=</code> <code class="n">GridSearchCV</code><code class="p">(</code><code class="n">estimator</code><code class="o">=</code><code class="n">model</code><code class="p">,</code> <code class="n">param_grid</code><code class="o">=</code><code class="n">param_grid</code><code class="p">,</code> <code class="n">scoring</code><code class="o">=</code><code class="n">scoring</code><code class="p">,</code> \
  <code class="n">cv</code><code class="o">=</code><code class="n">kfold</code><code class="p">)</code>
<code class="n">grid_result</code> <code class="o">=</code> <code class="n">grid</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">Y_train</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="s">"Best: %f using %s"</code> <code class="o">%</code> <code class="p">(</code><code class="n">grid_result</code><code class="o">.</code><code class="n">best_score_</code><code class="p">,</code> <code class="n">grid_result</code><code class="o">.</code><code class="n">best_params_</code><code class="p">))</code>
<code class="n">means</code> <code class="o">=</code> <code class="n">grid_result</code><code class="o">.</code><code class="n">cv_results_</code><code class="p">[</code><code class="s">'mean_test_score'</code><code class="p">]</code>
<code class="n">stds</code> <code class="o">=</code> <code class="n">grid_result</code><code class="o">.</code><code class="n">cv_results_</code><code class="p">[</code><code class="s">'std_test_score'</code><code class="p">]</code>
<code class="n">params</code> <code class="o">=</code> <code class="n">grid_result</code><code class="o">.</code><code class="n">cv_results_</code><code class="p">[</code><code class="s">'params'</code><code class="p">]</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">Best: 0.738632 using {'n_estimators': 250}</pre>

<p>Random forest with number of estimators as 250 is the best model after grid search.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="6. Finalize the model"><div class="sect3" id="idm45174925215272">
<h3>6. Finalize the model</h3>

<p><a data-type="indexterm" data-primary="risk tolerance and robo-advisor case study" data-secondary="finalizing the model" id="idm45174925214136"/>Let us look at the results on the test dataset and check the feature importance.</p>












<section data-type="sect4" data-pdf-bookmark="6.1. Results on the test dataset"><div class="sect4" id="idm45174925212808">
<h4>6.1. Results on the test dataset</h4>

<p>We prepare the random forest model with the number of estimators as 250:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">model</code> <code class="o">=</code> <code class="n">RandomForestRegressor</code><code class="p">(</code><code class="n">n_estimators</code> <code class="o">=</code> <code class="mi">250</code><code class="p">)</code>
<code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">Y_train</code><code class="p">)</code></pre>

<p>Let us look at the performance in the training set:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="k">import</code> <code class="n">r2_score</code>
<code class="n">predictions_train</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">r2_score</code><code class="p">(</code><code class="n">Y_train</code><code class="p">,</code> <code class="n">predictions_train</code><code class="p">))</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">0.9640632406817223</pre>

<p>The R<sup>2</sup> of the training set is 96%, which is a good result. Now let us look at the performance in the test set:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">predictions</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_validation</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">mean_squared_error</code><code class="p">(</code><code class="n">Y_validation</code><code class="p">,</code> <code class="n">predictions</code><code class="p">))</code>
<code class="nb">print</code><code class="p">(</code><code class="n">r2_score</code><code class="p">(</code><code class="n">Y_validation</code><code class="p">,</code> <code class="n">predictions</code><code class="p">))</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">0.007781840953471237
0.7614494526639909</pre>

<p>From the mean squared error and R<sup>2</sup> of 76% shown above for the test set, the random forest model does an excellent job of fitting the risk tolerance.</p>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="6.2. Feature importance and features intuition"><div class="sect3" id="idm45174925180776">
<h3>6.2. Feature importance and features intuition</h3>

<p>Let us look into the feature importance of the variables within the random forest model:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">import</code> <code class="nn">pandas</code> <code class="k">as</code> <code class="nn">pd</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">RandomForestRegressor</code><code class="p">(</code><code class="n">n_estimators</code><code class="o">=</code> <code class="mi">200</code><code class="p">,</code><code class="n">n_jobs</code><code class="o">=-</code><code class="mi">1</code><code class="p">)</code>
<code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code><code class="n">Y_train</code><code class="p">)</code>
<code class="c">#use inbuilt class feature_importances of tree based classifiers</code>
<code class="c">#plot graph of feature importances for better visualization</code>
<code class="n">feat_importances</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">Series</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">feature_importances_</code><code class="p">,</code> <code class="n">index</code><code class="o">=</code><code class="n">X</code><code class="o">.</code><code class="n">columns</code><code class="p">)</code>
<code class="n">feat_importances</code><code class="o">.</code><code class="n">nlargest</code><code class="p">(</code><code class="mi">10</code><code class="p">)</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">kind</code><code class="o">=</code><code class="s">'barh'</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>

<p><code>Output</code></p>

<figure><div class="figure">
<img src="Images/mlbf_05in19.png" alt="mlbf 05in19" width="412" height="233"/>
<h6/>
</div></figure>

<p>In the chart, the x-axis represents the magnitude of the importance of a feature. Hence, income and net worth, followed by age and willingness to take risk, are the key variables in determining risk tolerance.</p>












<section data-type="sect4" data-pdf-bookmark="6.3. Save model for later use"><div class="sect4" id="save_model">
<h4>6.3. Save model for later use</h4>

<p>In this step we save the model for later use. The saved model can be used directly for prediction given the set of input variables. The model is saved as <em>finalized_model.sav</em> using the <code>dump</code> module of the pickle package. This saved model can be loaded using the <code>load</code> module.</p>

<p>Let’s save the model as the first step:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c"># Save Model Using Pickle</code>
<code class="kn">from</code> <code class="nn">pickle</code> <code class="k">import</code> <code class="n">dump</code>
<code class="kn">from</code> <code class="nn">pickle</code> <code class="k">import</code> <code class="n">load</code>

<code class="c"># save the model to disk</code>
<code class="n">filename</code> <code class="o">=</code> <code class="s">'finalized_model.sav'</code>
<code class="n">dump</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="nb">open</code><code class="p">(</code><code class="n">filename</code><code class="p">,</code> <code class="s">'wb'</code><code class="p">))</code></pre>

<p>Now let’s load the saved model and use it for prediction:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c"># load the model from disk</code>
<code class="n">loaded_model</code> <code class="o">=</code> <code class="n">load</code><code class="p">(</code><code class="nb">open</code><code class="p">(</code><code class="n">filename</code><code class="p">,</code> <code class="s">'rb'</code><code class="p">))</code>
<code class="c"># estimate accuracy on validation set</code>
<code class="n">predictions</code> <code class="o">=</code> <code class="n">loaded_model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_validation</code><code class="p">)</code>
<code class="n">result</code> <code class="o">=</code> <code class="n">mean_squared_error</code><code class="p">(</code><code class="n">Y_validation</code><code class="p">,</code> <code class="n">predictions</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">r2_score</code><code class="p">(</code><code class="n">Y_validation</code><code class="p">,</code> <code class="n">predictions</code><code class="p">))</code>
<code class="nb">print</code><code class="p">(</code><code class="n">result</code><code class="p">)</code></pre>

<p class="pagebreak-before"><code>Output</code></p>

<pre data-type="programlisting">0.7683894847939692
0.007555447734714956</pre>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="7. Additional step: robo-advisor dashboard"><div class="sect3" id="idm45174924820696">
<h3>7. Additional step: robo-advisor dashboard</h3>

<p><a data-type="indexterm" data-primary="dashboard, for robo-advisor" id="ix_Chapter5-asciidoc17"/><a data-type="indexterm" data-primary="risk tolerance and robo-advisor case study" data-secondary="robo-advisor dashboard" id="ix_Chapter5-asciidoc18"/>We mentioned the robo-advisor dashboard in the beginning of this case study. The robo-advisor dashboard performs an automation of the portfolio management process and aims to overcome the problem of traditional risk tolerance profiling.</p>
<div data-type="tip"><h1>Python Code for Robo-Advisor Dashboard</h1>
<p>This robo-advisor dashboard is built in Python using the plotly dash package. <a href="https://dash.plot.ly">Dash</a> is a productive Python framework for building web applications with good user interfaces. The code for the robo-advisor dashboard is added to the <a href="https://oreil.ly/8fTDy">code repository for this book</a>. The code is in a Jupyter notebook called “Sample Robo-advisor”. A detailed description of the code is outside the scope of this case study. However, the codebase can be leveraged for creation of any new machine learning–enabled dashboard.</p>
</div>

<p>The dashboard has two panels:</p>

<ul>
<li>
<p>Inputs for investor characteristics</p>
</li>
<li>
<p>Asset allocation and portfolio performance</p>
</li>
</ul>












<section data-type="sect4" data-pdf-bookmark="Input for investor characteristics"><div class="sect4" id="idm45174924810776">
<h4>Input for investor characteristics</h4>

<p><a data-type="xref" href="#Robo2">Figure 5-7</a> shows the input panel for the investor characteristics. This panel takes all the input regarding the investor’s demographic, financial, and behavioral attributes. These inputs are for the predicted variables we used in the risk tolerance model created in the preceding steps. The interface is designed to input the categorical and continuous variables in the correct format.</p>

<p>Once the inputs are submitted, we leverage the model saved in <a data-type="xref" href="#save_model">“6.3. Save model for later use”</a>. This model takes all the inputs and produces the risk tolerance of an investor (refer to the <code>predict_riskTolerance</code> function of the “Sample Robo-advisor” Jupyter notebook in the code repository for this book for more details). The risk tolerance prediction model is embedded in this dashboard and is triggered once the “Calculate Risk Tolerance” button is pressed after submitting the inputs.</p>

<figure><div id="Robo2" class="figure">
<img src="Images/mlbf_0507.png" alt="mlbf 0507" width="401" height="667"/>
<h6><span class="label">Figure 5-7. </span>Robo-advisor input panel</h6>
</div></figure>
</div></section>













<section data-type="sect4" data-pdf-bookmark="7.2 Asset allocation and portfolio performance"><div class="sect4" id="idm45174924804040">
<h4>7.2 Asset allocation and portfolio performance</h4>

<p><a data-type="xref" href="#Robo3">Figure 5-8</a> shows the “Asset Allocation and Portfolio Performance” panel, which performs the following functionalities:</p>

<ul>
<li>
<p>Once the risk tolerance is computed using the model, it is displayed on the top of this panel.</p>
</li>
<li>
<p>In the next step, we pick the assets for our portfolio from the dropdown.</p>
</li>
<li>
<p>Once the list of assets are submitted, the traditional mean-variance portfolio allocation model is used to allocate the portfolio among the assets selected. Risk 
<span class="keep-together">tolerance</span> is one of the key inputs for this process. (Refer to the <code>get_asset_allocation</code> function of the “Sample Robo-advisor” Jupyter notebook in the code repository for this book for more details.)</p>
</li>
<li>
<p>The dashboard also shows the historical performance of the allocated portfolio for an initial investment of <em>$100</em>.</p>
</li>
</ul>

<figure><div id="Robo3" class="figure">
<img src="Images/mlbf_0508.png" alt="mlbf 0508" width="935" height="608"/>
<h6><span class="label">Figure 5-8. </span>Robo-advisor asset allocation and portfolio performance panel</h6>
</div></figure>

<p>Although the dashboard is a basic version of the robo-advisor dashboard, it performs end-to-end asset allocation for an investor and provides the portfolio view and historical performance of the portfolio over a selected period. There are several potential enhancements to this prototype in terms of the interface and underlying models used. The dashboard can be enhanced to include additional instruments and incorporate additional features such as real-time portfolio monitoring, portfolio rebalancing, and investment advisory. In terms of the underlying models used for asset allocation, we have used the traditional mean-variance optimization method, but it can be further enhanced to use the allocation algorithms based on machine learning techniques such as eigen-portfolio, hierarchical risk parity, or reinforcement learning–based models, described in Chapters <a href="ch07.xhtml#Chapter7">7</a>, <a href="ch08.xhtml#Chapter8">8</a> and <a href="ch09.xhtml#Chapter9">9</a>, respectively. The risk tolerance model can be further enhanced by using additional features or using the actual data of the investors rather than using data from the Survey of Consumer Finances.<a data-type="indexterm" data-startref="ix_Chapter5-asciidoc18" id="idm45174924790104"/><a data-type="indexterm" data-startref="ix_Chapter5-asciidoc17" id="idm45174924789400"/></p>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="Conclusion"><div class="sect3" id="idm45174924788600">
<h3>Conclusion</h3>

<p>In this case study, we introduced the regression-based algorithm applied to compute an investor’s risk tolerance, followed by a demonstration of the model in a robo-advisor setup. We showed that machine learning models might be able to objectively analyze the behavior of different investors in a changing market and attribute these changes to variables involved in determining risk appetite. With an increase in the volume of investors’ data and the availability of rich machine learning infrastructure, such models might prove to be more useful than existing manual processes.</p>

<p>We saw that there is a nonlinear relationship between the variables and the risk tolerance. We analyzed the feature importance and found that results of the case study are quite intuitive. Income and net worth, followed by age and willingness to take risk, are the key variables to deciding risk tolerance. These variables have been considered key variables to model risk tolerance across academic and industry literature.</p>

<p>Through the robo-advisor dashboard powered by machine learning, we demonstrated an effective combination of data science and machine learning implementation in wealth management. Robo-advisors and investment managers could leverage such models and platforms to enhance the portfolio management process with the help of machine learning.<a data-type="indexterm" data-startref="ix_Chapter5-asciidoc14" id="idm45174924784856"/></p>
<img class="blueprint bottom" src="Images/bracket_bottom.png" width="1625" height="32"/>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Case Study 4: Yield Curve Prediction"><div class="sect1" id="CaseStudy4SR">
<h1>Case Study 4: Yield Curve Prediction</h1>

<p><a data-type="indexterm" data-primary="yield curve prediction" id="ix_Chapter5-asciidoc19"/>A <em>yield curve</em> is a line that plots yields (interest rates) of bonds having equal credit quality but differing maturity dates. This yield curve is used as a benchmark for other debt in the market, such as mortgage rates or bank lending rates. The most frequently reported yield curve compares the 3-months, 2-years, 5-years, 10-years, and 30-years U.S. Treasury debt.</p>

<p><a data-type="indexterm" data-primary="fixed income market, yield curve and" id="idm45174924779672"/>The yield curve is the centerpiece in a fixed income market. Fixed income markets are important sources of finance for governments, national and supranational institutions, banks, and private and public corporations. In addition, yield curves are very important to investors in pension funds and insurance companies.</p>

<p><a data-type="indexterm" data-primary="bond market, yield curve and" id="idm45174924778184"/>The yield curve is a key representation of the state of the bond market. Investors watch the bond market closely as it is a strong predictor of future economic activity and levels of inflation, which affect prices of goods, financial assets, and real estate. The slope of the yield curve is an important indicator of short-term interest rates and is followed closely by investors.</p>

<p>Hence, an accurate yield curve forecasting is of critical importance in financial applications. Several statistical techniques and tools commonly used in econometrics and finance have been applied to model the yield curve.</p>

<p>In this case study we will use supervised learning–based models to predict the yield curve. This case study is inspired by the paper <em>Artificial Neural Networks in Fixed Income Markets for Yield Curve Forecasting</em> by Manuel Nunes et al. (2018).</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45174924744632">
<h5/>
<p>In this case study, we will focus on:</p>

<ul>
<li>
<p>Simultaneous modeling (producing multiple outputs at the same time) of the interest rates.</p>
</li>
<li>
<p>Comparison of neural network versus linear regression models.</p>
</li>
<li>
<p>Modeling a time series in a supervised regression–based framework.</p>
</li>
<li>
<p>Understanding the variable intuition and feature selection.</p>
</li>
</ul>
</div></aside>

<p>Overall, the case study is similar to the stock price prediction case study presented earlier in this chapter, with the following differences:</p>

<ul>
<li>
<p>We predict multiple outputs simultaneously, rather than a single output.</p>
</li>
<li>
<p>The predicted variable in this case study is not the return variable.</p>
</li>
<li>
<p>Given that we already covered time series models in case study 1, we focus on artificial neural networks for prediction in this case study.</p>
</li>
</ul>
<img class="blueprint top" src="Images/bracket_top.png" width="1625" height="32"/>








<section data-type="sect2" class="blueprint" data-pdf-bookmark="Blueprint for Using Supervised Learning Models to Predict the Yield Curve"><div class="sect2" id="idm45174924734568">
<h2>Blueprint for Using Supervised Learning Models to Predict the Yield Curve</h2>










<section data-type="sect3" data-pdf-bookmark="1. Problem definition"><div class="sect3" id="idm45174924733048">
<h3>1. Problem definition</h3>

<p>In the supervised regression framework used for this case study, three tenors (1M, 5Y, and 30Y) of the yield curve are the predicted variables. These tenors represent short-term, medium-term, and long-term tenors of the yield curve.</p>

<p>We need to understand what affects the movement of the yield curve and hence incorporate as much information into our model as we can. As a high-level overview, other than the historical price of the yield curve itself, we look at other correlated variables that can influence the yield curve. The independent or predictor variables we consider are:</p>

<ul>
<li>
<p><em>Previous value of the treasury curve for different tenors</em>. The tenors used are 1-month, 3-month, 1-year, 2-year, 5-year, 7-year, 10-year, and 30-year yields.</p>
</li>
<li>
<p><em>Percentage of the federal debt</em> held by the public, foreign governments, and the federal reserve.</p>
</li>
<li>
<p><em>Corporate spread</em> on Baa-rated debt relative to the 10-year treasury rate.</p>
</li>
</ul>

<p>The federal debt and corporate spread are correlated variables and can be potentially useful in modeling the yield curve. The dataset used for this case study is extracted from Yahoo Finance and <a href="https://fred.stlouisfed.org">FRED</a>. We will use the daily data of the last 10 years, from 2010 onward.</p>

<p>By the end of this case study, readers will be familiar with a general machine learning approach to yield curve modeling, from gathering and cleaning data to building and tuning different models.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="2. Getting started—loading the data and Python packages"><div class="sect3" id="idm45174924723992">
<h3>2. Getting started—loading the data and Python packages</h3>












<section data-type="sect4" data-pdf-bookmark="2.1. Loading the Python packages"><div class="sect4" id="idm45174924722904">
<h4>2.1. Loading the Python packages</h4>

<p><a data-type="indexterm" data-primary="yield curve prediction" data-secondary="loading data and Python packages" id="idm45174924721656"/>The loading of Python packages is similar to other case studies in this chapter. Refer to the Jupyter notebook of this case study for more details.</p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="2.2. Loading the data"><div class="sect4" id="idm45174924720072">
<h4>2.2. Loading the data</h4>

<p>The following steps demonstrate the loading of data using Pandas’s <code>DataReader</code> function:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c"># Get the data by webscraping using pandas datareader</code>
<code class="n">tsy_tickers</code> <code class="o">=</code> <code class="p">[</code><code class="s">'DGS1MO'</code><code class="p">,</code> <code class="s">'DGS3MO'</code><code class="p">,</code> <code class="s">'DGS1'</code><code class="p">,</code> <code class="s">'DGS2'</code><code class="p">,</code> <code class="s">'DGS5'</code><code class="p">,</code> <code class="s">'DGS7'</code><code class="p">,</code> <code class="s">'DGS10'</code><code class="p">,</code>
               <code class="s">'DGS30'</code><code class="p">,</code>
               <code class="s">'TREAST'</code><code class="p">,</code> <code class="c"># Treasury securities held by the Federal Reserve ($MM)</code>
               <code class="s">'FYGFDPUN'</code><code class="p">,</code> <code class="c"># Federal Debt Held by the Public ($MM)</code>
               <code class="s">'FDHBFIN'</code><code class="p">,</code> <code class="c"># Federal Debt Held by International Investors ($BN)</code>
               <code class="s">'GFDEBTN'</code><code class="p">,</code> <code class="c"># Federal Debt: Total Public Debt ($BN)</code>
               <code class="s">'BAA10Y'</code><code class="p">,</code> <code class="c"># Baa Corporate Bond Yield Relative to Yield on 10-Year</code>
              <code class="p">]</code>
<code class="n">tsy_data</code> <code class="o">=</code> <code class="n">web</code><code class="o">.</code><code class="n">DataReader</code><code class="p">(</code><code class="n">tsy_tickers</code><code class="p">,</code> <code class="s">'fred'</code><code class="p">)</code><code class="o">.</code><code class="n">dropna</code><code class="p">(</code><code class="n">how</code><code class="o">=</code><code class="s">'all'</code><code class="p">)</code><code class="o">.</code><code class="n">ffill</code><code class="p">()</code>
<code class="n">tsy_data</code><code class="p">[</code><code class="s">'FDHBFIN'</code><code class="p">]</code> <code class="o">=</code> <code class="n">tsy_data</code><code class="p">[</code><code class="s">'FDHBFIN'</code><code class="p">]</code> <code class="o">*</code> <code class="mi">1000</code>
<code class="n">tsy_data</code><code class="p">[</code><code class="s">'GOV_PCT'</code><code class="p">]</code> <code class="o">=</code> <code class="n">tsy_data</code><code class="p">[</code><code class="s">'TREAST'</code><code class="p">]</code> <code class="o">/</code> <code class="n">tsy_data</code><code class="p">[</code><code class="s">'GFDEBTN'</code><code class="p">]</code>
<code class="n">tsy_data</code><code class="p">[</code><code class="s">'HOM_PCT'</code><code class="p">]</code> <code class="o">=</code> <code class="n">tsy_data</code><code class="p">[</code><code class="s">'FYGFDPUN'</code><code class="p">]</code> <code class="o">/</code> <code class="n">tsy_data</code><code class="p">[</code><code class="s">'GFDEBTN'</code><code class="p">]</code>
<code class="n">tsy_data</code><code class="p">[</code><code class="s">'FOR_PCT'</code><code class="p">]</code> <code class="o">=</code> <code class="n">tsy_data</code><code class="p">[</code><code class="s">'FDHBFIN'</code><code class="p">]</code> <code class="o">/</code> <code class="n">tsy_data</code><code class="p">[</code><code class="s">'GFDEBTN'</code><code class="p">]</code></pre>

<p>Next, we define our dependent (<em>Y</em>) and independent (<em>X</em>) variables. The predicted variables are the rate for three tenors of the yield curve (i.e., 1M, 5Y, and 30Y) as mentioned before. The number of trading days in a week is assumed to be five, and we compute the lagged version of the variables mentioned in the problem definition section as independent variables using five trading day lag.</p>

<p>The lagged five-day variables embed the time series component by using a <em>time-delay approach</em>, where the lagged variable is included as one of the independent variables. This step reframes the time series data into a supervised regression–based model framework.</p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="3. Exploratory data analysis"><div class="sect4" id="idm45174924715160">
<h4>3. Exploratory data analysis</h4>

<p><a data-type="indexterm" data-primary="yield curve prediction" data-secondary="exploratory data analysis" id="ix_Chapter5-asciidoc20"/>We will look at descriptive statistics and data visualization in this section.</p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="3.1. Descriptive statistics"><div class="sect4" id="idm45174924630392">
<h4>3.1. Descriptive statistics</h4>

<p>Let us look at the shape and the columns in the dataset:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">dataset</code><code class="o">.</code><code class="n">shape</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">(505, 15)</pre>

<p>The data contains around 500 observations with 15 columns.</p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="3.2. Data visualization"><div class="sect4" id="idm45174924604632">
<h4>3.2. Data visualization</h4>

<p><a data-type="indexterm" data-primary="yield curve prediction" data-secondary="data visualization" id="ix_Chapter5-asciidoc21"/>Let us first plot the predicted variables and see their behavior:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">Y</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">style</code><code class="o">=</code><code class="p">[</code><code class="s">'-'</code><code class="p">,</code><code class="s">'--'</code><code class="p">,</code><code class="s">':'</code><code class="p">])</code></pre>

<p><code>Output</code></p>

<figure><div class="figure">
<img src="Images/mlbf_05in20.png" alt="mlbf 05in20" width="589" height="354"/>
<h6/>
</div></figure>

<p>In the plot, we see that the deviation among the short-term, medium-term, and long-term rates was higher in 2010 and has been decreasing since then. There was a drop in the long-term and medium-term rates during 2011, and they also have been declining since then. The order of the rates has been in line with the tenors. However, for a few months in recent years, the <em>5Y</em> rate has been lower than the <em>1M</em> rate. In the time series of all the tenors, we can see that the mean varies with time, resulting in an upward trend. Thus these series are nonstationary time series.</p>

<p>In some cases, the linear regression for such nonstationary dependent variables might not be valid. However, we are using the lagged variables, which are also nonstationary as independent variables. So we are effectively modeling a nonstationary time series against another nonstationary time series, which might still be valid.</p>

<p>Next, we look at the scatterplots (a correlation plot is skipped for this case study as it has a similar interpretation to that of a scatterplot). We can visualize the relationship between all the variables in the regression using the scatter matrix shown below:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c"># Scatterplot Matrix</code>
<code class="n">pyplot</code><code class="o">.</code><code class="n">figure</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">15</code><code class="p">,</code><code class="mi">15</code><code class="p">))</code>
<code class="n">scatter_matrix</code><code class="p">(</code><code class="n">dataset</code><code class="p">,</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">15</code><code class="p">,</code><code class="mi">16</code><code class="p">))</code>
<code class="n">pyplot</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>

<p><code>Output</code></p>

<figure><div class="figure">
<img src="Images/mlbf_05in21.png" alt="mlbf 05in21" width="882" height="920"/>
<h6/>
</div></figure>

<p>Looking at the scatterplot (full-size version available on <a href="https://oreil.ly/XIsvu">GitHub</a>), we see a significant linear relationship of the predicted variables with their lags and other tenors of the yield curve. There is also a linear relationship, with negative slope between 1M, 5Y rates versus corporate spread and changes in foreign government purchases. The 30Y rate shows a linear relationship with these variables, although the slope is negative. Overall, we see a lot of linear relationships, and we expect the linear models to perform well<a data-type="indexterm" data-startref="ix_Chapter5-asciidoc21" id="idm45174924459000"/>.<a data-type="indexterm" data-startref="ix_Chapter5-asciidoc20" id="idm45174924458168"/></p>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="4. Data preparation and analysis"><div class="sect3" id="idm45174924604008">
<h3>4. Data preparation and analysis</h3>

<p>We performed most of the data preparation steps (i.e., getting the dependent and independent variables) in the preceding steps, and so we’ll skip this step.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="5. Evaluate models"><div class="sect3" id="idm45174924455768">
<h3>5. Evaluate models</h3>

<p><a data-type="indexterm" data-primary="yield curve prediction" data-secondary="evaluation of models" id="idm45174924454568"/>In this step we evaluate the models. The Python code for this step is similar to dthat in case study 1, and some of the repetitive code is skipped. Readers can also refer to the Jupyter notebook of this case study in the code repository for this book for more details.</p>












<section data-type="sect4" data-pdf-bookmark="5.1. Train-test split and evaluation metrics"><div class="sect4" id="idm45174924453048">
<h4>5.1. Train-test split and evaluation metrics</h4>

<p>We will use 80% of the dataset for modeling and use 20% for testing. We will evaluate algorithms using the mean squared error metric. All the algorithms use default tuning parameters.</p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="5.2. Compare models and algorithms"><div class="sect4" id="idm45174924451272">
<h4>5.2. Compare models and algorithms</h4>

<p>In this case study, the primary purpose is to compare the linear models with the artificial neural network in yield curve modeling. So we stick to the linear regression (LR), regularized regression (LASSO and EN), and artificial neural network (shown as MLP). We also include a few other models such as KNN and CART, as these models are simpler with good interpretation, and if there is a nonlinear relationship between the variables, the CART and KNN models will be able to capture it and provide a good comparison benchmark for ANN.</p>

<p>Looking at the training and test error, we see a good performance of the linear regression model. We see that lasso and elastic net perform poorly. These are regularized regression models, and they reduce the number of variables in case they are not important. A decrease in the number of variables might have caused a loss of information leading to poor model performance. KNN and CART are good, but looking closely, we see that the test errors are higher than the training error. We also see that the performance of the artificial neural network (MLP) algorithm is comparable to the linear regression model. Despite its simplicity, the linear regression is a tough benchmark to beat for one-step-ahead forecasting when there is a significant linear relationship between the variables.</p>

<p class="pagebreak-before"><code>Output</code></p>

<figure><div class="figure">
<img src="Images/mlbf_05in22.png" alt="mlbf 05in22" width="1130" height="653"/>
<h6/>
</div></figure>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="6. Model tuning and grid search."><div class="sect3" id="idm45174924445304">
<h3>6. Model tuning and grid search.</h3>

<p><a data-type="indexterm" data-primary="yield curve prediction" data-secondary="model tuning and grid search" id="ix_Chapter5-asciidoc22"/>Similar to case study 2 of this chapter, we perform a grid search of the ANN model with different combinations of hidden layers. Several other hyperparameters such as learning rate, momentum, activation function, number of epochs, and batch size can be tuned during the grid search process, similar to the steps mentioned below.</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="sd">'''</code>
<code class="sd">hidden_layer_sizes : tuple, length = n_layers - 2, default (100,)</code>
<code class="sd">    The ith element represents the number of neurons in the ith</code>
<code class="sd">    hidden layer.</code>
<code class="sd">'''</code>
<code class="n">param_grid</code><code class="o">=</code><code class="p">{</code><code class="s">'hidden_layer_sizes'</code><code class="p">:</code> <code class="p">[(</code><code class="mi">20</code><code class="p">,),</code> <code class="p">(</code><code class="mi">50</code><code class="p">,),</code> <code class="p">(</code><code class="mi">20</code><code class="p">,</code><code class="mi">20</code><code class="p">),</code> <code class="p">(</code><code class="mi">20</code><code class="p">,</code> <code class="mi">30</code><code class="p">,</code> <code class="mi">20</code><code class="p">)]}</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">MLPRegressor</code><code class="p">()</code>
<code class="n">kfold</code> <code class="o">=</code> <code class="n">KFold</code><code class="p">(</code><code class="n">n_splits</code><code class="o">=</code><code class="n">num_folds</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="n">seed</code><code class="p">)</code>
<code class="n">grid</code> <code class="o">=</code> <code class="n">GridSearchCV</code><code class="p">(</code><code class="n">estimator</code><code class="o">=</code><code class="n">model</code><code class="p">,</code> <code class="n">param_grid</code><code class="o">=</code><code class="n">param_grid</code><code class="p">,</code> <code class="n">scoring</code><code class="o">=</code><code class="n">scoring</code><code class="p">,</code> \
  <code class="n">cv</code><code class="o">=</code><code class="n">kfold</code><code class="p">)</code>
<code class="n">grid_result</code> <code class="o">=</code> <code class="n">grid</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">Y_train</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="s">"Best: %f using %s"</code> <code class="o">%</code> <code class="p">(</code><code class="n">grid_result</code><code class="o">.</code><code class="n">best_score_</code><code class="p">,</code> <code class="n">grid_result</code><code class="o">.</code><code class="n">best_params_</code><code class="p">))</code>
<code class="n">means</code> <code class="o">=</code> <code class="n">grid_result</code><code class="o">.</code><code class="n">cv_results_</code><code class="p">[</code><code class="s">'mean_test_score'</code><code class="p">]</code>
<code class="n">stds</code> <code class="o">=</code> <code class="n">grid_result</code><code class="o">.</code><code class="n">cv_results_</code><code class="p">[</code><code class="s">'std_test_score'</code><code class="p">]</code>
<code class="n">params</code> <code class="o">=</code> <code class="n">grid_result</code><code class="o">.</code><code class="n">cv_results_</code><code class="p">[</code><code class="s">'params'</code><code class="p">]</code>
<code class="k">for</code> <code class="n">mean</code><code class="p">,</code> <code class="n">stdev</code><code class="p">,</code> <code class="n">param</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">means</code><code class="p">,</code> <code class="n">stds</code><code class="p">,</code> <code class="n">params</code><code class="p">):</code>
    <code class="nb">print</code><code class="p">(</code><code class="s">"%f (%f) with: %r"</code> <code class="o">%</code> <code class="p">(</code><code class="n">mean</code><code class="p">,</code> <code class="n">stdev</code><code class="p">,</code> <code class="n">param</code><code class="p">))</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">Best: -0.018006 using {'hidden_layer_sizes': (20, 30, 20)}
-0.036433 (0.019326) with: {'hidden_layer_sizes': (20,)}
-0.020793 (0.007075) with: {'hidden_layer_sizes': (50,)}
-0.026638 (0.010154) with: {'hidden_layer_sizes': (20, 20)}
-0.018006 (0.005637) with: {'hidden_layer_sizes': (20, 30, 20)}</pre>

<p>The best model is the model with three layers, with 20, 30, and 20 nodes in each hidden layer, respectively. Hence, we prepare a model with this configuration and check its performance on the test set. This is a crucial step, as a greater number of layers may lead to overfitting and have poor performance in the test set.</p>












<section data-type="sect4" data-pdf-bookmark="Prediction comparison"><div class="sect4" id="idm45174924433496">
<h4>Prediction comparison</h4>

<p>In the last step we look at the prediction plot of actual data versus the prediction from both linear regression and ANN models. Refer to the Jupyter notebook of this case study for the Python code of this section.</p>

<figure><div class="figure">
<img src="Images/mlbf_05in23.png" alt="mlbf 05in23" width="613" height="313"/>
<h6/>
</div></figure>

<figure><div class="figure">
<img src="Images/mlbf_05in24.png" alt="mlbf 05in24" width="613" height="313"/>
<h6/>
</div></figure>

<figure><div class="figure">
<img src="Images/mlbf_05in25.png" alt="mlbf 05in25" width="613" height="313"/>
<h6/>
</div></figure>

<p>Looking at the charts above, we see that the predictions of the linear regression and ANN are comparable. For 1M tenor, the fitting with ANN is slightly poor compared to the regression. However, for 5Y and 30Y tenors the ANN performs as well as the regression model.<a data-type="indexterm" data-startref="ix_Chapter5-asciidoc22" id="idm45174924265896"/></p>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="Conclusion"><div class="sect3" id="idm45174924265064">
<h3>Conclusion</h3>

<p>In this case study, we applied supervised regression to the prediction of several tenors of yield curve. The linear regression model, despite its simplicity, is a tough benchmark to beat for such one-step-ahead forecasting, given the dominant characteristic of the last available value of the variable to predict. The ANN results in this case study are comparable to the linear regression models. An additional benefit of ANN is that it is more flexible to changing market conditions. Also, ANN models can be enhanced by performing grid search on several other hyperparameters and the option of incorporating recurrent neural networks, such as LSTM.</p>

<p>Overall, we built a machine learning–based model using ANN with an encouraging outcome, in the context of fixed income instruments. This allows us to perform predictions using historical data to generate results and analyze risk and profitability before risking any actual capital in the fixed income market.<a data-type="indexterm" data-startref="ix_Chapter5-asciidoc19" id="idm45174924262360"/></p>
<img class="blueprint bottom" src="Images/bracket_bottom.png" width="1625" height="32"/>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Chapter Summary"><div class="sect1" id="idm45174924782792">
<h1>Chapter Summary</h1>

<p>In <a data-type="xref" href="#CaseStudy1SR">“Case Study 1: Stock Price Prediction”</a>, we covered a machine learning and time series–based framework for stock price prediction. We demonstrated the significance of visualization and compared time series against the machine learning models. In <a data-type="xref" href="#CaseStudy2SR">“Case Study 2: Derivative Pricing”</a>, we explored the use of machine learning for a traditional derivative pricing problem and demonstrated a high model performance. In <a data-type="xref" href="#CaseStudy3SR">“Case Study 3: Investor Risk Tolerance and Robo-Advisors”</a>, we demonstrated how supervised learning models can be used to model the risk tolerance of investors, which can lead to automation of the portfolio management process. <a data-type="xref" href="#CaseStudy4SR">“Case Study 4: Yield Curve Prediction”</a> was similar to the stock price prediction case study, providing another example of comparison of linear and nonlinear models in the context of fixed income markets.</p>

<p>We saw that time series and linear supervised learning models worked well for asset price prediction problems (i.e., case studies 1 and 4), where the predicted variable had a significant linear relationship with its lagged component. However, in derivative pricing and risk tolerance prediction, where there are nonlinear relationships, ensemble and ANN models performed better. Readers who are interested in implementing a case study using supervised regression or time series models are encouraged to understand the nuances in the variable relationships and model intuition before proceeding to model selection.</p>

<p>Overall, the concepts in Python, machine learning, time series, and finance presented in this chapter through the case studies can used as a blueprint for any other supervised regression–based problem in finance.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Exercises"><div class="sect1" id="idm45174924253560">
<h1>Exercises</h1>

<ul>
<li>
<p>Using the concepts and framework of machine learning and time series models specified in case study 1, develop a predictive model for another asset class—currency pair (EUR/USD, for example) or bitcoin.</p>
</li>
<li>
<p>In case study 1, add some technical indicators, such as trend or momentum, and check the enhancement in the model performance. Some of the ideas of the technical indicators can be borrowed from <a data-type="xref" href="ch06.xhtml#CaseStudy3SC">“Case Study 3: Bitcoin Trading Strategy”</a> in <a data-type="xref" href="ch06.xhtml#Chapter6">Chapter 6</a>.</p>
</li>
<li>
<p>Using the concepts in <a data-type="xref" href="#CaseStudy2SR">“Case Study 2: Derivative Pricing”</a>, develop a machine learning–based model to price <a href="https://oreil.ly/EMUXv">American options</a>.</p>
</li>
<li>
<p>Incorporate multivariate time series modeling using a variant of the ARIMA model, such as <a href="https://oreil.ly/t7s8q">VARMAX</a>, for rates prediction in the yield curve prediction case study and compare the performance against the machine learning–based models.</p>
</li>
<li>
<p>Enhance the robo-advisor dashboard presented in <a data-type="xref" href="#CaseStudy3SR">“Case Study 3: Investor Risk Tolerance and Robo-Advisors”</a> to incorporate instruments other than equities.<a data-type="indexterm" data-startref="ix_Chapter5-asciidoc0" id="idm45174924242344"/></p>
</li>
</ul>
</div></section>







<div data-type="footnotes"><p data-type="footnote" id="idm45174931460040"><sup><a href="ch05.xhtml#idm45174931460040-marker">1</a></sup> There may be reordering or renaming of the steps or substeps based on the appropriateness and intuitiveness of the steps/substeps.</p><p data-type="footnote" id="idm45174931449640"><sup><a href="ch05.xhtml#idm45174931449640-marker">2</a></sup> An exogenous variable is one whose value is determined outside the model and imposed on the model.</p><p data-type="footnote" id="idm45174931422552"><sup><a href="ch05.xhtml#idm45174931422552-marker">3</a></sup> These models are discussed later in this chapter.</p><p data-type="footnote" id="idm45174931421656"><sup><a href="ch05.xhtml#idm45174931421656-marker">4</a></sup> There may be reordering or renaming of the steps or substeps based on the appropriateness and intuitiveness of the steps/substeps.</p><p data-type="footnote" id="idm45174931308056"><sup><a href="ch05.xhtml#idm45174931308056-marker">5</a></sup> A white noise process is a random process of random variables that are uncorrelated and have a mean of zero and a finite variance.</p><p data-type="footnote" id="idm45174931138088"><sup><a href="ch05.xhtml#idm45174931138088-marker">6</a></sup> A detailed explanation of LSTM models can be found in this <a href="https://oreil.ly/4PDhr">blog post by Christopher Olah </a>.</p><p data-type="footnote" id="idm45174931122968"><sup><a href="ch05.xhtml#idm45174931122968-marker">7</a></sup> An ARIMA model and a Keras-based LSTM model will be demonstrated in one of the case studies.</p><p data-type="footnote" id="idm45174930951640"><sup><a href="ch05.xhtml#idm45174930951640-marker">8</a></sup> Refer to <a data-type="xref" href="ch06.xhtml#CaseStudy3SC">“Case Study 3: Bitcoin Trading Strategy”</a> presented in <a data-type="xref" href="ch06.xhtml#Chapter6">Chapter 6</a> and <a data-type="xref" href="ch10.xhtml#CaseStudy1NLP">“Case Study 1: NLP and Sentiment Analysis–Based Trading Strategies”</a> presented in <a data-type="xref" href="ch10.xhtml#Chapter10">Chapter 10</a> to understand the usage of technical indicators and news-based fundamental analysis as features in the price prediction.</p><p data-type="footnote" id="idm45174930944792"><sup><a href="ch05.xhtml#idm45174930944792-marker">9</a></sup> Equity markets have trading holidays, while currency markets do not. However, the alignment of the dates across all the time series is ensured before any modeling or analysis.</p><p data-type="footnote" id="idm45174930595336"><sup><a href="ch05.xhtml#idm45174930595336-marker">10</a></sup> In different case studies across the book we will demonstrate loading the data through different sources (e.g., CSV, and external websites like quandl).</p><p data-type="footnote" id="idm45174929976312"><sup><a href="ch05.xhtml#idm45174929976312-marker">11</a></sup> The time series is not the stock price but stock return, so the trend is mild compared to the stock price series.</p><p data-type="footnote" id="idm45174927997912"><sup><a href="ch05.xhtml#idm45174927997912-marker">12</a></sup> The predicted variable, which is the option price, should ideally be directly obtained for the market. Given this case study is more for demonstration purposes, we use model-generated option price for the sake of convenience.</p><p data-type="footnote" id="idm45174927663880"><sup><a href="ch05.xhtml#idm45174927663880-marker">13</a></sup> When the spot price is equal to the strike price, at-the-money option.</p><p data-type="footnote" id="idm45174927344984"><sup><a href="ch05.xhtml#idm45174927344984-marker">14</a></sup> Refer to the Jupyter notebook of this case study to go through other charts such as histogram plot and correlation plot.</p><p data-type="footnote" id="idm45174926472776"><sup><a href="ch05.xhtml#idm45174926472776-marker">15</a></sup> Given that the primary purpose of the model is to be used in the portfolio management context, the individual is also referred to as investor in the case study.</p><p data-type="footnote" id="idm45174926394984"><sup><a href="ch05.xhtml#idm45174926394984-marker">16</a></sup> There potentially can be several ways of computing the risk tolerance. In this case study, we use the intuitive ways to measure the risk tolerance of an individual.</p><p data-type="footnote" id="idm45174925517336"><sup><a href="ch05.xhtml#idm45174925517336-marker">17</a></sup> We could have chosen RMSE as the evaluation metric; however, R<sup>2</sup> was chosen as the evaluation metric given that we already used RMSE as the evaluation metric in the previous case studies.</p></div></div></section></div>



  </body></html>