<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 7. Unsupervised Learning: &#10;Dimensionality Reduction"><div class="chapter" id="Chapter7">
<h1><span class="label">Chapter 7. </span>Unsupervised Learning: 
<span class="keep-together">Dimensionality Reduction</span></h1>


<p><a data-type="indexterm" data-primary="dimensionality reduction" id="ix_Chapter7-asciidoc0"/>In previous chapters, we used supervised learning techniques to build machine learning models using data where the answer was already known (i.e., the class labels were available in our input data). <a data-type="indexterm" data-primary="unsupervised learning" data-secondary="defined" id="idm45174917949048"/>Now we will explore <em>unsupervised learning</em>, where we draw inferences from datasets consisting of input data when the answer is unknown. Unsupervised learning algorithms attempt to infer patterns from the data without any knowledge of the output the data is meant to yield. Without requiring labeled data, which can be time-consuming and impractical to create or acquire, this family of models allows for easy use of larger datasets for analysis and model development.</p>

<p><a data-type="indexterm" data-primary="dimensionality reduction" data-secondary="defined" id="idm45174917946824"/><em>Dimensionality reduction</em> is a key technique within unsupervised learning. It compresses the data by finding a smaller, different set of variables that capture what matters most in the original features, while minimizing the loss of information. Dimensionality reduction helps mitigate problems associated with high dimensionality and permits the visualization of salient aspects of higher-dimensional data that is otherwise difficult to explore.</p>

<p>In the context of finance, where datasets are often large and contain many dimensions, dimensionality reduction techniques prove to be quite practical and useful. Dimensionality reduction enables us to reduce noise and redundancy in the dataset and find an approximate version of the dataset using fewer features. With fewer variables to consider, exploration and visualization of a dataset becomes more straightforward. Dimensionality reduction techniques also enhance supervised learning–based models by reducing the number of features or by finding new ones. Practitioners use these dimensionality reduction techniques to allocate funds across asset classes and individual investments, identify trading strategies and signals, implement portfolio hedging and risk management, and develop instrument pricing models.</p>

<p>In this chapter, we will discuss fundamental dimensionality reduction techniques and walk through three case studies in the areas of portfolio management, interest rate modeling, and trading strategy development. The case studies are designed to not only cover diverse topics from a finance standpoint but also highlight multiple machine learning and data science concepts. The standardized template containing the detailed implementation of modeling steps in Python and machine learning and finance concepts can be used as a blueprint for any other dimensionality reduction–based problem in finance.</p>

<p>In <a data-type="xref" href="#CaseStudy1DR">“Case Study 1: Portfolio Management: Finding an Eigen Portfolio”</a>, we use a dimensionality reduction algorithm to allocate capital into different asset classes to maximize risk-adjusted returns. We also introduce a backtesting framework to assess the performance of the portfolio we constructed.</p>

<p>In <a data-type="xref" href="#CaseStudy2DR">“Case Study 2: Yield Curve Construction and Interest Rate Modeling”</a>, we use dimensionality reduction techniques to generate the typical movements of a yield curve. This will illustrate how dimensionality reduction techniques can be used for reducing the dimension of market variables across a number of asset classes to promote faster and effective portfolio management, trading, hedging, and risk management.</p>

<p>In <a data-type="xref" href="#CaseStudy3DR">“Case Study 3: Bitcoin Trading: Enhancing Speed and Accuracy”</a>, we 
<span class="keep-together">use dimensionality</span> reduction techniques for algorithmic trading. This case study demonstrates data exploration in low dimension.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45174917937496">
<h5/>
<p>In addition to the points mentioned above, readers will understand the following points by the end of this chapter:</p>

<ul>
<li>
<p>Basic concepts of models and techniques used for dimensionality reduction and how to implement them in Python.</p>
</li>
<li>
<p>Concepts of eigenvalues and eigenvectors of Principal Component Analysis (PCA), selecting the right number of principal components, and extracting the factor weights of principal components.</p>
</li>
<li>
<p>Usage of dimensionality reduction techniques such as singular value decomposition (SVD) and t-SNE to summarize high-dimensional data for effective data exploration and visualization.</p>
</li>
<li>
<p>How to reconstruct the original data using the reduced principal components.</p>
</li>
<li>
<p>How to enhance the speed and accuracy of supervised learning algorithms using dimensionality reduction.</p>
</li>
<li>
<p>A backtesting framework for the portfolio performance computing and analyzing portfolio performance metrics such as the Sharpe ratio and the annualized return of the portfolio.</p>
</li>
</ul>
</div></aside>
<div data-type="note" epub:type="note" class="note1"><h1>This Chapter’s Code Repository</h1>
<p>A Python-based master template for dimensionality reduction, along with the Jupyter notebook for all the case studies in this chapter, is included in the folder <a href="https://oreil.ly/tI-KJ"><em>Chapter 7 - Unsup. Learning - Dimensionality Reduction</em></a> in the code repository for this book. To work through any dimensionality reduction–modeling machine learning problems in Python involving the dimensionality reduction models (such as PCA, SVD, Kernel PCA, or t-SNE) presented in this chapter, readers need to modify the template slightly to align with their problem statement. All the case studies presented in this chapter use the standard Python master template with the standardized model development steps presented in <a data-type="xref" href="ch03.xhtml#Chapter3">Chapter 3</a>. For the dimensionality reduction case studies, steps 6 (i.e., model tuning) and 7 (i.e., finalizing the model) are relatively lighter compared to the supervised learning models, so these steps have been merged with step 5. For situations in which steps are irrelevant, they have been skipped or combined with others to make the flow of the case study more intuitive.</p>
</div>






<section data-type="sect1" data-pdf-bookmark="Dimensionality Reduction Techniques"><div class="sect1" id="idm45174917925048">
<h1>Dimensionality Reduction Techniques</h1>

<p><a data-type="indexterm" data-primary="dimensionality reduction" data-secondary="techniques" id="ix_Chapter7-asciidoc1"/>Dimensionality reduction represents the information in a given dataset more efficiently by using fewer features. These techniques project data onto a lower dimensional space by either discarding variation in the data that is not informative or identifying a lower dimensional subspace on or near where the data resides.</p>

<p>There are many types of dimensionality reduction techniques. In this chapter, we will cover these most frequently used techniques for dimensionality reduction:</p>

<ul>
<li>
<p>Principal component analysis (PCA)</p>
</li>
<li>
<p>Kernel principal component analysis (KPCA)</p>
</li>
<li>
<p>t-distributed stochastic neighbor embedding (t-SNE)</p>
</li>
</ul>

<p>After application of these dimensionality reduction techniques, the low-dimension feature subspace can be a linear or nonlinear function of the corresponding high-dimensional feature subspace. Hence, on a broad level these dimensionality reduction algorithms can be classified as linear and nonlinear. Linear algorithms, such as PCA, force the new variables to be linear combinations of the original features.</p>

<p>Nonlinear algorithms such KPCA and t-SNE can capture more complex structures in the data. However, given the infinite number of options, the algorithms still need to make assumptions to arrive at a solution.</p>








<section data-type="sect2" data-pdf-bookmark="Principal Component Analysis"><div class="sect2" id="idm45174917916568">
<h2>Principal Component Analysis</h2>

<p><a data-type="indexterm" data-primary="dimensionality reduction" data-secondary="principal component analysis (PCA)" id="ix_Chapter7-asciidoc2"/><a data-type="indexterm" data-primary="principal component analysis (PCA)" id="ix_Chapter7-asciidoc3"/>The idea of principal component analysis (PCA) is to reduce the dimensionality of a dataset with a large number of variables, while retaining as much variance in the data as possible. PCA allows us to understand whether there is a different representation of the data that can explain a majority of the original data points.</p>

<p>PCA finds a set of new variables that, through a linear combination, yield the original variables. The new variables are called <em>principal components</em> (PCs). These principal components are orthogonal (or independent) and can represent the original data. The number of components is a hyperparameter of the PCA algorithm that sets the target dimensionality.</p>

<p>The PCA algorithm works by projecting the original data onto the principal component space. It then identifies a sequence of principal components, each of which aligns with the direction of maximum variance in the data (after accounting for variation captured by previously computed components). The sequential optimization also ensures that new components are not correlated with existing components. Thus the resulting set constitutes an orthogonal basis for a vector space.</p>

<p>The decline in the amount of variance of the original data explained by each principal component reflects the extent of correlation among the original features. The number of components that capture, for example, 95% of the original variation relative to the total number of features provides an insight into the linearly independent information of the original data. In order to understand how PCA works, let’s consider the distribution of data shown in <a data-type="xref" href="#PCA1">Figure 7-1</a>.</p>

<figure><div id="PCA1" class="figure">
<img src="Images/mlbf_0701.png" alt="mlbf 0701" width="350" height="363"/>
<h6><span class="label">Figure 7-1. </span>PCA-1</h6>
</div></figure>

<p>PCA finds a new quadrant system (<em>y’</em> and <em>x’</em> axes) that is obtained from the original through translation and rotation. It will move the center of the coordinate system from the original point <em>(0, 0)</em> to the center of the distribution of data points. It will then move the x-axis into the principal axis of variation, which is the one with the most variation relative to data points (i.e., the direction of maximum spread). Then it moves the other axis orthogonally to the principal one, into a less important direction of variation.</p>

<p><a data-type="xref" href="#PCA2">Figure 7-2</a> shows an example of PCA in which two dimensions explain nearly all the variance of the underlying data.</p>

<figure><div id="PCA2" class="figure">
<img src="Images/mlbf_0702.png" alt="mlbf 0702" width="809" height="363"/>
<h6><span class="label">Figure 7-2. </span>PCA-2</h6>
</div></figure>

<p>These new directions that contain the maximum variance are called principal components and are orthogonal to each other by design.</p>

<p>There are two approaches to finding the principal components: <em>Eigen decomposition</em> and <em>singular value decomposition</em> (SVD).</p>










<section data-type="sect3" data-pdf-bookmark="Eigen decomposition"><div class="sect3" id="idm45174917898952">
<h3>Eigen decomposition</h3>

<p><a data-type="indexterm" data-primary="Eigen decomposition" id="idm45174917897512"/><a data-type="indexterm" data-primary="principal component analysis (PCA)" data-secondary="Eigen decomposition" id="idm45174917896808"/>The steps of Eigen decomposition are as follows:</p>
<ol>
<li>
<p>First, a covariance matrix is created for the features.</p>
</li>
<li>
<p>Once the covariance matrix is computed, the <em>eigenvectors</em> of the covariance matrix are calculated.<sup><a data-type="noteref" id="idm45174917893240-marker" href="ch07.xhtml#idm45174917893240">1</a></sup> These are the directions of maximum variance.</p>
</li>
<li>
<p>The <em>eigenvalues</em> are then created. They define the magnitude of the principal components.</p>
</li>

</ol>

<p>So, for <em>n</em> dimensions, there will be an <em>n</em> × <em>n</em> variance-covariance matrix, and as a result, we will have an eigenvector of <em>n</em> values and <em>n</em> eigenvalues.</p>

<p>Python’s sklearn library offers a powerful implementation of PCA. The <code>sklearn.decomposition.PCA</code> function computes the desired number of principal components and projects the data into the component space. The following code snippet illustrates how to create two principal components from a dataset.</p>

<p class="pagebreak-before"><code>Implementation</code></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c"># Import PCA Algorithm</code>
<code class="kn">from</code> <code class="nn">sklearn.decomposition</code> <code class="k">import</code> <code class="n">PCA</code>
<code class="c"># Initialize the algorithm and set the number of PC's</code>
<code class="n">pca</code> <code class="o">=</code> <code class="n">PCA</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>
<code class="c"># Fit the model to data</code>
<code class="n">pca</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">data</code><code class="p">)</code>
<code class="c"># Get list of PC's</code>
<code class="n">pca</code><code class="o">.</code><code class="n">components_</code>
<code class="c"># Transform the model to data</code>
<code class="n">pca</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">data</code><code class="p">)</code>
<code class="c"># Get the eigenvalues</code>
<code class="n">pca</code><code class="o">.</code><code class="n">explained_variance_ratio</code></pre>

<p>There are additional items, such as <em>factor loading</em>, that can be obtained using the functions in the sklearn library. Their use will be demonstrated in the case studies.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Singular value decomposition"><div class="sect3" id="idm45174917808952">
<h3>Singular value decomposition</h3>

<p><a data-type="indexterm" data-primary="principal component analysis (PCA)" data-secondary="singular value decomposition" id="idm45174917807480"/><a data-type="indexterm" data-primary="singular value decomposition (SVD)" id="idm45174917806472"/><a data-type="indexterm" data-primary="SVD (singular value decomposition)" id="idm45174917805784"/>Singular value decomposition (SVD) is factorization of a matrix into three matrices and is applicable to a more general case of <em>m</em> × <em>n</em> rectangular matrices.</p>

<p>If <em>A</em> is an <em>m</em> × <em>n</em> matrix, then SVD can express the matrix as:</p>
<div data-type="equation">
<math alttext="upper A equals upper U normal upper Sigma upper V Superscript upper T" display="block">
  <mrow>
    <mi>A</mi>
    <mo>=</mo>
    <mi>U</mi>
    <mi>Σ</mi>
    <msup><mi>V</mi> <mi>T</mi> </msup>
  </mrow>
</math>
</div>

<p>where <em>A</em> is an <em>m</em> × <em>n</em> matrix, <em>U</em> is an <em>(m</em> × <em>m)</em> orthogonal matrix, <em>Σ</em> is an <em>(m</em> × <em>n)</em> nonnegative rectangular diagonal matrix, and <em>V</em> is an <em>(n</em> × <em>n)</em> orthogonal matrix. SVD of a given matrix tells us exactly how we can decompose the matrix. <em>Σ</em> is a diagonal matrix with <em>m</em> diagonal values called <em>singular values</em>. Their magnitude indicates how significant they are to preserving the information of the original data. <em>V</em> contains the principal components as column vectors.</p>

<p>As shown above, both Eigen decomposition and SVD tell us that using PCA is effectively looking at the initial data from a different angle. Both will always give the same answer; however, SVD can be much more efficient than Eigen decomposition, as it is able to handle sparse matrices (those which contain very few nonzero elements). In addition, SVD yields better numerical stability, especially when some of the features are strongly correlated.</p>

<p><a data-type="indexterm" data-primary="truncated SVD" id="idm45174917787832"/><em>Truncated SVD</em> is a variant of SVD that computes only the largest singular values, where the number of computes is a user-specified parameter. This method is different from regular SVD in that it produces a factorization where the number of columns is equal to the specified truncation. For example, given an <em>n</em> × <em>n</em> matrix, SVD will produce matrices with <em>n</em> columns, whereas truncated SVD will produce matrices with a specified number of columns that may be less than <em>n</em>.</p>

<p><code>Implementation</code></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">from</code> <code class="nn">sklearn.decomposition</code> <code class="k">import</code> <code class="n">TruncatedSVD</code>
<code class="n">svd</code> <code class="o">=</code> <code class="n">TruncatedSVD</code><code class="p">(</code><code class="n">ncomps</code><code class="o">=</code><code class="mi">20</code><code class="p">)</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">)</code></pre>

<p>In terms of the weaknesses of the PCA technique, although it is very effective in reducing the number of dimensions, the resulting principal components may be less interpretable than the original features. Additionally, the results may be sensitive to the selected number of principal components. For example, too few principal components may miss some information compared to the original list of features. Also, PCA may not work well if the data is strongly nonlinear.<a data-type="indexterm" data-startref="ix_Chapter7-asciidoc3" id="idm45174917771864"/><a data-type="indexterm" data-startref="ix_Chapter7-asciidoc2" id="idm45174917730776"/></p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Kernel Principal Component Analysis"><div class="sect2" id="idm45174917916072">
<h2>Kernel Principal Component Analysis</h2>

<p><a data-type="indexterm" data-primary="dimensionality reduction" data-secondary="kernel PCA" id="idm45174917728968"/><a data-type="indexterm" data-primary="kernel PCA (KPCA)" id="idm45174917728024"/>A main limitation of PCA is that it only applies linear transformations. Kernel principal component analysis (KPCA) extends PCA to handle nonlinearity. It first maps the original data to some nonlinear feature space (usually one of higher dimension). Then it applies PCA to extract the principal components in that space.</p>

<p>A simple example of when KPCA is applicable is shown in <a data-type="xref" href="#KPCA">Figure 7-3</a>. Linear transformations are suitable for the blue and red data points on the left-hand plot. 
<span class="keep-together">However</span>, if all dots are arranged as per the graph on the right, the result is not linearly separable. We would then need to apply KPCA to separate the components.</p>

<figure><div id="KPCA" class="figure">
<img src="Images/mlbf_0703.png" alt="mlbf 0703" width="818" height="372"/>
<h6><span class="label">Figure 7-3. </span>Kernel PCA</h6>
</div></figure>

<p><code>Implementation</code></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">from</code> <code class="nn">sklearn.decomposition</code> <code class="k">import</code> <code class="n">KernelPCA</code>
<code class="n">kpca</code> <code class="o">=</code> <code class="n">KernelPCA</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mi">4</code><code class="p">,</code> <code class="n">kernel</code><code class="o">=</code><code class="s">'rbf'</code><code class="p">)</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">X</code><code class="p">)</code></pre>

<p><a data-type="indexterm" data-primary="radial basis function (rbf)" id="idm45174917685048"/><a data-type="indexterm" data-primary="rbf (radial basis function)" id="idm45174917711496"/>In the Python code, we specify <code>kernel='rbf'</code>, which is the <a href="https://oreil.ly/zCo-X">radial basis function kernel</a>. This is commonly used as a kernel in machine learning techniques, such as in SVMs (see <a data-type="xref" href="ch04.xhtml#Chapter4">Chapter 4</a>).</p>

<p>Using KPCA, component separation becomes easier in a higher dimensional space, as mapping into a higher dimensional space often provides greater classification power.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="t-distributed Stochastic Neighbor Embedding"><div class="sect2" id="idm45174917672856">
<h2>t-distributed Stochastic Neighbor Embedding</h2>

<p><a data-type="indexterm" data-primary="dimensionality reduction" data-secondary="t-distributed stochastic neighbor embedding" id="idm45174917671448"/><a data-type="indexterm" data-primary="t-distributed stochastic neighbor embedding (t-SNE)" id="idm45174917670408"/>t-distributed stochastic neighbor embedding (t-SNE) is a dimensionality reduction algorithm that reduces the dimensions by modeling the probability distribution of neighbors around each point. Here, the term <em>neighbors</em> refers to the set of points closest to a given point. The algorithm emphasizes keeping similar points together in low dimensions as opposed to maintaining the distance between points that are apart in high dimensions.</p>

<p>The algorithm starts by calculating the probability of similarity of data points in corresponding high and low dimensional space. The similarity of points is calculated as the conditional probability that a point <em>A</em> would choose point <em>B</em> as its neighbor if neighbors were picked in proportion to their probability density under a normal distribution centered at <em>A</em>. The algorithm then tries to minimize the difference between these conditional probabilities (or similarities) in the high and low dimensional spaces for a perfect representation of data points in the low dimensional space.</p>

<p><code>Implementation</code></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">from</code> <code class="nn">sklearn.manifold</code> <code class="k">import</code> <code class="n">TSNE</code>
<code class="n">X_tsne</code> <code class="o">=</code> <code class="n">TSNE</code><code class="p">()</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">X</code><code class="p">)</code></pre>

<p>An implementation of t-SNE is shown in the third case study presented in this 
<span class="keep-together">chapter</span>.<a data-type="indexterm" data-startref="ix_Chapter7-asciidoc1" id="idm45174917657464"/></p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Case Study 1: Portfolio Management: Finding an Eigen Portfolio"><div class="sect1" id="CaseStudy1DR">
<h1>Case Study 1: Portfolio Management: Finding an Eigen Portfolio</h1>

<p><a data-type="indexterm" data-primary="eigen portfolio" id="ix_Chapter7-asciidoc4"/>A primary objective of portfolio management is to allocate capital into different asset classes to maximize risk-adjusted returns. Mean-variance portfolio optimization is the most commonly used technique for asset allocation. This method requires an estimated covariance matrix and expected returns of the assets considered. However, the erratic nature of financial returns leads to estimation errors in these inputs, especially when the sample size of returns is insufficient compared to the number of assets being allocated. These errors greatly jeopardize the optimality of the resulting portfolios, leading to poor and unstable outcomes.</p>

<p>Dimensionality reduction is a technique we can use to address this issue. Using PCA, we can take an <em>n</em> × <em>n</em> covariance matrix of our assets and create a set of <em>n</em> linearly uncorrelated principal portfolios (sometimes referred to in literature as an <em>eigen portfolio</em>) made up of our assets and their corresponding variances. The principal components of the covariance matrix capture most of the covariation among the assets and are mutually uncorrelated. Moreover, we can use standardized principal components as the portfolio weights, with the statistical guarantee that the returns from these principal portfolios are linearly uncorrelated.</p>

<p>By the end of this case study, readers will be familiar with a general approach to finding an eigen portfolio for asset allocation, from understanding concepts of PCA to backtesting different principal components.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45174917618792">
<h5/>
<p>This case study will focus on:</p>

<ul>
<li>
<p>Understanding eigenvalues and eigenvectors of PCA and deriving portfolio weights using the principal components.</p>
</li>
<li>
<p>Developing a backtesting framework to evaluate portfolio performance.</p>
</li>
<li>
<p>Understanding how to work through a dimensionality reduction modeling problem from end to end.</p>
</li>
</ul>
</div></aside>
<img class="blueprint top" src="Images/bracket_top.png" width="1625" height="32"/>








<section data-type="sect2" class="blueprint" data-pdf-bookmark="Blueprint for Using Dimensionality Reduction for Asset Allocation"><div class="sect2" id="idm45174917613160">
<h2>Blueprint for Using Dimensionality Reduction for Asset Allocation</h2>










<section data-type="sect3" data-pdf-bookmark="1. Problem definition"><div class="sect3" id="idm45174917611800">
<h3>1. Problem definition</h3>

<p>Our goal in this case study is to maximize the risk-adjusted returns of an equity portfolio using PCA on a dataset of stock returns.</p>

<p>The dataset used for this case study is the Dow Jones Industrial Average (DJIA) index and its respective 30 stocks. The return data used will be from the year 2000 onwards and can be downloaded from Yahoo Finance.</p>

<p>We will also compare the performance of our hypothetical portfolios against a benchmark and backtest the model to evaluate the effectiveness of the approach.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="2. Getting started—loading the data and Python packages"><div class="sect3" id="idm45174917608840">
<h3>2. Getting started—loading the data and Python packages</h3>












<section data-type="sect4" data-pdf-bookmark="2.1. Loading the Python packages"><div class="sect4" id="idm45174917607832">
<h4>2.1. Loading the Python packages</h4>

<p><a data-type="indexterm" data-primary="eigen portfolio" data-secondary="loading data and Python packages" id="idm45174917606424"/>The list of the libraries used for data loading, data analysis, data preparation, model evaluation, and model tuning are shown below. The details of most of these packages and functions can be found in Chapters <a href="ch02.xhtml#Chapter2">2</a> and <a href="ch04.xhtml#Chapter4">4</a>.</p>

<p><code>Packages for dimensionality reduction</code></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">from</code> <code class="nn">sklearn.decomposition</code> <code class="k">import</code> <code class="n">PCA</code>
<code class="kn">from</code> <code class="nn">sklearn.decomposition</code> <code class="k">import</code> <code class="n">TruncatedSVD</code>
<code class="kn">from</code> <code class="nn">numpy.linalg</code> <code class="k">import</code> <code class="n">inv</code><code class="p">,</code> <code class="n">eig</code><code class="p">,</code> <code class="n">svd</code>
<code class="kn">from</code> <code class="nn">sklearn.manifold</code> <code class="k">import</code> <code class="n">TSNE</code>
<code class="kn">from</code> <code class="nn">sklearn.decomposition</code> <code class="k">import</code> <code class="n">KernelPCA</code></pre>

<p class="pagebreak-before"><code>Packages for data processing and visualization</code></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">import</code> <code class="nn">pandas</code> <code class="k">as</code> <code class="nn">pd</code>
<code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="k">as</code> <code class="nn">plt</code>
<code class="kn">from</code> <code class="nn">pandas</code> <code class="k">import</code> <code class="n">read_csv</code><code class="p">,</code> <code class="n">set_option</code>
<code class="kn">from</code> <code class="nn">pandas.plotting</code> <code class="k">import</code> <code class="n">scatter_matrix</code>
<code class="kn">import</code> <code class="nn">seaborn</code> <code class="k">as</code> <code class="nn">sns</code>
<code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="k">import</code> <code class="n">StandardScaler</code></pre>
</div></section>













<section data-type="sect4" data-pdf-bookmark="2.2. Loading the data"><div class="sect4" id="idm45174917456456">
<h4>2.2. Loading the data</h4>

<p>We import the dataframe containing the adjusted closing prices for all the companies in the DJIA index:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c"># load dataset</code>
<code class="n">dataset</code> <code class="o">=</code> <code class="n">read_csv</code><code class="p">(</code><code class="s">'Dow_adjcloses.csv'</code><code class="p">,</code> <code class="n">index_col</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code></pre>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="3. Exploratory data analysis"><div class="sect3" id="idm45174917485304">
<h3>3. Exploratory data analysis</h3>

<p><a data-type="indexterm" data-primary="eigen portfolio" data-secondary="exploratory data analysis" id="idm45174917484296"/>Next, we inspect the dataset.</p>












<section data-type="sect4" data-pdf-bookmark="3.1. Descriptive statistics"><div class="sect4" id="idm45174917483128">
<h4>3.1. Descriptive statistics</h4>

<p>Let’s look at the shape of the data:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">dataset</code><code class="o">.</code><code class="n">shape</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">(4804, 30)</pre>

<p>The data is comprised of 30 columns and 4,804 rows containing the daily closing 
<span class="keep-together">prices</span> of the 30 stocks in the index since 2000.</p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="3.2. Data visualization"><div class="sect4" id="idm45174917465064">
<h4>3.2. Data visualization</h4>

<p>The first thing we must do is gather a basic sense of our data. Let us take a look at the return correlations:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">correlation</code> <code class="o">=</code> <code class="n">dataset</code><code class="o">.</code><code class="n">corr</code><code class="p">()</code>
<code class="n">plt</code><code class="o">.</code><code class="n">figure</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">15</code><code class="p">,</code> <code class="mi">15</code><code class="p">))</code>
<code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="s">'Correlation Matrix'</code><code class="p">)</code>
<code class="n">sns</code><code class="o">.</code><code class="n">heatmap</code><code class="p">(</code><code class="n">correlation</code><code class="p">,</code> <code class="n">vmax</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">square</code><code class="o">=</code><code class="k">True</code><code class="p">,</code><code class="n">annot</code><code class="o">=</code><code class="k">True</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s">'cubehelix'</code><code class="p">)</code></pre>

<p>There is a significant positive correlation between the daily returns. The plot (full-size version available on <a href="https://oreil.ly/yFwu-">GitHub</a>) also indicates that the information embedded in the data may be represented by fewer variables (i.e., something smaller than the 30 dimensions we have now). We will perform another detailed look at the data after implementing dimensionality reduction.</p>

<p class="pagebreak-before"><code>Output</code></p>

<figure><div class="figure">
<img src="Images/mlbf_07in01.png" alt="mlbf 07in01" width="820" height="811"/>
<h6/>
</div></figure>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="4. Data preparation"><div class="sect3" id="idm45174917361384">
<h3>4. Data preparation</h3>

<p>We prepare the data for modeling in the following sections.</p>












<section data-type="sect4" data-pdf-bookmark="4.1. Data cleaning"><div class="sect4" id="idm45174917359928">
<h4>4.1. Data cleaning</h4>

<p><a data-type="indexterm" data-primary="eigen portfolio" data-secondary="data preparation" id="idm45174917358328"/>First, we check for NAs in the rows and either drop them or fill them with the mean of the column:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c">#Checking for any null values and removing the null values'''</code>
<code class="nb">print</code><code class="p">(</code><code class="s">'Null Values ='</code><code class="p">,</code><code class="n">dataset</code><code class="o">.</code><code class="n">isnull</code><code class="p">()</code><code class="o">.</code><code class="n">values</code><code class="o">.</code><code class="n">any</code><code class="p">())</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">Null Values = True</pre>

<p>Some stocks were added to the index after our start date. To ensure proper analysis, we will drop those with more than 30% missing values. Two stocks fit this criteria—Dow Chemicals and Visa:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">missing_fractions</code> <code class="o">=</code> <code class="n">dataset</code><code class="o">.</code><code class="n">isnull</code><code class="p">()</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code><code class="o">.</code><code class="n">sort_values</code><code class="p">(</code><code class="n">ascending</code><code class="o">=</code><code class="k">False</code><code class="p">)</code>
<code class="n">missing_fractions</code><code class="o">.</code><code class="n">head</code><code class="p">(</code><code class="mi">10</code><code class="p">)</code>
<code class="n">drop_list</code> <code class="o">=</code> <code class="nb">sorted</code><code class="p">(</code><code class="nb">list</code><code class="p">(</code><code class="n">missing_fractions</code><code class="p">[</code><code class="n">missing_fractions</code> <code class="o">&gt;</code> <code class="mf">0.3</code><code class="p">]</code><code class="o">.</code><code class="n">index</code><code class="p">))</code>
<code class="n">dataset</code><code class="o">.</code><code class="n">drop</code><code class="p">(</code><code class="n">labels</code><code class="o">=</code><code class="n">drop_list</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">inplace</code><code class="o">=</code><code class="k">True</code><code class="p">)</code>
<code class="n">dataset</code><code class="o">.</code><code class="n">shape</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">(4804, 28)</pre>

<p>We end up with return data for 28 companies and an additional one for the DJIA index. Now we fill the NAs with the mean of the columns:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c"># Fill the missing values with the last value available in the dataset.</code>
<code class="n">dataset</code><code class="o">=</code><code class="n">dataset</code><code class="o">.</code><code class="n">fillna</code><code class="p">(</code><code class="n">method</code><code class="o">=</code><code class="s">'ffill'</code><code class="p">)</code></pre>
</div></section>













<section data-type="sect4" data-pdf-bookmark="4.2. Data transformation"><div class="sect4" id="idm45174917359432">
<h4>4.2. Data transformation</h4>

<p>In addition to handling the missing values, we also want to standardize the dataset features onto a unit scale (mean = 0 and variance = 1). All the variables should be on the same scale before applying PCA; otherwise, a feature with large values will dominate the result. We use <code>StandardScaler</code> in sklearn to standardize the dataset, as shown below:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="k">import</code> <code class="n">StandardScaler</code>
<code class="n">scaler</code> <code class="o">=</code> <code class="n">StandardScaler</code><code class="p">()</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">datareturns</code><code class="p">)</code>
<code class="n">rescaledDataset</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">scaler</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">datareturns</code><code class="p">),</code><code class="n">columns</code> <code class="o">=</code>\
 <code class="n">datareturns</code><code class="o">.</code><code class="n">columns</code><code class="p">,</code> <code class="n">index</code> <code class="o">=</code> <code class="n">datareturns</code><code class="o">.</code><code class="n">index</code><code class="p">)</code>
<code class="c"># summarize transformed data</code>
<code class="n">datareturns</code><code class="o">.</code><code class="n">dropna</code><code class="p">(</code><code class="n">how</code><code class="o">=</code><code class="s">'any'</code><code class="p">,</code> <code class="n">inplace</code><code class="o">=</code><code class="k">True</code><code class="p">)</code>
<code class="n">rescaledDataset</code><code class="o">.</code><code class="n">dropna</code><code class="p">(</code><code class="n">how</code><code class="o">=</code><code class="s">'any'</code><code class="p">,</code> <code class="n">inplace</code><code class="o">=</code><code class="k">True</code><code class="p">)</code></pre>

<p>Overall, cleaning and standardizing the data is important in order to create a meaningful and reliable dataset to be used in dimensionality reduction without error.</p>

<p>Let us look at the returns of one of the stocks from the cleaned and standardized dataset:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c"># Visualizing Log Returns for the DJIA</code>
<code class="n">plt</code><code class="o">.</code><code class="n">figure</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">16</code><code class="p">,</code> <code class="mi">5</code><code class="p">))</code>
<code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="s">"AAPL Return"</code><code class="p">)</code>
<code class="n">rescaledDataset</code><code class="o">.</code><code class="n">AAPL</code><code class="o">.</code><code class="n">plot</code><code class="p">()</code>
<code class="n">plt</code><code class="o">.</code><code class="n">grid</code><code class="p">(</code><code class="k">True</code><code class="p">);</code>
<code class="n">plt</code><code class="o">.</code><code class="n">legend</code><code class="p">()</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>

<p class="pagebreak-before"><code>Output</code></p>

<figure><div class="figure">
<img src="Images/mlbf_07in02.png" alt="mlbf 07in02" width="922" height="316"/>
<h6/>
</div></figure>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="5. Evaluate algorithms and models"><div class="sect3" id="idm45174916983720">
<h3>5. Evaluate algorithms and models</h3>












<section data-type="sect4" data-pdf-bookmark="5.1. Train-test split"><div class="sect4" id="idm45174916982472">
<h4>5.1. Train-test split</h4>

<p><a data-type="indexterm" data-primary="eigen portfolio" data-secondary="evaluation of algorithms and models" id="ix_Chapter7-asciidoc5"/>The portfolio is divided into training and test sets to perform the analysis regarding the best portfolio and to perform backtesting:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c"># Dividing the dataset into training and testing sets</code>
<code class="n">percentage</code> <code class="o">=</code> <code class="nb">int</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">rescaledDataset</code><code class="p">)</code> <code class="o">*</code> <code class="mf">0.8</code><code class="p">)</code>
<code class="n">X_train</code> <code class="o">=</code> <code class="n">rescaledDataset</code><code class="p">[:</code><code class="n">percentage</code><code class="p">]</code>
<code class="n">X_test</code> <code class="o">=</code> <code class="n">rescaledDataset</code><code class="p">[</code><code class="n">percentage</code><code class="p">:]</code>

<code class="n">stock_tickers</code> <code class="o">=</code> <code class="n">rescaledDataset</code><code class="o">.</code><code class="n">columns</code><code class="o">.</code><code class="n">values</code>
<code class="n">n_tickers</code> <code class="o">=</code> <code class="nb">len</code><code class="p">(</code><code class="n">stock_tickers</code><code class="p">)</code></pre>
</div></section>













<section data-type="sect4" data-pdf-bookmark="5.2. Model evaluation: applying principal component analysis"><div class="sect4" id="idm45174917052168">
<h4>5.2. Model evaluation: applying principal component analysis</h4>

<p><a data-type="indexterm" data-primary="principal component analysis (PCA)" data-secondary="asset allocation" id="ix_Chapter7-asciidoc6"/>As the next step, we create a function to perform PCA using the sklearn library. This function generates the principal components from the data that will be used for further analysis:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">pca</code> <code class="o">=</code> <code class="n">PCA</code><code class="p">()</code>
<code class="n">PrincipalComponent</code><code class="o">=</code><code class="n">pca</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code></pre>
</div></section>













<section data-type="sect4" data-pdf-bookmark="5.2.1. Explained variance using PCA"><div class="sect4" id="idm45174916945192">
<h4>5.2.1. Explained variance using PCA</h4>

<p>In this step, we look at the variance explained using PCA. The decline in the amount of variance of the original data explained by each principal component reflects the extent of correlation among the original features. The first principal component captures the most variance in the original data, the second component is a representation of the second highest variance, and so on. The eigenvectors with the lowest eigenvalues describe the least amount of variation within the dataset. Therefore, these values can be dropped.</p>

<p>The following charts show the number of principal components and the variance explained by each.</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">NumEigenvalues</code><code class="o">=</code><code class="mi">20</code>
<code class="n">fig</code><code class="p">,</code> <code class="n">axes</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">ncols</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">14</code><code class="p">,</code><code class="mi">4</code><code class="p">))</code>
<code class="n">Series1</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">Series</code><code class="p">(</code><code class="n">pca</code><code class="o">.</code><code class="n">explained_variance_ratio_</code><code class="p">[:</code><code class="n">NumEigenvalues</code><code class="p">])</code><code class="o">.</code><code class="n">sort_values</code><code class="p">()</code>
<code class="n">Series2</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">Series</code><code class="p">(</code><code class="n">pca</code><code class="o">.</code><code class="n">explained_variance_ratio_</code><code class="p">[:</code><code class="n">NumEigenvalues</code><code class="p">])</code><code class="o">.</code><code class="n">cumsum</code><code class="p">()</code>
<code class="n">Series1</code><code class="o">.</code><code class="n">plot</code><code class="o">.</code><code class="n">barh</code><code class="p">(</code><code class="n">title</code><code class="o">=</code><code class="s">'Explained Variance Ratio by Top Factors'</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="n">axes</code><code class="p">[</code><code class="mi">0</code><code class="p">]);</code>
<code class="n">Series1</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">ylim</code><code class="o">=</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code><code class="mi">1</code><code class="p">),</code> <code class="n">ax</code><code class="o">=</code><code class="n">axes</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code> <code class="n">title</code><code class="o">=</code><code class="s">'Cumulative Explained Variance'</code><code class="p">);</code></pre>

<p><code>Output</code></p>

<figure><div class="figure">
<img src="Images/mlbf_07in03.png" alt="mlbf 07in03" width="797" height="249"/>
<h6/>
</div></figure>

<p>We find that the most important factor explains around 40% of the daily return variation. This dominant principal component is usually interpreted as the “market” factor. We will discuss the interpretation of this and the other factors when looking at the portfolio weights.</p>

<p>The plot on the right shows the cumulative explained variance and indicates that around ten factors explain 73% of the variance in returns of the 28 stocks analyzed.</p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="5.2.2. Looking at portfolio weights"><div class="sect4" id="idm45174916779064">
<h4>5.2.2. Looking at portfolio weights</h4>

<p><a data-type="indexterm" data-primary="portfolio weights" id="ix_Chapter7-asciidoc7"/>In this step, we look more closely at the individual principal components. These may be less interpretable than the original features. However, we can look at the weights of the factors on each principal component to assess any intuitive themes relative to the 28 stocks. We construct five portfolios, defining the weights of each stock as each of the first five principal components. We then create a scatterplot that visualizes an organized descending plot with the respective weight of every company at the current chosen principal component:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="k">def</code> <code class="nf">PCWeights</code><code class="p">():</code>
    <code class="c">#Principal Components (PC) weights for each 28 PCs</code>

    <code class="n">weights</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">()</code>
    <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">pca</code><code class="o">.</code><code class="n">components_</code><code class="p">)):</code>
        <code class="n">weights</code><code class="p">[</code><code class="s">"weights_{}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">i</code><code class="p">)]</code> <code class="o">=</code> \
        <code class="n">pca</code><code class="o">.</code><code class="n">components_</code><code class="p">[</code><code class="n">i</code><code class="p">]</code> <code class="o">/</code> <code class="nb">sum</code><code class="p">(</code><code class="n">pca</code><code class="o">.</code><code class="n">components_</code><code class="p">[</code><code class="n">i</code><code class="p">])</code>
    <code class="n">weights</code> <code class="o">=</code> <code class="n">weights</code><code class="o">.</code><code class="n">values</code><code class="o">.</code><code class="n">T</code>
    <code class="k">return</code> <code class="n">weights</code>
<code class="n">weights</code><code class="o">=</code><code class="n">PCWeights</code><code class="p">()</code></pre>

<pre data-type="programlisting" data-code-language="ipython3"><code class="nb">sum</code><code class="p">(</code><code class="n">pca</code><code class="o">.</code><code class="n">components_</code><code class="p">[</code><code class="mi">0</code><code class="p">])</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">-5.247808242068631</pre>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">NumComponents</code><code class="o">=</code><code class="mi">5</code>
<code class="n">topPortfolios</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">pca</code><code class="o">.</code><code class="n">components_</code><code class="p">[:</code><code class="n">NumComponents</code><code class="p">],</code>\
   <code class="n">columns</code><code class="o">=</code><code class="n">dataset</code><code class="o">.</code><code class="n">columns</code><code class="p">)</code>
<code class="n">eigen_portfolios</code> <code class="o">=</code> <code class="n">topPortfolios</code><code class="o">.</code><code class="n">div</code><code class="p">(</code><code class="n">topPortfolios</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="mi">1</code><code class="p">),</code> <code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
<code class="n">eigen_portfolios</code><code class="o">.</code><code class="n">index</code> <code class="o">=</code> <code class="p">[</code><code class="n">f</code><code class="s">'Portfolio {i}'</code> <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code> <code class="n">NumComponents</code><code class="p">)]</code>
<code class="n">np</code><code class="o">.</code><code class="n">sqrt</code><code class="p">(</code><code class="n">pca</code><code class="o">.</code><code class="n">explained_variance_</code><code class="p">)</code>
<code class="n">eigen_portfolios</code><code class="o">.</code><code class="n">T</code><code class="o">.</code><code class="n">plot</code><code class="o">.</code><code class="n">bar</code><code class="p">(</code><code class="n">subplots</code><code class="o">=</code><code class="k">True</code><code class="p">,</code> <code class="n">layout</code><code class="o">=</code><code class="p">(</code><code class="nb">int</code><code class="p">(</code><code class="n">NumComponents</code><code class="p">),</code><code class="mi">1</code><code class="p">),</code>  \
<code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">14</code><code class="p">,</code><code class="mi">10</code><code class="p">),</code> <code class="n">legend</code><code class="o">=</code><code class="k">False</code><code class="p">,</code> <code class="n">sharey</code><code class="o">=</code><code class="k">True</code><code class="p">,</code> <code class="n">ylim</code><code class="o">=</code> <code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code><code class="mi">1</code><code class="p">))</code></pre>

<p>Given that scale for the plots are the same, we can also look at the heatmap as follows:</p>

<p><code>Output</code></p>

<figure><div class="figure">
<img src="Images/mlbf_07in04.png" alt="mlbf 07in04" width="813" height="596"/>
<h6/>
</div></figure>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c"># plotting heatmap</code>
<code class="n">sns</code><code class="o">.</code><code class="n">heatmap</code><code class="p">(</code><code class="n">topPortfolios</code><code class="p">)</code></pre>

<p class="pagebreak-before"><code>Output</code></p>

<figure><div class="figure">
<img src="Images/mlbf_07in05.png" alt="mlbf 07in05" width="349" height="253"/>
<h6/>
</div></figure>

<p>The heatmap and barplots show the contribution of different stocks in each 
<span class="keep-together">eigenvector</span>.</p>

<p>Traditionally, the intuition behind each principal portfolio is that it represents some sort of independent risk factor. The manifestation of those risk factors depends on the assets in the portfolio. In our case study, the assets are all U.S. domestic equities. The principal portfolio with the largest variance is typically a systematic risk factor (i.e., “market” factor). Looking at the first principal component (<em>Portfolio 0</em>), we see that the weights are distributed homogeneously across the stocks. This nearly equal weighted portfolio explains 40% of the variance in the index and is a fair representation of a systematic risk factor.</p>

<p>The rest of the eigen portfolios typically correspond to sector or industry factors. For example, <em>Portfolio 1</em> assigns a high weight to JNJ and MRK, which are stocks from the health care sector. Similarly, <em>Portfolio 3</em> has high weights on technology and electronics companies, such AAPL, MSFT, and IBM.</p>

<p>When the asset universe for our portfolio is expanded to include broad, global investments, we may identify factors for international equity risk, interest rate risk, commodity exposure, geographic risk, and many others.<a data-type="indexterm" data-startref="ix_Chapter7-asciidoc7" id="idm45174916519784"/></p>

<p>In the next step, we find the best eigen portfolio.</p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="5.2.3. Finding the best eigen portfolio"><div class="sect4" id="idm45174916778568">
<h4>5.2.3. Finding the best eigen portfolio</h4>

<p><a data-type="indexterm" data-primary="eigen portfolio" data-secondary="Sharpe ratio to determine best portfolio" id="ix_Chapter7-asciidoc8"/><a data-type="indexterm" data-primary="Sharpe radio" id="ix_Chapter7-asciidoc9"/>To determine the best eigen portfolio, we use the <em>Sharpe ratio</em>. This is an assessment of risk-adjusted performance that explains the annualized returns against the annualized volatility of a portfolio. A high Sharpe ratio explains higher returns and/or lower volatility for the specified portfolio. The annualized Sharpe ratio is computed by dividing the annualized returns against the annualized volatility. For annualized return we apply the geometric average of all the returns in respect to the periods per year (days of operations in the exchange in a year). Annualized volatility is computed by taking the standard deviation of the returns and multiplying it by the square root of the periods per year.</p>

<p>The following code computes the Sharpe ratio of a portfolio:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c"># Sharpe Ratio Calculation</code>
<code class="c"># Calculation based on conventional number of trading days per year (i.e., 252).</code>
<code class="k">def</code> <code class="nf">sharpe_ratio</code><code class="p">(</code><code class="n">ts_returns</code><code class="p">,</code> <code class="n">periods_per_year</code><code class="o">=</code><code class="mi">252</code><code class="p">):</code>
    <code class="n">n_years</code> <code class="o">=</code> <code class="n">ts_returns</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">/</code> <code class="n">periods_per_year</code>
    <code class="n">annualized_return</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">power</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">prod</code><code class="p">(</code><code class="mi">1</code><code class="o">+</code><code class="n">ts_returns</code><code class="p">),</code> <code class="p">(</code><code class="mi">1</code><code class="o">/</code><code class="n">n_years</code><code class="p">))</code><code class="o">-</code><code class="mi">1</code>
    <code class="n">annualized_vol</code> <code class="o">=</code> <code class="n">ts_returns</code><code class="o">.</code><code class="n">std</code><code class="p">()</code> <code class="o">*</code> <code class="n">np</code><code class="o">.</code><code class="n">sqrt</code><code class="p">(</code><code class="n">periods_per_year</code><code class="p">)</code>
    <code class="n">annualized_sharpe</code> <code class="o">=</code> <code class="n">annualized_return</code> <code class="o">/</code> <code class="n">annualized_vol</code>

    <code class="k">return</code> <code class="n">annualized_return</code><code class="p">,</code> <code class="n">annualized_vol</code><code class="p">,</code> <code class="n">annualized_sharpe</code></pre>

<p>We construct a loop to compute the principal component weights for each eigen portfolio. Then it uses the Sharpe ratio function to look for the portfolio with the highest Sharpe ratio. Once we know which portfolio has the highest Sharpe ratio, we can visualize its performance against the index for comparison:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="k">def</code> <code class="nf">optimizedPortfolio</code><code class="p">():</code>
    <code class="n">n_portfolios</code> <code class="o">=</code> <code class="nb">len</code><code class="p">(</code><code class="n">pca</code><code class="o">.</code><code class="n">components_</code><code class="p">)</code>
    <code class="n">annualized_ret</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="mf">0.</code><code class="p">]</code> <code class="o">*</code> <code class="n">n_portfolios</code><code class="p">)</code>
    <code class="n">sharpe_metric</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="mf">0.</code><code class="p">]</code> <code class="o">*</code> <code class="n">n_portfolios</code><code class="p">)</code>
    <code class="n">annualized_vol</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="mf">0.</code><code class="p">]</code> <code class="o">*</code> <code class="n">n_portfolios</code><code class="p">)</code>
    <code class="n">highest_sharpe</code> <code class="o">=</code> <code class="mi">0</code>
    <code class="n">stock_tickers</code> <code class="o">=</code> <code class="n">rescaledDataset</code><code class="o">.</code><code class="n">columns</code><code class="o">.</code><code class="n">values</code>
    <code class="n">n_tickers</code> <code class="o">=</code> <code class="nb">len</code><code class="p">(</code><code class="n">stock_tickers</code><code class="p">)</code>
    <code class="n">pcs</code> <code class="o">=</code> <code class="n">pca</code><code class="o">.</code><code class="n">components_</code>

    <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">n_portfolios</code><code class="p">):</code>

        <code class="n">pc_w</code> <code class="o">=</code> <code class="n">pcs</code><code class="p">[</code><code class="n">i</code><code class="p">]</code> <code class="o">/</code> <code class="nb">sum</code><code class="p">(</code><code class="n">pcs</code><code class="p">[</code><code class="n">i</code><code class="p">])</code>
        <code class="n">eigen_prtfi</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code> <code class="o">=</code><code class="p">{</code><code class="s">'weights'</code><code class="p">:</code> <code class="n">pc_w</code><code class="o">.</code><code class="n">squeeze</code><code class="p">()</code><code class="o">*</code><code class="mi">100</code><code class="p">},</code> \
        <code class="n">index</code> <code class="o">=</code> <code class="n">stock_tickers</code><code class="p">)</code>
        <code class="n">eigen_prtfi</code><code class="o">.</code><code class="n">sort_values</code><code class="p">(</code><code class="n">by</code><code class="o">=</code><code class="p">[</code><code class="s">'weights'</code><code class="p">],</code> <code class="n">ascending</code><code class="o">=</code><code class="k">False</code><code class="p">,</code> <code class="n">inplace</code><code class="o">=</code><code class="k">True</code><code class="p">)</code>
        <code class="n">eigen_prti_returns</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">dot</code><code class="p">(</code><code class="n">X_train_raw</code><code class="o">.</code><code class="n">loc</code><code class="p">[:,</code> <code class="n">eigen_prtfi</code><code class="o">.</code><code class="n">index</code><code class="p">],</code> <code class="n">pc_w</code><code class="p">)</code>
        <code class="n">eigen_prti_returns</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">Series</code><code class="p">(</code><code class="n">eigen_prti_returns</code><code class="o">.</code><code class="n">squeeze</code><code class="p">(),</code>\
         <code class="n">index</code><code class="o">=</code><code class="n">X_train_raw</code><code class="o">.</code><code class="n">index</code><code class="p">)</code>
        <code class="n">er</code><code class="p">,</code> <code class="n">vol</code><code class="p">,</code> <code class="n">sharpe</code> <code class="o">=</code> <code class="n">sharpe_ratio</code><code class="p">(</code><code class="n">eigen_prti_returns</code><code class="p">)</code>
        <code class="n">annualized_ret</code><code class="p">[</code><code class="n">i</code><code class="p">]</code> <code class="o">=</code> <code class="n">er</code>
        <code class="n">annualized_vol</code><code class="p">[</code><code class="n">i</code><code class="p">]</code> <code class="o">=</code> <code class="n">vol</code>
        <code class="n">sharpe_metric</code><code class="p">[</code><code class="n">i</code><code class="p">]</code> <code class="o">=</code> <code class="n">sharpe</code>

        <code class="n">sharpe_metric</code><code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">nan_to_num</code><code class="p">(</code><code class="n">sharpe_metric</code><code class="p">)</code>

    <code class="c"># find portfolio with the highest Sharpe ratio</code>
    <code class="n">highest_sharpe</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">argmax</code><code class="p">(</code><code class="n">sharpe_metric</code><code class="p">)</code>

    <code class="nb">print</code><code class="p">(</code><code class="s">'Eigen portfolio #%d with the highest Sharpe. Return %.2f%%,</code><code class="se">\</code>
<code class="s">     vol = %.2f%%, Sharpe = %.2f'</code> <code class="o">%</code>
          <code class="p">(</code><code class="n">highest_sharpe</code><code class="p">,</code>
           <code class="n">annualized_ret</code><code class="p">[</code><code class="n">highest_sharpe</code><code class="p">]</code><code class="o">*</code><code class="mi">100</code><code class="p">,</code>
           <code class="n">annualized_vol</code><code class="p">[</code><code class="n">highest_sharpe</code><code class="p">]</code><code class="o">*</code><code class="mi">100</code><code class="p">,</code>
           <code class="n">sharpe_metric</code><code class="p">[</code><code class="n">highest_sharpe</code><code class="p">]))</code>


    <code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">()</code>
    <code class="n">fig</code><code class="o">.</code><code class="n">set_size_inches</code><code class="p">(</code><code class="mi">12</code><code class="p">,</code> <code class="mi">4</code><code class="p">)</code>
    <code class="n">ax</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">sharpe_metric</code><code class="p">,</code> <code class="n">linewidth</code><code class="o">=</code><code class="mi">3</code><code class="p">)</code>
    <code class="n">ax</code><code class="o">.</code><code class="n">set_title</code><code class="p">(</code><code class="s">'Sharpe ratio of eigen-portfolios'</code><code class="p">)</code>
    <code class="n">ax</code><code class="o">.</code><code class="n">set_ylabel</code><code class="p">(</code><code class="s">'Sharpe ratio'</code><code class="p">)</code>
    <code class="n">ax</code><code class="o">.</code><code class="n">set_xlabel</code><code class="p">(</code><code class="s">'Portfolios'</code><code class="p">)</code>

    <code class="n">results</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="p">{</code><code class="s">'Return'</code><code class="p">:</code> <code class="n">annualized_ret</code><code class="p">,</code>\
    <code class="s">'Vol'</code><code class="p">:</code> <code class="n">annualized_vol</code><code class="p">,</code>
    <code class="s">'Sharpe'</code><code class="p">:</code> <code class="n">sharpe_metric</code><code class="p">})</code>
    <code class="n">results</code><code class="o">.</code><code class="n">dropna</code><code class="p">(</code><code class="n">inplace</code><code class="o">=</code><code class="k">True</code><code class="p">)</code>
    <code class="n">results</code><code class="o">.</code><code class="n">sort_values</code><code class="p">(</code><code class="n">by</code><code class="o">=</code><code class="p">[</code><code class="s">'Sharpe'</code><code class="p">],</code> <code class="n">ascending</code><code class="o">=</code><code class="k">False</code><code class="p">,</code> <code class="n">inplace</code><code class="o">=</code><code class="k">True</code><code class="p">)</code>
    <code class="nb">print</code><code class="p">(</code><code class="n">results</code><code class="o">.</code><code class="n">head</code><code class="p">(</code><code class="mi">5</code><code class="p">))</code>

    <code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code>

<code class="n">optimizedPortfolio</code><code class="p">()</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">Eigen portfolio #0 with the highest Sharpe. Return 11.47%, vol = 13.31%, \
Sharpe = 0.86
    Return    Vol  Sharpe
0    0.115  0.133   0.862
7    0.096  0.693   0.138
5    0.100  0.845   0.118
1    0.057  0.670   0.084</pre>

<figure><div class="figure">
<img src="Images/mlbf_07in06.png" alt="mlbf 07in06" width="717" height="263"/>
<h6/>
</div></figure>

<p>As shown by the results above, <em>Portfolio 0</em> is the best performing one, with the highest return <em>and</em> the lowest volatility. Let us look at the composition of this portfolio:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">weights</code> <code class="o">=</code> <code class="n">PCWeights</code><code class="p">()</code>
<code class="n">portfolio</code> <code class="o">=</code> <code class="n">portfolio</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">()</code>

<code class="k">def</code> <code class="nf">plotEigen</code><code class="p">(</code><code class="n">weights</code><code class="p">,</code> <code class="n">plot</code><code class="o">=</code><code class="k">False</code><code class="p">,</code> <code class="n">portfolio</code><code class="o">=</code><code class="n">portfolio</code><code class="p">):</code>
    <code class="n">portfolio</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code> <code class="o">=</code><code class="p">{</code><code class="s">'weights'</code><code class="p">:</code> <code class="n">weights</code><code class="o">.</code><code class="n">squeeze</code><code class="p">()</code> <code class="o">*</code> <code class="mi">100</code><code class="p">},</code> \
    <code class="n">index</code> <code class="o">=</code> <code class="n">stock_tickers</code><code class="p">)</code>
    <code class="n">portfolio</code><code class="o">.</code><code class="n">sort_values</code><code class="p">(</code><code class="n">by</code><code class="o">=</code><code class="p">[</code><code class="s">'weights'</code><code class="p">],</code> <code class="n">ascending</code><code class="o">=</code><code class="k">False</code><code class="p">,</code> <code class="n">inplace</code><code class="o">=</code><code class="k">True</code><code class="p">)</code>
    <code class="k">if</code> <code class="n">plot</code><code class="p">:</code>
        <code class="n">portfolio</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">title</code><code class="o">=</code><code class="s">'Current Eigen-Portfolio Weights'</code><code class="p">,</code>
            <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">12</code><code class="p">,</code> <code class="mi">6</code><code class="p">),</code>
            <code class="n">xticks</code><code class="o">=</code><code class="nb">range</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="nb">len</code><code class="p">(</code><code class="n">stock_tickers</code><code class="p">),</code> <code class="mi">1</code><code class="p">),</code>
            <code class="n">rot</code><code class="o">=</code><code class="mi">45</code><code class="p">,</code>
            <code class="n">linewidth</code><code class="o">=</code><code class="mi">3</code>
            <code class="p">)</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code>


    <code class="k">return</code> <code class="n">portfolio</code>

<code class="c"># Weights are stored in arrays, where 0 is the first PC's weights.</code>
<code class="n">plotEigen</code><code class="p">(</code><code class="n">weights</code><code class="o">=</code><code class="n">weights</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="n">plot</code><code class="o">=</code><code class="k">True</code><code class="p">)</code></pre>

<p><code>Output</code></p>

<figure><div class="figure">
<img src="Images/mlbf_07in07.png" alt="mlbf 07in07" width="708" height="375"/>
<h6/>
</div></figure>

<p>Recall that this is the portfolio that explains 40% of the variance and represents the systematic risk factor. Looking at the portfolio weights (in percentages in the y-axis), they do not vary much and are in the range of 2.7% to 4.5% across all stocks. However, the weights seem to be higher in the financial sector, and stocks such as AXP, JPM, and GS have higher-than-average weights.<a data-type="indexterm" data-startref="ix_Chapter7-asciidoc9" id="idm45174915840888"/><a data-type="indexterm" data-startref="ix_Chapter7-asciidoc8" id="idm45174915840184"/></p>
</div></section>













<section data-type="sect4" class="pagebreak-before less_space" data-pdf-bookmark="5.2.4. Backtesting the eigen portfolios"><div class="sect4" id="idm45174915941128">
<h4>5.2.4. Backtesting the eigen portfolios</h4>

<p><a data-type="indexterm" data-primary="backtesting" data-secondary="eigen portfolio" id="ix_Chapter7-asciidoc10"/><a data-type="indexterm" data-primary="eigen portfolio" data-secondary="backtesting" id="ix_Chapter7-asciidoc11"/>We will now try to backtest this algorithm on the test set. We will look at a few of the top performers and the worst performer. For the top performers we look at the 3rd- and 4th-ranked eigen portfolios (<em>Portfolios 5</em> and <em>1</em>), while the worst performer reviewed was ranked 19th (<em>Portfolio 14</em>):</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="k">def</code> <code class="nf">Backtest</code><code class="p">(</code><code class="n">eigen</code><code class="p">):</code>

    <code class="sd">'''</code>
<code class="sd">    Plots principal components returns against real returns.</code>
<code class="sd">    '''</code>

    <code class="n">eigen_prtfi</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code> <code class="o">=</code><code class="p">{</code><code class="s">'weights'</code><code class="p">:</code> <code class="n">eigen</code><code class="o">.</code><code class="n">squeeze</code><code class="p">()},</code> \
    <code class="n">index</code><code class="o">=</code><code class="n">stock_tickers</code><code class="p">)</code>
    <code class="n">eigen_prtfi</code><code class="o">.</code><code class="n">sort_values</code><code class="p">(</code><code class="n">by</code><code class="o">=</code><code class="p">[</code><code class="s">'weights'</code><code class="p">],</code> <code class="n">ascending</code><code class="o">=</code><code class="k">False</code><code class="p">,</code> <code class="n">inplace</code><code class="o">=</code><code class="k">True</code><code class="p">)</code>

    <code class="n">eigen_prti_returns</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">dot</code><code class="p">(</code><code class="n">X_test_raw</code><code class="o">.</code><code class="n">loc</code><code class="p">[:,</code> <code class="n">eigen_prtfi</code><code class="o">.</code><code class="n">index</code><code class="p">],</code> <code class="n">eigen</code><code class="p">)</code>
    <code class="n">eigen_portfolio_returns</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">Series</code><code class="p">(</code><code class="n">eigen_prti_returns</code><code class="o">.</code><code class="n">squeeze</code><code class="p">(),</code>\
     <code class="n">index</code><code class="o">=</code><code class="n">X_test_raw</code><code class="o">.</code><code class="n">index</code><code class="p">)</code>
    <code class="n">returns</code><code class="p">,</code> <code class="n">vol</code><code class="p">,</code> <code class="n">sharpe</code> <code class="o">=</code> <code class="n">sharpe_ratio</code><code class="p">(</code><code class="n">eigen_portfolio_returns</code><code class="p">)</code>
    <code class="nb">print</code><code class="p">(</code><code class="s">'Current Eigen-Portfolio:</code><code class="se">\n</code><code class="s">Return = %.2f%%</code><code class="se">\n</code><code class="s">Volatility = %.2f%%</code><code class="se">\n\</code>
<code class="s">    Sharpe = %.2f'</code> <code class="o">%</code> <code class="p">(</code><code class="n">returns</code> <code class="o">*</code> <code class="mi">100</code><code class="p">,</code> <code class="n">vol</code> <code class="o">*</code> <code class="mi">100</code><code class="p">,</code> <code class="n">sharpe</code><code class="p">))</code>
    <code class="n">equal_weight_return</code><code class="o">=</code><code class="p">(</code><code class="n">X_test_raw</code> <code class="o">*</code> <code class="p">(</code><code class="mi">1</code><code class="o">/</code><code class="nb">len</code><code class="p">(</code><code class="n">pca</code><code class="o">.</code><code class="n">components_</code><code class="p">)))</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
    <code class="n">df_plot</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">({</code><code class="s">'EigenPorfolio Return'</code><code class="p">:</code> <code class="n">eigen_portfolio_returns</code><code class="p">,</code> \
    <code class="s">'Equal Weight Index'</code><code class="p">:</code> <code class="n">equal_weight_return</code><code class="p">},</code> <code class="n">index</code><code class="o">=</code><code class="n">X_test</code><code class="o">.</code><code class="n">index</code><code class="p">)</code>
    <code class="n">np</code><code class="o">.</code><code class="n">cumprod</code><code class="p">(</code><code class="n">df_plot</code> <code class="o">+</code> <code class="mi">1</code><code class="p">)</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">title</code><code class="o">=</code><code class="s">'Returns of the equal weighted</code><code class="se">\</code>
<code class="s">     index vs. First eigen-portfolio'</code><code class="p">,</code>
                          <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">12</code><code class="p">,</code> <code class="mi">6</code><code class="p">),</code> <code class="n">linewidth</code><code class="o">=</code><code class="mi">3</code><code class="p">)</code>
    <code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code>

<code class="n">Backtest</code><code class="p">(</code><code class="n">eigen</code><code class="o">=</code><code class="n">weights</code><code class="p">[</code><code class="mi">5</code><code class="p">])</code>
<code class="n">Backtest</code><code class="p">(</code><code class="n">eigen</code><code class="o">=</code><code class="n">weights</code><code class="p">[</code><code class="mi">1</code><code class="p">])</code>
<code class="n">Backtest</code><code class="p">(</code><code class="n">eigen</code><code class="o">=</code><code class="n">weights</code><code class="p">[</code><code class="mi">14</code><code class="p">])</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">Current Eigen-Portfolio:
Return = 32.76%
Volatility = 68.64%
Sharpe = 0.48</pre>

<figure><div class="figure">
<img src="Images/mlbf_07in08.png" alt="mlbf 07in08" width="699" height="371"/>
<h6/>
</div></figure>

<pre data-type="programlisting">Current Eigen-Portfolio:
Return = 99.80%
Volatility = 58.34%
Sharpe = 1.71</pre>

<figure><div class="figure">
<img src="Images/mlbf_07in09.png" alt="mlbf 07in09" width="699" height="371"/>
<h6/>
</div></figure>

<pre data-type="programlisting">Current Eigen-Portfolio:
Return = -79.42%
Volatility = 185.30%
Sharpe = -0.43</pre>

<figure><div class="figure">
<img src="Images/mlbf_07in10.png" alt="mlbf 07in10" width="699" height="371"/>
<h6/>
</div></figure>

<p>As shown in the preceding charts, the eigen portfolio return of the top portfolios outperforms the equally weighted index. The eigen portfolio ranked 19th underperformed the market significantly in the test set. The outperformance and underperformance are attributed to the weights of the stocks or sectors in the eigen portfolio. We can drill down further to understand the individual drivers of each portfolio. For example, <em>Portfolio 1</em> assigns high weight to several stocks in the health care sector, as discussed previously. This sector saw a significant increase in 2017 onwards, which is reflected in the chart for <em>Eigen Portfolio 1</em>.<a data-type="indexterm" data-startref="ix_Chapter7-asciidoc11" id="idm45174915558888"/><a data-type="indexterm" data-startref="ix_Chapter7-asciidoc10" id="idm45174915558152"/></p>

<p>Given that these eigen portfolios are independent, they also provide diversification opportunities. As such, we can invest across these uncorrelated eigen portfolios, providing other potential portfolio management benefits<a data-type="indexterm" data-startref="ix_Chapter7-asciidoc6" id="idm45174915556856"/>.<a data-type="indexterm" data-startref="ix_Chapter7-asciidoc5" id="idm45174915556024"/></p>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="Conclusion"><div class="sect3" id="idm45174915839256">
<h3>Conclusion</h3>

<p>In this case study, we applied dimensionality reduction techniques in the context of portfolio management, using eigenvalues and eigenvectors from PCA to perform asset allocation.</p>

<p>We demonstrated that, while some interpretability is lost, the initution behind the resulting portfolios can be matched to risk factors. In this example, the first eigen portfolio represented a systematic risk factor, while others exhibited sector or industry concentration.</p>

<p>Through backtesting, we found that the portfolio with the best result on the training set also achieved the strongest performance on the test set. Several of the portfolios outperformed the index based on the Sharpe ratio, the risk-adjusted performance metric used in this exercise.</p>

<p>Overall, we found that using PCA and analyzing eigen portfolios can yield a robust methodology for asset allocation and portfolio management.<a data-type="indexterm" data-startref="ix_Chapter7-asciidoc4" id="idm45174915551592"/></p>
<img class="blueprint bottom" src="Images/bracket_bottom.png" width="1625" height="32"/>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Case Study 2: Yield Curve Construction and Interest Rate Modeling"><div class="sect1" id="CaseStudy2DR">
<h1>Case Study 2: Yield Curve Construction and Interest Rate Modeling</h1>

<p><a data-type="indexterm" data-primary="yield curve construction/interest rate modeling case study" id="ix_Chapter7-asciidoc12"/>A number of problems in portfolio management, trading, and risk management require a deep understanding and modeling of yield curves.</p>

<p>A yield curve represents interest rates, or yields, across a range of maturities, usually depicted in a line graph, as discussed in <a data-type="xref" href="ch05.xhtml#CaseStudy4SR">“Case Study 4: Yield Curve Prediction”</a> in <a data-type="xref" href="ch05.xhtml#Chapter5">Chapter 5</a>. Yield curve illustrates the “price of funds” at a given point in time and, due to the time value of money, often shows interest rates rising as a function of maturity.</p>

<p>Researchers in finance have studied the yield curve and found that shifts or changes in the shape of the yield curve are attributable to a few unobservable factors. Specifically, empirical studies reveal that more than 99% of the movement of various U.S. Treasury bond yields are captured by three factors, which are often referred to as level, slope, and curvature. The names describe how each influences the yield curve shape in response to a shock. A level shock changes the interest rates of all maturities by almost identical amounts, inducing a <em>parallel shift</em> that changes the level of the entire curve up or down. A shock to the slope factor changes the difference in short-term and long-term rates. For instance, when long-term rates increase by a larger amount than do short-term rates, it results in a curve that becomes steeper (i.e., visually, the curve becomes more upward sloping). Changes in the short- and long-term rates can also produce a flatter yield curve. The main effects of the shock to the curvature factor focuses on medium-term interest rates, leading to hump, twist, or U-shaped characteristics.</p>

<p>Dimensionality reduction breaks down the movement of the yield curve into these three factors. Reducing the yield curve into fewer components means we can focus on a few intuitive dimensions in the yield curve. Traders and risk managers use this technique to condense the curve in risk factors for hedging the interest rate risk. Similarly, portfolio managers then have fewer dimensions to analyze when allocating funds. Interest rate structurers use this technique to model the yield curve and analyze its shape. Overall, it promotes faster and more effective portfolio management, trading, hedging, and risk management.</p>

<p>In this case study, we use PCA to generate typical movements of a yield curve and show that the first three principal components correspond to a yield curve’s level, slope, and curvature, respectively.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45174915540952">
<h5/>
<p>This case study will focus on:</p>

<ul>
<li>
<p>Understanding the intuition behind eigenvectors.</p>
</li>
<li>
<p>Using dimensions resulting from dimensionality reduction to reproduce the original data.</p>
</li>
</ul>
</div></aside>
<img class="blueprint top" src="Images/bracket_top.png" width="1625" height="32"/>








<section data-type="sect2" class="blueprint" data-pdf-bookmark="Blueprint for Using Dimensionality Reduction to Generate a Yield Curve"><div class="sect2" id="idm45174915536312">
<h2>Blueprint for Using Dimensionality Reduction to Generate a Yield Curve</h2>










<section data-type="sect3" data-pdf-bookmark="1. Problem definition"><div class="sect3" id="idm45174915534792">
<h3>1. Problem definition</h3>

<p>Our goal in this case study is to use dimensionality reduction techniques to generate the typical movements of a yield curve.</p>

<p>The data used for this case study is obtained from <a href="https://www.quandl.com">Quandl</a>, a premier source for financial, economic, and alternative datasets. We use the data of 11 tenors (or maturities), from 1-month to 30-years, of Treasury curves. These are of daily frequency and are available from 1960 onwards.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="2. Getting started—loading the data and Python packages"><div class="sect3" id="idm45174915531720">
<h3>2. Getting started—loading the data and Python packages</h3>












<section data-type="sect4" data-pdf-bookmark="2.1. Loading the Python packages"><div class="sect4" id="idm45174915530712">
<h4>2.1. Loading the Python packages</h4>

<p><a data-type="indexterm" data-primary="yield curve construction/interest rate modeling case study" data-secondary="loading data and Python packages" id="idm45174915529544"/>The loading of Python packages is similar to the previous dimensionality reduction case study. Please refer to the Jupyter notebook of this case study for more details.</p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="2.2. Loading the data"><div class="sect4" id="idm45174915528008">
<h4>2.2. Loading the data</h4>

<p>In the first step, we load the data of different tenors of the Treasury curves from Quandl:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c"># In order to use quandl, ApiConfig.api_key will need to be</code>
<code class="c"># set to identify you to the quandl API. Please see API</code>
<code class="c"># Documentation of quandl for more details</code>
<code class="n">quandl</code><code class="o">.</code><code class="n">ApiConfig</code><code class="o">.</code><code class="n">api_key</code> <code class="o">=</code> <code class="s">'API Key'</code></pre>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">treasury</code> <code class="o">=</code> <code class="p">[</code><code class="s">'FRED/DGS1MO'</code><code class="p">,</code><code class="s">'FRED/DGS3MO'</code><code class="p">,</code><code class="s">'FRED/DGS6MO'</code><code class="p">,</code><code class="s">'FRED/DGS1'</code><code class="p">,</code>\
<code class="s">'FRED/DGS2'</code><code class="p">,</code><code class="s">'FRED/DGS3'</code><code class="p">,</code><code class="s">'FRED/DGS5'</code><code class="p">,</code><code class="s">'FRED/DGS7'</code><code class="p">,</code><code class="s">'FRED/DGS10'</code><code class="p">,</code>\
<code class="s">'FRED/DGS20'</code><code class="p">,</code><code class="s">'FRED/DGS30'</code><code class="p">]</code>

<code class="n">treasury_df</code> <code class="o">=</code> <code class="n">quandl</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="n">treasury</code><code class="p">)</code>
<code class="n">treasury_df</code><code class="o">.</code><code class="n">columns</code> <code class="o">=</code> <code class="p">[</code><code class="s">'TRESY1mo'</code><code class="p">,</code><code class="s">'TRESY3mo'</code><code class="p">,</code><code class="s">'TRESY6mo'</code><code class="p">,</code><code class="s">'TRESY1y'</code><code class="p">,</code>\
<code class="s">'TRESY2y'</code><code class="p">,</code><code class="s">'TRESY3y'</code><code class="p">,</code><code class="s">'TRESY5y'</code><code class="p">,</code><code class="s">'TRESY7y'</code><code class="p">,</code><code class="s">'TRESY10y'</code><code class="p">,</code>\<code class="s">'TRESY20y'</code><code class="p">,</code><code class="s">'TRESY30y'</code><code class="p">]</code>
<code class="n">dataset</code> <code class="o">=</code> <code class="n">treasury_df</code></pre>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="3. Exploratory data analysis"><div class="sect3" id="idm45174915518984">
<h3>3. Exploratory data analysis</h3>

<p><a data-type="indexterm" data-primary="yield curve construction/interest rate modeling case study" data-secondary="exploratory data analysis" id="ix_Chapter7-asciidoc13"/>Here, we will take our first look at the data.</p>












<section data-type="sect4" data-pdf-bookmark="3.1. Descriptive statistics"><div class="sect4" id="idm45174915417512">
<h4>3.1. Descriptive statistics</h4>

<p>In the next step we look at the shape of the dataset:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c"># shape</code>
<code class="n">dataset</code><code class="o">.</code><code class="n">shape</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">(14420, 11)</pre>

<p>The dataset has 14,420 rows and has the data of 11 tenors of the Treasury curve for more than 50 years.</p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="3.2. Data visualization"><div class="sect4" id="idm45174915249784">
<h4>3.2. Data visualization</h4>

<p><a data-type="indexterm" data-primary="yield curve construction/interest rate modeling case study" data-secondary="data visualization" id="ix_Chapter7-asciidoc14"/>Let us look at the movement of the rates from the downloaded data:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">dataset</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code><code class="mi">5</code><code class="p">))</code>
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s">"Rate"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">legend</code><code class="p">(</code><code class="n">bbox_to_anchor</code><code class="o">=</code><code class="p">(</code><code class="mf">1.01</code><code class="p">,</code> <code class="mf">0.9</code><code class="p">),</code> <code class="n">loc</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>

<p><code>Output</code></p>

<figure><div class="figure">
<img src="Images/mlbf_07in11.png" alt="mlbf 07in11" width="703" height="285"/>
<h6/>
</div></figure>

<p>In the next step we look at the correlations across tenors:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c"># correlation</code>
<code class="n">correlation</code> <code class="o">=</code> <code class="n">dataset</code><code class="o">.</code><code class="n">corr</code><code class="p">()</code>
<code class="n">plt</code><code class="o">.</code><code class="n">figure</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">15</code><code class="p">,</code> <code class="mi">15</code><code class="p">))</code>
<code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="s">'Correlation Matrix'</code><code class="p">)</code>
<code class="n">sns</code><code class="o">.</code><code class="n">heatmap</code><code class="p">(</code><code class="n">correlation</code><code class="p">,</code> <code class="n">vmax</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">square</code><code class="o">=</code><code class="k">True</code><code class="p">,</code> <code class="n">annot</code><code class="o">=</code><code class="k">True</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s">'cubehelix'</code><code class="p">)</code></pre>

<p class="pagebreak-before"><code>Output</code></p>

<figure><div class="figure">
<img src="Images/mlbf_07in12.png" alt="mlbf 07in12" width="799" height="811"/>
<h6/>
</div></figure>

<p>There is a significant positive correlation between the tenors, as you can see in the output (full-size version available on <a href="https://oreil.ly/hjQG7">GitHub</a>). This is an indication that reducing the number dimensions may be useful when modeling with the data. Additional visualizations of the data will be performed after implementing the dimensionality reduction models<a data-type="indexterm" data-startref="ix_Chapter7-asciidoc14" id="idm45174915123576"/>.<a data-type="indexterm" data-startref="ix_Chapter7-asciidoc13" id="idm45174915122744"/></p>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="4. Data preparation"><div class="sect3" id="idm45174915121912">
<h3>4. Data preparation</h3>

<p><a data-type="indexterm" data-primary="yield curve construction/interest rate modeling case study" data-secondary="data preparation" id="idm45174915120744"/>Data cleaning and transformation are a necessary modeling prerequisite in this case study.</p>












<section data-type="sect4" data-pdf-bookmark="4.1. Data cleaning"><div class="sect4" id="idm45174915119432">
<h4>4.1. Data cleaning</h4>

<p>Here, we check for NAs in the data and either drop them or fill them with the mean of the column.</p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="4.2. Data transformation"><div class="sect4" id="idm45174915117768">
<h4>4.2. Data transformation</h4>

<p>We standardize the variables on the same scale before applying PCA in order to prevent a feature with large values from dominating the result. We use the <code>StandardScaler</code> function in sklearn to standardize the dataset’s features onto a unit scale (mean = 0 and variance = 1):</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="k">import</code> <code class="n">StandardScaler</code>
<code class="n">scaler</code> <code class="o">=</code> <code class="n">StandardScaler</code><code class="p">()</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">dataset</code><code class="p">)</code>
<code class="n">rescaledDataset</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">scaler</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">dataset</code><code class="p">),</code>\
<code class="n">columns</code> <code class="o">=</code> <code class="n">dataset</code><code class="o">.</code><code class="n">columns</code><code class="p">,</code>
<code class="n">index</code> <code class="o">=</code> <code class="n">dataset</code><code class="o">.</code><code class="n">index</code><code class="p">)</code>
<code class="c"># summarize transformed data</code>
<code class="n">dataset</code><code class="o">.</code><code class="n">dropna</code><code class="p">(</code><code class="n">how</code><code class="o">=</code><code class="s">'any'</code><code class="p">,</code> <code class="n">inplace</code><code class="o">=</code><code class="k">True</code><code class="p">)</code>
<code class="n">rescaledDataset</code><code class="o">.</code><code class="n">dropna</code><code class="p">(</code><code class="n">how</code><code class="o">=</code><code class="s">'any'</code><code class="p">,</code> <code class="n">inplace</code><code class="o">=</code><code class="k">True</code><code class="p">)</code></pre>

<p><code>Visualizing the standardized dataset</code></p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">rescaledDataset</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">14</code><code class="p">,</code> <code class="mi">10</code><code class="p">))</code>
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s">"Rate"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">legend</code><code class="p">(</code><code class="n">bbox_to_anchor</code><code class="o">=</code><code class="p">(</code><code class="mf">1.01</code><code class="p">,</code> <code class="mf">0.9</code><code class="p">),</code> <code class="n">loc</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>

<p><code>Output</code></p>

<figure><div class="figure">
<img src="Images/mlbf_07in13.png" alt="mlbf 07in13" width="920" height="531"/>
<h6/>
</div></figure>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="5. Evaluate algorithms and models"><div class="sect3" id="idm45174914986824">
<h3>5. Evaluate algorithms and models</h3>












<section data-type="sect4" data-pdf-bookmark="5.2. Model evaluation—applying principal component analysis"><div class="sect4" id="idm45174914985576">
<h4>5.2. Model evaluation—applying principal component analysis</h4>

<p><a data-type="indexterm" data-primary="yield curve construction/interest rate modeling case study" data-secondary="evaluation of algorithms and models" id="ix_Chapter7-asciidoc15"/><a data-type="indexterm" data-primary="principal component analysis (PCA)" data-secondary="yield curve construction" id="ix_Chapter7-asciidoc16"/>As a next step, we create a function to perform PCA using the sklearn library. This function generates the principal components from the data that will be used for further analysis:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">pca</code> <code class="o">=</code> <code class="n">PCA</code><code class="p">()</code>
<code class="n">PrincipalComponent</code><code class="o">=</code><code class="n">pca</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">rescaledDataset</code><code class="p">)</code></pre>
</div></section>













<section data-type="sect4" data-pdf-bookmark="5.2.1. Explained variance using PCA"><div class="sect4" id="idm45174914884024">
<h4>5.2.1. Explained variance using PCA</h4>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">NumEigenvalues</code><code class="o">=</code><code class="mi">5</code>
<code class="n">fig</code><code class="p">,</code> <code class="n">axes</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">ncols</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">14</code><code class="p">,</code> <code class="mi">4</code><code class="p">))</code>
<code class="n">pd</code><code class="o">.</code><code class="n">Series</code><code class="p">(</code><code class="n">pca</code><code class="o">.</code><code class="n">explained_variance_ratio_</code><code class="p">[:</code><code class="n">NumEigenvalues</code><code class="p">])</code><code class="o">.</code><code class="n">sort_values</code><code class="p">()</code><code class="o">.</code>\
<code class="n">plot</code><code class="o">.</code><code class="n">barh</code><code class="p">(</code><code class="n">title</code><code class="o">=</code><code class="s">'Explained Variance Ratio by Top Factors'</code><code class="p">,</code><code class="n">ax</code><code class="o">=</code><code class="n">axes</code><code class="p">[</code><code class="mi">0</code><code class="p">]);</code>
<code class="n">pd</code><code class="o">.</code><code class="n">Series</code><code class="p">(</code><code class="n">pca</code><code class="o">.</code><code class="n">explained_variance_ratio_</code><code class="p">[:</code><code class="n">NumEigenvalues</code><code class="p">])</code><code class="o">.</code><code class="n">cumsum</code><code class="p">()</code>\
<code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">ylim</code><code class="o">=</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code><code class="mi">1</code><code class="p">),</code><code class="n">ax</code><code class="o">=</code><code class="n">axes</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code> <code class="n">title</code><code class="o">=</code><code class="s">'Cumulative Explained Variance'</code><code class="p">);</code>
<code class="c"># explained_variance</code>
<code class="n">pd</code><code class="o">.</code><code class="n">Series</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">cumsum</code><code class="p">(</code><code class="n">pca</code><code class="o">.</code><code class="n">explained_variance_ratio_</code><code class="p">))</code><code class="o">.</code><code class="n">to_frame</code>\
<code class="p">(</code><code class="s">'Explained Variance_Top 5'</code><code class="p">)</code><code class="o">.</code><code class="n">head</code><code class="p">(</code><code class="n">NumEigenvalues</code><code class="p">)</code><code class="o">.</code><code class="n">style</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="s">'{:,.2%}'</code><code class="o">.</code><code class="n">format</code><code class="p">)</code></pre>

<p><code>Output</code></p>
<table>

<thead>
<tr>
<th/>
<th>Explained Variance_Top 5</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>0</p></td>
<td><p>84.36%</p></td>
</tr>
<tr>
<td><p>1</p></td>
<td><p>98.44%</p></td>
</tr>
<tr>
<td><p>2</p></td>
<td><p>99.53%</p></td>
</tr>
<tr>
<td><p>3</p></td>
<td><p>99.83%</p></td>
</tr>
<tr>
<td><p>4</p></td>
<td><p>99.94%</p></td>
</tr>
</tbody>
</table>

<figure><div class="figure">
<img src="Images/mlbf_07in14.png" alt="mlbf 07in14" width="801" height="249"/>
<h6/>
</div></figure>

<p class="pagebreak-before">The first three principal components account for 84.4%, 14.08%, and 1.09% of variance, respectively. Cumulatively, they describe over 99.5% of all movement in the data. This is an incredibly efficient reduction in dimensions. Recall that in the first case study, we saw the first 10 components account for only 73% of variance.</p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="5.2.2. Intuition behind the principal components"><div class="sect4" id="idm45174914787272">
<h4>5.2.2. Intuition behind the principal components</h4>

<p>Ideally, we can have some intuition and interpretation of these principal components. To explore this, we first have a function to determine the weights of each principal component, and then perform the visualization of the principal components:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="k">def</code> <code class="nf">PCWeights</code><code class="p">():</code>
    <code class="sd">'''</code>
<code class="sd">    Principal Components (PC) weights for each 28 PCs</code>
<code class="sd">    '''</code>
    <code class="n">weights</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">()</code>

    <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">pca</code><code class="o">.</code><code class="n">components_</code><code class="p">)):</code>
        <code class="n">weights</code><code class="p">[</code><code class="s">"weights_{}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">i</code><code class="p">)]</code> <code class="o">=</code> \
        <code class="n">pca</code><code class="o">.</code><code class="n">components_</code><code class="p">[</code><code class="n">i</code><code class="p">]</code> <code class="o">/</code> <code class="nb">sum</code><code class="p">(</code><code class="n">pca</code><code class="o">.</code><code class="n">components_</code><code class="p">[</code><code class="n">i</code><code class="p">])</code>

    <code class="n">weights</code> <code class="o">=</code> <code class="n">weights</code><code class="o">.</code><code class="n">values</code><code class="o">.</code><code class="n">T</code>
    <code class="k">return</code> <code class="n">weights</code>

<code class="n">weights</code><code class="o">=</code><code class="n">PCWeights</code><code class="p">()</code></pre>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">weights</code> <code class="o">=</code> <code class="n">PCWeights</code><code class="p">()</code>
<code class="n">NumComponents</code><code class="o">=</code><code class="mi">3</code>

<code class="n">topPortfolios</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">weights</code><code class="p">[:</code><code class="n">NumComponents</code><code class="p">],</code> <code class="n">columns</code><code class="o">=</code><code class="n">dataset</code><code class="o">.</code><code class="n">columns</code><code class="p">)</code>
<code class="n">topPortfolios</code><code class="o">.</code><code class="n">index</code> <code class="o">=</code> <code class="p">[</code><code class="n">f</code><code class="s">'Principal Component {i}'</code> \
<code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="n">NumComponents</code><code class="o">+</code><code class="mi">1</code><code class="p">)]</code>

<code class="n">axes</code> <code class="o">=</code> <code class="n">topPortfolios</code><code class="o">.</code><code class="n">T</code><code class="o">.</code><code class="n">plot</code><code class="o">.</code><code class="n">bar</code><code class="p">(</code><code class="n">subplots</code><code class="o">=</code><code class="k">True</code><code class="p">,</code> <code class="n">legend</code><code class="o">=</code><code class="k">False</code><code class="p">,</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">14</code><code class="p">,</code> <code class="mi">10</code><code class="p">))</code>
<code class="n">plt</code><code class="o">.</code><code class="n">subplots_adjust</code><code class="p">(</code><code class="n">hspace</code><code class="o">=</code><code class="mf">0.35</code><code class="p">)</code>
<code class="n">axes</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">set_ylim</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="o">.</code><code class="mi">2</code><code class="p">);</code></pre>

<p class="pagebreak-before"><code>Output</code></p>

<figure><div class="figure">
<img src="Images/mlbf_07in15.png" alt="mlbf 07in15" width="811" height="621"/>
<h6/>
</div></figure>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">pca</code><code class="o">.</code><code class="n">components_</code><code class="p">[</code><code class="mi">0</code><code class="p">:</code><code class="mi">3</code><code class="p">]</code><code class="o">.</code><code class="n">T</code><code class="p">)</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">style</code><code class="o">=</code> <code class="p">[</code><code class="s">'s-'</code><code class="p">,</code><code class="s">'o-'</code><code class="p">,</code><code class="s">'^-'</code><code class="p">],</code> \
                            <code class="n">legend</code><code class="o">=</code><code class="k">False</code><code class="p">,</code> <code class="n">title</code><code class="o">=</code><code class="s">"Principal Component"</code><code class="p">)</code></pre>

<p><code>Output</code></p>

<figure class="width-50"><div class="figure">
<img src="Images/mlbf_07in16.png" alt="mlbf 07in16" width="370" height="249"/>
<h6/>
</div></figure>

<p>By plotting the components of the eigenvectors we can make the following 
<span class="keep-together">interpretation</span>:</p>
<dl>
<dt>Principal Component 1</dt>
<dd>
<p>This eigenvector has all positive values, with all tenors weighted in the same direction. This means that the first principal component reflects movements that cause all maturities to move in the same direction, corresponding to <em>directional movements</em> in the yield curve. These are movements that shift the entire yield curve up or down.</p>
</dd>
<dt>Principal Component 2</dt>
<dd>
<p>The second eigenvector has the first half of the components negative and the second half positive. Treasury rates on the short end (long end) of the curve are weighted positively (negatively). This means that the second principal component reflects movements that cause the short end to go in one direction and the long end in the other. Consequently, it represents <em>slope movements</em> in the yield curve.</p>
</dd>
<dt>Principal Component 3</dt>
<dd>
<p>The third eigenvector has the first third of the components negative, the second third positive, and the last third negative. This means that the third principal component reflects movements that cause the short and long end to go in one direction, and the middle to go in the other, resulting in <em>curvature movements</em> of the yield curve.</p>
</dd>
</dl>
</div></section>













<section data-type="sect4" data-pdf-bookmark="5.2.3. Reconstructing the curve using principal components"><div class="sect4" id="idm45174914786648">
<h4>5.2.3. Reconstructing the curve using principal components</h4>

<p>One of the key features of PCA is the ability to reconstruct the initial dataset using the outputs of PCA. Using simple matrix reconstruction, we can generate a near exact replica of the initial data:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">pca</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">rescaledDataset</code><code class="p">)[:,</code> <code class="p">:</code><code class="mi">2</code><code class="p">]</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">array([[ 4.97514826, -0.48514999],
       [ 5.03634891, -0.52005102],
       [ 5.14497849, -0.58385444],
       ...,
       [-1.82544584,  2.82360062],
       [-1.69938513,  2.6936174 ],
       [-1.73186029,  2.73073137]])</pre>

<p>Mechanically, PCA is just a matrix multiplication:</p>
<div data-type="equation">
<math display="block"><mi>Y</mi><mo>=</mo><mi>X</mi><mi>W</mi></math>
</div>

<p>where <em>Y</em> is the principal components, <em>X</em> is input data, and <em>W</em> is a matrix of coefficients, which we can use to recover the original matrix as per the equation below:</p>
<div data-type="equation">
<math display="block"><mi>X</mi><mo>=</mo><mi>Y</mi><mi>W</mi><mi>′</mi></math>
</div>

<p>where <em>W′</em> is the inverse of the matrix of coefficients <em>W</em>.</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">nComp</code><code class="o">=</code><code class="mi">3</code>
<code class="n">reconst</code><code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">dot</code><code class="p">(</code><code class="n">pca</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">rescaledDataset</code><code class="p">)[:,</code> <code class="p">:</code><code class="n">nComp</code><code class="p">],</code>\
<code class="n">pca</code><code class="o">.</code><code class="n">components_</code><code class="p">[:</code><code class="n">nComp</code><code class="p">,:]),</code><code class="n">columns</code><code class="o">=</code><code class="n">dataset</code><code class="o">.</code><code class="n">columns</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">figure</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code><code class="mi">8</code><code class="p">))</code>
<code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">reconst</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s">"Treasury Rate"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="s">"Reconstructed Dataset"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>

<p>This figure shows the replicated Treasury rate chart and demonstrates that, using just the first three principal components, we are able to replicate the original chart. Despite reducing the data from 11 dimensions to three, we still retain more than 99% of the information and can reproduce the original data easily. Additionally, we also have intuition around these three drivers of yield curve moments. Reducing the yield curve into fewer components means practictioners can focus on fewer factors that influence interest rates. For example, in order to hedge a portfolio, it may be sufficient to protect the portfolio against moves in the first three principal components only<a data-type="indexterm" data-startref="ix_Chapter7-asciidoc16" id="idm45174914315992"/>.<a data-type="indexterm" data-startref="ix_Chapter7-asciidoc15" id="idm45174914315256"/></p>

<p><code>Output</code></p>

<figure><div class="figure">
<img src="Images/mlbf_07in17.png" alt="mlbf 07in17" width="596" height="466"/>
<h6/>
</div></figure>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="Conclusion"><div class="sect3" id="idm45174914493016">
<h3>Conclusion</h3>

<p>In this case study, we introduced dimensionality reduction to break down the Treasury rate curve into fewer components. We saw that the principal components are quite intuitive for this case study. The first three principal components explain more than 99.5% of the variation and represent directional movements, slope movements, and curvature movements, respectively.</p>

<p>By using principal component analysis, analyzing the eigenvectors, and understanding the intuition behind them, we demonstrated how using dimensionality reduction led to fewer intuitive dimensions in the yield curve. Such dimensionality reduction of the yield curve can potentially lead to faster and more effective portfolio management, trading, hedging, and risk management.<a data-type="indexterm" data-startref="ix_Chapter7-asciidoc12" id="idm45174914310408"/></p>
<img class="blueprint bottom" src="Images/bracket_bottom.png" width="1625" height="32"/>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Case Study 3: Bitcoin Trading: Enhancing Speed and Accuracy"><div class="sect1" id="CaseStudy3DR">
<h1>Case Study 3: Bitcoin Trading: Enhancing Speed and Accuracy</h1>

<p><a data-type="indexterm" data-primary="bitcoin trading: enhancing speed and accuracy" id="ix_Chapter7-asciidoc17"/>As trading becomes more automated, traders will continue to seek to use as many features and technical indicators as they can to make their strategies more accurate and efficient. One of the many challenges in this is that adding more variables leads to ever more complexity, making it increasingly difficult to arrive at solid conclusions. Using dimensionality reduction techniques, we can compress many features and technical indicators into a few logical collections, while still maintaining a significant amount of the variance of the original data. This helps speed up model training and tuning. Additionally, it helps prevent overfitting by getting rid of correlated variables, which can ultimately cause more harm than good. Dimensionality reduction also enhances exploration and visualization of a dataset to understand grouping or relationships, an important task when building and continuously monitoring trading strategies.</p>

<p>In this case study, we will use dimensionality reduction to enhance <a data-type="xref" href="ch06.xhtml#CaseStudy3SC">“Case Study 3: Bitcoin Trading Strategy”</a> presented in <a data-type="xref" href="ch06.xhtml#Chapter6">Chapter 6</a>. In this case study, we design a trading strategy for bitcoin that considers the relationship between the short-term and long-term prices to predict a buy or sell signal. We create several new intuitive, technical indicator features, including trend, volume, volatility, and momentum. We apply  dimensionality reduction techniques on these features in order to achieve better results.</p>
<aside data-type="sidebar" epub:type="sidebar" class="pagebreak-before less_space"><div class="sidebar" id="idm45174914302568">
<h5/>
<p>In this case study, we will focus on:</p>

<ul>
<li>
<p>Reducing the dimensions of a dataset to yield better and faster results for supervised learning.</p>
</li>
<li>
<p>Using SVD and t-SNE to visualize data in lower dimensions.</p>
</li>
</ul>
</div></aside>
<img class="blueprint top" src="Images/bracket_top.png" width="1625" height="32"/>








<section data-type="sect2" class="blueprint" data-pdf-bookmark="Blueprint for Using Dimensionality Reduction to Enhance a Trading Strategy"><div class="sect2" id="idm45174914297816">
<h2>Blueprint for Using Dimensionality Reduction to Enhance a Trading Strategy</h2>










<section data-type="sect3" data-pdf-bookmark="1. Problem definition"><div class="sect3" id="idm45174914296296">
<h3>1. Problem definition</h3>

<p>Our goal in this case study is to use dimensionality reduction techniques to enhance an algorithmic trading strategy. The data and the variables used in this case study are the same as in <a data-type="xref" href="ch06.xhtml#CaseStudy3SC">“Case Study 3: Bitcoin Trading Strategy”</a>. For reference, we are using intraday bitcoin price data, volume, and weighted bitcoin price from January 2012 to October 2017. Steps 3 and 4 presented in this case study use the same steps as the case study in <a data-type="xref" href="ch06.xhtml#Chapter6">Chapter 6</a>. As such, these steps are condensed in this case study to avoid repetition.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="2. Getting started—loading the data and Python packages"><div class="sect3" id="idm45174914292648">
<h3>2. Getting started—loading the data and Python packages</h3>












<section data-type="sect4" data-pdf-bookmark="2.1. Loading the Python packages"><div class="sect4" id="idm45174914291672">
<h4>2.1. Loading the Python packages</h4>

<p><a data-type="indexterm" data-primary="bitcoin trading: enhancing speed and accuracy" data-secondary="loading data and Python packages" id="idm45174914290504"/>The Python packages used for this case study are the same as those presented in the previous two case studies in this chapter.</p>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="3. Exploratory data analysis"><div class="sect3" id="idm45174914288936">
<h3>3. Exploratory data analysis</h3>

<p>Refer to <a data-type="xref" href="ch06.xhtml#data_analysis_ch6">“3. Exploratory data analysis”</a> for more details of this step.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="4. Data preparation"><div class="sect3" id="idm45174914286552">
<h3>4. Data preparation</h3>

<p>We prepare the data for modeling in the following sections.</p>












<section data-type="sect4" data-pdf-bookmark="4.1. Data cleaning"><div class="sect4" id="idm45174914284888">
<h4>4.1. Data cleaning</h4>

<p><a data-type="indexterm" data-primary="bitcoin trading: enhancing speed and accuracy" data-secondary="data preparation" id="ix_Chapter7-asciidoc18"/>We clean the data by filling the NAs with the last available values:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">dataset</code><code class="p">[</code><code class="n">dataset</code><code class="o">.</code><code class="n">columns</code><code class="p">]</code> <code class="o">=</code> <code class="n">dataset</code><code class="p">[</code><code class="n">dataset</code><code class="o">.</code><code class="n">columns</code><code class="p">]</code><code class="o">.</code><code class="n">ffill</code><code class="p">()</code></pre>
</div></section>













<section data-type="sect4" data-pdf-bookmark="4.2. Preparing the data for classification"><div class="sect4" id="idm45174914271896">
<h4>4.2. Preparing the data for classification</h4>

<p>We attach the following label to each movement: 1 if the short-term price increases compared to the long-term price; 0 if the short-term price decreases compared to the long-term price. This label is assigned to a variable we will call <em>signal</em>, which is the predicted variable for this case study. Let us look at the data for prediction:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">dataset</code><code class="o">.</code><code class="n">tail</code><code class="p">(</code><code class="mi">5</code><code class="p">)</code></pre>

<p><code>Output</code></p>

<figure><div class="figure">
<img src="Images/mlbf_07in18.png" alt="mlbf 07in18" width="905" height="155"/>
<h6/>
</div></figure>

<p>The dataset contains the signal column along with all other columns.</p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="4.3. Feature engineering"><div class="sect4" id="idm45174914255624">
<h4>4.3. Feature engineering</h4>

<p>In this step, we construct a dataset that contains the predictors that will be used to make the signal  prediction. Using the bitcoin intraday price data, including daily open, high, low, close, and volume, we compute the following technical indicators:</p>

<ul>
<li>
<p>Moving Average</p>
</li>
<li>
<p>Stochastic Oscillator %K and %D</p>
</li>
<li>
<p>Relative Strength Index (RSI)</p>
</li>
<li>
<p>Rate Of Change (ROC)</p>
</li>
<li>
<p>Momentum (MOM)</p>
</li>
</ul>

<p>The code for the construction of all of the indicators, along with their descriptions, is presented in <a data-type="xref" href="ch06.xhtml#Chapter6">Chapter 6</a>. The final dataset and the columns used are as follows:</p>

<figure><div class="figure">
<img src="Images/mlbf_07in19.png" alt="mlbf 07in19" width="1046" height="155"/>
<h6/>
</div></figure>
</div></section>













<section data-type="sect4" data-pdf-bookmark="4.4. Data visualization"><div class="sect4" id="idm45174914174248">
<h4>4.4. Data visualization</h4>

<p>Let us look at the distribution of the predicted variable:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">fig</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">figure</code><code class="p">()</code>
<code class="n">plot</code> <code class="o">=</code> <code class="n">dataset</code><code class="o">.</code><code class="n">groupby</code><code class="p">([</code><code class="s">'signal'</code><code class="p">])</code><code class="o">.</code><code class="n">size</code><code class="p">()</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">kind</code><code class="o">=</code><code class="s">'barh'</code><code class="p">,</code> <code class="n">color</code><code class="o">=</code><code class="s">'red'</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>

<p class="pagebreak-before"><code>Output</code></p>

<figure><div class="figure">
<img src="Images/mlbf_07in20.png" alt="mlbf 07in20" width="374" height="234"/>
<h6/>
</div></figure>

<p>The predicted signal is “buy” 52.9% of the time.<a data-type="indexterm" data-startref="ix_Chapter7-asciidoc18" id="idm45174914128680"/></p>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="5. Evaluate algorithms and models"><div class="sect3" id="idm45174914127848">
<h3>5. Evaluate algorithms and models</h3>

<p>Next, we perform dimensionality reduction and evaluate the models.</p>












<section data-type="sect4" data-pdf-bookmark="5.1. Train-test split"><div class="sect4" id="idm45174914126216">
<h4>5.1. Train-test split</h4>

<p><a data-type="indexterm" data-primary="bitcoin trading: enhancing speed and accuracy" data-secondary="evaluation of algorithms and models" id="ix_Chapter7-asciidoc19"/>In this step, we split the dataset into training and test sets:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">Y</code><code class="o">=</code> <code class="n">subset_dataset</code><code class="p">[</code><code class="s">"signal"</code><code class="p">]</code>
<code class="n">X</code> <code class="o">=</code> <code class="n">subset_dataset</code><code class="o">.</code><code class="n">loc</code><code class="p">[:,</code> <code class="n">dataset</code><code class="o">.</code><code class="n">columns</code> <code class="o">!=</code> <code class="s">'signal'</code><code class="p">]</code> <code class="n">validation_size</code> <code class="o">=</code> <code class="mf">0.2</code>
<code class="n">X_train</code><code class="p">,</code> <code class="n">X_validation</code><code class="p">,</code> <code class="n">Y_train</code><code class="p">,</code> <code class="n">Y_validation</code> <code class="o">=</code> <code class="n">train_test_split</code>\
<code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">Y</code><code class="p">,</code> <code class="n">test_size</code><code class="o">=</code><code class="n">validation_size</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code></pre>

<p>We standardize the variables on the same scale before applying dimensionality reduction. Data standardization is performed using the following Python code:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="k">import</code> <code class="n">StandardScaler</code>
<code class="n">scaler</code> <code class="o">=</code> <code class="n">StandardScaler</code><code class="p">()</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code>
<code class="n">rescaledDataset</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">scaler</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">X_train</code><code class="p">),</code>\
<code class="n">columns</code> <code class="o">=</code> <code class="n">X_train</code><code class="o">.</code><code class="n">columns</code><code class="p">,</code> <code class="n">index</code> <code class="o">=</code> <code class="n">X_train</code><code class="o">.</code><code class="n">index</code><code class="p">)</code>
<code class="c"># summarize transformed data</code>
<code class="n">X_train</code><code class="o">.</code><code class="n">dropna</code><code class="p">(</code><code class="n">how</code><code class="o">=</code><code class="s">'any'</code><code class="p">,</code> <code class="n">inplace</code><code class="o">=</code><code class="k">True</code><code class="p">)</code>
<code class="n">rescaledDataset</code><code class="o">.</code><code class="n">dropna</code><code class="p">(</code><code class="n">how</code><code class="o">=</code><code class="s">'any'</code><code class="p">,</code> <code class="n">inplace</code><code class="o">=</code><code class="k">True</code><code class="p">)</code>
<code class="n">rescaledDataset</code><code class="o">.</code><code class="n">head</code><code class="p">(</code><code class="mi">2</code><code class="p">)</code></pre>

<p><code>Output</code></p>

<figure><div class="figure">
<img src="Images/mlbf_07in21.png" alt="mlbf 07in21" width="1057" height="65"/>
<h6/>
</div></figure>
</div></section>













<section data-type="sect4" data-pdf-bookmark="5.2. Singular value decomposition (feature reduction)"><div class="sect4" id="idm45174913947272">
<h4>5.2. Singular value decomposition (feature reduction)</h4>

<p><a data-type="indexterm" data-primary="principal component analysis (PCA)" data-secondary="bitcoin trading speed/accuracy enhancement" id="ix_Chapter7-asciidoc20"/><a data-type="indexterm" data-primary="singular value decomposition (SVD)" id="ix_Chapter7-asciidoc21"/><a data-type="indexterm" data-primary="SVD (singular value decomposition)" id="ix_Chapter7-asciidoc22"/><a data-type="indexterm" data-primary="truncated SVD" id="ix_Chapter7-asciidoc23"/>Here we will use SVD to perform PCA. Specifically, we are using the <code>TruncatedSVD</code> method in the sklearn package to transform the full dataset into a representation using the top five components:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">ncomps</code> <code class="o">=</code> <code class="mi">5</code>
<code class="n">svd</code> <code class="o">=</code> <code class="n">TruncatedSVD</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="n">ncomps</code><code class="p">)</code>
<code class="n">svd_fit</code> <code class="o">=</code> <code class="n">svd</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">rescaledDataset</code><code class="p">)</code>
<code class="n">Y_pred</code> <code class="o">=</code> <code class="n">svd</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">rescaledDataset</code><code class="p">)</code>
<code class="n">ax</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">Series</code><code class="p">(</code><code class="n">svd_fit</code><code class="o">.</code><code class="n">explained_variance_ratio_</code><code class="o">.</code><code class="n">cumsum</code><code class="p">())</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">kind</code><code class="o">=</code><code class="s">'line'</code><code class="p">,</code> \
<code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="mi">3</code><code class="p">))</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_xlabel</code><code class="p">(</code><code class="s">"Eigenvalues"</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_ylabel</code><code class="p">(</code><code class="s">"Percentage Explained"</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="s">'Variance preserved by first 5 components == {:.2%}'</code><code class="o">.</code>\
<code class="nb">format</code><code class="p">(</code><code class="n">svd_fit</code><code class="o">.</code><code class="n">explained_variance_ratio_</code><code class="o">.</code><code class="n">cumsum</code><code class="p">()[</code><code class="o">-</code><code class="mi">1</code><code class="p">]))</code></pre>

<p><code>Output</code></p>

<figure><div class="figure">
<img src="Images/mlbf_07in22.png" alt="mlbf 07in22" width="596" height="249"/>
<h6/>
</div></figure>

<p>Following the computation, we preserve 92.75% of the variance by using just five components rather than the full 25+ original features. This is a tremendously useful compression for the analysis and iterations of the model.</p>

<p class="pagebreak-before">For convenience, we will create a Python dataframe specifically for these top five components:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">dfsvd</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">Y_pred</code><code class="p">,</code> <code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s">'c{}'</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">c</code><code class="p">)</code> <code class="k">for</code> \
<code class="n">c</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">ncomps</code><code class="p">)],</code> <code class="n">index</code><code class="o">=</code><code class="n">rescaledDataset</code><code class="o">.</code><code class="n">index</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">dfsvd</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code>
<code class="n">dfsvd</code><code class="o">.</code><code class="n">head</code><code class="p">()</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">(8000, 5)</pre>
<table>

<thead>
<tr>
<th/>
<th>c0</th>
<th>c1</th>
<th>c2</th>
<th>c3</th>
<th>c4</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>2834071</p></td>
<td><p>–2.252</p></td>
<td><p>1.920</p></td>
<td><p>0.538</p></td>
<td><p>–0.019</p></td>
<td><p>–0.967</p></td>
</tr>
<tr>
<td><p>2836517</p></td>
<td><p>5.303</p></td>
<td><p>–1.689</p></td>
<td><p>–0.678</p></td>
<td><p>0.473</p></td>
<td><p>0.643</p></td>
</tr>
<tr>
<td><p>2833945</p></td>
<td><p>–2.315</p></td>
<td><p>–0.042</p></td>
<td><p>1.697</p></td>
<td><p>–1.704</p></td>
<td><p>1.672</p></td>
</tr>
<tr>
<td><p>2835048</p></td>
<td><p>–0.977</p></td>
<td><p>0.782</p></td>
<td><p>3.706</p></td>
<td><p>–0.697</p></td>
<td><p>0.057</p></td>
</tr>
<tr>
<td><p>2838804</p></td>
<td><p>2.115</p></td>
<td><p>–1.915</p></td>
<td><p>0.475</p></td>
<td><p>–0.174</p></td>
<td><p>–0.299</p></td>
</tr>
</tbody>
</table>
</div></section>













<section data-type="sect4" data-pdf-bookmark="5.2.1. Basic visualization of reduced features"><div class="sect4" id="idm45174913946648">
<h4>5.2.1. Basic visualization of reduced features</h4>

<p>Let us visualize the compressed dataset:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">svdcols</code> <code class="o">=</code> <code class="p">[</code><code class="n">c</code> <code class="k">for</code> <code class="n">c</code> <code class="ow">in</code> <code class="n">dfsvd</code><code class="o">.</code><code class="n">columns</code> <code class="k">if</code> <code class="n">c</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code> <code class="o">==</code> <code class="s">'c'</code><code class="p">]</code></pre>
<dl>
<dt>Pairs-plots</dt>
<dd>
<p>Pairs-plots are a simple representation of a set of 2D scatterplots, with each component plotted against every other component. The data points are colored according to their signal classification:</p>
</dd>
</dl>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">plotdims</code> <code class="o">=</code> <code class="mi">5</code>
<code class="n">ploteorows</code> <code class="o">=</code> <code class="mi">1</code>
<code class="n">dfsvdplot</code> <code class="o">=</code> <code class="n">dfsvd</code><code class="p">[</code><code class="n">svdcols</code><code class="p">]</code><code class="o">.</code><code class="n">iloc</code><code class="p">[:,</code> <code class="p">:</code><code class="n">plotdims</code><code class="p">]</code>
<code class="n">dfsvdplot</code><code class="p">[</code><code class="s">'signal'</code><code class="p">]</code><code class="o">=</code><code class="n">Y_train</code>
<code class="n">ax</code> <code class="o">=</code> <code class="n">sns</code><code class="o">.</code><code class="n">pairplot</code><code class="p">(</code><code class="n">dfsvdplot</code><code class="o">.</code><code class="n">iloc</code><code class="p">[::</code><code class="n">ploteorows</code><code class="p">,</code> <code class="p">:],</code> <code class="n">hue</code><code class="o">=</code><code class="s">'signal'</code><code class="p">,</code> <code class="n">size</code><code class="o">=</code><code class="mf">1.8</code><code class="p">)</code></pre>

<p class="pagebreak-before"><code>Output</code></p>

<figure><div class="figure">
<img src="Images/mlbf_07in23.png" alt="mlbf 07in23" width="810" height="746"/>
<h6/>
</div></figure>

<p>We can see that there is clear separation of the colored dots (full color version available on <a href="https://oreil.ly/GWfug">GitHub</a>), meaning that data points from the same signal tend to cluster together. The separation is more distinct for the first components, with the characteristics of signal distributions growing more similar as you progress from the first to the fifth component. That said, the plot provides support for using all five components in our model.<a data-type="indexterm" data-startref="ix_Chapter7-asciidoc23" id="idm45174913585608"/><a data-type="indexterm" data-startref="ix_Chapter7-asciidoc22" id="idm45174913584904"/><a data-type="indexterm" data-startref="ix_Chapter7-asciidoc21" id="idm45174913584232"/><a data-type="indexterm" data-startref="ix_Chapter7-asciidoc20" id="idm45174913583560"/></p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="5.3. t-SNE visualization"><div class="sect4" id="idm45174913582760">
<h4>5.3. t-SNE visualization</h4>

<p><a data-type="indexterm" data-primary="t-distributed stochastic neighbor embedding (t-SNE)" id="idm45174913581384"/>In this step, we implement t-SNE and look at the related visualization. We will use the basic implementation available in Scikit-learn:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">tsne</code> <code class="o">=</code> <code class="n">TSNE</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>

<code class="n">Z</code> <code class="o">=</code> <code class="n">tsne</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">dfsvd</code><code class="p">[</code><code class="n">svdcols</code><code class="p">])</code>
<code class="n">dftsne</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">Z</code><code class="p">,</code> <code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s">'x'</code><code class="p">,</code><code class="s">'y'</code><code class="p">],</code> <code class="n">index</code><code class="o">=</code><code class="n">dfsvd</code><code class="o">.</code><code class="n">index</code><code class="p">)</code>

<code class="n">dftsne</code><code class="p">[</code><code class="s">'signal'</code><code class="p">]</code> <code class="o">=</code> <code class="n">Y_train</code>

<code class="n">g</code> <code class="o">=</code> <code class="n">sns</code><code class="o">.</code><code class="n">lmplot</code><code class="p">(</code><code class="s">'x'</code><code class="p">,</code> <code class="s">'y'</code><code class="p">,</code> <code class="n">dftsne</code><code class="p">,</code> <code class="n">hue</code><code class="o">=</code><code class="s">'signal'</code><code class="p">,</code> <code class="n">fit_reg</code><code class="o">=</code><code class="k">False</code><code class="p">,</code> <code class="n">size</code><code class="o">=</code><code class="mi">8</code>
                <code class="p">,</code> <code class="n">scatter_kws</code><code class="o">=</code><code class="p">{</code><code class="s">'alpha'</code><code class="p">:</code><code class="mf">0.7</code><code class="p">,</code><code class="s">'s'</code><code class="p">:</code><code class="mi">60</code><code class="p">})</code></pre>

<p><code>Output</code></p>

<figure class="width-75"><div class="figure">
<img src="Images/mlbf_07in24.png" alt="mlbf 07in24" width="603" height="556"/>
<h6/>
</div></figure>

<p>The plot shows us that there is a good degree of clustering for the trading signal. There is some overlap of the long and short signals, but they can be distinguished quite well using the reduced number of features.</p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="5.4. Compare models with and without dimensionality reduction"><div class="sect4" id="idm45174913475368">
<h4>5.4. Compare models with and without dimensionality reduction</h4>

<p>In this step, we analyze the impact of the dimensionality reduction on the classification and the impact on the overall accuracy and computation time:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="c"># test options for classification</code>
<code class="n">scoring</code> <code class="o">=</code> <code class="s">'accuracy'</code></pre>
</div></section>













<section data-type="sect4" data-pdf-bookmark="5.4.1. Models"><div class="sect4" id="idm45174913445992">
<h4>5.4.1. Models</h4>

<p>We first look at the time taken by the model without dimensionality reduction, where we have all the technical indicators:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="kn">import</code> <code class="nn">time</code>
<code class="n">start_time</code> <code class="o">=</code> <code class="n">time</code><code class="o">.</code><code class="n">time</code><code class="p">()</code>

<code class="c"># spot-check the algorithms</code>
<code class="n">models</code> <code class="o">=</code>  <code class="n">RandomForestClassifier</code><code class="p">(</code><code class="n">n_jobs</code><code class="o">=-</code><code class="mi">1</code><code class="p">)</code>
<code class="n">cv_results_XTrain</code><code class="o">=</code> <code class="n">cross_val_score</code><code class="p">(</code><code class="n">models</code><code class="p">,</code> <code class="n">X_train</code><code class="p">,</code> <code class="n">Y_train</code><code class="p">,</code> <code class="n">cv</code><code class="o">=</code><code class="n">kfold</code><code class="p">,</code> \
  <code class="n">scoring</code><code class="o">=</code><code class="n">scoring</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="s">"Time Without Dimensionality Reduction--- %s seconds ---"</code> <code class="o">%</code> \
<code class="p">(</code><code class="n">time</code><code class="o">.</code><code class="n">time</code><code class="p">()</code> <code class="o">-</code> <code class="n">start_time</code><code class="p">))</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">Time Without Dimensionality Reduction
7.781347990036011 seconds</pre>

<p>The total time taken without dimensionality reduction is around eight seconds. Let us look at the time it takes with dimensionality reduction, when only the five principal components from the truncated SVD are used:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="n">start_time</code> <code class="o">=</code> <code class="n">time</code><code class="o">.</code><code class="n">time</code><code class="p">()</code>
<code class="n">X_SVD</code><code class="o">=</code> <code class="n">dfsvd</code><code class="p">[</code><code class="n">svdcols</code><code class="p">]</code><code class="o">.</code><code class="n">iloc</code><code class="p">[:,</code> <code class="p">:</code><code class="mi">5</code><code class="p">]</code>
<code class="n">cv_results_SVD</code> <code class="o">=</code> <code class="n">cross_val_score</code><code class="p">(</code><code class="n">models</code><code class="p">,</code> <code class="n">X_SVD</code><code class="p">,</code> <code class="n">Y_train</code><code class="p">,</code> <code class="n">cv</code><code class="o">=</code><code class="n">kfold</code><code class="p">,</code> \
  <code class="n">scoring</code><code class="o">=</code><code class="n">scoring</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="s">"Time with Dimensionality Reduction--- %s seconds ---"</code> <code class="o">%</code> \
<code class="p">(</code><code class="n">time</code><code class="o">.</code><code class="n">time</code><code class="p">()</code> <code class="o">-</code> <code class="n">start_time</code><code class="p">))</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">Time with Dimensionality Reduction
2.281977653503418 seconds</pre>

<p>The total time taken with dimensionality reduction is around two seconds—four times a reduction in time, which is a significant improvement. Let us investigate whether there is any decline in the accuracy when using the condensed dataset:</p>

<pre data-type="programlisting" data-code-language="ipython3"><code class="nb">print</code><code class="p">(</code><code class="s">"Result without dimensionality Reduction: %f (%f)"</code> <code class="o">%</code>\
 <code class="p">(</code><code class="n">cv_results_XTrain</code><code class="o">.</code><code class="n">mean</code><code class="p">(),</code> <code class="n">cv_results_XTrain</code><code class="o">.</code><code class="n">std</code><code class="p">()))</code>
<code class="nb">print</code><code class="p">(</code><code class="s">"Result with dimensionality Reduction: %f (%f)"</code> <code class="o">%</code>\
 <code class="p">(</code><code class="n">cv_results_SVD</code><code class="o">.</code><code class="n">mean</code><code class="p">(),</code> <code class="n">cv_results_SVD</code><code class="o">.</code><code class="n">std</code><code class="p">()))</code></pre>

<p><code>Output</code></p>

<pre data-type="programlisting">Result without dimensionality Reduction: 0.936375 (0.010774)
Result with dimensionality Reduction: 0.887500 (0.012698)</pre>

<p>Accuracy declines roughly 5%, from 93.6% to 88.7%. The improvement in speed has to be balanced against this loss in accuracy. Whether the loss in accuracy is acceptable likely depends on the problem. If this is a model that needs to be recalibrated very frequently, then a lower computation time will be essential, especially when handling large, high-velocity datasets. The improvement in the computation time does have other benefits, especially in the early stages of trading strategy development. It enables us to test a greater number of features (or technical indicators) in less time.<a data-type="indexterm" data-startref="ix_Chapter7-asciidoc19" id="idm45174913206520"/></p>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="Conclusion"><div class="sect3" id="idm45174913445528">
<h3>Conclusion</h3>

<p>In this case study, we demonstrated the efficiency of dimensionality reduction and principal components analysis in reducing the number of dimensions in the context of a trading strategy. Through dimensionality reduction, we achieved a commensurate accuracy rate with a fourfold improvement in the modeling speed. In trading strategy development involving expansive datasets, such speed enhancements can lead to improvements for the entire process.</p>

<p>We demonstrated that both SVD and t-SNE yield reduced datasets that can easily be visualized for evaluating trading signal data. This allowed us to distinguish the long and short signals of this trading strategy in ways not possible with the original number of features.<a data-type="indexterm" data-startref="ix_Chapter7-asciidoc17" id="idm45174913203736"/></p>
<img class="blueprint bottom" src="Images/bracket_bottom.png" width="1625" height="32"/>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Chapter Summary"><div class="sect1" id="idm45174913202008">
<h1>Chapter Summary</h1>

<p>The case studies presented in this chapter focused on understanding the concepts of the different dimensionality reduction methods, developing intuition around the principal components, and visualizing the condensed datasets.</p>

<p>Overall, the concepts in Python, machine learning, and finance presented in this chapter through the case studies can used as a blueprint for any other dimensionality reduction–based problem in finance.</p>

<p>In the next chapter, we explore concepts and case studies for another type of unsupervised learning—clustering.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Exercises"><div class="sect1" id="idm45174913199000">
<h1>Exercises</h1>
<ol>
<li>
<p>Using dimensionality reduction, extract the different factors from the stocks within a different index and use them to build a trading strategy.</p>
</li>
<li>
<p>Pick any of the regression-based case studies in <a data-type="xref" href="ch05.xhtml#Chapter5">Chapter 5</a> and use dimensionality reduction to see whether there is any improvement in computation time. Explain the components using the factor loading and develop some high-level intuition of them.</p>
</li>
<li>
<p>For case study 3 presented in this chapter, perform factor loading of the principal components and understand the intuition of the different components.</p>
</li>
<li>
<p>Get the principal components of different currency pairs or different commodity prices. Identify the drivers of the primary principal components and link them to some intuitive macroeconomic variables.<a data-type="indexterm" data-startref="ix_Chapter7-asciidoc0" id="idm45174913192808"/></p>
</li>

</ol>
</div></section>







<div data-type="footnotes"><p data-type="footnote" id="idm45174917893240"><sup><a href="ch07.xhtml#idm45174917893240-marker">1</a></sup> <a href="https://oreil.ly/fDaLg">Eigenvectors and eigenvalues</a> are concepts of linear algebra.</p></div></div></section></div>



  </body></html>