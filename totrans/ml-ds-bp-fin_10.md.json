["```py\n# Import PCA Algorithm\nfrom sklearn.decomposition import PCA\n# Initialize the algorithm and set the number of PC's\npca = PCA(n_components=2)\n# Fit the model to data\npca.fit(data)\n# Get list of PC's\npca.components_\n# Transform the model to data\npca.transform(data)\n# Get the eigenvalues\npca.explained_variance_ratio\n```", "```py\nfrom sklearn.decomposition import TruncatedSVD\nsvd = TruncatedSVD(ncomps=20).fit(X)\n```", "```py\nfrom sklearn.decomposition import KernelPCA\nkpca = KernelPCA(n_components=4, kernel='rbf').fit_transform(X)\n```", "```py\nfrom sklearn.manifold import TSNE\nX_tsne = TSNE().fit_transform(X)\n```", "```py\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import TruncatedSVD\nfrom numpy.linalg import inv, eig, svd\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import KernelPCA\n```", "```py\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pandas import read_csv, set_option\nfrom pandas.plotting import scatter_matrix\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\n```", "```py\n# load dataset\ndataset = read_csv('Dow_adjcloses.csv', index_col=0)\n```", "```py\ndataset.shape\n```", "```py\n(4804, 30)\n```", "```py\ncorrelation = dataset.corr()\nplt.figure(figsize=(15, 15))\nplt.title('Correlation Matrix')\nsns.heatmap(correlation, vmax=1, square=True,annot=True, cmap='cubehelix')\n```", "```py\n#Checking for any null values and removing the null values'''\nprint('Null Values =',dataset.isnull().values.any())\n```", "```py\nNull Values = True\n```", "```py\nmissing_fractions = dataset.isnull().mean().sort_values(ascending=False)\nmissing_fractions.head(10)\ndrop_list = sorted(list(missing_fractions[missing_fractions > 0.3].index))\ndataset.drop(labels=drop_list, axis=1, inplace=True)\ndataset.shape\n```", "```py\n(4804, 28)\n```", "```py\n# Fill the missing values with the last value available in the dataset.\ndataset=dataset.fillna(method='ffill')\n```", "```py\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler().fit(datareturns)\nrescaledDataset = pd.DataFrame(scaler.fit_transform(datareturns),columns =\\\n datareturns.columns, index = datareturns.index)\n# summarize transformed data\ndatareturns.dropna(how='any', inplace=True)\nrescaledDataset.dropna(how='any', inplace=True)\n```", "```py\n# Visualizing Log Returns for the DJIA\nplt.figure(figsize=(16, 5))\nplt.title(\"AAPL Return\")\nrescaledDataset.AAPL.plot()\nplt.grid(True);\nplt.legend()\nplt.show()\n```", "```py\n# Dividing the dataset into training and testing sets\npercentage = int(len(rescaledDataset) * 0.8)\nX_train = rescaledDataset[:percentage]\nX_test = rescaledDataset[percentage:]\n\nstock_tickers = rescaledDataset.columns.values\nn_tickers = len(stock_tickers)\n```", "```py\npca = PCA()\nPrincipalComponent=pca.fit(X_train)\n```", "```py\nNumEigenvalues=20\nfig, axes = plt.subplots(ncols=2, figsize=(14,4))\nSeries1 = pd.Series(pca.explained_variance_ratio_[:NumEigenvalues]).sort_values()\nSeries2 = pd.Series(pca.explained_variance_ratio_[:NumEigenvalues]).cumsum()\nSeries1.plot.barh(title='Explained Variance Ratio by Top Factors', ax=axes[0]);\nSeries1.plot(ylim=(0,1), ax=axes[1], title='Cumulative Explained Variance');\n```", "```py\ndef PCWeights():\n    #Principal Components (PC) weights for each 28 PCs\n\n    weights = pd.DataFrame()\n    for i in range(len(pca.components_)):\n        weights[\"weights_{}\".format(i)] = \\\n        pca.components_[i] / sum(pca.components_[i])\n    weights = weights.values.T\n    return weights\nweights=PCWeights()\n```", "```py\nsum(pca.components_[0])\n```", "```py\n-5.247808242068631\n```", "```py\nNumComponents=5\ntopPortfolios = pd.DataFrame(pca.components_[:NumComponents],\\\n   columns=dataset.columns)\neigen_portfolios = topPortfolios.div(topPortfolios.sum(1), axis=0)\neigen_portfolios.index = [f'Portfolio {i}' for i in range( NumComponents)]\nnp.sqrt(pca.explained_variance_)\neigen_portfolios.T.plot.bar(subplots=True, layout=(int(NumComponents),1),  \\\nfigsize=(14,10), legend=False, sharey=True, ylim= (-1,1))\n```", "```py\n# plotting heatmap\nsns.heatmap(topPortfolios)\n```", "```py\n# Sharpe Ratio Calculation\n# Calculation based on conventional number of trading days per year (i.e., 252).\ndef sharpe_ratio(ts_returns, periods_per_year=252):\n    n_years = ts_returns.shape[0]/ periods_per_year\n    annualized_return = np.power(np.prod(1+ts_returns), (1/n_years))-1\n    annualized_vol = ts_returns.std() * np.sqrt(periods_per_year)\n    annualized_sharpe = annualized_return / annualized_vol\n\n    return annualized_return, annualized_vol, annualized_sharpe\n```", "```py\ndef optimizedPortfolio():\n    n_portfolios = len(pca.components_)\n    annualized_ret = np.array([0.] * n_portfolios)\n    sharpe_metric = np.array([0.] * n_portfolios)\n    annualized_vol = np.array([0.] * n_portfolios)\n    highest_sharpe = 0\n    stock_tickers = rescaledDataset.columns.values\n    n_tickers = len(stock_tickers)\n    pcs = pca.components_\n\n    for i in range(n_portfolios):\n\n        pc_w = pcs[i] / sum(pcs[i])\n        eigen_prtfi = pd.DataFrame(data ={'weights': pc_w.squeeze()*100}, \\\n        index = stock_tickers)\n        eigen_prtfi.sort_values(by=['weights'], ascending=False, inplace=True)\n        eigen_prti_returns = np.dot(X_train_raw.loc[:, eigen_prtfi.index], pc_w)\n        eigen_prti_returns = pd.Series(eigen_prti_returns.squeeze(),\\\n         index=X_train_raw.index)\n        er, vol, sharpe = sharpe_ratio(eigen_prti_returns)\n        annualized_ret[i] = er\n        annualized_vol[i] = vol\n        sharpe_metric[i] = sharpe\n\n        sharpe_metric= np.nan_to_num(sharpe_metric)\n\n    # find portfolio with the highest Sharpe ratio\n    highest_sharpe = np.argmax(sharpe_metric)\n\n    print('Eigen portfolio #%d with the highest Sharpe. Return %.2f%%,\\\n vol = %.2f%%, Sharpe = %.2f' %\n          (highest_sharpe,\n           annualized_ret[highest_sharpe]*100,\n           annualized_vol[highest_sharpe]*100,\n           sharpe_metric[highest_sharpe]))\n\n    fig, ax = plt.subplots()\n    fig.set_size_inches(12, 4)\n    ax.plot(sharpe_metric, linewidth=3)\n    ax.set_title('Sharpe ratio of eigen-portfolios')\n    ax.set_ylabel('Sharpe ratio')\n    ax.set_xlabel('Portfolios')\n\n    results = pd.DataFrame(data={'Return': annualized_ret,\\\n    'Vol': annualized_vol,\n    'Sharpe': sharpe_metric})\n    results.dropna(inplace=True)\n    results.sort_values(by=['Sharpe'], ascending=False, inplace=True)\n    print(results.head(5))\n\n    plt.show()\n\noptimizedPortfolio()\n```", "```py\nEigen portfolio #0 with the highest Sharpe. Return 11.47%, vol = 13.31%, \\\nSharpe = 0.86\n    Return    Vol  Sharpe\n0    0.115  0.133   0.862\n7    0.096  0.693   0.138\n5    0.100  0.845   0.118\n1    0.057  0.670   0.084\n```", "```py\nweights = PCWeights()\nportfolio = portfolio = pd.DataFrame()\n\ndef plotEigen(weights, plot=False, portfolio=portfolio):\n    portfolio = pd.DataFrame(data ={'weights': weights.squeeze() * 100}, \\\n    index = stock_tickers)\n    portfolio.sort_values(by=['weights'], ascending=False, inplace=True)\n    if plot:\n        portfolio.plot(title='Current Eigen-Portfolio Weights',\n            figsize=(12, 6),\n            xticks=range(0, len(stock_tickers), 1),\n            rot=45,\n            linewidth=3\n            )\n        plt.show()\n\n    return portfolio\n\n# Weights are stored in arrays, where 0 is the first PC's weights.\nplotEigen(weights=weights[0], plot=True)\n```", "```py\ndef Backtest(eigen):\n\n    '''\n Plots principal components returns against real returns.\n '''\n\n    eigen_prtfi = pd.DataFrame(data ={'weights': eigen.squeeze()}, \\\n    index=stock_tickers)\n    eigen_prtfi.sort_values(by=['weights'], ascending=False, inplace=True)\n\n    eigen_prti_returns = np.dot(X_test_raw.loc[:, eigen_prtfi.index], eigen)\n    eigen_portfolio_returns = pd.Series(eigen_prti_returns.squeeze(),\\\n     index=X_test_raw.index)\n    returns, vol, sharpe = sharpe_ratio(eigen_portfolio_returns)\n    print('Current Eigen-Portfolio:\\nReturn = %.2f%%\\nVolatility = %.2f%%\\n\\\n Sharpe = %.2f' % (returns * 100, vol * 100, sharpe))\n    equal_weight_return=(X_test_raw * (1/len(pca.components_))).sum(axis=1)\n    df_plot = pd.DataFrame({'EigenPorfolio Return': eigen_portfolio_returns, \\\n    'Equal Weight Index': equal_weight_return}, index=X_test.index)\n    np.cumprod(df_plot + 1).plot(title='Returns of the equal weighted\\\n index vs. First eigen-portfolio',\n                          figsize=(12, 6), linewidth=3)\n    plt.show()\n\nBacktest(eigen=weights[5])\nBacktest(eigen=weights[1])\nBacktest(eigen=weights[14])\n```", "```py\nCurrent Eigen-Portfolio:\nReturn = 32.76%\nVolatility = 68.64%\nSharpe = 0.48\n```", "```py\nCurrent Eigen-Portfolio:\nReturn = 99.80%\nVolatility = 58.34%\nSharpe = 1.71\n```", "```py\nCurrent Eigen-Portfolio:\nReturn = -79.42%\nVolatility = 185.30%\nSharpe = -0.43\n```", "```py\n# In order to use quandl, ApiConfig.api_key will need to be\n# set to identify you to the quandl API. Please see API\n# Documentation of quandl for more details\nquandl.ApiConfig.api_key = 'API Key'\n```", "```py\ntreasury = ['FRED/DGS1MO','FRED/DGS3MO','FRED/DGS6MO','FRED/DGS1',\\\n'FRED/DGS2','FRED/DGS3','FRED/DGS5','FRED/DGS7','FRED/DGS10',\\\n'FRED/DGS20','FRED/DGS30']\n\ntreasury_df = quandl.get(treasury)\ntreasury_df.columns = ['TRESY1mo','TRESY3mo','TRESY6mo','TRESY1y',\\\n'TRESY2y','TRESY3y','TRESY5y','TRESY7y','TRESY10y',\\'TRESY20y','TRESY30y']\ndataset = treasury_df\n```", "```py\n# shape\ndataset.shape\n```", "```py\n(14420, 11)\n```", "```py\ndataset.plot(figsize=(10,5))\nplt.ylabel(\"Rate\")\nplt.legend(bbox_to_anchor=(1.01, 0.9), loc=2)\nplt.show()\n```", "```py\n# correlation\ncorrelation = dataset.corr()\nplt.figure(figsize=(15, 15))\nplt.title('Correlation Matrix')\nsns.heatmap(correlation, vmax=1, square=True, annot=True, cmap='cubehelix')\n```", "```py\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler().fit(dataset)\nrescaledDataset = pd.DataFrame(scaler.fit_transform(dataset),\\\ncolumns = dataset.columns,\nindex = dataset.index)\n# summarize transformed data\ndataset.dropna(how='any', inplace=True)\nrescaledDataset.dropna(how='any', inplace=True)\n```", "```py\nrescaledDataset.plot(figsize=(14, 10))\nplt.ylabel(\"Rate\")\nplt.legend(bbox_to_anchor=(1.01, 0.9), loc=2)\nplt.show()\n```", "```py\npca = PCA()\nPrincipalComponent=pca.fit(rescaledDataset)\n```", "```py\nNumEigenvalues=5\nfig, axes = plt.subplots(ncols=2, figsize=(14, 4))\npd.Series(pca.explained_variance_ratio_[:NumEigenvalues]).sort_values().\\\nplot.barh(title='Explained Variance Ratio by Top Factors',ax=axes[0]);\npd.Series(pca.explained_variance_ratio_[:NumEigenvalues]).cumsum()\\\n.plot(ylim=(0,1),ax=axes[1], title='Cumulative Explained Variance');\n# explained_variance\npd.Series(np.cumsum(pca.explained_variance_ratio_)).to_frame\\\n('Explained Variance_Top 5').head(NumEigenvalues).style.format('{:,.2%}'.format)\n```", "```py\ndef PCWeights():\n    '''\n Principal Components (PC) weights for each 28 PCs\n '''\n    weights = pd.DataFrame()\n\n    for i in range(len(pca.components_)):\n        weights[\"weights_{}\".format(i)] = \\\n        pca.components_[i] / sum(pca.components_[i])\n\n    weights = weights.values.T\n    return weights\n\nweights=PCWeights()\n```", "```py\nweights = PCWeights()\nNumComponents=3\n\ntopPortfolios = pd.DataFrame(weights[:NumComponents], columns=dataset.columns)\ntopPortfolios.index = [f'Principal Component {i}' \\\nfor i in range(1, NumComponents+1)]\n\naxes = topPortfolios.T.plot.bar(subplots=True, legend=False, figsize=(14, 10))\nplt.subplots_adjust(hspace=0.35)\naxes[0].set_ylim(0, .2);\n```", "```py\npd.DataFrame(pca.components_[0:3].T).plot(style= ['s-','o-','^-'], \\\n                            legend=False, title=\"Principal Component\")\n```", "```py\npca.transform(rescaledDataset)[:, :2]\n```", "```py\narray([[ 4.97514826, -0.48514999],\n       [ 5.03634891, -0.52005102],\n       [ 5.14497849, -0.58385444],\n       ...,\n       [-1.82544584,  2.82360062],\n       [-1.69938513,  2.6936174 ],\n       [-1.73186029,  2.73073137]])\n```", "```py\nnComp=3\nreconst= pd.DataFrame(np.dot(pca.transform(rescaledDataset)[:, :nComp],\\\npca.components_[:nComp,:]),columns=dataset.columns)\nplt.figure(figsize=(10,8))\nplt.plot(reconst)\nplt.ylabel(\"Treasury Rate\")\nplt.title(\"Reconstructed Dataset\")\nplt.show()\n```", "```py\ndataset[dataset.columns] = dataset[dataset.columns].ffill()\n```", "```py\ndataset.tail(5)\n```", "```py\nfig = plt.figure()\nplot = dataset.groupby(['signal']).size().plot(kind='barh', color='red')\nplt.show()\n```", "```py\nY= subset_dataset[\"signal\"]\nX = subset_dataset.loc[:, dataset.columns != 'signal'] validation_size = 0.2\nX_train, X_validation, Y_train, Y_validation = train_test_split\\\n(X, Y, test_size=validation_size, random_state=1)\n```", "```py\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler().fit(X_train)\nrescaledDataset = pd.DataFrame(scaler.fit_transform(X_train),\\\ncolumns = X_train.columns, index = X_train.index)\n# summarize transformed data\nX_train.dropna(how='any', inplace=True)\nrescaledDataset.dropna(how='any', inplace=True)\nrescaledDataset.head(2)\n```", "```py\nncomps = 5\nsvd = TruncatedSVD(n_components=ncomps)\nsvd_fit = svd.fit(rescaledDataset)\nY_pred = svd.fit_transform(rescaledDataset)\nax = pd.Series(svd_fit.explained_variance_ratio_.cumsum()).plot(kind='line', \\\nfigsize=(10, 3))\nax.set_xlabel(\"Eigenvalues\")\nax.set_ylabel(\"Percentage Explained\")\nprint('Variance preserved by first 5 components == {:.2%}'.\\\nformat(svd_fit.explained_variance_ratio_.cumsum()[-1]))\n```", "```py\ndfsvd = pd.DataFrame(Y_pred, columns=['c{}'.format(c) for \\\nc in range(ncomps)], index=rescaledDataset.index)\nprint(dfsvd.shape)\ndfsvd.head()\n```", "```py\n(8000, 5)\n```", "```py\nsvdcols = [c for c in dfsvd.columns if c[0] == 'c']\n```", "```py\nplotdims = 5\nploteorows = 1\ndfsvdplot = dfsvd[svdcols].iloc[:, :plotdims]\ndfsvdplot['signal']=Y_train\nax = sns.pairplot(dfsvdplot.iloc[::ploteorows, :], hue='signal', size=1.8)\n```", "```py\ntsne = TSNE(n_components=2, random_state=0)\n\nZ = tsne.fit_transform(dfsvd[svdcols])\ndftsne = pd.DataFrame(Z, columns=['x','y'], index=dfsvd.index)\n\ndftsne['signal'] = Y_train\n\ng = sns.lmplot('x', 'y', dftsne, hue='signal', fit_reg=False, size=8\n                , scatter_kws={'alpha':0.7,'s':60})\n```", "```py\n# test options for classification\nscoring = 'accuracy'\n```", "```py\nimport time\nstart_time = time.time()\n\n# spot-check the algorithms\nmodels =  RandomForestClassifier(n_jobs=-1)\ncv_results_XTrain= cross_val_score(models, X_train, Y_train, cv=kfold, \\\n  scoring=scoring)\nprint(\"Time Without Dimensionality Reduction--- %s seconds ---\" % \\\n(time.time() - start_time))\n```", "```py\nTime Without Dimensionality Reduction\n7.781347990036011 seconds\n```", "```py\nstart_time = time.time()\nX_SVD= dfsvd[svdcols].iloc[:, :5]\ncv_results_SVD = cross_val_score(models, X_SVD, Y_train, cv=kfold, \\\n  scoring=scoring)\nprint(\"Time with Dimensionality Reduction--- %s seconds ---\" % \\\n(time.time() - start_time))\n```", "```py\nTime with Dimensionality Reduction\n2.281977653503418 seconds\n```", "```py\nprint(\"Result without dimensionality Reduction: %f (%f)\" %\\\n (cv_results_XTrain.mean(), cv_results_XTrain.std()))\nprint(\"Result with dimensionality Reduction: %f (%f)\" %\\\n (cv_results_SVD.mean(), cv_results_SVD.std()))\n```", "```py\nResult without dimensionality Reduction: 0.936375 (0.010774)\nResult with dimensionality Reduction: 0.887500 (0.012698)\n```"]