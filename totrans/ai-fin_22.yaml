- en: Appendix B. Neural Network Classes
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building on the foundations from [Appendix A](app01.xhtml#app_interactive_neural_networks),
    this appendix provides simple, class-based implementations of neural networks
    that mimic the APIs of packages such as `scikit-learn`. The implementation is
    based on pure, simple Python code and is for illustration and instruction. The
    classes presented in this appendix cannot replace robust, efficient, and scalable
    implementations found in the standard Python packages, such as `scikit-learn`
    or `TensorFlow` in combination with `Keras`.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'The appendix comprises the following sections:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '[“Activation Functions”](#app_nnc_activation) introduces a Python function
    with different activation functions.'
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[“Simple Neural Networks”](#app_nnc_sinn) presents a Python class for *simple
    neural networks*.'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[“Shallow Neural Networks”](#app_nnc_shnn) presents a Python class for *shallow
    neural networks*.'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[“Predicting Market Direction”](#app_nnc_fin) applies the class for shallow
    neural networks to financial data.'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The implementations and examples in this appendix are simple and straightforward.
    The Python classes are not well suited to attack larger estimation or classification
    problems. The idea is rather to show easy-to-understand Python implementations
    from scratch.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Activation Functions
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Appendix A](app01.xhtml#app_interactive_neural_networks) uses two activation
    functions implicitly or explicitly: linear function and sigmoid function. The
    Python function `activation` adds the `relu` (rectified linear unit) and `softplus`
    functions to the set of options. For all these activation functions, the first
    derivative is also defined:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Simple Neural Networks
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section presents a class for *simple neural networks* that has an API
    similar to those of models from standard Python packages for machine or deep learning
    (in particular, `scikit-learn` and `Keras`). Consider the class `sinn` as presented
    in the following Python code. It implements a simple neural network and defines
    the two main methods `.fit()` and `.predict()`. The `.metrics()` method calculates
    typical performance metrics: the mean-squared error (MSE) for estimation and the
    accuracy for classification. The class also implements two methods for the forward
    and backward propagation steps:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Estimation
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First is an estimation problem that can be solved by the use of regression
    techniques:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[![1](Images/1.png)](#co_neural_network_classes_CO1-1)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Exact solution by regression
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: 'Applying the `sinn` class to the estimation problem requires quite some effort
    in the form of repeated learning steps. However, by increasing the number of steps,
    one can make the estimate arbitrarily precise:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[![1](Images/1.png)](#co_neural_network_classes_CO2-1)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Residual errors of the neural network estimation
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Classification
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Second is a classification problem that can also be attacked with the `sinn`
    class. Here, standard regression techniques are in general of no use. For the
    particular set of random features and labels, the `sinn` model reaches an accuracy
    of 100%. Again, quite some effort is required in the form of repeated learning
    steps. [Figure B-1](#figure_nnc_01) shows how the prediction accuracy changes
    with the number of learning steps:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[![1](Images/1.png)](#co_neural_network_classes_CO3-1)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: The sigmoid function is used for activation
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_neural_network_classes_CO3-2)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Perfect accuracy on this particular data set
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '![aiif 1601](Images/aiif_1601.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
- en: Figure B-1\. Prediction accuracy versus the number of learning steps
  id: totrans-31
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Shallow Neural Networks
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section applies the class `shnn`, which implements *shallow neural networks*
    with one hidden layer, to estimation and classification problems. The class structure
    is along the lines of the `sinn` class from the previous section:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Estimation
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Again, the estimation problem comes first. For 5 features and 10 samples, a
    perfect regression solution is unlikely to exist. As a result, the MSE value of
    the regression is relatively high:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'However, the shallow neural network estimate based on the `shnn` class is quite
    good and shows a relatively low MSE value compared to the regression value:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Classification
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The classification example takes the estimation numbers and applies rounding
    to them. The shallow neural network converges quickly to predict the labels with
    100% accuracy (see [Figure B-2](#figure_nnc_02)):'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![aiif 1602](Images/aiif_1602.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
- en: Figure B-2\. Performance metrics for the shallow neural network (classification)
  id: totrans-44
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Predicting Market Direction
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section applies the `shnn` class to predict the future direction of the
    EUR/USD exchange rate. The analysis is in-sample only to illustrate the application
    of `shnn` to real-world data. See [Chapter 10](ch10.xhtml#vectorized_backtesting)
    for the implementation of a more realistic setup for the vectorized backtesting
    of such prediction-based strategies.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: 'The following Python code imports the financial data—10 years’ worth of EOD
    data—and creates lagged, normalized log returns used as the features. The labels
    data is the direction of the price series as a binary data set:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[![1](Images/1.png)](#co_neural_network_classes_CO4-1)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Market direction as the labels data
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_neural_network_classes_CO4-2)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Lagged log returns as the features data
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_neural_network_classes_CO4-3)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Gaussian normalization of the features data
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: 'With the data preprocessing accomplished, the application of the shallow neural
    network class `shnn` for a supervised classification is straightforward. [Figure B-3](#figure_nnc_03)
    shows that the prediction-based strategy in-sample significantly outperforms the
    passive benchmark investment:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[![1](Images/1.png)](#co_neural_network_classes_CO5-1)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Derives the position values from the prediction values
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_neural_network_classes_CO5-3)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_neural_network_classes_CO5-3)'
- en: Calculates the strategy returns from the position values and the log returns
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 从仓位价值和对数收益率计算策略收益
- en: '[![3](Images/3.png)](#co_neural_network_classes_CO5-4)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_neural_network_classes_CO5-4)'
- en: Calculates the gross performance of the strategy and the benchmark investment
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 计算策略和基准投资的总体表现
- en: '[![4](Images/4.png)](#co_neural_network_classes_CO5-5)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_neural_network_classes_CO5-5)'
- en: Shows the gross performance of the strategy and the benchmark investment over
    time
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 显示策略和基准投资随时间的总体表现
- en: '![aiif 1603](Images/aiif_1603.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![aiif 1603](Images/aiif_1603.png)'
- en: Figure B-3\. Gross performance of prediction-based strategy compared to passive
    benchmark investment (in-sample)
  id: totrans-66
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 B-3\. 基于预测的策略与被动基准投资的总体表现（样本内）
